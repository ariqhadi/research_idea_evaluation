{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ideas_file = 'sample_ideas/agentic_ai_for_idea_generation_ideas.json'\n",
    "\n",
    "with open(ideas_file, 'r') as f:\n",
    "    ideas_data = json.load(f)\n",
    "\n",
    "for ideas in ideas_data['ideas']:\n",
    "    for idea, items in ideas.items():\n",
    "        print(idea)\n",
    "        for item, i in items.items():\n",
    "            print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f575aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from typing import Literal\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0,  # Set the temperature for the model's responses\n",
    "    model_name=\"gpt-5-nano\",  # Specify the model name\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb13e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\",\n",
    "                             google_api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd02d825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a versatile, high-level programming language known for its readability and extensive libraries, making it popular for web development, data science, automation, and more.\n"
     ]
    }
   ],
   "source": [
    "## Testing LLM Call\n",
    "\n",
    "# \n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"Tell me about Python programming in one sentence\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6965b7c",
   "metadata": {},
   "source": [
    "## Retrieve Paper with SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b750afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "class getReferencePaper():\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "\n",
    "\n",
    "    def query_search(self, query):\n",
    "        url=\"https://api.semanticscholar.org/graph/v1/paper/search/\"\n",
    "        \n",
    "        query_params = {\n",
    "            \"query\": query,\n",
    "            \"fields\": \"title,citationCount,tldr,url,publicationTypes,publicationDate,openAccessPdf,abstract\",\n",
    "            \"year\": \"2020-2025\",\n",
    "            \"limit\": 50,\n",
    "            \"sort\": \"relevance\",\n",
    "            \"minCitationCount\": 10\n",
    "        }\n",
    "        headers = {\"x-api-key\": self.api_key}\n",
    "        response = requests.get(url, params=query_params, headers=headers).json()\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def PaperDetails(self, paper_id, fields=\"title,year,abstract,authors,citationCount,venue,citations,references,tldr\"):\n",
    "        \n",
    "        url = \"https://api.semanticscholar.org/graph/v1/paper/\"\n",
    "        \n",
    "        paper_data_query_params = {\"fields\": fields}\n",
    "        headers = {\"x-api-key\": self.api_key}\n",
    "        response = requests.get(\n",
    "            url = url + paper_id, params=paper_data_query_params, headers=headers\n",
    "        )\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_papers_for_llm(list_of_papers):\n",
    "        unique_papers = {}\n",
    "\n",
    "        for query_string, query_data in list_of_papers.items():\n",
    "            for paper in query_data.get('data', []):\n",
    "                paper_id = paper.get('paperId')\n",
    "                if paper_id and paper_id not in unique_papers:  # Skips if paper_id is None\n",
    "                    paper_str = f\"\"\"Paper ID: {paper_id}\n",
    "                                    Title: {paper.get('title')}\n",
    "                                    Abstract: {paper.get('abstract')}\n",
    "                                \"\"\"\n",
    "                    unique_papers[paper_id] = paper_str\n",
    "                    \n",
    "        paper_list = list(unique_papers.values())\n",
    "                \n",
    "        papers_for_llm = \"\\n\\n---\\n\\n\".join(paper_list)\n",
    "        return papers_for_llm\n",
    "\n",
    " \n",
    "# query = 'Computing Machinery and Intelligence'\n",
    "\n",
    "# search_paper = getReferencePaper()\n",
    "# search_paper_response = search_paper.query_search(query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d15a9",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3b096",
   "metadata": {},
   "source": [
    "### Agent 1: Idea Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb69f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "\n",
    "class IdeaParser(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 1: Parse Idea from user input into structured format.\n",
    "    This is the state of the model that holds the parsed idea details throughout the process.\n",
    "    \"\"\"\n",
    "    research_question: str\n",
    "    problem_domain: str \n",
    "    methodology_keywords: List[str] \n",
    "    key_concepts: List[str]\n",
    "    existing_methods: List[str] \n",
    "    claimed_novelty: List[str] \n",
    "    \n",
    "\n",
    "    \n",
    "    @field_validator('key_concepts')\n",
    "    @classmethod\n",
    "    def validate_key_concepts_counts(cls, v):\n",
    "        \"\"\"Ensure that there are a reasonable number of key concepts extracted\"\"\"\n",
    "        if len(v) < 3 or len(v) > 15:\n",
    "            raise ValueError('At least 3 and at most 15 key concepts are required.')\n",
    "        return v\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "    def to_summary(self) -> str:\n",
    "        return f\"\"\"Research Question: {self.research_question}\\n\n",
    "                    Problem Domain: {self.problem_domain}\\n\n",
    "                    Key Concepts: {', '.join(self.key_concepts)}\\n\n",
    "                    Claimed Novelty: {', '.join(self.claimed_novelty)}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b5727ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "idea_parser_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a research analysis assistant that has a deep understanding of extracting structured information from research proposals.\n",
    "\n",
    "            INPUT:\n",
    "            You will receive a research idea description with these fields:\n",
    "            - Problem: The research problem being addressed\n",
    "            - Existing Methods: Current approaches and their limitations\n",
    "            - Motivation: Why this research is needed\n",
    "            - Proposed Method: The new approach being proposed\n",
    "            - Experiment Plan: How the approach will be evaluated\n",
    "\n",
    "            YOUR TASK:\n",
    "            Extract and structure the key information needed for finding similar work.\n",
    "\n",
    "            OUTPUT REQUIREMENTS:\n",
    "            Return ONLY valid JSON with NO additional text, markdown formatting, or code blocks.\n",
    "            Do not include ```json or ``` markers.\n",
    "            Your entire response must be parseable by JSON.parse().\n",
    "\n",
    "            CRITICAL: Be precise and specific in extraction. Extract actual technical terms, not generic descriptions.\n",
    "\n",
    "            OUTPUT SCHEMA:\n",
    "            {{\n",
    "            \"research_question\": \"string - The main research question in one concise sentence\",\n",
    "            \"problem_domain\": \"string - The specific research area/field (e.g., 'natural language processing', 'computer vision')\",\n",
    "            \"methodology_keywords\": [\n",
    "                \"string - Specific technical methods mentioned (e.g., 'reinforcement learning', 'transformer architecture')\"\n",
    "            ],\n",
    "            \"key_concepts\": [\n",
    "                \"string - Core concepts and techniques (e.g., 'prompt optimization', 'context window management')\"\n",
    "            ],\n",
    "            \"existing_methods\": [\n",
    "                \"string - Baseline methods or prior work explicitly mentioned\"\n",
    "            ],\n",
    "            \"claimed_novelty\": [\n",
    "                \"string - What the proposal claims is novel (extract from Motivation and Proposed Method)\"\n",
    "            ]\n",
    "            }}\n",
    "\n",
    "            EXTRACTION RULES:\n",
    "            1. Be specific: Extract \"transformer architecture\" not \"neural network\"\n",
    "            2. Preserve technical terms exactly as written\n",
    "            3. For methodology_keywords: Include only actionable technical terms\n",
    "            4. For key_concepts: Include 5-8 most important concepts\n",
    "            5. For claimed_novelty: Extract 2-4 specific novel claims\n",
    "            6. If a field has no relevant information, use empty array [] or empty string \"\"\n",
    "\n",
    "            INPUT: \n",
    "            research_idea.\n",
    "\n",
    "            OUTPUT (valid JSON only):\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"research_idea\")\n",
    "    ])\n",
    "\n",
    "## this meant that the input is going to be prompt in idea_parser_prompt and the output is going to be structured based on IdeaParser class\n",
    "idea_parser_agent = idea_parser_prompt |  llm.with_structured_output(IdeaParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a0f58820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_idea_parser(state: MessagesState):\n",
    "    user_message = state[\"messages\"][-1] # Get the last message from the state\n",
    "    \n",
    "    # it's going to invoke or run the llm prompt with user input message content\n",
    "    response = idea_parser_agent.invoke({\n",
    "        \"research_idea\": [HumanMessage(content=user_message.content)]\n",
    "    })\n",
    "    # response is now an IdeaParser object\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941980b",
   "metadata": {},
   "source": [
    "### Agent 2: Literature Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9626e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryGenerator(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 2: Generate Search Queries from parsed research idea.\n",
    "    1. Create effective search queries to find related work.\n",
    "    \"\"\"\n",
    "    query_string: str \n",
    "    rationale: str \n",
    "    priority_concept: str \n",
    "    \n",
    "    @field_validator('query_string')\n",
    "    @classmethod\n",
    "    def validate_query_string_length(cls, v):\n",
    "        \"\"\"Ensure that the query is not too long\"\"\"\n",
    "        if len(v.split()) > 8:\n",
    "            raise ValueError('Query string must be less than 8 words.')\n",
    "        return v\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "class QueryGeneratorOutput(BaseModel):\n",
    "    \"\"\"Multiple search queries\"\"\"\n",
    "    queries: List[QueryGenerator] = Field(\n",
    "        description=\"List of 5 diverse search queries\"\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"queries\": [q.to_dict() for q in self.queries]\n",
    "        }\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a683c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query_generator_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced professor with an established search query strategy skill for academic literature databases.\n",
    "\n",
    "            CONTEXT:\n",
    "            You have a parsed research idea and need to generate optimal search queries for Semantic Scholar API.\n",
    "            Your queries will retrieve papers to assess the novelty of the proposed research.\n",
    "\n",
    "            PARSED RESEARCH IDEA:\n",
    "            {parsed_idea_json}\n",
    "\n",
    "\n",
    "            YOUR TASK:\n",
    "            Generate 5 (five) diverse search queries that will find the most relevant existing work.\n",
    "\n",
    "\n",
    "            QUERY OPTIMIZATION RULES:\n",
    "            1. Keep queries SHORT: 2-6 words maximum for best results\n",
    "            2. Use technical terms, not natural language\n",
    "            3. Combine 2-3 concepts maximum per query\n",
    "            4. NO operators: Don't use \"AND\", \"OR\", \"-\", quotes, or \"site:\"\n",
    "            5. Prioritize precision over recall\n",
    "\n",
    "            OUTPUT REQUIREMENTS:\n",
    "            Return ONLY valid JSON with NO additional text or formatting.\n",
    "            Do not include ```json or ``` markers.\n",
    "\n",
    "            OUTPUT SCHEMA:\n",
    "            {{\n",
    "            \"queries\": [\n",
    "                {{\n",
    "                \"query_string\": \"string - The actual search query (2-6 words)\",\n",
    "                \"rationale\": \"string - Why this query will find relevant papers\",\n",
    "                \"priority_concepts\" : \"string - Top 3-5 concepts that should appear in similar papers\"\n",
    "                }}\n",
    "            ]\n",
    "            }}\n",
    "\n",
    "            EXAMPLES OF GOOD QUERIES:\n",
    "            - \"adaptive prompt dialogue coherence\"\n",
    "            - \"dynamic context management LLM\"\n",
    "            - \"conversational continuity language models\"\n",
    "            - \"iterative prompt optimization\"\n",
    "\n",
    "            EXAMPLES OF BAD QUERIES:\n",
    "            - \"papers about improving language model coherence\" (too natural language)\n",
    "            - \"dynamic AND adaptive OR iterative -static\" (operators not supported)\n",
    "            - \"comprehensive survey of prompt engineering techniques\" (too long/broad)\n",
    "\n",
    "            OUTPUT (valid JSON only):\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"parsed_idea_json\")\n",
    "    ])\n",
    "\n",
    "\n",
    "query_generator_agent = query_generator_prompt |  llm.with_structured_output(QueryGeneratorOutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d42ce32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_query_generator(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  \n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = query_generator_agent.invoke({\n",
    "        \"parsed_idea_json\": [HumanMessage(content=json.dumps(json.loads(last_message.content)))]\n",
    "    })\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319927ef",
   "metadata": {},
   "source": [
    "#### AGENT 3 : search paper using semantic scholar api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a92d90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_paper_search(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  \n",
    "    queries_json = json.loads(last_message.content)\n",
    "    all_search_results = {}\n",
    "    \n",
    "    search_paper = getReferencePaper()\n",
    "\n",
    "    for search_query in queries_json['queries']:\n",
    "        query_string = search_query['query_string']\n",
    "        search_results = search_paper.query_search(query_string)\n",
    "        all_search_results[query_string] = search_results\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(all_search_results, indent=2))]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cfaeb",
   "metadata": {},
   "source": [
    "### Agent 4 : Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04f52e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalyzer(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 4: Generate Search Queries from parsed research idea.\n",
    "    \"\"\"\n",
    "    paper_id: str \n",
    "    title: str \n",
    "    overlap_score: float = Field(\n",
    "        description=\"float 0.0-1.0 - overlap similarity with proposed idea\"\n",
    "    )\n",
    "    methodology_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Methodology overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )\n",
    "    problem_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Problem overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )\n",
    "    domain_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Domain overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )   \n",
    "    key_overlaps: List[str] = Field(\n",
    "        description=\"Specific overlapping aspects\"\n",
    "    )\n",
    "    key_differences: List[str] = Field(\n",
    "        description=\"How proposed idea differs\"\n",
    "    )\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "class PaperAnalyzerOutput(BaseModel):\n",
    "    \"\"\"Multiple search queries\"\"\"\n",
    "    queries: List[PaperAnalyzer] = Field(\n",
    "        description=\"List of all analyzed papers\"\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"papers\": [q.to_dict() for q in self.queries]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04b64098",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_work_analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experience researcher with years of expertise in academic literature review and analysis.\n",
    "\n",
    "                PROPOSED RESEARCH IDEA:\n",
    "                {research_idea}\n",
    "                \n",
    "                LIST OF RETRIEVED PAPERS:\n",
    "                {paper_list}\n",
    "                \n",
    "                YOUR TASK:\n",
    "                For each paper, assess the degree of overlap with the proposed research idea.\n",
    "                \n",
    "                ANALYSIS CRITERIA:\n",
    "                Methodology overlap: Do they use similar approaches?\n",
    "                Problem overlap: Do they address the same problem?\n",
    "                Domain overlap: Same application area?\n",
    "                Overall score: The average of the three overlap scores.\n",
    "                \n",
    "                OUTPUT REQUIREMENTS:\n",
    "                Return ONLY valid JSON with NO additional text.\n",
    "                Do not include ```json or ``` markers.\n",
    "                \n",
    "                OUTPUT SCHEMA:\n",
    "                {{\n",
    "                \"paper_analyses\": [\n",
    "                   {{\n",
    "                      \"paper_id\": \"string - Semantic Scholar paper ID\",\n",
    "                      \"title\": \"string\",\n",
    "                      \"overlap_score\": \"float 0.0-1.0 - Overall similarity\",\n",
    "                      \"methodology_overlap\": \"float 0.0-1.0\",\n",
    "                      \"problem_overlap\": \"float 0.0-1.0\", \n",
    "                      \"domain_overlap\": \"float 0.0-1.0\",\n",
    "                      \"key_overlaps\": [\n",
    "                        \"string - Specific overlapping aspects\"\n",
    "                      ],\n",
    "                      \"key_differences\": [\n",
    "                        \"string - How proposed idea differs\"\n",
    "                      ]\n",
    "                    }}\n",
    "                  ]\n",
    "                }}\n",
    "                \n",
    "                SCORING GUIDELINES:\n",
    "                overlap_score 0.8-1.0: Nearly identical approach\n",
    "                overlap_score 0.6-0.8: High similarity, incremental difference\n",
    "                overlap_score 0.4-0.6: Moderate similarity, related work\n",
    "                overlap_score 0.2-0.4: Tangentially related\n",
    "                overlap_score 0.0-0.2: Different approach, same domain\n",
    "                \n",
    "                Be precise and evidence-based. Cite specific aspects from paper titles/abstracts.\n",
    "                \n",
    "                OUTPUT (valid JSON only):\n",
    "                \n",
    "            \"\"\",\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "prior_work_analysis_agent = prior_work_analysis_prompt |  llm.with_structured_output(PaperAnalyzerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "67fb8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_prior_work_analysis(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  # HumanMessage\n",
    "    initial_user_input = state[\"messages\"][0]  # User input research idea\n",
    "    \n",
    "    list_of_papers = getReferencePaper.prepare_papers_for_llm(\n",
    "        json.loads(last_message.content)\n",
    "    )\n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = prior_work_analysis_agent.invoke({\n",
    "        \"research_idea\":initial_user_input.content, #json.dumps(json.loads(last_message.content)),\n",
    "        \"paper_list\": list_of_papers\n",
    "    })\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912274f2",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3afc5516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c1762be0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"idea_parser\", call_idea_parser)\n",
    "workflow.add_node(\"search_query\", call_query_generator)\n",
    "workflow.add_node(\"search_paper\", call_paper_search)\n",
    "workflow.add_node(\"prior_work_analysis\", call_prior_work_analysis)\n",
    "\n",
    "workflow.add_edge(START, \"idea_parser\")\n",
    "workflow.add_edge(\"idea_parser\", \"search_query\")\n",
    "workflow.add_edge(\"search_query\", \"search_paper\")\n",
    "workflow.add_edge(\"search_paper\", END)\n",
    "# workflow.add_edge(\"search_paper\", \"prior_work_analysis\")\n",
    "# workflow.add_edge(\"prior_work_analysis\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969dfdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "823bc0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ee237047-156b-46ff-b2d5-4c6075ddc235')]}\n",
      "\u001b[1m[updates]\u001b[0m {'idea_parser': {'messages': [AIMessage(content='{\\n  \"research_question\": \"How can prompts be dynamically adapted to maintain coherence and engagement in extended LLM interactions?\",\\n  \"problem_domain\": \"natural language processing\",\\n  \"methodology_keywords\": [\\n    \"prompt engineering\",\\n    \"contextual analysis\",\\n    \"natural language generation\"\\n  ],\\n  \"key_concepts\": [\\n    \"dynamic prompt adaptation\",\\n    \"thematic consistency\",\\n    \"coherence\",\\n    \"iterative context update\",\\n    \"adaptive prompt generation\",\\n    \"reader engagement\",\\n    \"LLM interaction\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation method\",\\n    \"three-phase approach (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update)\",\\n    \"maintaining coherence and adaptability across LLM interactions\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='d4be8654-b546-4e18-ba54-8feb3a5492be')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ee237047-156b-46ff-b2d5-4c6075ddc235'), AIMessage(content='{\\n  \"research_question\": \"How can prompts be dynamically adapted to maintain coherence and engagement in extended LLM interactions?\",\\n  \"problem_domain\": \"natural language processing\",\\n  \"methodology_keywords\": [\\n    \"prompt engineering\",\\n    \"contextual analysis\",\\n    \"natural language generation\"\\n  ],\\n  \"key_concepts\": [\\n    \"dynamic prompt adaptation\",\\n    \"thematic consistency\",\\n    \"coherence\",\\n    \"iterative context update\",\\n    \"adaptive prompt generation\",\\n    \"reader engagement\",\\n    \"LLM interaction\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation method\",\\n    \"three-phase approach (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update)\",\\n    \"maintaining coherence and adaptability across LLM interactions\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='d4be8654-b546-4e18-ba54-8feb3a5492be')]}\n",
      "\u001b[1m[updates]\u001b[0m {'search_query': {'messages': [AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation LLM\",\\n      \"rationale\": \"Directly targets the core concept of dynamically adapting prompts for LLMs, a key aspect of the research question.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"coherence in extended LLM\",\\n      \"rationale\": \"Focuses on the problem domain of maintaining coherence, which is central to the research question\\'s goal in long interactions.\",\\n      \"priority_concept\": \"coherence, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Highlights the methodology of generating prompts in an adaptive manner, aligning with the proposed novel approach.\",\\n      \"priority_concept\": \"adaptive prompt generation, prompt engineering\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Addresses the \\'Iterative Context Update\\' phase of the proposed method, crucial for maintaining state in extended interactions.\",\\n      \"priority_concept\": \"iterative context update, contextual analysis\"\\n    },\\n    {\\n      \"query_string\": \"thematic consistency LLM\",\\n      \"rationale\": \"Searches for work related to maintaining thematic consistency, a specific aspect of coherence in the context of LLM interactions.\",\\n      \"priority_concept\": \"thematic consistency, LLM interaction\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='df3e3d38-4dc6-473a-bd42-348c81dbc69c')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ee237047-156b-46ff-b2d5-4c6075ddc235'), AIMessage(content='{\\n  \"research_question\": \"How can prompts be dynamically adapted to maintain coherence and engagement in extended LLM interactions?\",\\n  \"problem_domain\": \"natural language processing\",\\n  \"methodology_keywords\": [\\n    \"prompt engineering\",\\n    \"contextual analysis\",\\n    \"natural language generation\"\\n  ],\\n  \"key_concepts\": [\\n    \"dynamic prompt adaptation\",\\n    \"thematic consistency\",\\n    \"coherence\",\\n    \"iterative context update\",\\n    \"adaptive prompt generation\",\\n    \"reader engagement\",\\n    \"LLM interaction\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation method\",\\n    \"three-phase approach (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update)\",\\n    \"maintaining coherence and adaptability across LLM interactions\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='d4be8654-b546-4e18-ba54-8feb3a5492be'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation LLM\",\\n      \"rationale\": \"Directly targets the core concept of dynamically adapting prompts for LLMs, a key aspect of the research question.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"coherence in extended LLM\",\\n      \"rationale\": \"Focuses on the problem domain of maintaining coherence, which is central to the research question\\'s goal in long interactions.\",\\n      \"priority_concept\": \"coherence, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Highlights the methodology of generating prompts in an adaptive manner, aligning with the proposed novel approach.\",\\n      \"priority_concept\": \"adaptive prompt generation, prompt engineering\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Addresses the \\'Iterative Context Update\\' phase of the proposed method, crucial for maintaining state in extended interactions.\",\\n      \"priority_concept\": \"iterative context update, contextual analysis\"\\n    },\\n    {\\n      \"query_string\": \"thematic consistency LLM\",\\n      \"rationale\": \"Searches for work related to maintaining thematic consistency, a specific aspect of coherence in the context of LLM interactions.\",\\n      \"priority_concept\": \"thematic consistency, LLM interaction\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='df3e3d38-4dc6-473a-bd42-348c81dbc69c')]}\n",
      "\u001b[1m[updates]\u001b[0m {'search_paper': {'messages': [AIMessage(content='{\\n  \"dynamic prompt adaptation LLM\": {\\n    \"total\": 467,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"cf95279b1da9de1aad9e7c651f5048f69af295ed\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf95279b1da9de1aad9e7c651f5048f69af295ed\",\\n        \"title\": \"AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents\",\\n        \"citationCount\": 76,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.13352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2175939276\",\\n            \"name\": \"Edoardo Debenedetti\"\\n          },\\n          {\\n            \"authorId\": \"2299061721\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2138580250\",\\n            \"name\": \"Mislav Balunovi\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2150869869\",\\n            \"name\": \"Luca Beurer-Kellner\"\\n          },\\n          {\\n            \"authorId\": \"2307472727\",\\n            \"name\": \"Marc Fischer\"\\n          },\\n          {\\n            \"authorId\": \"2267733649\",\\n            \"name\": \"Florian Tram\\\\u00e8r\"\\n          }\\n        ],\\n        \"abstract\": \"AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.\"\\n      },\\n      {\\n        \"paperId\": \"3967189742efab8859da542ce3953d4c72957aca\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3967189742efab8859da542ce3953d4c72957aca\",\\n        \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.23904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI), designed to design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275054108\",\\n            \"name\": \"Qinqian Lei\"\\n          },\\n          {\\n            \"authorId\": \"2313081973\",\\n            \"name\": \"Bo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256998291\",\\n            \"name\": \"Robby T. Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\"\\n      },\\n      {\\n        \"paperId\": \"659e0b3303caa860348dee52f41476e3fddc9573\",\\n        \"url\": \"https://www.semanticscholar.org/paper/659e0b3303caa860348dee52f41476e3fddc9573\",\\n        \"title\": \"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method is introduced, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages and allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2237800256\",\\n            \"name\": \"Qichen Fu\"\\n          },\\n          {\\n            \"authorId\": \"2237803694\",\\n            \"name\": \"Minsik Cho\"\\n          },\\n          {\\n            \"authorId\": \"2178316365\",\\n            \"name\": \"Thomas Merth\"\\n          },\\n          {\\n            \"authorId\": \"2256998189\",\\n            \"name\": \"Sachin Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2284683934\",\\n            \"name\": \"Mohammad Rastegari\"\\n          },\\n          {\\n            \"authorId\": \"40465379\",\\n            \"name\": \"Mahyar Najibi\"\\n          }\\n        ],\\n        \"abstract\": \"The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.\"\\n      },\\n      {\\n        \"paperId\": \"00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\\n        \"title\": \"From ideas to ventures: building entrepreneurship knowledge with LLM, prompt engineering, and conversational agents\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10639-024-12775-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10639-024-12775-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Large Language Models like the Generative Pre-trained Transformer 4 (GPT-4), recognized for their exceptional performance on public datasets, are examined in this study for their potential adaptability and interactivity nature, which align well with the dynamic nature of entrepreneurship learning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1424283019\",\\n            \"name\": \"Marsela Thanasi-Bo\\\\u00e7e\"\\n          },\\n          {\\n            \"authorId\": \"2290624379\",\\n            \"name\": \"Julian Hoxha\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"b67a6181fad5c5838945583ccdc7f39187e29332\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b67a6181fad5c5838945583ccdc7f39187e29332\",\\n        \"title\": \"Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.20911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Mantis is a defensive framework that exploits LLMs\\' susceptibility to adversarial inputs to undermine malicious operations, leading the attacker\\'s LLM to disrupt their own operations or even compromise the attacker\\'s machine.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50610174\",\\n            \"name\": \"Dario Pasquini\"\\n          },\\n          {\\n            \"authorId\": \"2762279\",\\n            \"name\": \"Evgenios M. Kornaropoulos\"\\n          },\\n          {\\n            \"authorId\": \"1700850\",\\n            \"name\": \"G. Ateniese\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs\\' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker\\'s LLM to disrupt their own operations (passive defense) or even compromise the attacker\\'s machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker\\'s LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project_mantis\"\\n      },\\n      {\\n        \"paperId\": \"2f274db9aa447a13c019114e327057d4b161b6d5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f274db9aa447a13c019114e327057d4b161b6d5\",\\n        \"title\": \"LLM-controller: Dynamic robot control adaptation using large language models\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.robot.2024.104913?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.robot.2024.104913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"90182090\",\\n            \"name\": \"Rasoul Zahedifar\"\\n          },\\n          {\\n            \"authorId\": \"1799503\",\\n            \"name\": \"M. Baghshah\"\\n          },\\n          {\\n            \"authorId\": \"2273939584\",\\n            \"name\": \"Alireza Taheri\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\\n        \"url\": \"https://www.semanticscholar.org/paper/47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\\n        \"title\": \"HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.19878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE is proposed, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-09-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2211429378\",\\n            \"name\": \"Bingshen Mu\"\\n          },\\n          {\\n            \"authorId\": \"2299944267\",\\n            \"name\": \"Kun Wei\"\\n          },\\n          {\\n            \"authorId\": \"2061559378\",\\n            \"name\": \"Qijie Shao\"\\n          },\\n          {\\n            \"authorId\": \"2323714781\",\\n            \"name\": \"Yong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2249732546\",\\n            \"name\": \"Lei Xie\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.\"\\n      },\\n      {\\n        \"paperId\": \"bcac614f9774488447221ebb4f16f05e3975ec1e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bcac614f9774488447221ebb4f16f05e3975ec1e\",\\n        \"title\": \"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\",\\n        \"citationCount\": 75,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.04669\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A well-designed visual tokenizer is introduced to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read, which empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2239056878\",\\n            \"name\": \"Yang Jin\"\\n          },\\n          {\\n            \"authorId\": \"2266735757\",\\n            \"name\": \"Kun Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266735757\",\\n            \"name\": \"Kun Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266419021\",\\n            \"name\": \"Liwei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2239059653\",\\n            \"name\": \"Chao Liao\"\\n          },\\n          {\\n            \"authorId\": \"2239091862\",\\n            \"name\": \"Jianchao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2007771781\",\\n            \"name\": \"Quzhe Huang\"\\n          },\\n          {\\n            \"authorId\": \"2230906921\",\\n            \"name\": \"Bin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2366079231\",\\n            \"name\": \"Chenyi Lei\"\\n          },\\n          {\\n            \"authorId\": \"2239069665\",\\n            \"name\": \"An Liu\"\\n          },\\n          {\\n            \"authorId\": \"2241686105\",\\n            \"name\": \"Chengru Song\"\\n          },\\n          {\\n            \"authorId\": \"2238955477\",\\n            \"name\": \"Xiaoqiang Lei\"\\n          },\\n          {\\n            \"authorId\": \"2228125963\",\\n            \"name\": \"Di Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2238953778\",\\n            \"name\": \"Wenwu Ou\"\\n          },\\n          {\\n            \"authorId\": \"2238953242\",\\n            \"name\": \"Kun Gai\"\\n          },\\n          {\\n            \"authorId\": \"2238953689\",\\n            \"name\": \"Yadong Mu\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model\\'s potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.\"\\n      },\\n      {\\n        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"58700f3740105e3422eb030305372b6d8bc44986\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58700f3740105e3422eb030305372b6d8bc44986\",\\n        \"title\": \"Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours, and can serve as an orthogonal optimization for synergistic integration with existing speculative decoding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2242179319\",\\n            \"name\": \"H. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2303652170\",\\n            \"name\": \"Wayne Luk\"\\n          },\\n          {\\n            \"authorId\": \"2301711440\",\\n            \"name\": \"Ka-Fai Cedric Yiu\"\\n          },\\n          {\\n            \"authorId\": \"2152153064\",\\n            \"name\": \"Rui Li\"\\n          },\\n          {\\n            \"authorId\": \"2303652428\",\\n            \"name\": \"Konstantin Mishchenko\"\\n          },\\n          {\\n            \"authorId\": \"2115955596\",\\n            \"name\": \"Stylianos I. Venieris\"\\n          },\\n          {\\n            \"authorId\": \"10001427\",\\n            \"name\": \"Hongxiang Fan\"\\n          }\\n        ],\\n        \"abstract\": \"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\\\\\\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\\\\\\\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.\"\\n      },\\n      {\\n        \"paperId\": \"427c3a502d132b6e1cea2d5565460d284db6e3f7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/427c3a502d132b6e1cea2d5565460d284db6e3f7\",\\n        \"title\": \"Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2507.00601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies and introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-06-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2345186311\",\\n            \"name\": \"Shuangquan Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2353085449\",\\n            \"name\": \"Yingnan Deng\"\\n          },\\n          {\\n            \"authorId\": \"2372425942\",\\n            \"name\": \"Guiran Liu\"\\n          },\\n          {\\n            \"authorId\": \"2374351029\",\\n            \"name\": \"Zhen Qi\"\\n          },\\n          {\\n            \"authorId\": \"2372322790\",\\n            \"name\": \"Ruotong Wang\"\\n          }\\n        ],\\n        \"abstract\": \"This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model\\'s original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method\\'s applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.\"\\n      },\\n      {\\n        \"paperId\": \"c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"title\": \"Dynamic and Adaptive Feature Generation with LLM\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research introduces a novel approach adopting large language models and feature-generating prompts to address challenges of explainability, applicability, and inflexible strategy and proposes a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130031226\",\\n            \"name\": \"XinHao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108045855\",\\n            \"name\": \"Jinghan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1966492\",\\n            \"name\": \"Banafsheh Rekabdar\"\\n          },\\n          {\\n            \"authorId\": \"2145108199\",\\n            \"name\": \"Yuanchun Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2301248160\",\\n            \"name\": \"Pengfei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293571072\",\\n            \"name\": \"Kunpeng Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\\n        \"title\": \"Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.04295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process and demonstrates measurable performance improvements compared to content-only optimization methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2344555074\",\\n            \"name\": \"Yuanye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257094139\",\\n            \"name\": \"Jiahang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2274195530\",\\n            \"name\": \"L. Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2344193091\",\\n            \"name\": \"Qi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2341721557\",\\n            \"name\": \"Xuan Feng\"\\n          },\\n          {\\n            \"authorId\": \"2344520491\",\\n            \"name\": \"Yang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2339241318\",\\n            \"name\": \"Zhongxin Guo\"\\n          },\\n          {\\n            \"authorId\": \"2344097630\",\\n            \"name\": \"Yuqing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2296029993\",\\n            \"name\": \"Peng Cheng\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 130,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"e3e3aa5576de899b755100db211501bb405aba3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3e3aa5576de899b755100db211501bb405aba3e\",\\n        \"title\": \"Firewalls to Secure Dynamic LLM Agentic Networks\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.01822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work identifies required properties for agent communication: proactivity, adaptability, privacy, privacy, and security, and proposes a practical design and protocol inspired by network security principles.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2343752567\",\\n            \"name\": \"Sahar Abdelnabi\"\\n          },\\n          {\\n            \"authorId\": \"2249532110\",\\n            \"name\": \"Amr Gomaa\"\\n          },\\n          {\\n            \"authorId\": \"36103467\",\\n            \"name\": \"Eugene Bagdasarian\"\\n          },\\n          {\\n            \"authorId\": \"2237674591\",\\n            \"name\": \"P. O. Kristensson\"\\n          },\\n          {\\n            \"authorId\": \"2346834097\",\\n            \"name\": \"Reza Shokri\"\\n          }\\n        ],\\n        \"abstract\": \"LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.\"\\n      },\\n      {\\n        \"paperId\": \"ee552989a03693a441863af4c29dc594bfcd1ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ee552989a03693a441863af4c29dc594bfcd1ab5\",\\n        \"title\": \"AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2503.18891?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2503.18891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316787389\",\\n            \"name\": \"Zhexuan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2306069252\",\\n            \"name\": \"Yutong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256344322\",\\n            \"name\": \"Xuebo Liu\"\\n          },\\n          {\\n            \"authorId\": \"46573238\",\\n            \"name\": \"Liang Ding\"\\n          },\\n          {\\n            \"authorId\": \"2187384924\",\\n            \"name\": \"Miao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2348727938\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2346352158\",\\n            \"name\": \"Min Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents\\\\u2019 communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout , which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yufeng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"273b2c64d675edd522cd6f679891756ad5207296\",\\n        \"url\": \"https://www.semanticscholar.org/paper/273b2c64d675edd522cd6f679891756ad5207296\",\\n        \"title\": \"Exploring the Effectiveness of LLM Domain Adaptation for Business IT Machine Translation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://aclanthology.org/2024.eamt-1.51, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that while LLMs can translate on-par with SAP\\\\u2019s MT models on general domain data, it is difficult to close the gap on SAP\\\\u2019s domain-specific data, even with extensive training and carefully curated data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2322393313\",\\n            \"name\": \"Johannes Eschbach-Dymanus\"\\n          },\\n          {\\n            \"authorId\": \"2322400027\",\\n            \"name\": \"Frank Essenberger\"\\n          },\\n          {\\n            \"authorId\": \"1403814959\",\\n            \"name\": \"Bianka Buschbeck-Wolf\"\\n          },\\n          {\\n            \"authorId\": \"70124681\",\\n            \"name\": \"Miriam Exel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\\n        \"title\": \"ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities\",\\n        \"citationCount\": 81,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283831590\",\\n            \"name\": \"Jiarui Lu\"\\n          },\\n          {\\n            \"authorId\": \"2315811087\",\\n            \"name\": \"Thomas Holleis\"\\n          },\\n          {\\n            \"authorId\": \"2313695880\",\\n            \"name\": \"Yizhe Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2315810003\",\\n            \"name\": \"Bernhard Aumayer\"\\n          },\\n          {\\n            \"authorId\": \"2313640225\",\\n            \"name\": \"Feng Nan\"\\n          },\\n          {\\n            \"authorId\": \"2313910532\",\\n            \"name\": \"Felix Bai\"\\n          },\\n          {\\n            \"authorId\": \"2313694040\",\\n            \"name\": \"Shuang Ma\"\\n          },\\n          {\\n            \"authorId\": \"2313694042\",\\n            \"name\": \"Shen Ma\"\\n          },\\n          {\\n            \"authorId\": \"2315946702\",\\n            \"name\": \"Mengyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2293171017\",\\n            \"name\": \"Guoli Yin\"\\n          },\\n          {\\n            \"authorId\": \"2313671930\",\\n            \"name\": \"Zirui Wang\"\\n          },\\n          {\\n            \"authorId\": \"2238621132\",\\n            \"name\": \"Ruoming Pang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at https://github.com/apple/ToolSandbox\"\\n      },\\n      {\\n        \"paperId\": \"44b0d2e884efa5344e50424dbe2edf616981f201\",\\n        \"url\": \"https://www.semanticscholar.org/paper/44b0d2e884efa5344e50424dbe2edf616981f201\",\\n        \"title\": \"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.00807\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.00807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work develops and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply and achieves substantially lower latency than standard reranking methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2127522115\",\\n            \"name\": \"Jon Saad-Falcon\"\\n          },\\n          {\\n            \"authorId\": \"144112155\",\\n            \"name\": \"O. Khattab\"\\n          },\\n          {\\n            \"authorId\": \"50818255\",\\n            \"name\": \"Keshav Santhanam\"\\n          },\\n          {\\n            \"authorId\": \"1707117\",\\n            \"name\": \"Radu Florian\"\\n          },\\n          {\\n            \"authorId\": \"39038065\",\\n            \"name\": \"M. Franz\"\\n          },\\n          {\\n            \"authorId\": \"1781292\",\\n            \"name\": \"S. Roukos\"\\n          },\\n          {\\n            \"authorId\": \"2707234\",\\n            \"name\": \"Avirup Sil\"\\n          },\\n          {\\n            \"authorId\": \"2937809\",\\n            \"name\": \"Md Arafat Sultan\"\\n          },\\n          {\\n            \"authorId\": \"144922861\",\\n            \"name\": \"Christopher Potts\"\\n          }\\n        ],\\n        \"abstract\": \"Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.\"\\n      },\\n      {\\n        \"paperId\": \"9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\\n        \"title\": \"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\",\\n        \"citationCount\": 212,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.02490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing, is introduced, which effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2181120463\",\\n            \"name\": \"Huiqiang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"1527099159\",\\n            \"name\": \"Yucheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2284970741\",\\n            \"name\": \"Chengruidong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108728536\",\\n            \"name\": \"Qianhui Wu\"\\n          },\\n          {\\n            \"authorId\": \"13289447\",\\n            \"name\": \"Xufang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2309738728\",\\n            \"name\": \"Surin Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2281867465\",\\n            \"name\": \"Zhenhua Han\"\\n          },\\n          {\\n            \"authorId\": \"2309244780\",\\n            \"name\": \"Amir H. Abdi\"\\n          },\\n          {\\n            \"authorId\": \"2305587638\",\\n            \"name\": \"Dongsheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2257359863\",\\n            \"name\": \"Chin-Yew Lin\"\\n          },\\n          {\\n            \"authorId\": \"2125051198\",\\n            \"name\": \"Yuqing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2160727304\",\\n            \"name\": \"Lili Qiu\"\\n          }\\n        ],\\n        \"abstract\": \"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\"\\n      },\\n      {\\n        \"paperId\": \"34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\\n        \"title\": \"GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting\",\\n        \"citationCount\": 91,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/30383/32447\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i21.30383?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i21.30383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously simultaneously, named GPT4MTS, and proposes a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2293923697\",\\n            \"name\": \"Kevin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2260272787\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.\"\\n      },\\n      {\\n        \"paperId\": \"c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\\n        \"title\": \"Do LLMs Understand Visual Anomalies? Uncovering LLM\\'s Capabilities in Zero-shot Anomaly Detection\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.09654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM), enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108678212\",\\n            \"name\": \"Jiaqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"51459472\",\\n            \"name\": \"Shaofeng Cai\"\\n          },\\n          {\\n            \"authorId\": \"2276607267\",\\n            \"name\": \"Fang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2188240935\",\\n            \"name\": \"Bengchin Ooi\"\\n          },\\n          {\\n            \"authorId\": \"2296743990\",\\n            \"name\": \"Junran Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA\\'s effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.\"\\n      },\\n      {\\n        \"paperId\": \"8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"title\": \"FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.15813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2247164321\",\\n            \"name\": \"Yu Lu\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"3446334\",\\n            \"name\": \"Hehe Fan\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n      },\\n      {\\n        \"paperId\": \"162f33c7799683ca9b0f193275fe7eec5a0b973f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/162f33c7799683ca9b0f193275fe7eec5a0b973f\",\\n        \"title\": \"EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.09618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EasyRef is introduced, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt and surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1571400317\",\\n            \"name\": \"Zhuofan Zong\"\\n          },\\n          {\\n            \"authorId\": \"2293242031\",\\n            \"name\": \"Dongzhi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2261489892\",\\n            \"name\": \"Bingqi Ma\"\\n          },\\n          {\\n            \"authorId\": \"12920342\",\\n            \"name\": \"Guanglu Song\"\\n          },\\n          {\\n            \"authorId\": \"2075457131\",\\n            \"name\": \"Hao Shao\"\\n          },\\n          {\\n            \"authorId\": \"2292263397\",\\n            \"name\": \"Dazhong Shen\"\\n          },\\n          {\\n            \"authorId\": \"2292207974\",\\n            \"name\": \"Yu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2261394248\",\\n            \"name\": \"Hongsheng Li\"\\n          }\\n        ],\\n        \"abstract\": \"Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although the tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent elements within multiple images through the training process, it necessitates specific finetuning for each distinct image group. This paper introduces EasyRef, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM\\'s representations into the diffusion process through adapters can easily generalize to unseen domains, mining the consistent visual elements within unseen data. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\\n      },\\n      {\\n        \"paperId\": \"ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"title\": \"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.10462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation, is proposed and experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2241468593\",\\n            \"name\": \"Hossein Rajabzadeh\"\\n          },\\n          {\\n            \"authorId\": \"9200111\",\\n            \"name\": \"Mojtaba Valipour\"\\n          },\\n          {\\n            \"authorId\": \"2284643707\",\\n            \"name\": \"Tianshu Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1996315\",\\n            \"name\": \"Marzieh S. Tahaei\"\\n          },\\n          {\\n            \"authorId\": \"2241480742\",\\n            \"name\": \"Hyock Ju Kwon\"\\n          },\\n          {\\n            \"authorId\": \"2237425782\",\\n            \"name\": \"Ali Ghodsi\"\\n          },\\n          {\\n            \"authorId\": \"2237517964\",\\n            \"name\": \"Boxing Chen\"\\n          },\\n          {\\n            \"authorId\": \"2066076226\",\\n            \"name\": \"Mehdi Rezagholizadeh\"\\n          }\\n        ],\\n        \"abstract\": \"Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n      },\\n      {\\n        \"paperId\": \"9a73effed8775962c86587feb0f9ef841fa2ff4c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a73effed8775962c86587feb0f9ef841fa2ff4c\",\\n        \"title\": \"LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258550477\",\\n            \"name\": \"Yash Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2258751660\",\\n            \"name\": \"Wenchang Gao\"\\n          },\\n          {\\n            \"authorId\": \"3379438\",\\n            \"name\": \"Vasanth Sarathy\"\\n          },\\n          {\\n            \"authorId\": \"2258715054\",\\n            \"name\": \"Alvaro Velasquez\"\\n          },\\n          {\\n            \"authorId\": \"2258551993\",\\n            \"name\": \"Robert Wright\"\\n          },\\n          {\\n            \"authorId\": \"1715858\",\\n            \"name\": \"Jivko Sinapov\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a propreitary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the LLM proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.\"\\n      },\\n      {\\n        \"paperId\": \"108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\\n        \"url\": \"https://www.semanticscholar.org/paper/108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\\n        \"title\": \"TimeRAG: Boosting LLM Time Series Forecasting via Retrieval-Augmented Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.16643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TimeRAG is proposed, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2206558646\",\\n            \"name\": \"Si-Nan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2337389407\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2336916957\",\\n            \"name\": \"Haoqi Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2336884221\",\\n            \"name\": \"Ruochun Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.\"\\n      },\\n      {\\n        \"paperId\": \"1e6325865e809670765bea9dadd3c40b2014eb6d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e6325865e809670765bea9dadd3c40b2014eb6d\",\\n        \"title\": \"VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The performance contribution of different prompt components is explored, the vision for future improvement in visual accessibility, and the way for LLMs in video anomaly detection and vision-language understanding is paved.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256768778\",\\n            \"name\": \"Hao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292146470\",\\n            \"name\": \"Jiayou Qin\"\\n          },\\n          {\\n            \"authorId\": \"2260725391\",\\n            \"name\": \"Ashish Bastola\"\\n          },\\n          {\\n            \"authorId\": \"2024833342\",\\n            \"name\": \"Xiwen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2292183597\",\\n            \"name\": \"John Suchanek\"\\n          },\\n          {\\n            \"authorId\": \"2292143727\",\\n            \"name\": \"Zihao Gong\"\\n          },\\n          {\\n            \"authorId\": \"2064311884\",\\n            \"name\": \"Abolfazl Razi\"\\n          }\\n        ],\\n        \"abstract\": \"This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.\"\\n      },\\n      {\\n        \"paperId\": \"7f96bb27a8fca35b1f7d02ee319a64be04114809\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7f96bb27a8fca35b1f7d02ee319a64be04114809\",\\n        \"title\": \"LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.08337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties, and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265927745\",\\n            \"name\": \"Maonan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2219695355\",\\n            \"name\": \"Aoyu Pang\"\\n          },\\n          {\\n            \"authorId\": \"7592365\",\\n            \"name\": \"Yuheng Kan\"\\n          },\\n          {\\n            \"authorId\": \"144305489\",\\n            \"name\": \"Man-On Pun\"\\n          },\\n          {\\n            \"authorId\": \"2292117616\",\\n            \"name\": \"Chung Shue Chen\"\\n          },\\n          {\\n            \"authorId\": \"2291077490\",\\n            \"name\": \"Bo Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system\\'s adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\\\\\\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.\"\\n      },\\n      {\\n        \"paperId\": \"650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"title\": \"Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.acl-srw.90?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.acl-srw.90, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Time-LlaMA framework achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2346152667\",\\n            \"name\": \"Juyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334602368\",\\n            \"name\": \"Jiechao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2374971695\",\\n            \"name\": \"Wenwen Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2334718086\",\\n            \"name\": \"Wei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2321408906\",\\n            \"name\": \"Hui Yi Leong\"\\n          }\\n        ],\\n        \"abstract\": \"Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model\\\\u2019s predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 212,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\\n        \"title\": \"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair\",\\n        \"citationCount\": 188,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2176865575\",\\n            \"name\": \"Islem Bouzenia\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"2260683361\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": \"Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent\\'s effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI\\'s GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\\n      },\\n      {\\n        \"paperId\": \"3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\\n        \"title\": \"Token-Budget-Aware LLM Reasoning\",\\n        \"citationCount\": 114,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem is proposed, offering a practical solution to balance efficiency and accuracy in LLM reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170360833\",\\n            \"name\": \"Tingxu Han\"\\n          },\\n          {\\n            \"authorId\": \"2154723145\",\\n            \"name\": \"Zhenting Wang\"\\n          },\\n          {\\n            \"authorId\": \"2239197945\",\\n            \"name\": \"Chunrong Fang\"\\n          },\\n          {\\n            \"authorId\": \"2110773055\",\\n            \"name\": \"Shiyun Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2333472479\",\\n            \"name\": \"Shiqing Ma\"\\n          },\\n          {\\n            \"authorId\": \"2238950128\",\\n            \"name\": \"Zhenyu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE\"\\n      },\\n      {\\n        \"paperId\": \"20843eaa59db5e2af416d7db47d51d0aab3de230\",\\n        \"url\": \"https://www.semanticscholar.org/paper/20843eaa59db5e2af416d7db47d51d0aab3de230\",\\n        \"title\": \"Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue Resolution\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.11709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper analyzes 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution and identifies key textual and code-related heuristics\\\\u2014Specificity, Contextual Richness, and Clarity\\\\u2014that are associated with successful issue closure and help assess prompt quality.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2147154362\",\\n            \"name\": \"Ramtin Ehsani\"\\n          },\\n          {\\n            \"authorId\": \"2341336471\",\\n            \"name\": \"Sakshi Pathak\"\\n          },\\n          {\\n            \"authorId\": \"9728244\",\\n            \"name\": \"Preetha Chatterjee\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have become essential in software development, especially for issue resolution. However, despite their widespread use, significant challenges persist in the quality of LLM responses to issue resolution queries. LLM interactions often yield incorrect, incomplete, or ambiguous information, largely due to knowledge gaps in prompt design, which can lead to unproductive exchanges and reduced developer productivity.In this paper, we analyze 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution. We identify four main knowledge gaps in developer prompts: Missing Context, Missing Specifications, Multiple Context, and Unclear Instructions. Assuming that conversations within closed issues contributed to successful resolutions while those in open issues did not, we find that ineffective conversations contain knowledge gaps in $44.6 \\\\\\\\%$ of prompts, compared to only $12.6 \\\\\\\\%$ in effective ones. Additionally, we observe seven distinct conversational styles, with Directive Prompting, Chain of Thought, and Responsive Feedback being the most prevalent. We find that knowledge gaps are present in all styles of conversations, with Missing Context being the most repeated challenge developers face in issue-resolution conversations.Based on our analysis, we identify key textual and code-related heuristics\\\\u2014Specificity, Contextual Richness, and Clarity\\\\u2014that are associated with successful issue closure and help assess prompt quality. These heuristics lay the foundation for an automated tool that can dynamically flag unclear prompts and suggest structured improvements. To test feasibility, we developed a lightweight browser extension prototype for detecting prompt gaps, that can be easily adapted to other tools within developer workflows.\"\\n      },\\n      {\\n        \"paperId\": \"25cee84e3a1541697a7c97443d7526574127c344\",\\n        \"url\": \"https://www.semanticscholar.org/paper/25cee84e3a1541697a7c97443d7526574127c344\",\\n        \"title\": \"Don\\'t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\",\\n        \"citationCount\": 158,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.00367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs, and proposes two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2114887261\",\\n            \"name\": \"Shangbin Feng\"\\n          },\\n          {\\n            \"authorId\": \"2254168375\",\\n            \"name\": \"Weijia Shi\"\\n          },\\n          {\\n            \"authorId\": \"2108853330\",\\n            \"name\": \"Yike Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282214127\",\\n            \"name\": \"Wenxuan Ding\"\\n          },\\n          {\\n            \"authorId\": \"143820870\",\\n            \"name\": \"Vidhisha Balachandran\"\\n          },\\n          {\\n            \"authorId\": \"2249583325\",\\n            \"name\": \"Yulia Tsvetkov\"\\n          }\\n        ],\\n        \"abstract\": \"Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"d5342fce96175f83550cfae471a0a46d16401481\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d5342fce96175f83550cfae471a0a46d16401481\",\\n        \"title\": \"ST-LLM: Large Language Models Are Effective Temporal Learners\",\\n        \"citationCount\": 120,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM, and develops a dynamic masking strategy with tailor-made training objectives to address the overhead and stability issues introduced by uncompressed video tokens within LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"12287885\",\\n            \"name\": \"Ruyang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256784925\",\\n            \"name\": \"Chen Li\"\\n          },\\n          {\\n            \"authorId\": \"2294629231\",\\n            \"name\": \"Haoran Tang\"\\n          },\\n          {\\n            \"authorId\": \"152988335\",\\n            \"name\": \"Yixiao Ge\"\\n          },\\n          {\\n            \"authorId\": \"2265579883\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2294517847\",\\n            \"name\": \"Ge Li\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at https://github.com/TencentARC/ST-LLM.\"\\n      },\\n      {\\n        \"paperId\": \"12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\\n        \"title\": \"Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.17282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with, and performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218970025\",\\n            \"name\": \"Yi-Kai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2274937712\",\\n            \"name\": \"De-Chuan Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2151459740\",\\n            \"name\": \"Han-Jia Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"edfff0e15449f438a13a7341290c008bf6486afc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edfff0e15449f438a13a7341290c008bf6486afc\",\\n        \"title\": \"MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.18035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism, which significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2327293591\",\\n            \"name\": \"Jingfan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2322997112\",\\n            \"name\": \"Yi Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2327694040\",\\n            \"name\": \"Dan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2293390999\",\\n            \"name\": \"Xing Tian\"\\n          },\\n          {\\n            \"authorId\": \"2179528564\",\\n            \"name\": \"Huanran Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2322888603\",\\n            \"name\": \"Wei Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 203,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"ab4800a924508f49d644ced8ba236ec92f54f566\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ab4800a924508f49d644ced8ba236ec92f54f566\",\\n        \"title\": \"Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2505.16552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach and enhances CoLaR through reinforcement learning (RL) that leverages the latent head\\'s non-deterministic nature to explore diverse reasoning paths and exploit more compact ones.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218557953\",\\n            \"name\": \"Wenhui Tan\"\\n          },\\n          {\\n            \"authorId\": \"2362865102\",\\n            \"name\": \"Jiaze Li\"\\n          },\\n          {\\n            \"authorId\": \"2317982861\",\\n            \"name\": \"Jianzhong Ju\"\\n          },\\n          {\\n            \"authorId\": \"2363405807\",\\n            \"name\": \"Zhenbo Luo\"\\n          },\\n          {\\n            \"authorId\": \"2317980688\",\\n            \"name\": \"Jian Luan\"\\n          },\\n          {\\n            \"authorId\": \"2290923147\",\\n            \"name\": \"Ruihua Song\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head\\'s non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance.\"\\n      },\\n      {\\n        \"paperId\": \"268e28f8d5235031dcd7bfae0f857439e27e8564\",\\n        \"url\": \"https://www.semanticscholar.org/paper/268e28f8d5235031dcd7bfae0f857439e27e8564\",\\n        \"title\": \"SteP: Stacked LLM Policies for Web Actions\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.03720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Stacked LLM Policies for Web Actions is proposed, an approach to dynamically compose policies to solve a diverse set of web tasks and is competitive with prior works while using significantly less data.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-10-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2254267756\",\\n            \"name\": \"Paloma Sodhi\"\\n          },\\n          {\\n            \"authorId\": \"1741598\",\\n            \"name\": \"S. Branavan\"\\n          },\\n          {\\n            \"authorId\": \"2066324938\",\\n            \"name\": \"Yoav Artzi\"\\n          },\\n          {\\n            \"authorId\": \"2254260284\",\\n            \"name\": \"Ryan McDonald\"\\n          }\\n        ],\\n        \"abstract\": \"Performing tasks on the web presents fundamental challenges to large language models (LLMs), including combinatorially large open-world tasks and variations across web interfaces. Simply specifying a large prompt to handle all possible behaviors and states is extremely complex, and results in behavior leaks between unrelated behaviors. Decomposition to distinct policies can address this challenge, but requires carefully handing off control between policies. We propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically compose policies to solve a diverse set of web tasks. SteP defines a Markov Decision Process where the state is a stack of policies representing the control state, i.e., the chain of policy calls. Unlike traditional methods that are restricted to static hierarchies, SteP enables dynamic control that adapts to the complexity of the task. We evaluate SteP against multiple baselines and web environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP improves (14.9\\\\\\\\% to 33.5\\\\\\\\%) over SOTA that use GPT-4 policies, while on MiniWob++, SteP is competitive with prior works while using significantly less data. Our code and data are available at https://asappresearch.github.io/webagents-step.\"\\n      },\\n      {\\n        \"paperId\": \"1f9822022f586e375461660db792f23e891c7123\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1f9822022f586e375461660db792f23e891c7123\",\\n        \"title\": \"Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems\",\\n        \"citationCount\": 62,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2307.06187\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.06187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models, grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2658311\",\\n            \"name\": \"N. Nascimento\"\\n          },\\n          {\\n            \"authorId\": \"40761174\",\\n            \"name\": \"Paulo Alencar\"\\n          },\\n          {\\n            \"authorId\": \"2149928782\",\\n            \"name\": \"Donald D. Cowan\"\\n          }\\n        ],\\n        \"abstract\": \"The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs\\' capabilities and indicating further research opportunities to assess LLMs\\' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.\"\\n      },\\n      {\\n        \"paperId\": \"3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\\n        \"title\": \"Bridging the Gulf of Envisioning: Cognitive Challenges in Prompt Based Interactions with LLMs\",\\n        \"citationCount\": 91,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642754\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work theorizes how end-users \\\\u2018envision\\\\u2019 translating their goals into clear intentions and craft prompts to obtain the desired LLM response.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2301051721\",\\n            \"name\": \"Hariharan Subramonyam\"\\n          },\\n          {\\n            \"authorId\": \"2246886383\",\\n            \"name\": \"Roy D. Pea\"\\n          },\\n          {\\n            \"authorId\": \"1825757380\",\\n            \"name\": \"Christopher Pondoc\"\\n          },\\n          {\\n            \"authorId\": \"1820412\",\\n            \"name\": \"Maneesh Agrawala\"\\n          },\\n          {\\n            \"authorId\": \"2289103973\",\\n            \"name\": \"Colleen M. Seifert\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman\\\\u2019s gulfs of execution and evaluation. To address this gap, we theorize how end-users \\\\u2018envision\\\\u2019 translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments on not knowing: (1) what the task should be, (2) how to instruct the LLM to do the task, and (3) what to expect for the LLM\\\\u2019s output in meeting the goal. Finally, we make recommendations to narrow the gulf of envisioning in human-LLM interactions.\"\\n      },\\n      {\\n        \"paperId\": \"28d6411019f448f54834c2a5cff723cd350345b5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/28d6411019f448f54834c2a5cff723cd350345b5\",\\n        \"title\": \"Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC) involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2289252454\",\\n            \"name\": \"Haokun Liu\"\\n          },\\n          {\\n            \"authorId\": \"8247318\",\\n            \"name\": \"Yaonan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2110285301\",\\n            \"name\": \"Kenji Kato\"\\n          },\\n          {\\n            \"authorId\": \"2307380233\",\\n            \"name\": \"Atsushi Tsukahara\"\\n          },\\n          {\\n            \"authorId\": \"2282115338\",\\n            \"name\": \"Izumi Kondo\"\\n          },\\n          {\\n            \"authorId\": \"1752849\",\\n            \"name\": \"T. Aoyama\"\\n          },\\n          {\\n            \"authorId\": \"2237520520\",\\n            \"name\": \"Yasuhisa Hasegawa\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This letter proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.\"\\n      },\\n      {\\n        \"paperId\": \"77a9c310df0d7896d297da90fc4a1131819c341e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/77a9c310df0d7896d297da90fc4a1131819c341e\",\\n        \"title\": \"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding, and proposes LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293615241\",\\n            \"name\": \"Mingxing Peng\"\\n          },\\n          {\\n            \"authorId\": \"2293665950\",\\n            \"name\": \"Xusen Guo\"\\n          },\\n          {\\n            \"authorId\": \"2146413818\",\\n            \"name\": \"Xianda Chen\"\\n          },\\n          {\\n            \"authorId\": \"2241024418\",\\n            \"name\": \"Meixin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2267078966\",\\n            \"name\": \"Kehua Chen\"\\n          },\\n          {\\n            \"authorId\": \"2293775145\",\\n            \"name\": \"Hao Yang\"\\n          },\\n          {\\n            \"authorId\": \"2258778041\",\\n            \"name\": \"Xuesong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2258755636\",\\n            \"name\": \"Yinhai Wang\"\\n          }\\n        ],\\n        \"abstract\": \"To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.\"\\n      }\\n    ]\\n  },\\n  \"coherence in extended LLM\": {\\n    \"total\": 23,\\n    \"offset\": 0,\\n    \"data\": [\\n      {\\n        \"paperId\": \"d0da372b4b6f422e13556ce272595a0c9002fe90\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d0da372b4b6f422e13556ce272595a0c9002fe90\",\\n        \"title\": \"GPT-4 in Education: Evaluating Aptness, Reliability, and Loss of Coherence in Solving Calculus Problems and Grading Submissions\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s40593-024-00403-3.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40593-024-00403-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40593-024-00403-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256969876\",\\n            \"name\": \"Alberto Gandolfi\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we initially investigate the capabilities of GPT-3 5 and GPT-4 in solving college-level calculus problems, an essential segment of mathematics that remains under-explored so far. Although improving upon earlier versions, GPT-4 attains approximately 65% accuracy for standard problems and decreases to 20% for competition-like scenarios. Overall, the models prove to be unreliable due to common arithmetic errors. Our primary contribution lies then in examining the use of ChatGPT for grading solutions to calculus exercises. Our objectives are to probe an in-context learning task with less emphasis over direct calculations; recognize positive applications of ChatGPT in educational contexts; highlight a potentially emerging facet of AI that could necessitate oversight; and introduce unconventional AI benchmarks, for which models like GPT are untrained. Pertaining to the latter, we uncover a tendency for loss of coherence in extended contexts. Our findings suggest that while the current ChatGPT exhibits comprehension of the grading task and often provides relevant outputs, the consistency of grading is marred by occasional loss of coherence and hallucinations. Intriguingly, GPT-4\\'s overall scores, delivered in mere moments, align closely with human graders, although its detailed accuracy remains suboptimal. This work suggests that, when appropriately orchestrated, collaboration between human graders and LLMs like GPT-4 might combine their unique strengths while mitigating their respective shortcomings In this direction, it is imperative to consider implementing transparency, fairness, and appropriate regulations in the near future.\"\\n      },\\n      {\\n        \"paperId\": \"49b7bd275a0386392769f5b33028500754dbc69d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/49b7bd275a0386392769f5b33028500754dbc69d\",\\n        \"title\": \"AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE), and reveals that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2317040111\",\\n            \"name\": \"Abhay Gupta\"\\n          },\\n          {\\n            \"authorId\": \"2317010916\",\\n            \"name\": \"Philip Meng\"\\n          },\\n          {\\n            \"authorId\": \"2317007185\",\\n            \"name\": \"Ece Yurtseven\"\\n          },\\n          {\\n            \"authorId\": \"2241351144\",\\n            \"name\": \"Sean O\\'Brien\"\\n          },\\n          {\\n            \"authorId\": \"2312105716\",\\n            \"name\": \"Kevin Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting biases in natural language understanding (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language processing (NLP) systems. To address dialect-induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE). AAVENUE builds upon and extends existing benchmarks like VALUE, replacing deterministic syntactic and morphological transformations with a more flexible methodology leveraging LLM-based translation with few-shot prompting, improving performance across our evaluation metrics when translating key tasks from the GLUE and SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics including fluency, BARTScore, quality, coherence, and understandability. Additionally, we recruit fluent AAVE speakers to validate our translations for authenticity. Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\\n      },\\n      {\\n        \"paperId\": \"e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\\n        \"title\": \"DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.findings-emnlp.606.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME), which proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2110546424\",\\n            \"name\": \"Weijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261645232\",\\n            \"name\": \"Wenxiang Hu\"\\n          },\\n          {\\n            \"authorId\": \"2261413304\",\\n            \"name\": \"Fanyou Wu\"\\n          },\\n          {\\n            \"authorId\": \"1757518\",\\n            \"name\": \"Srinivasan H. Sengamedu\"\\n          }\\n        ],\\n        \"abstract\": \"In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME\\'s potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\\n      },\\n      {\\n        \"paperId\": \"0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\\n        \"title\": \"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An evaluation framework for mental health chatbots is validated, proving its effectiveness in improving safety and reliability and highlighting the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-08-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2315914561\",\\n            \"name\": \"Jung In Park\"\\n          },\\n          {\\n            \"authorId\": \"2182148069\",\\n            \"name\": \"Mahyar Abbasian\"\\n          },\\n          {\\n            \"authorId\": \"2241201441\",\\n            \"name\": \"Iman Azimi\"\\n          },\\n          {\\n            \"authorId\": \"2315810363\",\\n            \"name\": \"Dawn Bounds\"\\n          },\\n          {\\n            \"authorId\": \"2315810053\",\\n            \"name\": \"Angela Jun\"\\n          },\\n          {\\n            \"authorId\": \"2315889398\",\\n            \"name\": \"Jaesu Han\"\\n          },\\n          {\\n            \"authorId\": \"2315811390\",\\n            \"name\": \"Robert McCarron\"\\n          },\\n          {\\n            \"authorId\": \"2297708916\",\\n            \"name\": \"Jessica Borelli\"\\n          },\\n          {\\n            \"authorId\": \"2348273518\",\\n            \"name\": \"Parmida Safavi\"\\n          },\\n          {\\n            \"authorId\": \"2348305728\",\\n            \"name\": \"Sanaz Mirbaha\"\\n          },\\n          {\\n            \"authorId\": \"2315875066\",\\n            \"name\": \"Jia Li\"\\n          },\\n          {\\n            \"authorId\": \"2315811652\",\\n            \"name\": \"Mona Mahmoudi\"\\n          },\\n          {\\n            \"authorId\": \"2315811354\",\\n            \"name\": \"Carmen Wiedenhoeft\"\\n          },\\n          {\\n            \"authorId\": \"2311169857\",\\n            \"name\": \"Amir M. Rahmani\"\\n          }\\n        ],\\n        \"abstract\": \"Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Materials and Methods: We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. Results: The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Discussion: Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. Conclusion: The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.\"\\n      },\\n      {\\n        \"paperId\": \"09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\\n        \"url\": \"https://www.semanticscholar.org/paper/09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\\n        \"title\": \"Asynchronous LLM Function Calling\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.07017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An in-context protocol for function calls and interrupts is designed, an fine-tuning strategy is provided to adapt LLMs to the interrupt semantics, and mechanisms to asynchronously notify the LLM in-flight when function calls return are implemented.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265756791\",\\n            \"name\": \"In Gim\"\\n          },\\n          {\\n            \"authorId\": \"2118065368\",\\n            \"name\": \"Seung-seob Lee\"\\n          },\\n          {\\n            \"authorId\": \"2323908057\",\\n            \"name\": \"Lin Zhong\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM\\'s operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call\\'s completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.\"\\n      },\\n      {\\n        \"paperId\": \"3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\\n        \"title\": \"Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-024-56309-6.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10925587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies, and demonstrating a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260831947\",\\n            \"name\": \"Juho Jung\"\\n          },\\n          {\\n            \"authorId\": \"2152569226\",\\n            \"name\": \"Jinyoung Han\"\\n          },\\n          {\\n            \"authorId\": \"6777367\",\\n            \"name\": \"J. Han\"\\n          },\\n          {\\n            \"authorId\": \"2088247339\",\\n            \"name\": \"Junseo Ko\"\\n          },\\n          {\\n            \"authorId\": \"1677558107\",\\n            \"name\": \"Jeewoo Yoon\"\\n          },\\n          {\\n            \"authorId\": \"39548326\",\\n            \"name\": \"J. Hwang\"\\n          },\\n          {\\n            \"authorId\": \"2265721342\",\\n            \"name\": \"Ji In Park\"\\n          },\\n          {\\n            \"authorId\": \"2054436718\",\\n            \"name\": \"Gyudeok Hwang\"\\n          },\\n          {\\n            \"authorId\": \"2290973453\",\\n            \"name\": \"Jae Ho Jung\"\\n          },\\n          {\\n            \"authorId\": \"2003794778\",\\n            \"name\": \"Daniel Duck-Jin Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"Neovascular age-related macular degeneration (nAMD) can result in blindness if left untreated, and patients often require repeated anti-vascular endothelial growth factor injections. Although, the treat-and-extend method is becoming popular to reduce vision loss attributed to recurrence, it may pose a risk of overtreatment. This study aimed to develop a deep learning model based on DenseNet201 to predict nAMD recurrence within 3 months after confirming dry-up 1 month following three loading injections in treatment-na\\\\u00efve patients. A dataset of 1076 spectral domain optical coherence tomography (OCT) images from 269 patients diagnosed with nAMD was used. The performance of the model was compared with that of 6 ophthalmologists, using 100 randomly selected samples. The DenseNet201-based model achieved 53.0% accuracy in predicting nAMD recurrence using a single pre-injection image and 60.2% accuracy after viewing all the images immediately after the 1st, 2nd, and 3rd injections. The model outperformed experienced ophthalmologists, with an average accuracy of 52.17% using a single pre-injection image and 53.3% after examining four images before and after three loading injections. In conclusion, the artificial intelligence model demonstrated a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists. These findings suggest that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies.\"\\n      },\\n      {\\n        \"paperId\": \"b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\\n        \"title\": \"Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.03275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a hybrid representation of the reasoning process, where the initial reasoning steps are partially abstracted away using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2325888815\",\\n            \"name\": \"DiJia Su\"\\n          },\\n          {\\n            \"authorId\": \"2255310892\",\\n            \"name\": \"Hanlin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2269737738\",\\n            \"name\": \"Yingchen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          },\\n          {\\n            \"authorId\": \"2285362895\",\\n            \"name\": \"Yuandong Tian\"\\n          },\\n          {\\n            \"authorId\": \"2326106870\",\\n            \"name\": \"Qinqing Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\\n        \"title\": \"13.4 A 48GB 16-High 1280GB/s HBM3E DRAM with All-Around Power TSV and a 6-Phase RDQS Scheme for TSV Area Optimization\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISSCC49657.2024.10454440?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISSCC49657.2024.10454440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287764300\",\\n            \"name\": \"Jinhyung Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287959684\",\\n            \"name\": \"Kyungjun Cho\"\\n          },\\n          {\\n            \"authorId\": \"2109114249\",\\n            \"name\": \"C. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287956252\",\\n            \"name\": \"Yeonho Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287119443\",\\n            \"name\": \"Jae-Hyung Park\"\\n          },\\n          {\\n            \"authorId\": \"2286899300\",\\n            \"name\": \"Su-Hyun Oh\"\\n          },\\n          {\\n            \"authorId\": \"1640393542\",\\n            \"name\": \"Yucheon Ju\"\\n          },\\n          {\\n            \"authorId\": \"3365606\",\\n            \"name\": \"Chunseok Jeong\"\\n          },\\n          {\\n            \"authorId\": \"123947284\",\\n            \"name\": \"H. Cho\"\\n          },\\n          {\\n            \"authorId\": \"2198615241\",\\n            \"name\": \"Jaeseung Lee\"\\n          },\\n          {\\n            \"authorId\": \"3376046\",\\n            \"name\": \"T. Yun\"\\n          },\\n          {\\n            \"authorId\": \"2292215366\",\\n            \"name\": \"Jin Hee Cho\"\\n          },\\n          {\\n            \"authorId\": \"3375969\",\\n            \"name\": \"Sangmuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"1640397693\",\\n            \"name\": \"J. Moon\"\\n          },\\n          {\\n            \"authorId\": \"2110426052\",\\n            \"name\": \"Y. Park\"\\n          },\\n          {\\n            \"authorId\": \"2291083018\",\\n            \"name\": \"Hong-Seok Choi\"\\n          },\\n          {\\n            \"authorId\": \"2159532637\",\\n            \"name\": \"In-Keun Kim\"\\n          },\\n          {\\n            \"authorId\": \"2286904054\",\\n            \"name\": \"Seung Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"2286933095\",\\n            \"name\": \"Sun-Yeol Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291078685\",\\n            \"name\": \"Jaemin Jang\"\\n          },\\n          {\\n            \"authorId\": \"2292140319\",\\n            \"name\": \"Jinwook Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108644317\",\\n            \"name\": \"S. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2286906698\",\\n            \"name\": \"Younghyun Jeon\"\\n          },\\n          {\\n            \"authorId\": \"2292172289\",\\n            \"name\": \"Juhyung Park\"\\n          },\\n          {\\n            \"authorId\": \"2159571158\",\\n            \"name\": \"Tae-Kyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"82375019\",\\n            \"name\": \"D. Ka\"\\n          },\\n          {\\n            \"authorId\": \"2159321700\",\\n            \"name\": \"Sanghoon Oh\"\\n          },\\n          {\\n            \"authorId\": \"2292143044\",\\n            \"name\": \"Jinse Kim\"\\n          },\\n          {\\n            \"authorId\": \"2159244890\",\\n            \"name\": \"Junyeol Jeon\"\\n          },\\n          {\\n            \"authorId\": \"2292173125\",\\n            \"name\": \"Seonhong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291075551\",\\n            \"name\": \"Kyeong Tae Kim\"\\n          },\\n          {\\n            \"authorId\": \"2154855420\",\\n            \"name\": \"Tae-Hwan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291083403\",\\n            \"name\": \"Hyeonjin Yang\"\\n          },\\n          {\\n            \"authorId\": \"2291072681\",\\n            \"name\": \"Dongju Yang\"\\n          },\\n          {\\n            \"authorId\": \"2291084224\",\\n            \"name\": \"Minseop Lee\"\\n          },\\n          {\\n            \"authorId\": \"30684992\",\\n            \"name\": \"Heewoong Song\"\\n          },\\n          {\\n            \"authorId\": \"2291069292\",\\n            \"name\": \"Dongwook Jang\"\\n          },\\n          {\\n            \"authorId\": \"2287120235\",\\n            \"name\": \"Junghyun Shin\"\\n          },\\n          {\\n            \"authorId\": \"2287261607\",\\n            \"name\": \"Hyunsik Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291041862\",\\n            \"name\": \"Changki Baek\"\\n          },\\n          {\\n            \"authorId\": \"2291023817\",\\n            \"name\": \"Hajun Jeong\"\\n          },\\n          {\\n            \"authorId\": \"2291081668\",\\n            \"name\": \"Jongchan Yoon\"\\n          },\\n          {\\n            \"authorId\": \"1641325386\",\\n            \"name\": \"SeungGyeon Lim\"\\n          },\\n          {\\n            \"authorId\": \"2110630984\",\\n            \"name\": \"Kyo Yun Lee\"\\n          },\\n          {\\n            \"authorId\": \"2159499427\",\\n            \"name\": \"Young Jun Koo\"\\n          },\\n          {\\n            \"authorId\": \"2287107051\",\\n            \"name\": \"Myeong-Jae Park\"\\n          },\\n          {\\n            \"authorId\": \"2510417\",\\n            \"name\": \"Joohwan Cho\"\\n          },\\n          {\\n            \"authorId\": \"2291056079\",\\n            \"name\": \"Jonghwan Kim\"\\n          }\\n        ],\\n        \"abstract\": \"With the emergence of large-language models (LLM) and generative AI, which require an enormous amount of model parameters, the required memory bandwidth and capacity for high-end systems is on an unprecedented increase. To meet this need, we present an extended version of the high-bandwidth memory-3 (HBM3 DRAM), HBM3E, which achieves a 1280GB/s bandwidth with a cube density of 48GB. New design schemes and features, such as all-around power-through-silicon via (TSV), a 6-phase read-data-strobe (RDQS) scheme, a byte-mapping swap scheme, and a voltage-drift compensator for write data strobe (WDQS), are implemented to achieve extended bandwidth and capacity with enhanced reliability. The overall architecture and specifications, such as bump map footprint, the number of channel and I/Os, and the operation voltage, are identical to the latest HBM3 [1, 2]; therefore, backward compatibility is provided, avoiding system modification.\"\\n      },\\n      {\\n        \"paperId\": \"76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\\n        \"title\": \"NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper details the design and construction of NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35504092\",\\n            \"name\": \"Cunxiang Wang\"\\n          },\\n          {\\n            \"authorId\": \"30819687\",\\n            \"name\": \"Ruoxi Ning\"\\n          },\\n          {\\n            \"authorId\": \"2292184774\",\\n            \"name\": \"Boqi Pan\"\\n          },\\n          {\\n            \"authorId\": \"2292208424\",\\n            \"name\": \"Tonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"3187768\",\\n            \"name\": \"Qipeng Guo\"\\n          },\\n          {\\n            \"authorId\": \"2292147095\",\\n            \"name\": \"Cheng Deng\"\\n          },\\n          {\\n            \"authorId\": \"1993226927\",\\n            \"name\": \"Guangsheng Bao\"\\n          },\\n          {\\n            \"authorId\": \"2292261580\",\\n            \"name\": \"Qian Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261496744\",\\n            \"name\": \"Yue Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models\\' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.\"\\n      },\\n      {\\n        \"paperId\": \"dda8031682684655744c7001374e6cb88c9503bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dda8031682684655744c7001374e6cb88c9503bd\",\\n        \"title\": \"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.10902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Soda-Eval is introduced, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, and the performance of several open-access instruction-tuned LLMs is studied, finding that dialogue evaluation remains challenging.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2007581062\",\\n            \"name\": \"John Mendon\\\\u00e7a\"\\n          },\\n          {\\n            \"authorId\": \"2268558660\",\\n            \"name\": \"Isabel Trancoso\"\\n          },\\n          {\\n            \"authorId\": \"1784914\",\\n            \"name\": \"A. Lavie\"\\n          }\\n        ],\\n        \"abstract\": \"Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.\"\\n      },\\n      {\\n        \"paperId\": \"2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"title\": \"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\",\\n        \"citationCount\": 100,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.07542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155795167\",\\n            \"name\": \"Chengzu Li\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2339967968\",\\n            \"name\": \"Huanyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2273419590\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2294850817\",\\n            \"name\": \"Li Dong\"\\n          },\\n          {\\n            \"authorId\": \"2339667880\",\\n            \"name\": \"Ivan Vuli\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"78875987dc674fc556873df037cf114f04932e80\",\\n        \"url\": \"https://www.semanticscholar.org/paper/78875987dc674fc556873df037cf114f04932e80\",\\n        \"title\": \"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\",\\n        \"citationCount\": 83,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2401.07764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.07764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A split learning system for LLM agents in 6G networks is proposed, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1454018677\",\\n            \"name\": \"Minrui Xu\"\\n          },\\n          {\\n            \"authorId\": \"1713586\",\\n            \"name\": \"D. Niyato\"\\n          },\\n          {\\n            \"authorId\": \"2261731446\",\\n            \"name\": \"Jiawen Kang\"\\n          },\\n          {\\n            \"authorId\": \"2943819\",\\n            \"name\": \"Zehui Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2237802924\",\\n            \"name\": \"Shiwen Mao\"\\n          },\\n          {\\n            \"authorId\": \"2264568786\",\\n            \"name\": \"Zhu Han\"\\n          },\\n          {\\n            \"authorId\": \"2228302663\",\\n            \"name\": \"Dong In Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269994509\",\\n            \"name\": \"K. B. Letaief\"\\n          }\\n        ],\\n        \"abstract\": \"AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.\"\\n      },\\n      {\\n        \"paperId\": \"807ba70b6dc5ce8104268ef8e579d6ff67051230\",\\n        \"url\": \"https://www.semanticscholar.org/paper/807ba70b6dc5ce8104268ef8e579d6ff67051230\",\\n        \"title\": \"A Survey on Post-training of Large Language Models\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2187039492\",\\n            \"name\": \"Guiyao Tie\"\\n          },\\n          {\\n            \"authorId\": \"2349394246\",\\n            \"name\": \"Zeli Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2347232612\",\\n            \"name\": \"Dingjie Song\"\\n          },\\n          {\\n            \"authorId\": \"2349375946\",\\n            \"name\": \"Fuyang Wei\"\\n          },\\n          {\\n            \"authorId\": \"2349949677\",\\n            \"name\": \"Rong Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2349402229\",\\n            \"name\": \"Yurou Dai\"\\n          },\\n          {\\n            \"authorId\": \"2349357634\",\\n            \"name\": \"Wen Yin\"\\n          },\\n          {\\n            \"authorId\": \"121937496\",\\n            \"name\": \"Zhejian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2349498657\",\\n            \"name\": \"Jiangyue Yan\"\\n          },\\n          {\\n            \"authorId\": \"2342866931\",\\n            \"name\": \"Yao Su\"\\n          },\\n          {\\n            \"authorId\": \"2349400490\",\\n            \"name\": \"Zhenhan Dai\"\\n          },\\n          {\\n            \"authorId\": \"2324567791\",\\n            \"name\": \"Yifeng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2211165440\",\\n            \"name\": \"Yihan Cao\"\\n          },\\n          {\\n            \"authorId\": \"2301109277\",\\n            \"name\": \"Lichao Sun\"\\n          },\\n          {\\n            \"authorId\": \"2221116622\",\\n            \"name\": \"Pan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2254874151\",\\n            \"name\": \"Lifang He\"\\n          },\\n          {\\n            \"authorId\": \"2280102292\",\\n            \"name\": \"Hechang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2349483665\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2284983420\",\\n            \"name\": \"Qingsong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2348862207\",\\n            \"name\": \"Tianming Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249536787\",\\n            \"name\": \"Neil Zhenqiang Gong\"\\n          },\\n          {\\n            \"authorId\": \"2279062891\",\\n            \"name\": \"Jiliang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2265549448\",\\n            \"name\": \"Caiming Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2349551882\",\\n            \"name\": \"Heng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2293777434\",\\n            \"name\": \"Philip S. Yu\"\\n          },\\n          {\\n            \"authorId\": \"2288029761\",\\n            \"name\": \"Jianfeng Gao\"\\n          }\\n        ],\\n        \"abstract\": \"The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT\\'s alignment strategies to DeepSeek-R1\\'s innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.\"\\n      },\\n      {\\n        \"paperId\": \"ecb1002427e476ec76463e0a8b5a453471a1931f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ecb1002427e476ec76463e0a8b5a453471a1931f\",\\n        \"title\": \"The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks are illustrated, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283841922\",\\n            \"name\": \"Ayo Adedeji\"\\n          },\\n          {\\n            \"authorId\": \"2283935118\",\\n            \"name\": \"Sarita Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2283841541\",\\n            \"name\": \"Brendan Doohan\"\\n          }\\n        ],\\n        \"abstract\": \"In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\\n      },\\n      {\\n        \"paperId\": \"a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\\n        \"title\": \"Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.03151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A thorough and up-to-date comparison of reasoning techniques in both textual and multimodal LLMs is offered, highlighting practical methods for post-training optimization and test-time inference.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2066776633\",\\n            \"name\": \"Jing Bi\"\\n          },\\n          {\\n            \"authorId\": \"2153545235\",\\n            \"name\": \"Susan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2322454494\",\\n            \"name\": \"Xiaofei Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2279760245\",\\n            \"name\": \"Pinxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2331365304\",\\n            \"name\": \"Junjia Guo\"\\n          },\\n          {\\n            \"authorId\": \"2119309562\",\\n            \"name\": \"Yunlong Tang\"\\n          },\\n          {\\n            \"authorId\": \"2242154602\",\\n            \"name\": \"Luchuan Song\"\\n          },\\n          {\\n            \"authorId\": \"2161012966\",\\n            \"name\": \"Chao Huang\"\\n          },\\n          {\\n            \"authorId\": \"2350866278\",\\n            \"name\": \"Guangyu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2350999609\",\\n            \"name\": \"Jinxi He\"\\n          },\\n          {\\n            \"authorId\": \"2350428628\",\\n            \"name\": \"Jiarui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2354168235\",\\n            \"name\": \"Shu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2266412651\",\\n            \"name\": \"Daoan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2350433568\",\\n            \"name\": \"Chen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2152129821\",\\n            \"name\": \"L. Wen\"\\n          },\\n          {\\n            \"authorId\": \"2337241442\",\\n            \"name\": \"Zhang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2320811177\",\\n            \"name\": \"Jiebo Luo\"\\n          },\\n          {\\n            \"authorId\": \"2293314762\",\\n            \"name\": \"Chenliang Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.\"\\n      },\\n      {\\n        \"paperId\": \"5550118041a89121e9d7274f83aef420cd9ed487\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5550118041a89121e9d7274f83aef420cd9ed487\",\\n        \"title\": \"Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.11849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311451390\",\\n            \"name\": \"Yinghao Aaron Li\"\\n          },\\n          {\\n            \"authorId\": \"2243118841\",\\n            \"name\": \"Xilin Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2162961620\",\\n            \"name\": \"Jordan Darefsky\"\\n          },\\n          {\\n            \"authorId\": \"2316835793\",\\n            \"name\": \"Ge Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1686269\",\\n            \"name\": \"N. Mesgarani\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end speech-to-speech conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and speech styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the speech, which is then played back to the user. While the response speech is being played, the input speech undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input speech. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and speech-to-speech baselines in terms of both dialogue naturalness and coherence while being more than 50% faster.\"\\n      },\\n      {\\n        \"paperId\": \"afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\\n        \"title\": \"What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This survey summarizes existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual, and proposes a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-08-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2302816545\",\\n            \"name\": \"Dingyi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2317013378\",\\n            \"name\": \"Qin Jin\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories. Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations.\"\\n      },\\n      {\\n        \"paperId\": \"bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\\n        \"title\": \"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.19846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Quest is introduced, a query-centric data synthesis method aggregating semantically relevant yet diverse documents that achieves superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152644192\",\\n            \"name\": \"Chaochen Gao\"\\n          },\\n          {\\n            \"authorId\": \"2155226596\",\\n            \"name\": \"Xing Wu\"\\n          },\\n          {\\n            \"authorId\": \"2176771084\",\\n            \"name\": \"Qingfang Fu\"\\n          },\\n          {\\n            \"authorId\": \"2257376973\",\\n            \"name\": \"Songlin Hu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest\\'s superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\\n      },\\n      {\\n        \"paperId\": \"1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\\n        \"title\": \"When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.13476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AnchorAttention is developed, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training, and reduces training time by over 50% compared to standard full attention mechanisms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2267866973\",\\n            \"name\": \"Haonan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284062049\",\\n            \"name\": \"Qian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2325201427\",\\n            \"name\": \"Chao Du\"\\n          },\\n          {\\n            \"authorId\": \"2291015783\",\\n            \"name\": \"Tongyao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2325980280\",\\n            \"name\": \"Cunxiao Du\"\\n          },\\n          {\\n            \"authorId\": \"2256995496\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"19201674\",\\n            \"name\": \"Tianyu Pang\"\\n          }\\n        ],\\n        \"abstract\": \"Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16\\'s limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\\\\\\\% compared to standard full attention mechanisms, while preserving the original LLM\\'s capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.\"\\n      },\\n      {\\n        \"paperId\": \"38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\\n        \"title\": \"Ultrafast imaging of coherent polariton propagation and interactions\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2143435931\",\\n            \"name\": \"Ding Xu\"\\n          },\\n          {\\n            \"authorId\": \"13643895\",\\n            \"name\": \"Arkajit Mandal\"\\n          },\\n          {\\n            \"authorId\": \"2053177998\",\\n            \"name\": \"James M. Baxter\"\\n          },\\n          {\\n            \"authorId\": \"2004406278\",\\n            \"name\": \"Shangjun Cheng\"\\n          },\\n          {\\n            \"authorId\": \"49805255\",\\n            \"name\": \"I. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1849913502\",\\n            \"name\": \"Haowen (Vicky) Su\"\\n          },\\n          {\\n            \"authorId\": \"2144363449\",\\n            \"name\": \"Song Liu\"\\n          },\\n          {\\n            \"authorId\": \"6834462\",\\n            \"name\": \"D. Reichman\"\\n          },\\n          {\\n            \"authorId\": \"3895968\",\\n            \"name\": \"Milan Delor\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\\n        \"title\": \"Extrapolation-Based Video Retargeting With Backward Warping Using an Image-to-Warping Vector Generation Network\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LSP.2020.2977206?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LSP.2020.2977206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46445100\",\\n            \"name\": \"Sung In Cho\"\\n          },\\n          {\\n            \"authorId\": \"1739537\",\\n            \"name\": \"Suk-ju Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Video retargeting is a technique used to transform a given video to a target aspect ratio. Current methods often cause severe visual distortion due to frequent temporal incoherence during the retargeting. In this study, we propose a new extrapolation-based video retargeting method using an image-to-warping vector generation network to maintain temporal coherence and prevent deformation of an input frame by extending the side area of an input frame. Backward warping-based extrapolation is performed using a displacement vector (DV) that is generated by a proposed convolutional neural network (CNN). The DV is defined as the displacement between the current hole to be filled in the extended area and a pixel in the input frame used to fill the hole. We also propose a technique to efficiently train the CNN including a method for ground-truth DV generation. After the extrapolation, we propose a technique for the maintenance of temporal coherence of the extended region and a distortion suppression scheme (DSC) for minimizing visual artifacts. The simulation results demonstrated that the proposed method improved bidirectional similarity (BDS) up to 3.69, which is a measure of the quality of video retargeting, compared with existing video retargeting methods.\"\\n      },\\n      {\\n        \"paperId\": \"0df45f6ab09cc6ddcaf6829c131c777732a73731\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0df45f6ab09cc6ddcaf6829c131c777732a73731\",\\n        \"title\": \"Temporal Incoherence-Free Video Retargeting Using Foreground Aware Extrapolation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2020.2977171?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2020.2977171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46445100\",\\n            \"name\": \"Sung In Cho\"\\n          },\\n          {\\n            \"authorId\": \"1739537\",\\n            \"name\": \"Suk-ju Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Video retargeting is a method of adjusting the aspect ratio of a given video to the target aspect ratio. However, temporal incoherence of video contents, which can occur frequently by video retargeting, is the most dominant factor that degrades the quality of retargeted videos. Current methods to maintain temporal coherence use the entire frames of the input videos; however, these methods cannot be implemented as on-time systems because of their tremendous computational complexity. As far as we know, there is no existing on-time video retargeting method that can avoid spatial distortion while perfectly maintaining temporal coherence. In this paper, we propose a novel on-time video retargeting method that can perfectly maintain temporal coherence and prevent the spatial distortion by using only two consecutive input frames. In our method, the maximum a posteriori-based foreground aware-block matching is used for the extrapolation that extends the side area of a given video to adjust its aspect ratio to the target. To maintain the temporal coherence of the extended area, the result of block matching for backward warping-based extrapolation of the start frame after the scene change occurs, is reused for the other frames until the next scene change occurs. In addition, we propose a scene scenario-adaptive fallback scheme to prevent severe distortions that can occur with reusing block matching results or extrapolation-based side extension. The simulation results showed that the proposed method greatly improved the bidirectional similarity value, which can measure the quality of video retargeting, by up to 10.26 compared with the existing on-time video retargeting methods.\"\\n      },\\n      {\\n        \"paperId\": \"cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\\n        \"title\": \"Posterior-GRPO: Rewarding Reasoning Processes in Code Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2508.05170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a unified framework that can effectively incorporate the quality of the reasoning process during RL and introduces Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-08-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310403349\",\\n            \"name\": \"Lishui Fan\"\\n          },\\n          {\\n            \"authorId\": \"2375147757\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2125101083\",\\n            \"name\": \"Mouxiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2331676092\",\\n            \"name\": \"Zhongxin Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model\\'s internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.\"\\n      }\\n    ]\\n  },\\n  \"adaptive prompt generation\": {\\n    \"total\": 13544,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"title\": \"Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions, which is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which the authors call flocking.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2171964328\",\\n            \"name\": \"Harry Dong\"\\n          },\\n          {\\n            \"authorId\": \"2282555057\",\\n            \"name\": \"Beidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284063779\",\\n            \"name\": \"Yuejie Chi\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method\\'s simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model\\'s performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\\\\\times$ and 1.25$\\\\\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\"\\n      },\\n      {\\n        \"paperId\": \"bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"title\": \"Prompt Expansion for Adaptive Text-to-Image Generation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A human evaluation study shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2070966889\",\\n            \"name\": \"Siddhartha Datta\"\\n          },\\n          {\\n            \"authorId\": \"2276608298\",\\n            \"name\": \"Alexander Ku\"\\n          },\\n          {\\n            \"authorId\": \"2275054270\",\\n            \"name\": \"Deepak Ramachandran\"\\n          },\\n          {\\n            \"authorId\": \"2276610768\",\\n            \"name\": \"Peter Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\"\\n      },\\n      {\\n        \"paperId\": \"1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"title\": \"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\",\\n        \"citationCount\": 52,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.12761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights and incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13563486\",\\n            \"name\": \"Jaehong Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2164249715\",\\n            \"name\": \"Shoubin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2061083016\",\\n            \"name\": \"Vaidehi Patil\"\\n          },\\n          {\\n            \"authorId\": \"2267311471\",\\n            \"name\": \"Huaxiu Yao\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\"\\n      },\\n      {\\n        \"paperId\": \"a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"title\": \"Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.06401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2273557458\",\\n            \"name\": \"Hao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2273559489\",\\n            \"name\": \"Jun Li\"\\n          },\\n          {\\n            \"authorId\": \"2118764798\",\\n            \"name\": \"Yizhuang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2273589717\",\\n            \"name\": \"Jun Wan\"\\n          },\\n          {\\n            \"authorId\": \"2113457514\",\\n            \"name\": \"Zhen Lei\"\\n          },\\n          {\\n            \"authorId\": \"2274088311\",\\n            \"name\": \"Xiangyu Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\"\\n      },\\n      {\\n        \"paperId\": \"bc9f1025246efeed568650934b6e183415aca279\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bc9f1025246efeed568650934b6e183415aca279\",\\n        \"title\": \"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.03214\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation, outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on the authors\\' main metrics for each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221313264\",\\n            \"name\": \"Jonathan Pei\"\\n          },\\n          {\\n            \"authorId\": \"1410652795\",\\n            \"name\": \"Kevin Yang\"\\n          },\\n          {\\n            \"authorId\": \"38666915\",\\n            \"name\": \"D. Klein\"\\n          }\\n        ],\\n        \"abstract\": \"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\"\\n      },\\n      {\\n        \"paperId\": \"cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"title\": \"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\",\\n        \"citationCount\": 115,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds, and shows that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1443432623\",\\n            \"name\": \"Anselm Paulus\"\\n          },\\n          {\\n            \"authorId\": \"3461866\",\\n            \"name\": \"Arman Zharmagambetov\"\\n          },\\n          {\\n            \"authorId\": \"2298951327\",\\n            \"name\": \"Chuan Guo\"\\n          },\\n          {\\n            \"authorId\": \"2298758184\",\\n            \"name\": \"Brandon Amos\"\\n          },\\n          {\\n            \"authorId\": \"2253746559\",\\n            \"name\": \"Yuandong Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n      },\\n      {\\n        \"paperId\": \"1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"title\": \"WavLLM: Towards Robust and Adaptive Speech Large Language Model\",\\n        \"citationCount\": 104,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"WavLLM is introduced, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277450543\",\\n            \"name\": \"Shujie Hu\"\\n          },\\n          {\\n            \"authorId\": \"2135918679\",\\n            \"name\": \"Long Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2107983441\",\\n            \"name\": \"Shujie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107970655\",\\n            \"name\": \"Sanyuan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2294360053\",\\n            \"name\": \"Hongkun Hao\"\\n          },\\n          {\\n            \"authorId\": \"2258308585\",\\n            \"name\": \"Jing Pan\"\\n          },\\n          {\\n            \"authorId\": \"2274190703\",\\n            \"name\": \"Xunying Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280887661\",\\n            \"name\": \"Jinyu Li\"\\n          },\\n          {\\n            \"authorId\": \"9075412\",\\n            \"name\": \"S. Sivasankaran\"\\n          },\\n          {\\n            \"authorId\": \"2294832157\",\\n            \"name\": \"Linquan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277299355\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker\\'s identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\\\\\url{aka.ms/wavllm}.\"\\n      },\\n      {\\n        \"paperId\": \"5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"title\": \"OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.18866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniAvatar is introduced, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements, and introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293272991\",\\n            \"name\": \"Qijun Gan\"\\n          },\\n          {\\n            \"authorId\": \"2294510159\",\\n            \"name\": \"Ruizi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2314648986\",\\n            \"name\": \"Jianke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2370937157\",\\n            \"name\": \"Shaofei Xue\"\\n          },\\n          {\\n            \"authorId\": \"2370937932\",\\n            \"name\": \"Steven Hoi\"\\n          }\\n        ],\\n        \"abstract\": \"Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"title\": \"AdaPlanner: Adaptive Planning from Feedback with Language Models\",\\n        \"citationCount\": 175,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16653\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118180896\",\\n            \"name\": \"Haotian Sun\"\\n          },\\n          {\\n            \"authorId\": \"8103389\",\\n            \"name\": \"Yuchen Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2865034\",\\n            \"name\": \"Lingkai Kong\"\\n          },\\n          {\\n            \"authorId\": \"2218437288\",\\n            \"name\": \"Bo Dai\"\\n          },\\n          {\\n            \"authorId\": \"145657504\",\\n            \"name\": \"Chao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"title\": \"Adaptive Machine Translation with Large Language Models\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.13294\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.13294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9400076\",\\n            \"name\": \"Yasmin Moslem\"\\n          },\\n          {\\n            \"authorId\": \"1748844\",\\n            \"name\": \"Rejwanul Haque\"\\n          },\\n          {\\n            \"authorId\": \"144315616\",\\n            \"name\": \"Andy Way\"\\n          }\\n        ],\\n        \"abstract\": \"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\\n      },\\n      {\\n        \"paperId\": \"7665642af9e682e012bec045102a4d009421067c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7665642af9e682e012bec045102a4d009421067c\",\\n        \"title\": \"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\",\\n        \"citationCount\": 98,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.17061\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An efficient yet effective framework that generates high-quality 3D humans with fine-grained geometry and realistic appearance, based on the insight that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257708193\",\\n            \"name\": \"Xian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2260342453\",\\n            \"name\": \"Xiaohang Zhan\"\\n          },\\n          {\\n            \"authorId\": \"1397711601\",\\n            \"name\": \"Jiaxiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2260340529\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2247995148\",\\n            \"name\": \"Gang Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2258618427\",\\n            \"name\": \"Dahua Lin\"\\n          },\\n          {\\n            \"authorId\": \"2257370021\",\\n            \"name\": \"Xihui Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249080787\",\\n            \"name\": \"Ziwei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"title\": \"ChatUniTest: a ChatGPT-based automated unit test generation tool\",\\n        \"citationCount\": 87,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.04764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.04764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.04764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework that outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and generates assertions while utilizing mock objects and re\\\\ufb02ection to achieve test objectives.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1387638000\",\\n            \"name\": \"Zhuo-Qi Xie\"\\n          },\\n          {\\n            \"authorId\": \"2300176046\",\\n            \"name\": \"Yinghao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2064478633\",\\n            \"name\": \"Chen Zhi\"\\n          },\\n          {\\n            \"authorId\": \"145590434\",\\n            \"name\": \"Shuiguang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2116398505\",\\n            \"name\": \"Jianwei Yin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"836e3069a83f455f916114e7265e00187e511838\",\\n        \"url\": \"https://www.semanticscholar.org/paper/836e3069a83f455f916114e7265e00187e511838\",\\n        \"title\": \"Locally Differentially Private Document Generation Using Zero Shot Prompting\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.16111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1512255229\",\\n            \"name\": \"Saiteja Utpala\"\\n          },\\n          {\\n            \"authorId\": \"2261493078\",\\n            \"name\": \"Sara Hooker\"\\n          },\\n          {\\n            \"authorId\": \"2261697074\",\\n            \"name\": \"Pin Yu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\\\\\% reduction in author identification F1 score against static attackers and a 26\\\\\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\"\\n      },\\n      {\\n        \"paperId\": \"f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"title\": \"Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"153003087\",\\n            \"name\": \"Xiangteng He\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"143753918\",\\n            \"name\": \"Yuxin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\"\\n      },\\n      {\\n        \"paperId\": \"107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"title\": \"The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.09576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1847858\",\\n            \"name\": \"Subhankar Maity\"\\n          },\\n          {\\n            \"authorId\": \"2144085844\",\\n            \"name\": \"Aniket Deroy\"\\n          }\\n        ],\\n        \"abstract\": \"In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n      },\\n      {\\n        \"paperId\": \"b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"title\": \"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.00330?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.00330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"StyleCrafter is introduced, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image, and designs a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269171464\",\\n            \"name\": \"Gongye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257035878\",\\n            \"name\": \"Menghan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149052351\",\\n            \"name\": \"Haoxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2087273800\",\\n            \"name\": \"Jinbo Xing\"\\n          },\\n          {\\n            \"authorId\": \"2253795356\",\\n            \"name\": \"Xintao Wang\"\\n          },\\n          {\\n            \"authorId\": \"3001727\",\\n            \"name\": \"Yujiu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257019659\",\\n            \"name\": \"Ying Shan\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0f78249a5ff64441cc51e55bbe5b97e28f801240\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f78249a5ff64441cc51e55bbe5b97e28f801240\",\\n        \"title\": \"Prompt Tuning for Generative Multimodal Pretrained Models\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2208.02532\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2208.02532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work implements prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks, with a focus on generative multimodal pretrained models, instead of contrastive ones.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-08-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50841357\",\\n            \"name\": \"Han Yang\"\\n          },\\n          {\\n            \"authorId\": \"35996608\",\\n            \"name\": \"Junyang Lin\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"An Yang\"\\n          },\\n          {\\n            \"authorId\": \"2155302144\",\\n            \"name\": \"Peng Wang\"\\n          },\\n          {\\n            \"authorId\": \"144161025\",\\n            \"name\": \"Chang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"38385080\",\\n            \"name\": \"Hongxia Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at \\\\\\\\url{https://github.com/OFA-Sys/OFA}\"\\n      },\\n      {\\n        \"paperId\": \"192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"title\": \"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2312.01663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.01663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269468811\",\\n            \"name\": \"Runze He\"\\n          },\\n          {\\n            \"authorId\": \"2052151521\",\\n            \"name\": \"Shaofei Huang\"\\n          },\\n          {\\n            \"authorId\": \"2269461105\",\\n            \"name\": \"Xuecheng Nie\"\\n          },\\n          {\\n            \"authorId\": \"151475424\",\\n            \"name\": \"Tianrui Hui\"\\n          },\\n          {\\n            \"authorId\": \"1776665\",\\n            \"name\": \"Luoqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2108984\",\\n            \"name\": \"Jiao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2269685669\",\\n            \"name\": \"Jizhong Han\"\\n          },\\n          {\\n            \"authorId\": \"2269748083\",\\n            \"name\": \"Guanbin Li\"\\n          },\\n          {\\n            \"authorId\": \"2269687302\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\"\\n      },\\n      {\\n        \"paperId\": \"817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\\n        \"url\": \"https://www.semanticscholar.org/paper/817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\\n        \"title\": \"TaskCraft: Automated Generation of Agentic Tasks\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.10055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories, and empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2367198602\",\\n            \"name\": \"Dingfeng Shi\"\\n          },\\n          {\\n            \"authorId\": \"2366609463\",\\n            \"name\": \"Jingyi Cao\"\\n          },\\n          {\\n            \"authorId\": \"2368654631\",\\n            \"name\": \"Qianben Chen\"\\n          },\\n          {\\n            \"authorId\": \"2367090248\",\\n            \"name\": \"Weichen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2366569457\",\\n            \"name\": \"Weizhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2366571583\",\\n            \"name\": \"Hongxuan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2366522296\",\\n            \"name\": \"Fangchen Dong\"\\n          },\\n          {\\n            \"authorId\": \"2366567233\",\\n            \"name\": \"Tianrui Qin\"\\n          },\\n          {\\n            \"authorId\": \"2368705239\",\\n            \"name\": \"King Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2283080546\",\\n            \"name\": \"Minghao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2366695720\",\\n            \"name\": \"Jian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2366560952\",\\n            \"name\": \"Ge Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2182423032\",\\n            \"name\": \"Jiaheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2351712181\",\\n            \"name\": \"Changwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2366603044\",\\n            \"name\": \"Jun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2134457930\",\\n            \"name\": \"Y. Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2284803168\",\\n            \"name\": \"Wangchunshu Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce \\\\\\\\textsc{TaskCraft}, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.\"\\n      },\\n      {\\n        \"paperId\": \"f373c5569b45bf580b7502729a83761a791ee209\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f373c5569b45bf580b7502729a83761a791ee209\",\\n        \"title\": \"MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work is the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing, and Experimental results on four benchmarks suggest that the method outperforms the previous method on each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238430925\",\\n            \"name\": \"Haoyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2238207741\",\\n            \"name\": \"Tianyi Lu\"\\n          },\\n          {\\n            \"authorId\": \"2239092619\",\\n            \"name\": \"Jiaxi Gu\"\\n          },\\n          {\\n            \"authorId\": \"2238449354\",\\n            \"name\": \"Xing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2311476511\",\\n            \"name\": \"Qingping Zheng\"\\n          },\\n          {\\n            \"authorId\": \"3099139\",\\n            \"name\": \"Zuxuan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2258963974\",\\n            \"name\": \"Hang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238451522\",\\n            \"name\": \"Yu-Gang Jiang\"\\n          }\\n        ],\\n        \"abstract\": \"The diffusion model is widely leveraged for either video generation or video editing. As each field has its task-specific problems, it is difficult to merely develop a single diffusion for completing both tasks simultaneously. Video diffusion sorely relying on the text prompt can be adapted to unify the two tasks. However, it lacks a high capability of aligning heterogeneous modalities between text and image, leading to various misalignment problems. In this work, we are the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing. The proposed MagDiff introduces three types of alignments, including subject-driven alignment, adaptive prompts alignment, and high-fidelity alignment. Particularly, the subject-driven alignment is put forward to trade off the image and text prompts, serving as a unified foundation generative model for both tasks. The adaptive prompts alignment is introduced to emphasize different strengths of homogeneous and heterogeneous alignments by assigning different values of weights to the image and the text prompts. The high-fidelity alignment is developed to further enhance the fidelity of both video generation and editing by taking the subject image as an additional model input. Experimental results on four benchmarks suggest that our method outperforms the previous method on each task.\"\\n      },\\n      {\\n        \"paperId\": \"a17aee1261b56ee828f029b1caeef78033acea83\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a17aee1261b56ee828f029b1caeef78033acea83\",\\n        \"title\": \"Adaptive Ship Detection From Optical to SAR Images\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LGRS.2023.3317321?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LGRS.2023.3317321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243328855\",\\n            \"name\": \"Yuxuan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2221128858\",\\n            \"name\": \"Zhijie Rao\"\\n          },\\n          {\\n            \"authorId\": \"2232100687\",\\n            \"name\": \"Chuyang Lin\"\\n          },\\n          {\\n            \"authorId\": \"1950637\",\\n            \"name\": \"Yue Huang\"\\n          },\\n          {\\n            \"authorId\": \"2713947\",\\n            \"name\": \"Xinghao Ding\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in synthetic aperture radar (SAR) ship detection have witnessed remarkable success by using large-scale annotated datasets. However, the annotation of SAR images requires strong domain-specific expertise, significantly hindering the prompt adoption of modern object detectors in this regime. Compared to SAR data, optical data in geoscience are considerably easier to label. Motivated by this, we investigate a new and challenging problem\\\\u2014adaptive ship detection\\\\u2014with the goal of enhancing ship detection performance on SAR images by leveraging knowledge transferred from optical images. Considering the large distributional discrepancy between the source (optical) and target (SAR) domains, we present OmniAdapt, a novel framework that progressively narrows the distance between the two types of images at the pixel, feature, and classifier levels. Specifically, OmniAdapt consists of three main modules, target-like generation module (TLGM), multifeature alignment module (MFAM), and common specific decomposition module (CSDM). TLGM minimizes the visual disparity by infusing the target-domain style into the source domain. MFAM aligns local- and global-level feature representations in an adversarial manner. Finally, CSDM decomposes the classifier into two independent components, that is, the domain-common component and the domain-specific component, and promotes the recognition ability of the former via regularization learning. Experimental results demonstrate the effectiveness of the proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\\n        \"title\": \"Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.05256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate, is introduced, a reinforcement learning objective tailoring generation length to per-prompt solve rate that delivers higher accuracy on the hardest problems with higher cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274104658\",\\n            \"name\": \"Violet Xiang\"\\n          },\\n          {\\n            \"authorId\": \"2326300653\",\\n            \"name\": \"Chase Blagden\"\\n          },\\n          {\\n            \"authorId\": \"102801230\",\\n            \"name\": \"Rafael Rafailov\"\\n          },\\n          {\\n            \"authorId\": \"2283848553\",\\n            \"name\": \"nathan lile\"\\n          },\\n          {\\n            \"authorId\": \"2366009773\",\\n            \"name\": \"Sang Truong\"\\n          },\\n          {\\n            \"authorId\": \"2284774407\",\\n            \"name\": \"Chelsea Finn\"\\n          },\\n          {\\n            \"authorId\": \"2274104149\",\\n            \"name\": \"Nick Haber\"\\n          }\\n        ],\\n        \"abstract\": \"Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt\\'s online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\\\\\\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.\"\\n      },\\n      {\\n        \"paperId\": \"44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\\n        \"title\": \"OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.10825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2190109518\",\\n            \"name\": \"Dianbing Xi\"\\n          },\\n          {\\n            \"authorId\": \"2356794181\",\\n            \"name\": \"Jiepeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2337074199\",\\n            \"name\": \"Yuanzhi Liang\"\\n          },\\n          {\\n            \"authorId\": \"2336910859\",\\n            \"name\": \"Xi Qiu\"\\n          },\\n          {\\n            \"authorId\": \"3131188\",\\n            \"name\": \"Yuchi Huo\"\\n          },\\n          {\\n            \"authorId\": \"2325437281\",\\n            \"name\": \"Rui Wang\"\\n          },\\n          {\\n            \"authorId\": \"2336934367\",\\n            \"name\": \"Chi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2336880377\",\\n            \"name\": \"Xuelong Li\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff , aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. Our framework supports three key capabilities: (1) Text-conditioned video generation, where all modalities are jointly synthesized from a textual prompt; (2) Video understanding, where structural modalities are predicted from rgb inputs in a coherent manner; and (3) X-conditioned video generation, where video synthesis is guided by finegrained inputs such as depth, canny and segmentation. Extensive experiments demonstrate that OmniVDiff achieves state-of-the-art performance in video generation tasks and competitive results in video understanding. Its flexibility and scalability make it well-suited for downstream applications such as video-to-video translation, modality adaptation for visual tasks, and scene reconstruction.\"\\n      },\\n      {\\n        \"paperId\": \"42117d01d498eb9f8c21b788c3565bc6855d620b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/42117d01d498eb9f8c21b788c3565bc6855d620b\",\\n        \"title\": \"Learning to Transfer Prompts for Text Generation\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.01543\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.01543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper improves this technique and proposes a novel prompt-based method (PTG) for text generation in a transferable setting that learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2018027\",\\n            \"name\": \"Junyi Li\"\\n          },\\n          {\\n            \"authorId\": \"1997234792\",\\n            \"name\": \"Tianyi Tang\"\\n          },\\n          {\\n            \"authorId\": \"50204644\",\\n            \"name\": \"J. Nie\"\\n          },\\n          {\\n            \"authorId\": \"153693432\",\\n            \"name\": \"Ji-rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2542603\",\\n            \"name\": \"Wayne Xin Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.\"\\n      },\\n      {\\n        \"paperId\": \"7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\\n        \"title\": \"CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer\",\\n        \"citationCount\": 1175,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions, and develops an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2303231681\",\\n            \"name\": \"Zhuoyi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2238205354\",\\n            \"name\": \"Jiayan Teng\"\\n          },\\n          {\\n            \"authorId\": \"2163967642\",\\n            \"name\": \"Wendi Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2055623340\",\\n            \"name\": \"Ming Ding\"\\n          },\\n          {\\n            \"authorId\": \"2305795673\",\\n            \"name\": \"Shiyu Huang\"\\n          },\\n          {\\n            \"authorId\": \"2214082934\",\\n            \"name\": \"Jiazheng Xu\"\\n          },\\n          {\\n            \"authorId\": \"2315948290\",\\n            \"name\": \"Yuanming Yang\"\\n          },\\n          {\\n            \"authorId\": \"2105844599\",\\n            \"name\": \"Wenyi Hong\"\\n          },\\n          {\\n            \"authorId\": \"2268628279\",\\n            \"name\": \"Xiaohan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2307077651\",\\n            \"name\": \"Guanyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2307075814\",\\n            \"name\": \"Da Yin\"\\n          },\\n          {\\n            \"authorId\": \"2290625851\",\\n            \"name\": \"Xiaotao Gu\"\\n          },\\n          {\\n            \"authorId\": \"2316099643\",\\n            \"name\": \"Yuxuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265518149\",\\n            \"name\": \"Weihan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2306161782\",\\n            \"name\": \"Yean Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2315952736\",\\n            \"name\": \"Ting Liu\"\\n          },\\n          {\\n            \"authorId\": \"2288066971\",\\n            \"name\": \"Bin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2243402027\",\\n            \"name\": \"Yuxiao Dong\"\\n          },\\n          {\\n            \"authorId\": \"2238207092\",\\n            \"name\": \"Jie Tang\"\\n          }\\n        ],\\n        \"abstract\": \"We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.\"\\n      },\\n      {\\n        \"paperId\": \"f37a5c2bf4266d031533d5e029b74b00b48ef038\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f37a5c2bf4266d031533d5e029b74b00b48ef038\",\\n        \"title\": \"Latency-Aware Generative Semantic Communications With Pre-Trained Diffusion Models\",\\n        \"citationCount\": 62,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.17256\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A latency-aware semantic communications framework with pre-trained generative models that designs a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258983485\",\\n            \"name\": \"Li Qiao\"\\n          },\\n          {\\n            \"authorId\": \"3202702\",\\n            \"name\": \"Mahdi Boloursaz Mashhadi\"\\n          },\\n          {\\n            \"authorId\": \"2293693238\",\\n            \"name\": \"Zhen Gao\"\\n          },\\n          {\\n            \"authorId\": \"1690137\",\\n            \"name\": \"C. Foh\"\\n          },\\n          {\\n            \"authorId\": \"2293392561\",\\n            \"name\": \"Pei Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2279548875\",\\n            \"name\": \"Mehdi Bennis\"\\n          }\\n        ],\\n        \"abstract\": \"Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this letter, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.\"\\n      },\\n      {\\n        \"paperId\": \"759b95f7f90addc4c526cd92557e486ab143fbec\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b95f7f90addc4c526cd92557e486ab143fbec\",\\n        \"title\": \"Style Vectors for Steering Generative Large Language Models\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.01618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches, demonstrating the effectiveness of activation engineering using such style vectors.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2098959640\",\\n            \"name\": \"Kai Konen\"\\n          },\\n          {\\n            \"authorId\": \"2282467369\",\\n            \"name\": \"Sophie Jentzsch\"\\n          },\\n          {\\n            \"authorId\": \"2274662002\",\\n            \"name\": \"Diaoul\\\\u00e9 Diallo\"\\n          },\\n          {\\n            \"authorId\": \"2282467364\",\\n            \"name\": \"Peer Schutt\"\\n          },\\n          {\\n            \"authorId\": \"2282467405\",\\n            \"name\": \"Oliver Bensch\"\\n          },\\n          {\\n            \"authorId\": \"51185829\",\\n            \"name\": \"Roxanne El Baff\"\\n          },\\n          {\\n            \"authorId\": \"2282467346\",\\n            \"name\": \"Dominik Opitz\"\\n          },\\n          {\\n            \"authorId\": \"2282467403\",\\n            \"name\": \"Tobias Hecking\"\\n          }\\n        ],\\n        \"abstract\": \"This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.\"\\n      },\\n      {\\n        \"paperId\": \"621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\\n        \"title\": \"EdgeLLM: Fast On-Device LLM Inference With Speculative Decoding\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMC.2024.3513457?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMC.2024.3513457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238890503\",\\n            \"name\": \"Daliang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238575108\",\\n            \"name\": \"Wangsong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2306091156\",\\n            \"name\": \"Hao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239060901\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"2326531487\",\\n            \"name\": \"Ying Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2238910539\",\\n            \"name\": \"Shiyun Wei\"\\n          },\\n          {\\n            \"authorId\": \"2326083763\",\\n            \"name\": \"Mengwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2237080638\",\\n            \"name\": \"Xuanzhe Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device\\'s memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3\\\\u00d7 faster than existing engines.\"\\n      },\\n      {\\n        \"paperId\": \"411b16add23976ffcdf6422f932453f6ebcca119\",\\n        \"url\": \"https://www.semanticscholar.org/paper/411b16add23976ffcdf6422f932453f6ebcca119\",\\n        \"title\": \"EvoPrompting: Language Models for Code-Level Neural Architecture Search\",\\n        \"citationCount\": 118,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.14838\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.14838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13336152\",\\n            \"name\": \"Angelica Chen\"\\n          },\\n          {\\n            \"authorId\": \"35363891\",\\n            \"name\": \"David Dohan\"\\n          },\\n          {\\n            \"authorId\": \"48165870\",\\n            \"name\": \"David R. So\"\\n          }\\n        ],\\n        \"abstract\": \"Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\\n      },\\n      {\\n        \"paperId\": \"1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\\n        \"title\": \"Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.18950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Adaptive Guided Erasure (AGE) method is proposed, which selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2007884558\",\\n            \"name\": \"Anh-Vu Bui\"\\n          },\\n          {\\n            \"authorId\": \"2299801919\",\\n            \"name\": \"T. V\\\\u0169\"\\n          },\\n          {\\n            \"authorId\": \"67329496\",\\n            \"name\": \"Tung-Long Vuong\"\\n          },\\n          {\\n            \"authorId\": \"2249909946\",\\n            \"name\": \"Trung Le\"\\n          },\\n          {\\n            \"authorId\": \"2292198330\",\\n            \"name\": \"Paul Montague\"\\n          },\\n          {\\n            \"authorId\": \"2059248789\",\\n            \"name\": \"Tamas Abraham\"\\n          },\\n          {\\n            \"authorId\": \"2275034108\",\\n            \"name\": \"Junae Kim\"\\n          },\\n          {\\n            \"authorId\": \"1400659302\",\\n            \"name\": \"Dinh Q. Phung\"\\n          }\\n        ],\\n        \"abstract\": \"Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a neutral concept or just an empty text prompt. In this paper, we demonstrate that this fixed-target strategy is suboptimal, as it fails to account for the impact of erasing one concept on the others. To address this limitation, we model the concept space as a graph and empirically analyze the effects of erasing one concept on the remaining concepts. Our analysis uncovers intriguing geometric properties of the concept space, where the influence of erasing a concept is confined to a local region. Building on this insight, we propose the Adaptive Guided Erasure (AGE) method, which \\\\\\\\emph{dynamically} selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects. Experimental results show that AGE significantly outperforms state-of-the-art erasure methods on preserving unrelated concepts while maintaining effective erasure performance. Our code is published at {https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.\"\\n      },\\n      {\\n        \"paperId\": \"af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\\n        \"title\": \"StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.08503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A cross-modal Adaptive Instance Normalization mechanism is introduced for better integration of style and text features, enhancing alignment and a Style-based Classifier-Free Guidance approach is developed that enables selective control over stylistic elements, reducing irrelevant influences.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2334740500\",\\n            \"name\": \"Mingkun Lei\"\\n          },\\n          {\\n            \"authorId\": \"2334824597\",\\n            \"name\": \"Xue Song\"\\n          },\\n          {\\n            \"authorId\": \"2336265253\",\\n            \"name\": \"Beier Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2334818360\",\\n            \"name\": \"Hao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2334822819\",\\n            \"name\": \"Chi Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.\"\\n      },\\n      {\\n        \"paperId\": \"5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\\n        \"title\": \"TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.16425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information, and designs a Dynamic Map Scaling mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333781842\",\\n            \"name\": \"Linqing Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2105618628\",\\n            \"name\": \"Chen Gao\"\\n          },\\n          {\\n            \"authorId\": \"2264574237\",\\n            \"name\": \"Zihan Ding\"\\n          },\\n          {\\n            \"authorId\": \"2325825544\",\\n            \"name\": \"Yue Liao\"\\n          },\\n          {\\n            \"authorId\": \"2325537018\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM\\'s spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.\"\\n      },\\n      {\\n        \"paperId\": \"4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\\n        \"title\": \"A new rapid deflagration-to-detonation transition in a short smooth tube\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1063/5.0191500?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1063/5.0191500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"92832821\",\\n            \"name\": \"Wandong Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290316859\",\\n            \"name\": \"Caizhi Fan\"\\n          },\\n          {\\n            \"authorId\": \"1734241\",\\n            \"name\": \"R. Deiterding\"\\n          },\\n          {\\n            \"authorId\": \"2290237139\",\\n            \"name\": \"Xiaokang Li\"\\n          },\\n          {\\n            \"authorId\": \"36072040\",\\n            \"name\": \"Jianhan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290198596\",\\n            \"name\": \"Xiong Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Obtaining a rapid deflagration-to-detonation transition (DDT) within a short smooth tube is a challenging task. Here, an unconventional means of flame acceleration propagating upstream in subsonic and supersonic mixtures within a smooth tube was introduced to acquire a speedy DDT. The Navier\\\\u2013Stokes equations with an adaptive mesh refinement technique and a detailed hydrogen\\\\u2013air chemistry reaction mechanism of 11 species and 27 steps were utilized to resolve the entire DDT characteristics. The effect of the initial Mach number on flame acceleration and DDT mechanism was revealed comprehensively. The results demonstrated that a prompt oblique shock wave (SW) occurs when the flame propagates upstream along the boundary walls due to the boundary layer influence. An intense coupling between the SW and the leading flame front is enhanced by increasing the initial Mach number of the mixture. The speedy generation of the oblique SW is formed at the incipient stage, mainly produced by the boundary layer influence and the coalescences of the compression waves. Consequently, the run-up time to detonation is shortened accordingly through a fierce reflected SW due to the intense leading SW after it reflects from the confined wall. Furthermore, three kinds of DDT evolution are revealed from the obtained results: (1) localized ignition in the upper boundary wall after the reflected and transverse shock waves propagate in the upper wall regions; (2) autoignition is formed in the confined wall corner after the reflected SW; and (3) direct detonation transition occurs at the end wall behind a strongly reflected SW in the supersonic case.\"\\n      },\\n      {\\n        \"paperId\": \"815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\\n        \"title\": \"Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2405.14713\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Generative AI capabilities are introduced to assist educators in creating tutor interfaces that meet their needs while adhering to design principles and raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170697820\",\\n            \"name\": \"Tommaso Cal\\\\u00f2\"\\n          },\\n          {\\n            \"authorId\": \"2257237899\",\\n            \"name\": \"Christopher J. MacLellan\"\\n          }\\n        ],\\n        \"abstract\": \"Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\\n      },\\n      {\\n        \"paperId\": \"e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\\n        \"title\": \"Transformer-Based Variable-Rate Image Compression with Region-of-Interest Control\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.10807\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.10807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a transformer-based learned image compression system that is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality while confirming its superiority over the other competing methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2140736758\",\\n            \"name\": \"Chia-Hao Kao\"\\n          },\\n          {\\n            \"authorId\": \"1723619\",\\n            \"name\": \"Ying Weng\"\\n          },\\n          {\\n            \"authorId\": \"2116613919\",\\n            \"name\": \"Yi-Hsin Chen\"\\n          },\\n          {\\n            \"authorId\": \"37811787\",\\n            \"name\": \"Wei-Chen Chiu\"\\n          },\\n          {\\n            \"authorId\": \"123608804\",\\n            \"name\": \"Wenmin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"This paper proposes a transformer-based learned image compression system. It is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality. Inspired by prompt tuning, we introduce prompt generation networks to condition the transformer-based autoencoder of compression. Our prompt generation networks generate content-adaptive tokens according to the input image, an ROI mask, and a rate parameter. The separation of the ROI mask and the rate parameter allows an intuitive way to achieve variable-rate and ROI coding simultaneously. Extensive experiments validate the effectiveness of our proposed method and confirm its superiority over the other competing methods.\"\\n      },\\n      {\\n        \"paperId\": \"c7acf9250926100f531bcf46d63d7da06e73928e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c7acf9250926100f531bcf46d63d7da06e73928e\",\\n        \"title\": \"MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work identifies that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images, and advances the cross-modality relation learning between the prompt and the generated images to better align the prompt and image content.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238903147\",\\n            \"name\": \"Yupeng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"18119920\",\\n            \"name\": \"Daquan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2238887924\",\\n            \"name\": \"Zuo-Liang Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2238889090\",\\n            \"name\": \"Yaxing Wang\"\\n          },\\n          {\\n            \"authorId\": \"3298532\",\\n            \"name\": \"Qibin Hou\"\\n          },\\n          {\\n            \"authorId\": \"33221685\",\\n            \"name\": \"Jiashi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in diffusion models have showcased their impressive capacity to generate visually striking images. However, ensuring a close match between the generated image and the given prompt remains a persistent challenge. In this work, we identify that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images. To better align the prompt and image content, we advance the cross-attention with an adaptive mask, which is conditioned on the attention maps and the prompt embeddings, to dynamically adjust the contribution of each text token to the image features. This mechanism explicitly diminishes the ambiguity in the semantic information embedding of the text encoder, leading to a boost of text-to-image consistency in the synthesized images. Our method, termed MaskDiffusion, is training-free and hot-pluggable for popular pre-trained diffusion models. When applied to the latent diffusion models, our MaskDiffusion can largely enhance their capability to correctly generate objects and their attributes, with negligible computation overhead compared to the original diffusion models. Our project page is https://github.com/HVision-NKU/MaskDiffusion.\"\\n      },\\n      {\\n        \"paperId\": \"66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\\n        \"url\": \"https://www.semanticscholar.org/paper/66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\\n        \"title\": \"TEMPERA: Test-Time Prompting via Reinforcement Learning\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2211.11890\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.11890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work designs a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers, and achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-11-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1993655237\",\\n            \"name\": \"Tianjun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275277634\",\\n            \"name\": \"Xuezhi Wang\"\\n          },\\n          {\\n            \"authorId\": \"65855107\",\\n            \"name\": \"Denny Zhou\"\\n          },\\n          {\\n            \"authorId\": \"50319359\",\\n            \"name\": \"D. Schuurmans\"\\n          },\\n          {\\n            \"authorId\": \"49988044\",\\n            \"name\": \"Joseph E. Gonzalez\"\\n          }\\n        ],\\n        \"abstract\": \"Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.\"\\n      },\\n      {\\n        \"paperId\": \"88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\\n        \"title\": \"QA-CLIMS: Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.09883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2280039282\",\\n            \"name\": \"Songhe Deng\"\\n          },\\n          {\\n            \"authorId\": \"2279830536\",\\n            \"name\": \"Wei Zhuo\"\\n          },\\n          {\\n            \"authorId\": \"2220635949\",\\n            \"name\": \"Jinheng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2265520934\",\\n            \"name\": \"Linlin Shen\"\\n          }\\n        ],\\n        \"abstract\": \"Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model\\'s ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets.\"\\n      },\\n      {\\n        \"paperId\": \"d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\\n        \"title\": \"Immunosenescence, Inflammaging, and Lung Senescence in Asthma in the Elderly\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2218-273X/12/10/1456/pdf?version=1665574999\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9599177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Immunosenescence\\\\u2019s contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly is discussed and an overview of age-related features in the immune system and lung structure is presented.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47522049\",\\n            \"name\": \"T. Soma\"\\n          },\\n          {\\n            \"authorId\": \"46375440\",\\n            \"name\": \"M. Nagata\"\\n          }\\n        ],\\n        \"abstract\": \"Prevalence of asthma in older adults is growing along with increasing global life expectancy. Due to poor clinical consequences such as high mortality, advancement in understanding the pathophysiology of asthma in older patients has been sought to provide prompt treatment for them. Age-related alterations of functions in the immune system and lung parenchyma occur throughout life. Alterations with advancing age are promoted by various stimuli, including pathobionts, fungi, viruses, pollutants, and damage-associated molecular patterns derived from impaired cells, abandoned cell debris, and senescent cells. Age-related changes in the innate and adaptive immune response, termed immunosenescence, includes impairment of phagocytosis and antigen presentation, enhancement of proinflammatory mediator generation, and production of senescence-associated secretory phenotype. Immnunosenescence could promote inflammaging (chronic low-grade inflammation) and contribute to late-onset adult asthma and asthma in the elderly, along with age-related pulmonary disease, such as chronic obstructive pulmonary disease and pulmonary fibrosis, due to lung parenchyma senescence. Aged patients with asthma exhibit local and systemic type 2 and non-type 2 inflammation, associated with clinical manifestations. Here, we discuss immunosenescence\\\\u2019s contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly and present an overview of age-related features in the immune system and lung structure.\"\\n      },\\n      {\\n        \"paperId\": \"94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\\n        \"title\": \"DAFT-E: Feature-Based Multivariate and Multi-Step-Ahead Wind Power Forecasting\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tste.2021.3130949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tste.2021.3130949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1978743481\",\\n            \"name\": \"F. De Caro\"\\n          },\\n          {\\n            \"authorId\": \"2142416821\",\\n            \"name\": \"Jacopo De Stefani\"\\n          },\\n          {\\n            \"authorId\": \"33858225\",\\n            \"name\": \"A. Vaccaro\"\\n          },\\n          {\\n            \"authorId\": \"1772497\",\\n            \"name\": \"Gianluca Bontempi\"\\n          }\\n        ],\\n        \"abstract\": \"Wind energy is one of the most promising resources for the mitigation of greenhouse gas emissions that contribute to anthropogenic global warming. However, the large proliferation of wind power generators is causing several critical issues in power systems due to their variable power generated profiles. For this reason, a large number of learning techniques, e.g. integrating Vector Auto-Regressive and Neural Network-based models, were proposed in the literature for mitigating wind power uncertainty issues. Unfortunately, these methodologies show several limitations, e.g. the huge number of parameters and/or the heavy computational cost, which hinder their deployment in modern power system operation, where prompt and reliable wide-area wind power generation forecasts are requested for supporting time-critical decision making on several time horizons. To try addressing this issue, this paper proposes the Dynamic Adaptive Feature-based Temporal Ensemble (DAFT-E) forecasting approach, which relies on an extensive feature engineering, a fast feature selection step and an ensemble of computationally inexpensive models to reduce the computational complexity of the forecasting task, while still preserving predictive accuracy. The experimental results, which benchmark DAFT-E against multivariate (VAR and deep learning) alternatives on two real case studies, show that the proposed approach outperforms state-of-the-art and representation learning models according to several forecasting accuracy metrics.\"\\n      },\\n      {\\n        \"paperId\": \"f6023556221018f779a63a60874973195aea8352\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6023556221018f779a63a60874973195aea8352\",\\n        \"title\": \"Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.09857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel framework named Attention-aware Self-adaptive Prompt (ASP), which prevents overfitting on base task and does not require enormous data in few-shot incremental tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275745354\",\\n            \"name\": \"Chenxi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2254008335\",\\n            \"name\": \"Zhenyi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2249155683\",\\n            \"name\": \"Tianyi Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2262968852\",\\n            \"name\": \"Ruibo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2254326623\",\\n            \"name\": \"Yihan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2275765198\",\\n            \"name\": \"Junfeng Guo\"\\n          },\\n          {\\n            \"authorId\": \"2261394090\",\\n            \"name\": \"Heng Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensive experiments on three benchmark datasets validate that ASP consistently outperforms state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning new classes and mitigating forgetting.\"\\n      },\\n      {\\n        \"paperId\": \"c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"title\": \"Soft Prompt Generation for Domain Generalization\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG), which consists of a two-stage training phase and an inference phase, aiming to incorporate the generative model domain knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2290728057\",\\n            \"name\": \"Yuedi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\"\\n      },\\n      {\\n        \"paperId\": \"37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"title\": \"Adapting to Distribution Shift by Visual Domain Prompt Generation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.02797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction and outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35793956\",\\n            \"name\": \"Zhixiang Chi\"\\n          },\\n          {\\n            \"authorId\": \"2300096585\",\\n            \"name\": \"Li Gu\"\\n          },\\n          {\\n            \"authorId\": \"2300089295\",\\n            \"name\": \"Tao Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2277793919\",\\n            \"name\": \"Huan Liu\"\\n          },\\n          {\\n            \"authorId\": \"1787848\",\\n            \"name\": \"Yuanhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2277598061\",\\n            \"name\": \"Konstantinos N. Plataniotis\"\\n          },\\n          {\\n            \"authorId\": \"2277695392\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n      },\\n      {\\n        \"paperId\": \"fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\\n        \"title\": \"Query-Based Adversarial Prompt Generation\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.12329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work improves on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268494505\",\\n            \"name\": \"Jonathan Hayase\"\\n          },\\n          {\\n            \"authorId\": \"2284689404\",\\n            \"name\": \"Ema Borevkovic\"\\n          },\\n          {\\n            \"authorId\": \"2483738\",\\n            \"name\": \"Nicholas Carlini\"\\n          },\\n          {\\n            \"authorId\": \"2444919\",\\n            \"name\": \"Florian Tram\\\\u00e8r\"\\n          },\\n          {\\n            \"authorId\": \"3490923\",\\n            \"name\": \"Milad Nasr\"\\n          }\\n        ],\\n        \"abstract\": \"Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI\\'s safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.\"\\n      },\\n      {\\n        \"paperId\": \"91b6158978b248e9a0e65d0d588bc1054e72bc16\",\\n        \"url\": \"https://www.semanticscholar.org/paper/91b6158978b248e9a0e65d0d588bc1054e72bc16\",\\n        \"title\": \"MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.findings-emnlp.215.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP and proposes a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260750930\",\\n            \"name\": \"Yuyan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2260655625\",\\n            \"name\": \"Zhihao Wen\"\\n          },\\n          {\\n            \"authorId\": \"2260651904\",\\n            \"name\": \"Ge Fan\"\\n          },\\n          {\\n            \"authorId\": \"2273721608\",\\n            \"name\": \"Zhengyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2273816877\",\\n            \"name\": \"Wei Wu\"\\n          },\\n          {\\n            \"authorId\": \"2260908086\",\\n            \"name\": \"Dayiheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2243457917\",\\n            \"name\": \"Zhixu Li\"\\n          },\\n          {\\n            \"authorId\": \"2163832089\",\\n            \"name\": \"Bang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2265724350\",\\n            \"name\": \"Yanghua Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.\"\\n      },\\n      {\\n        \"paperId\": \"135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\\n        \"url\": \"https://www.semanticscholar.org/paper/135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\\n        \"title\": \"Federated Text-driven Prompt Generation for Vision-Language Models\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256983385\",\\n            \"name\": \"Chen Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2257324808\",\\n            \"name\": \"Xingyu Li\"\\n          },\\n          {\\n            \"authorId\": \"29359383\",\\n            \"name\": \"Chaithanya Kumar Mummadi\"\\n          },\\n          {\\n            \"authorId\": \"144487556\",\\n            \"name\": \"M. Ganesh\"\\n          },\\n          {\\n            \"authorId\": \"2257091754\",\\n            \"name\": \"Zhenzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2257130661\",\\n            \"name\": \"Lu Peng\"\\n          },\\n          {\\n            \"authorId\": \"2257132255\",\\n            \"name\": \"Wan-Yi Lin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"title\": \"EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2024.3482175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2024.3482175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145205189\",\\n            \"name\": \"Yinsong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266873357\",\\n            \"name\": \"Jiaqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2266387679\",\\n            \"name\": \"Aidong Men\"\\n          },\\n          {\\n            \"authorId\": \"2266583142\",\\n            \"name\": \"Qingchao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\"\\n      },\\n      {\\n        \"paperId\": \"1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"title\": \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\\n        \"citationCount\": 92,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new approach to code generation by LLMs is proposed, which is called AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72729733\",\\n            \"name\": \"T. Ridnik\"\\n          },\\n          {\\n            \"authorId\": \"2279758170\",\\n            \"name\": \"Dedy Kredo\"\\n          },\\n          {\\n            \"authorId\": \"49668367\",\\n            \"name\": \"Itamar Friedman\"\\n          }\\n        ],\\n        \"abstract\": \"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\"\\n      },\\n      {\\n        \"paperId\": \"211e915b2e1e0753ddd581f10362fc82f28cc606\",\\n        \"url\": \"https://www.semanticscholar.org/paper/211e915b2e1e0753ddd581f10362fc82f28cc606\",\\n        \"title\": \"DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DiTCtrl is proposed, a training-free multi-prompt video generation method under MM-DiT architectures for the first time that achieves state-of-the-art performance without additional training and presents MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2337083029\",\\n            \"name\": \"Minghong Cai\"\\n          },\\n          {\\n            \"authorId\": \"30176430\",\\n            \"name\": \"Xiaodong Cun\"\\n          },\\n          {\\n            \"authorId\": \"2257035102\",\\n            \"name\": \"Xiaoyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2308556703\",\\n            \"name\": \"Wenze Liu\"\\n          },\\n          {\\n            \"authorId\": \"2303078452\",\\n            \"name\": \"Zhaoyang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2268490605\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2316484241\",\\n            \"name\": \"Xiangyu Yue\"\\n          }\\n        ],\\n        \"abstract\": \"Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer (MM-DiT) architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT\\\\u2019s attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training. Code is available at https://github.com/TencentARC/DiTCtrl.\"\\n      },\\n      {\\n        \"paperId\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04095\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces the Prompt Auto-Editing (PAE) method, which employs an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148661301\",\\n            \"name\": \"Wenyi Mo\"\\n          },\\n          {\\n            \"authorId\": \"2146332319\",\\n            \"name\": \"Tianyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2281418241\",\\n            \"name\": \"Yalong Bai\"\\n          },\\n          {\\n            \"authorId\": \"2295513824\",\\n            \"name\": \"Bing Su\"\\n          },\\n          {\\n            \"authorId\": \"2293310016\",\\n            \"name\": \"Ji-Rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2281323801\",\\n            \"name\": \"Qing Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\"\\n      }\\n    ]\\n  },\\n  \"iterative context update\": {\\n    \"total\": 9219,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 288,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 211,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 40,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\', \\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"fca9a4508863025d95a581ead47032d497825053\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fca9a4508863025d95a581ead47032d497825053\",\\n        \"title\": \"VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.02186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"VideoICL is a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach, improving OOD video understanding performance by extending effective context length without incurring high costs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333526847\",\\n            \"name\": \"Kangsan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2307075970\",\\n            \"name\": \"Geon Park\"\\n          },\\n          {\\n            \"authorId\": \"3445691\",\\n            \"name\": \"Youngwan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2119578055\",\\n            \"name\": \"Woongyeong Yeo\"\\n          },\\n          {\\n            \"authorId\": \"2265627157\",\\n            \"name\": \"Sung Ju Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in video large multimodal models (LMMs) have significantly improved their video understanding and reasoning capabilities. However, their performance drops on out-of-distribution (OOD) tasks that are underrepresented in training data. Traditional methods like fine-tuning on OOD datasets are impractical due to high computational costs. While In-context learning (ICL) with demonstration examples has shown promising generalization performance in language tasks and image-language tasks without fine-tuning, applying ICL to video-language tasks faces challenges due to the limited context length in Video LMMs, as videos require longer token lengths. To address these issues, we propose VideoICL, a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach. This allows to select the most relevant examples and rank them based on similarity, to be used for inference. If the generated response has low confidence, our framework selects new examples and performs inference again, iteratively refining the results until a high-confidence response is obtained. This approach improves OOD video understanding performance by extending effective context length without incurring high costs. The experimental results on multiple benchmarks demonstrate significant performance gains, especially in domain-specific scenarios, laying the groundwork for broader video comprehension applications. Code is released at https://github.com/KangsanKim07/VideoICL\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 72,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\\n        \"title\": \"Single image deraining using scale constraint iterative update network\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2023.121339?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2023.121339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1390863714\",\\n            \"name\": \"Yitong Yang\"\\n          },\\n          {\\n            \"authorId\": \"1591131546\",\\n            \"name\": \"Yongjun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"150356192\",\\n            \"name\": \"Zhongwei Cui\"\\n          },\\n          {\\n            \"authorId\": \"2112674491\",\\n            \"name\": \"Haoliang Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2210993430\",\\n            \"name\": \"Ting Ouyang\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 188,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 366,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 330,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 48,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2891,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      }\\n    ]\\n  },\\n  \"thematic consistency LLM\": {\\n    \"total\": 887,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\\n        \"title\": \"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.02896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This experimental study seeks to lay the groundwork for the understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction?\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282535211\",\\n            \"name\": \"Ivar Frisch\"\\n          },\\n          {\\n            \"authorId\": \"24068173\",\\n            \"name\": \"Mario Giulianelli\"\\n          }\\n        ],\\n        \"abstract\": \"Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model (LLM) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction? We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.\"\\n      },\\n      {\\n        \"paperId\": \"477a16bdbb43589e5feac3881b3370e3a4ab5624\",\\n        \"url\": \"https://www.semanticscholar.org/paper/477a16bdbb43589e5feac3881b3370e3a4ab5624\",\\n        \"title\": \"Evaluating the Consistency of LLM Evaluators\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.00543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Examining the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models demonstrates that strong proprietary models are not necessarily consistent evaluators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2291076200\",\\n            \"name\": \"Noah Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290955335\",\\n            \"name\": \"Jiwoo Hong\"\\n          },\\n          {\\n            \"authorId\": \"2290905396\",\\n            \"name\": \"James Thorne\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have shown potential as general evaluators along with the evident benefits of speed and cost. While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators. In this paper, we conduct extensive studies on the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models. Our comprehensive analysis demonstrates that strong proprietary models are not necessarily consistent evaluators, highlighting the importance of considering consistency in assessing the capability of LLM evaluators.\"\\n      },\\n      {\\n        \"paperId\": \"75062b58398b6e9409e5fec855f6912534331eaf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/75062b58398b6e9409e5fec855f6912534331eaf\",\\n        \"title\": \"Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales, facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-08-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": \"Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths,but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority voting, enabling more informed choices on when to halt sampling and which rationale to select. Our comprehensive experiments across diverse question-answering datasets demonstrate that RASC outperforms existing methods, reducing sample usage by approximately 70% while maintaining accuracy. Moreover, RASC facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs. Our approach effectively addresses the efficiency-accuracy trade-off in LLM reasoning tasks, offering a new perspective for more nuanced, faithful, and effective utilization of LLMs in resource-constrained environments.\"\\n      },\\n      {\\n        \"paperId\": \"3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\\n        \"title\": \"Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1186/s12911-024-02656-3\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11382443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare is emphasized, aiming to improve clinical outcomes ethically and effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-09-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"145770274\",\\n            \"name\": \"Tala Mirzaei\"\\n          },\\n          {\\n            \"authorId\": \"2320339088\",\\n            \"name\": \"Leila Amini\"\\n          },\\n          {\\n            \"authorId\": \"2574575\",\\n            \"name\": \"Pouyan Esmaeilzadeh\"\\n          }\\n        ],\\n        \"abstract\": \"This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions. We analyzed 3049 posts and comments extracted from a self-identified clinician subreddit using unsupervised machine learning via Latent Dirichlet Allocation and a structured qualitative analysis methodology. Analysis uncovered 14 salient themes of ethical implications, which we further consolidated into 4 overarching domains reflecting ethical issues around various clinical applications of LLM in healthcare, LLM coding, algorithm, and data governance, LLM\\\\u2019s role in health equity and the distribution of public health services, and the relationship between users (human) and LLM systems (machine). Mapping themes to ethical frameworks in literature illustrated multifaceted issues covering transparent LLM decisions, fairness, privacy, access disparities, user experiences, and reliability. This study emphasizes the need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare, aiming to improve clinical outcomes ethically and effectively.\"\\n      },\\n      {\\n        \"paperId\": \"6b7c5fc0f6b401962153f68f8250951f75da929e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b7c5fc0f6b401962153f68f8250951f75da929e\",\\n        \"title\": \"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.06503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper analyzes two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and finds that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-04-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295732707\",\\n            \"name\": \"Nathan Brake\"\\n          },\\n          {\\n            \"authorId\": \"2295732451\",\\n            \"name\": \"Thomas Schaaf\"\\n          }\\n        ],\\n        \"abstract\": \"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\\n      },\\n      {\\n        \"paperId\": \"e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"title\": \"LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\",\\n        \"citationCount\": 110,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL), which yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3400291\",\\n            \"name\": \"Shih-Chieh Dai\"\\n          },\\n          {\\n            \"authorId\": \"2261362789\",\\n            \"name\": \"Aiping Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1746959\",\\n            \"name\": \"Lun-Wei Ku\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n      },\\n      {\\n        \"paperId\": \"32426b96ff3c680125bde3b835bfa931288b8ade\",\\n        \"url\": \"https://www.semanticscholar.org/paper/32426b96ff3c680125bde3b835bfa931288b8ade\",\\n        \"title\": \"Better Patching Using LLM Prompting, via Self-Consistency\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2306.00108\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.00108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an application of the $\\\\\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots, on the MODIT dataset.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language models (LLMs) can be induced to solve non-trivial problems with \\\\u201cfew-shot\\\\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \\\\u201cchain of thought\\\\u201d ($\\\\\\\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \\\\u201cexplained\\\\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\\\\\\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\\\\\\\mathcal{S}-C$ (or even $\\\\\\\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\\\\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.\"\\n      },\\n      {\\n        \"paperId\": \"e24424283c02fbe7f641e5b3490d7bb059f8355a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e24424283c02fbe7f641e5b3490d7bb059f8355a\",\\n        \"title\": \"A Survey on LLM-as-a-Judge\",\\n        \"citationCount\": 776,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.15594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built?\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-11-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216587705\",\\n            \"name\": \"Jiawei Gu\"\\n          },\\n          {\\n            \"authorId\": \"144267788\",\\n            \"name\": \"Xuhui Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2287881684\",\\n            \"name\": \"Zhichao Shi\"\\n          },\\n          {\\n            \"authorId\": \"2274159320\",\\n            \"name\": \"Hexiang Tan\"\\n          },\\n          {\\n            \"authorId\": \"2332093190\",\\n            \"name\": \"Xuehao Zhai\"\\n          },\\n          {\\n            \"authorId\": \"2250617116\",\\n            \"name\": \"Chengjin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2330714501\",\\n            \"name\": \"Wei Li\"\\n          },\\n          {\\n            \"authorId\": \"1944248313\",\\n            \"name\": \"Yinghan Shen\"\\n          },\\n          {\\n            \"authorId\": \"2311556497\",\\n            \"name\": \"Shengjie Ma\"\\n          },\\n          {\\n            \"authorId\": \"2332306096\",\\n            \"name\": \"Honghao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257058703\",\\n            \"name\": \"Yuanzhuo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284217200\",\\n            \"name\": \"Jian Guo\"\\n          }\\n        ],\\n        \"abstract\": \"Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of\\\\\"LLM-as-a-Judge,\\\\\"where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.\"\\n      },\\n      {\\n        \"paperId\": \"0bf3a1867f7245b8a702093901c66b08b518eafc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0bf3a1867f7245b8a702093901c66b08b518eafc\",\\n        \"title\": \"Evaluating Very Long-Term Conversational Memory of LLM Agents\",\\n        \"citationCount\": 166,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A machine-human pipeline is introduced to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs, and presents a comprehensive evaluation benchmark to measure long-term memory in models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8785371\",\\n            \"name\": \"Adyasha Maharana\"\\n          },\\n          {\\n            \"authorId\": \"2266803131\",\\n            \"name\": \"Dong-Ho Lee\"\\n          },\\n          {\\n            \"authorId\": \"145582202\",\\n            \"name\": \"S. Tulyakov\"\\n          },\\n          {\\n            \"authorId\": \"2285969697\",\\n            \"name\": \"Mohit Bansal\"\\n          },\\n          {\\n            \"authorId\": \"2266751000\",\\n            \"name\": \"Francesco Barbieri\"\\n          },\\n          {\\n            \"authorId\": \"2267220081\",\\n            \"name\": \"Yuwei Fang\"\\n          }\\n        ],\\n        \"abstract\": \"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.\"\\n      },\\n      {\\n        \"paperId\": \"7c04ab297b59d4fe29285f339350882a3120b27f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7c04ab297b59d4fe29285f339350882a3120b27f\",\\n        \"title\": \"CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs\",\\n        \"citationCount\": 205,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642773\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.11314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions, is developed, revealing four design considerations for future educational AI assistants.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3136345\",\\n            \"name\": \"Majeed Kazemitabaar\"\\n          },\\n          {\\n            \"authorId\": \"2184253123\",\\n            \"name\": \"Runlong Ye\"\\n          },\\n          {\\n            \"authorId\": \"2280281736\",\\n            \"name\": \"Xiaoning Wang\"\\n          },\\n          {\\n            \"authorId\": \"2280145055\",\\n            \"name\": \"Austin Z Henley\"\\n          },\\n          {\\n            \"authorId\": \"2243041721\",\\n            \"name\": \"Paul Denny\"\\n          },\\n          {\\n            \"authorId\": \"2280145218\",\\n            \"name\": \"Michelle Craig\"\\n          },\\n          {\\n            \"authorId\": \"2280146888\",\\n            \"name\": \"Tovi Grossman\"\\n          }\\n        ],\\n        \"abstract\": \"Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student\\\\u2019s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI\\\\u2019s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.\"\\n      },\\n      {\\n        \"paperId\": \"da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\\n        \"url\": \"https://www.semanticscholar.org/paper/da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\\n        \"title\": \"Exploring and Evaluating Hallucinations in LLM-Powered Code Generation\",\\n        \"citationCount\": 221,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation, and proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290969862\",\\n            \"name\": \"Fang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2294504414\",\\n            \"name\": \"Yang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2295165194\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2294528116\",\\n            \"name\": \"Houkun Huang\"\\n          },\\n          {\\n            \"authorId\": \"2294510508\",\\n            \"name\": \"Ruifeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2294664033\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290433096\",\\n            \"name\": \"Li Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users\\' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.\"\\n      },\\n      {\\n        \"paperId\": \"a06d3e9e90008c64c45a0029d580541d5f646771\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771\",\\n        \"title\": \"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.00812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An overview of the various benefits of integrating code into LLMs\\' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277527247\",\\n            \"name\": \"Ke Yang\"\\n          },\\n          {\\n            \"authorId\": \"33456794\",\\n            \"name\": \"Jiateng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277421308\",\\n            \"name\": \"John Wu\"\\n          },\\n          {\\n            \"authorId\": \"2277597831\",\\n            \"name\": \"Chaoqi Yang\"\\n          },\\n          {\\n            \"authorId\": \"51135899\",\\n            \"name\": \"Y. Fung\"\\n          },\\n          {\\n            \"authorId\": \"2262396117\",\\n            \"name\": \"Sha Li\"\\n          },\\n          {\\n            \"authorId\": \"2277416897\",\\n            \"name\": \"Zixuan Huang\"\\n          },\\n          {\\n            \"authorId\": \"2344961610\",\\n            \"name\": \"Xu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2144803999\",\\n            \"name\": \"Xingyao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2277247982\",\\n            \"name\": \"Yiquan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2277409745\",\\n            \"name\": \"Heng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2261082008\",\\n            \"name\": \"ChengXiang Zhai\"\\n          }\\n        ],\\n        \"abstract\": \"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs\\' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.\"\\n      },\\n      {\\n        \"paperId\": \"2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\\n        \"title\": \"PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.12532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The number of crucial keys and values that influence future generations decreases layer by layer and can be extracted by the consistency in attention weights, leading to PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2182432556\",\\n            \"name\": \"Dongjie Yang\"\\n          },\\n          {\\n            \"authorId\": \"2302683712\",\\n            \"name\": \"Xiaodong Han\"\\n          },\\n          {\\n            \"authorId\": \"2302558089\",\\n            \"name\": \"Yan Gao\"\\n          },\\n          {\\n            \"authorId\": \"2302556666\",\\n            \"name\": \"Yao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2302704855\",\\n            \"name\": \"Shilin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2302545224\",\\n            \"name\": \"Hai Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.\"\\n      },\\n      {\\n        \"paperId\": \"36b9dac525fc93100b18d8e489bd97460cd49a5e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/36b9dac525fc93100b18d8e489bd97460cd49a5e\",\\n        \"title\": \"Thematic-LM: A LLM-based Multi-agent System for Large-scale Thematic Analysis\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3696410.3714595?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3696410.3714595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study applies Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-04-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2343740149\",\\n            \"name\": \"Tingrui Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2343749049\",\\n            \"name\": \"Caroline Walker\"\\n          },\\n          {\\n            \"authorId\": \"2343746435\",\\n            \"name\": \"Chris Cunningham\"\\n          },\\n          {\\n            \"authorId\": \"2310725786\",\\n            \"name\": \"Yun Sing Koh\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) is a widely used qualitative method for identifying underlying meanings within unstructured text. However, TA requires manual processes, which become increasingly labour-intensive and time-consuming as datasets grow. While large language models (LLMs) have been introduced to assist with TA on small-scale datasets, three key limitations hinder their effectiveness. First, current approaches often depend on interactions between an LLM agent and a human coder, a process that becomes challenging with larger datasets. Second, with feedback from the human coder, the LLM tends to mirror the human coder, which provides a narrower viewpoint of the data. Third, existing methods follow a sequential process, where codes are generated for individual samples without recalling previous codes and associated data, reducing the ability to analyse data holistically. To address these limitations, we propose Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis. Thematic-LM assigns specialised tasks to each agent, such as coding, aggregating codes, and maintaining and updating the codebook. We assign coder agents different identity perspectives to simulate the subjective nature of TA, fostering a more diverse interpretation of the data. We applied Thematic-LM to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change. We evaluate the resulting themes based on trustworthiness principles in qualitative research. Our study reveals insights such as assigning different identities to coder agents promotes divergence in codes and themes.\"\\n      },\\n      {\\n        \"paperId\": \"54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\\n        \"title\": \"Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation\",\\n        \"citationCount\": 88,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2285255408\",\\n            \"name\": \"Jiawei Wang\"\\n          },\\n          {\\n            \"authorId\": \"31279896\",\\n            \"name\": \"Renhe Jiang\"\\n          },\\n          {\\n            \"authorId\": \"46962297\",\\n            \"name\": \"Chuang Yang\"\\n          },\\n          {\\n            \"authorId\": \"2157765133\",\\n            \"name\": \"Zengqing Wu\"\\n          },\\n          {\\n            \"authorId\": \"2266396584\",\\n            \"name\": \"Makoto Onizuka\"\\n          },\\n          {\\n            \"authorId\": \"2239490643\",\\n            \"name\": \"Ryosuke Shibasaki\"\\n          },\\n          {\\n            \"authorId\": \"2284717877\",\\n            \"name\": \"Chuan Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\\n      },\\n      {\\n        \"paperId\": \"7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\\n        \"title\": \"ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification\",\\n        \"citationCount\": 75,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3660810?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3660810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions, and can effectively facilitate the practical application of LLMs in real-world development environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1994579604\",\\n            \"name\": \"Fangwen Mu\"\\n          },\\n          {\\n            \"authorId\": \"2305416699\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2259571951\",\\n            \"name\": \"Song Wang\"\\n          },\\n          {\\n            \"authorId\": \"2259613131\",\\n            \"name\": \"Zhuohao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2259874187\",\\n            \"name\": \"Binquan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2259824656\",\\n            \"name\": \"ChenXue Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260294248\",\\n            \"name\": \"Shichao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2157214565\",\\n            \"name\": \"Qing Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.\"\\n      },\\n      {\\n        \"paperId\": \"630c47372317164fc367153f938903e1d5b76059\",\\n        \"url\": \"https://www.semanticscholar.org/paper/630c47372317164fc367153f938903e1d5b76059\",\\n        \"title\": \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52202/079017-1077?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52202/079017-1077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1602820179\",\\n            \"name\": \"Gaurang Sriramanan\"\\n          },\\n          {\\n            \"authorId\": \"2344249306\",\\n            \"name\": \"Siddhant Bharti\"\\n          },\\n          {\\n            \"authorId\": \"150333898\",\\n            \"name\": \"Vinu Sankar Sadasivan\"\\n          },\\n          {\\n            \"authorId\": \"152623528\",\\n            \"name\": \"Shoumik Saha\"\\n          },\\n          {\\n            \"authorId\": \"2305809801\",\\n            \"name\": \"Priyatham Kattakinda\"\\n          },\\n          {\\n            \"authorId\": \"34389431\",\\n            \"name\": \"S. Feizi\"\\n          }\\n        ],\\n        \"abstract\": \"While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations\\\\u2014 outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.\"\\n      },\\n      {\\n        \"paperId\": \"038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\\n        \"title\": \"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt), which eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1966961\",\\n            \"name\": \"Yanshen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2239274607\",\\n            \"name\": \"Jianfeng He\"\\n          },\\n          {\\n            \"authorId\": \"2293779433\",\\n            \"name\": \"Limeng Cui\"\\n          },\\n          {\\n            \"authorId\": \"3433489\",\\n            \"name\": \"Shuo Lei\"\\n          },\\n          {\\n            \"authorId\": \"2249846863\",\\n            \"name\": \"Chang-Tien Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.\"\\n      },\\n      {\\n        \"paperId\": \"f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\\n        \"title\": \"Reasoning Runtime Behavior of a Program with LLM: How Far are We?\",\\n        \"citationCount\": 49,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a framework, namely $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution, and utilizes existing code benchmarks and adapt them to new benchmarks within this framework.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293350098\",\\n            \"name\": \"Junkai Chen\"\\n          },\\n          {\\n            \"authorId\": \"2276184077\",\\n            \"name\": \"Zhiyuan Pan\"\\n          },\\n          {\\n            \"authorId\": \"2110049191\",\\n            \"name\": \"Xing Hu\"\\n          },\\n          {\\n            \"authorId\": \"2293350478\",\\n            \"name\": \"Zhenhao Li\"\\n          },\\n          {\\n            \"authorId\": \"2286413567\",\\n            \"name\": \"Ge Li\"\\n          },\\n          {\\n            \"authorId\": \"2265241871\",\\n            \"name\": \"Xin Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs (i.e., predicting code execution behaviors such as program output and execution path), but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs. Our code, data and $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$ leaderboard are available at https://r-eval.github.io.\"\\n      },\\n      {\\n        \"paperId\": \"8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\\n        \"title\": \"Don\\'t Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper leverages the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics, they can be prompted to autoformalize informal mathematical statements into formal Isabelle code -- which provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287813125\",\\n            \"name\": \"Jin Peng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2144884927\",\\n            \"name\": \"Charles Staats\"\\n          },\\n          {\\n            \"authorId\": \"2293653101\",\\n            \"name\": \"Wenda Li\"\\n          },\\n          {\\n            \"authorId\": \"2574060\",\\n            \"name\": \"Christian Szegedy\"\\n          },\\n          {\\n            \"authorId\": \"7446832\",\\n            \"name\": \"Kilian Q. Weinberger\"\\n          },\\n          {\\n            \"authorId\": \"2287780080\",\\n            \"name\": \"Yuhuai Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLM), such as Google\\'s Minerva and OpenAI\\'s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.\"\\n      },\\n      {\\n        \"paperId\": \"1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\\n        \"title\": \"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.17578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MM-Eval is introduced, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages designed with multilingual-specific challenges in mind and finds that evaluators are unfair and inconsistent when evaluating lower-resourced languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2321452295\",\\n            \"name\": \"Guijin Son\"\\n          },\\n          {\\n            \"authorId\": \"29830817\",\\n            \"name\": \"Dongkeun Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2299329316\",\\n            \"name\": \"Juyoung Suk\"\\n          },\\n          {\\n            \"authorId\": \"2301578911\",\\n            \"name\": \"Javier Aula-Blasco\"\\n          },\\n          {\\n            \"authorId\": \"2327215494\",\\n            \"name\": \"Mano Aslan\"\\n          },\\n          {\\n            \"authorId\": \"2327216625\",\\n            \"name\": \"Vu Trong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2232783785\",\\n            \"name\": \"Shayekh Bin Islam\"\\n          },\\n          {\\n            \"authorId\": \"2327215436\",\\n            \"name\": \"Jaume Prats-Cristi\\\\u00e0\"\\n          },\\n          {\\n            \"authorId\": \"2327217057\",\\n            \"name\": \"Luc\\\\u00eda Tormo-Ba\\\\u00f1uelos\"\\n          },\\n          {\\n            \"authorId\": \"2184037220\",\\n            \"name\": \"Seungone Kim\"\\n          }\\n        ],\\n        \"abstract\": \"As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\\\\\"meta-evaluation benchmarks\\\\\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.\"\\n      },\\n      {\\n        \"paperId\": \"f1366e505de4f1d0e901903e3c17471033758a96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f1366e505de4f1d0e901903e3c17471033758a96\",\\n        \"title\": \"Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based Chatbots\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.11876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Rescriber is designed, built, and evaluated, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts, presenting a promising approach to address the privacy and trust challenges of AI.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2326261436\",\\n            \"name\": \"Jijie Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2326116321\",\\n            \"name\": \"Eryue Xu\"\\n          },\\n          {\\n            \"authorId\": \"2326262935\",\\n            \"name\": \"Yaoyao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2326228781\",\\n            \"name\": \"Tianshi Li\"\\n          }\\n        ],\\n        \"abstract\": \"The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users\\\\u2019 personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=Rescriber) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users\\\\u2019 subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4o. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users\\\\u2019 trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.\"\\n      },\\n      {\\n        \"paperId\": \"3ec06fe8d8764123490544ab5dc956143e84b443\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3ec06fe8d8764123490544ab5dc956143e84b443\",\\n        \"title\": \"Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.11977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise and the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310565909\",\\n            \"name\": \"Guangzhi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2089532795\",\\n            \"name\": \"Xiao Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2275248675\",\\n            \"name\": \"Jose Such\"\\n          }\\n        ],\\n        \"abstract\": \"The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\\n      },\\n      {\\n        \"paperId\": \"dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\\n        \"title\": \"Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.07791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating position bias in LLM judges across pairwise and list-wise comparison settings confirms that position bias is not due to random chance and varies significantly across judges and tasks, and provides insights into the distribution of judging difficulty across the dataset.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305925735\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2262963382\",\\n            \"name\": \"Chiyu Ma\"\\n          },\\n          {\\n            \"authorId\": \"2330065663\",\\n            \"name\": \"Wenhua Liang\"\\n          },\\n          {\\n            \"authorId\": \"2227771\",\\n            \"name\": \"Weicheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"1918441\",\\n            \"name\": \"Soroush Vosoughi\"\\n          }\\n        ],\\n        \"abstract\": \"LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases - particularly position bias, the tendency to favor solutions based on their position within the prompt - compromise its reliability. This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 15 LLM judges across MTBench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.\"\\n      },\\n      {\\n        \"paperId\": \"638d8d1f3865ebf065605535a7aa50727d5ffabe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/638d8d1f3865ebf065605535a7aa50727d5ffabe\",\\n        \"title\": \"TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.18919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation, significantly improves semantic and contextual consistency in synthesized images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2298945483\",\\n            \"name\": \"Junhao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2298032416\",\\n            \"name\": \"Baiqiao Yin\"\\n          },\\n          {\\n            \"authorId\": \"2229014859\",\\n            \"name\": \"Kaixin Cai\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2276604489\",\\n            \"name\": \"Hanhui Li\"\\n          },\\n          {\\n            \"authorId\": \"2299161673\",\\n            \"name\": \"Yuxin He\"\\n          },\\n          {\\n            \"authorId\": \"2298943419\",\\n            \"name\": \"Xi Lu\"\\n          },\\n          {\\n            \"authorId\": \"2298043252\",\\n            \"name\": \"Yue Li\"\\n          },\\n          {\\n            \"authorId\": \"2298926852\",\\n            \"name\": \"Yifei Li\"\\n          },\\n          {\\n            \"authorId\": \"2298644153\",\\n            \"name\": \"Yuhao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"144880586\",\\n            \"name\": \"Yiqiang Yan\"\\n          },\\n          {\\n            \"authorId\": \"2291389227\",\\n            \"name\": \"Xiaodan Liang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a\\\\\"Screenwriter\\\\\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the\\\\\"Rehearsal\\\\\". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the\\\\\"Final Performance\\\\\". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.\"\\n      },\\n      {\\n        \"paperId\": \"4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"title\": \"Improving Text-to-Image Consistency via Automatic Prompt Optimization\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models and paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1796269096\",\\n            \"name\": \"Oscar Ma\\\\u00f1as\"\\n          },\\n          {\\n            \"authorId\": \"2274101827\",\\n            \"name\": \"Pietro Astolfi\"\\n          },\\n          {\\n            \"authorId\": \"2293590162\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"2256372432\",\\n            \"name\": \"Candace Ross\"\\n          },\\n          {\\n            \"authorId\": \"39219656\",\\n            \"name\": \"Jack Urbanek\"\\n          },\\n          {\\n            \"authorId\": \"2293907712\",\\n            \"name\": \"Adina Williams\"\\n          },\\n          {\\n            \"authorId\": \"2801949\",\\n            \"name\": \"Aishwarya Agrawal\"\\n          },\\n          {\\n            \"authorId\": \"1456285042\",\\n            \"name\": \"Adriana Romero-Soriano\"\\n          },\\n          {\\n            \"authorId\": \"3325894\",\\n            \"name\": \"M. Drozdzal\"\\n          }\\n        ],\\n        \"abstract\": \"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"275a3955a83867dd36a3683788e0e053e00f8a89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/275a3955a83867dd36a3683788e0e053e00f8a89\",\\n        \"title\": \"Collaborative Enhancement of Consistency and Accuracy in US Diagnosis of Thyroid Nodules Using Large Language Models.\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1148/radiol.232255?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1148/radiol.232255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging and ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1678966811\",\\n            \"name\": \"Shaohong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2272038393\",\\n            \"name\": \"Wen-juan Tong\"\\n          },\\n          {\\n            \"authorId\": \"2127991969\",\\n            \"name\": \"Ming-De Li\"\\n          },\\n          {\\n            \"authorId\": \"28890237\",\\n            \"name\": \"Hang-tong Hu\"\\n          },\\n          {\\n            \"authorId\": \"2187182790\",\\n            \"name\": \"Xiao-zhou Lu\"\\n          },\\n          {\\n            \"authorId\": \"2185218332\",\\n            \"name\": \"Ze-Rong Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290915880\",\\n            \"name\": \"Xin-Xin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290897544\",\\n            \"name\": \"Ruifang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2261915754\",\\n            \"name\": \"Ming-De Lu\"\\n          },\\n          {\\n            \"authorId\": \"6457299\",\\n            \"name\": \"Li-da Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290866279\",\\n            \"name\": \"Wei Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Background Large language models (LLMs) hold substantial promise for medical imaging interpretation. However, there is a lack of studies on their feasibility in handling reasoning questions associated with medical diagnosis. Purpose To investigate the viability of leveraging three publicly available LLMs to enhance consistency and diagnostic accuracy in medical imaging based on standardized reporting, with pathology as the reference standard. Materials and Methods US images of thyroid nodules with pathologic results were retrospectively collected from a tertiary referral hospital between July 2022 and December 2022 and used to evaluate malignancy diagnoses generated by three LLMs-OpenAI\\'s ChatGPT 3.5, ChatGPT 4.0, and Google\\'s Bard. Inter- and intra-LLM agreement of diagnosis were evaluated. Then, diagnostic performance, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), was evaluated and compared for the LLMs and three interactive approaches: human reader combined with LLMs, image-to-text model combined with LLMs, and an end-to-end convolutional neural network model. Results A total of 1161 US images of thyroid nodules (498 benign, 663 malignant) from 725 patients (mean age, 42.2 years \\\\u00b1 14.1 [SD]; 516 women) were evaluated. ChatGPT 4.0 and Bard displayed substantial to almost perfect intra-LLM agreement (\\\\u03ba range, 0.65-0.86 [95% CI: 0.64, 0.86]), while ChatGPT 3.5 showed fair to substantial agreement (\\\\u03ba range, 0.36-0.68 [95% CI: 0.36, 0.68]). ChatGPT 4.0 had an accuracy of 78%-86% (95% CI: 76%, 88%) and sensitivity of 86%-95% (95% CI: 83%, 96%), compared with 74%-86% (95% CI: 71%, 88%) and 74%-91% (95% CI: 71%, 93%), respectively, for Bard. Moreover, with ChatGPT 4.0, the image-to-text-LLM strategy exhibited an AUC (0.83 [95% CI: 0.80, 0.85]) and accuracy (84% [95% CI: 82%, 86%]) comparable to those of the human-LLM interaction strategy with two senior readers and one junior reader and exceeding those of the human-LLM interaction strategy with one junior reader. Conclusion LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging. ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5. \\\\u00a9 RSNA, 2024 Supplemental material is available for this article.\"\\n      },\\n      {\\n        \"paperId\": \"b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\\n        \"title\": \"Mitigating LLM Hallucinations via Conformal Abstention\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.01563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1388837087\",\\n            \"name\": \"Yasin Abbasi-Yadkori\"\\n          },\\n          {\\n            \"authorId\": \"3150458\",\\n            \"name\": \"Ilja Kuzborskij\"\\n          },\\n          {\\n            \"authorId\": \"2298902427\",\\n            \"name\": \"David Stutz\"\\n          },\\n          {\\n            \"authorId\": \"2305592785\",\\n            \"name\": \"Andr\\\\u00e1s Gy\\\\u00f6rgy\"\\n          },\\n          {\\n            \"authorId\": \"2299943725\",\\n            \"name\": \"Adam Fisch\"\\n          },\\n          {\\n            \"authorId\": \"2299943677\",\\n            \"name\": \"Arnaud Doucet\"\\n          },\\n          {\\n            \"authorId\": \"2299943992\",\\n            \"name\": \"Iuliya Beloshapka\"\\n          },\\n          {\\n            \"authorId\": \"2239098855\",\\n            \"name\": \"Wei-Hung Weng\"\\n          },\\n          {\\n            \"authorId\": \"2300022831\",\\n            \"name\": \"Yao-Yuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257346986\",\\n            \"name\": \"Csaba Szepesv\\'ari\"\\n          },\\n          {\\n            \"authorId\": \"9235290\",\\n            \"name\": \"Ali Taylan Cemgil\"\\n          },\\n          {\\n            \"authorId\": \"2359197879\",\\n            \"name\": \"Nenad Tomasev\"\\n          }\\n        ],\\n        \"abstract\": \"We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying\\\\\"I don\\'t know\\\\\") in a general domain, instead of resorting to possibly\\\\\"hallucinating\\\\\"a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.\"\\n      },\\n      {\\n        \"paperId\": \"023a98af94a3e7e8e538a6183da8ec05024fec56\",\\n        \"url\": \"https://www.semanticscholar.org/paper/023a98af94a3e7e8e538a6183da8ec05024fec56\",\\n        \"title\": \"How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work prompts different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions, and finds that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2307916998\",\\n            \"name\": \"Julia Kharchenko\"\\n          },\\n          {\\n            \"authorId\": \"2284066307\",\\n            \"name\": \"Tanya Roosta\"\\n          },\\n          {\\n            \"authorId\": \"2284065969\",\\n            \"name\": \"Aman Chadha\"\\n          },\\n          {\\n            \"authorId\": \"2234352974\",\\n            \"name\": \"Chirag Shah\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user\\'s known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs\\'cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"a1849a77644ff411a03833b5aa7a65ff57158c50\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1849a77644ff411a03833b5aa7a65ff57158c50\",\\n        \"title\": \"CLLMs: Consistency Large Language Models\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.00835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work develops a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory by refining the target LLM to consistently predict the fixed point given any state as input.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258963117\",\\n            \"name\": \"Siqi Kou\"\\n          },\\n          {\\n            \"authorId\": \"2258334187\",\\n            \"name\": \"Lanxiang Hu\"\\n          },\\n          {\\n            \"authorId\": \"2116778591\",\\n            \"name\": \"Zhe He\"\\n          },\\n          {\\n            \"authorId\": \"2260296481\",\\n            \"name\": \"Zhijie Deng\"\\n          },\\n          {\\n            \"authorId\": \"2289837431\",\\n            \"name\": \"Hao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\\\\\\\times$ to 3.4$\\\\\\\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\\n        \"title\": \"PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children\\'s Collaborative Learning\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613905.3651008\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.14227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded and thus highlight potential design improvements and considerations for peer agents in both roles.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-03-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2296401615\",\\n            \"name\": \"Jiawen Liu\"\\n          },\\n          {\\n            \"authorId\": \"2292671914\",\\n            \"name\": \"Yuanyuan Yao\"\\n          },\\n          {\\n            \"authorId\": \"2283762773\",\\n            \"name\": \"Pengcheng An\"\\n          },\\n          {\\n            \"authorId\": \"2301178822\",\\n            \"name\": \"Qi Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In children\\\\u2019s collaborative learning, effective peer conversations can significantly enhance the quality of children\\\\u2019s collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children\\\\u2019s creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.\"\\n      },\\n      {\\n        \"paperId\": \"5e317746c0f38d1149f33a19807af47d513bdd27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5e317746c0f38d1149f33a19807af47d513bdd27\",\\n        \"title\": \"Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.12509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald\\'s omega, and demonstrates the limitations of fixed randomness and the importance of considering multiple samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2328309355\",\\n            \"name\": \"Kayla Schroeder\"\\n          },\\n          {\\n            \"authorId\": \"1411379613\",\\n            \"name\": \"Zach Wood-Doughty\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs. While deterministic settings can improve consistency, they do not guarantee reliability, as a single sample from the model\\'s probability distribution can still be misleading. Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald\\'s omega. We evaluate the reliability of LLMs when judging the outputs of other LLMs on standard single-turn and multi-turn benchmarks, simultaneously investigating the impact of temperature on reliability. By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implications for downstream applications. Our findings highlight the need for a nuanced understanding of LLM reliability and the potential risks associated with over-reliance on single-shot evaluations. This work provides a crucial step towards building more trustworthy and reliable LLM-based systems and applications.\"\\n      },\\n      {\\n        \"paperId\": \"235a8bd57a6b53ecab756780a45bce6e4743cecd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/235a8bd57a6b53ecab756780a45bce6e4743cecd\",\\n        \"title\": \"LLM-based Multi-Level Knowledge Generation for Few-shot Knowledge Graph Completion\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2024/236?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2024/236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a generation-based FKGC paradigm facilitated by LLM distillation, which achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284861197\",\\n            \"name\": \"Qian Li\"\\n          },\\n          {\\n            \"authorId\": \"2313599793\",\\n            \"name\": \"Zhuo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2052296239\",\\n            \"name\": \"Cheng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2313658619\",\\n            \"name\": \"Shiqi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2274552581\",\\n            \"name\": \"Jianxin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Knowledge Graphs (KGs) are pivotal in various NLP applications but often grapple with incompleteness, especially due to the long-tail problem where infrequent, unpopular relationships drastically reduce the KG completion performance. In this paper, we focus on Few-shot Knowledge Graph Completion (FKGC), a task addressing these gaps in long-tail scenarios. Amidst the rapid evolution of Large Language Models, we propose a generation-based FKGC paradigm facilitated by LLM distillation. Our MuKDC framework employs multi-level knowledge distillation for few-shot KG completion, generating supplementary knowledge to mitigate data scarcity in few-shot environments. MuKDC comprises two primary components: Multi-level Knowledge Generation, which enriches the KG at various levels, and Consistency Assessment, to ensure the coherence and reliability of the generated knowledge. Most notably, our method achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\\n      },\\n      {\\n        \"paperId\": \"9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\\n        \"title\": \"Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2144511530\",\\n            \"name\": \"Xuan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2265432385\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2302819855\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"2302765389\",\\n            \"name\": \"Haoyang Shang\"\\n          },\\n          {\\n            \"authorId\": \"2302927286\",\\n            \"name\": \"Chengxu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2302915902\",\\n            \"name\": \"Quanyan Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents\\' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents\\' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\"\\n      },\\n      {\\n        \"paperId\": \"086046d38b3a7066aa39e3d350905e8065c8f1b5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/086046d38b3a7066aa39e3d350905e8065c8f1b5\",\\n        \"title\": \"Efficiently Scaling LLM Reasoning with Certaindex\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.20993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy, so this work introduces Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-12-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282896192\",\\n            \"name\": \"Yichao Fu\"\\n          },\\n          {\\n            \"authorId\": \"2279862923\",\\n            \"name\": \"Junda Chen\"\\n          },\\n          {\\n            \"authorId\": \"2317134948\",\\n            \"name\": \"Siqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2337869460\",\\n            \"name\": \"Zheyu Fu\"\\n          },\\n          {\\n            \"authorId\": \"2351053054\",\\n            \"name\": \"Zhongdongming Dai\"\\n          },\\n          {\\n            \"authorId\": \"2152482391\",\\n            \"name\": \"Yonghao Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2363671676\",\\n            \"name\": \"Yian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2317112099\",\\n            \"name\": \"Aurick Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2379937619\",\\n            \"name\": \"Tajana Rosing\"\\n          },\\n          {\\n            \"authorId\": \"2344601177\",\\n            \"name\": \"Ion Stoica\"\\n          },\\n          {\\n            \"authorId\": \"2337807823\",\\n            \"name\": \"Hao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50% compute savings and 3.3x higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git\"\\n      },\\n      {\\n        \"paperId\": \"43fedc4430be030c083626b3e64b7093916b429a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43fedc4430be030c083626b3e64b7093916b429a\",\\n        \"title\": \"Enhancing clinical reasoning skills for medical students: a qualitative comparison of LLM-powered social robotic versus computer-based virtual patients within rheumatology\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1007/s00296-024-05731-0\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11618132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform and shows promise in training CR skills, communication, and adaptive thinking.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290951787\",\\n            \"name\": \"Alexander Borg\"\\n          },\\n          {\\n            \"authorId\": \"2326093338\",\\n            \"name\": \"Benjamin Jobs\"\\n          },\\n          {\\n            \"authorId\": \"2164303442\",\\n            \"name\": \"Viking Huss\"\\n          },\\n          {\\n            \"authorId\": \"23717264\",\\n            \"name\": \"C. Gentline\"\\n          },\\n          {\\n            \"authorId\": \"2326093767\",\\n            \"name\": \"Fabricio Espinosa\"\\n          },\\n          {\\n            \"authorId\": \"2290722393\",\\n            \"name\": \"Mini Ruiz\"\\n          },\\n          {\\n            \"authorId\": \"2758537\",\\n            \"name\": \"Samuel Edelbring\"\\n          },\\n          {\\n            \"authorId\": \"2326094583\",\\n            \"name\": \"Carina Georg\"\\n          },\\n          {\\n            \"authorId\": \"103081544\",\\n            \"name\": \"G. Skantze\"\\n          },\\n          {\\n            \"authorId\": \"8637952\",\\n            \"name\": \"Ioannis Parodis\"\\n          }\\n        ],\\n        \"abstract\": \"Virtual patients (VPs) are increasingly used in medical education to train clinical reasoning (CR) skills. However, optimal VP design for enhancing interactivity and authenticity remains unclear. Novel interactive modalities, such as large language model (LLM)-enhanced social robotic VPs might increase interactivity and authenticity in CR skill practice. To evaluate medical students\\\\u2019 perceptions of CR training using an LLM-enhanced social robotic VP platform compared with a conventional computer-based VP platform. A qualitative study involved 23 third-year medical students from Karolinska Institutet, who completed VP cases on an LLM-enhanced social robotic platform and a computer-based semi-linear platform. In-depth interviews assessed students\\\\u2019 self-perceived acquirement of CR skills using the two platforms. Thematic analysis was employed to identify themes and sub-themes. Three main themes were identified: authenticity, VP application, and strengths and limitations. Students found the social robotic platform more authentic and engaging. It enabled highly interactive communication and expressed emotions, collectively offering a realistic experience. It facilitated active learning, hypothesis generation, and adaptive thinking. Limitations included lack of physical examination options and, occasionally, mechanical dialogue. The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform. Despite some limitations, it shows promise in training CR skills, communication, and adaptive thinking. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. \\\\u2022An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. \\\\u2022Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. \\\\u2022Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. \\\\u2022An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. \\\\u2022Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. \\\\u2022Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations.\"\\n      },\\n      {\\n        \"paperId\": \"f04c25fcf3247ff4d8eca72d862b22090b884b75\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f04c25fcf3247ff4d8eca72d862b22090b884b75\",\\n        \"title\": \"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.06461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49606614\",\\n            \"name\": \"Junlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282521448\",\\n            \"name\": \"Siddhartha Jain\"\\n          },\\n          {\\n            \"authorId\": \"2305691523\",\\n            \"name\": \"Dejiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2282366776\",\\n            \"name\": \"Baishakhi Ray\"\\n          },\\n          {\\n            \"authorId\": \"40574366\",\\n            \"name\": \"Varun Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2304481349\",\\n            \"name\": \"Ben Athiwaratkun\"\\n          }\\n        ],\\n        \"abstract\": \"A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don\\\\u2019t surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.\"\\n      },\\n      {\\n        \"paperId\": \"7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\\n        \"title\": \"What Did I Do Wrong? Quantifying LLMs\\' Sensitivity and Consistency to Prompt Engineering\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.12334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Two metrics for classification tasks, namely sensitivity and consistency, are introduced, which are complementary to task performance, and it is hoped that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2307085791\",\\n            \"name\": \"Federico Errica\"\\n          },\\n          {\\n            \"authorId\": \"2009237\",\\n            \"name\": \"G. Siracusano\"\\n          },\\n          {\\n            \"authorId\": \"3109801\",\\n            \"name\": \"D. Sanvito\"\\n          },\\n          {\\n            \"authorId\": \"2269460793\",\\n            \"name\": \"Roberto Bifulco\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs\\'inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely sensitivity and consistency, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\\n      },\\n      {\\n        \"paperId\": \"024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\\n        \"title\": \"PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2403.09732?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2403.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt representation, called reference-enhanced representation, is introduced, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries and proposes using cross-consistency across different LLMs rather than self-consistency within a particular LLM.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2262448530\",\\n            \"name\": \"Zhishuai Li\"\\n          },\\n          {\\n            \"authorId\": \"2292059965\",\\n            \"name\": \"Xiang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2291921433\",\\n            \"name\": \"Jingjing Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2262480162\",\\n            \"name\": \"Sun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2228059114\",\\n            \"name\": \"Guoqing Du\"\\n          },\\n          {\\n            \"authorId\": \"2267589674\",\\n            \"name\": \"Xiaoru Hu\"\\n          },\\n          {\\n            \"authorId\": \"2315291036\",\\n            \"name\": \"Bin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2276235755\",\\n            \"name\": \"Yuxiao Ye\"\\n          },\\n          {\\n            \"authorId\": \"2262543561\",\\n            \"name\": \"Ziyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2263456785\",\\n            \"name\": \"Rui Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2262446566\",\\n            \"name\": \"Hangyu Mao\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0e314ddbf28514d92f2405b73941242c162ae0ba\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0e314ddbf28514d92f2405b73941242c162ae0ba\",\\n        \"title\": \"AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.11073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AFaCTA (Automatic Factual Claim deTection Annotator) is introduced, a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs) and calibrates its annotation confidence with consistency along three predefined reasoning paths.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046974354\",\\n            \"name\": \"Jingwei Ni\"\\n          },\\n          {\\n            \"authorId\": \"2284947386\",\\n            \"name\": \"Minjing Shi\"\\n          },\\n          {\\n            \"authorId\": \"146552774\",\\n            \"name\": \"Dominik Stammbach\"\\n          },\\n          {\\n            \"authorId\": \"2790926\",\\n            \"name\": \"Mrinmaya Sachan\"\\n          },\\n          {\\n            \"authorId\": \"2261279066\",\\n            \"name\": \"Elliott Ash\"\\n          },\\n          {\\n            \"authorId\": \"3073566\",\\n            \"name\": \"Markus Leippold\"\\n          }\\n        ],\\n        \"abstract\": \"With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.\"\\n      },\\n      {\\n        \"paperId\": \"1c5a097b4e376897545f153370425cf7e0c2d8fd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1c5a097b4e376897545f153370425cf7e0c2d8fd\",\\n        \"title\": \"Explaining Length Bias in LLM-Based Preference Evaluations\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.01085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AdapAlpaca is proposed, a simple yet effective adjustment to win rate measurement that ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Zhengyu Hu\"\\n          },\\n          {\\n            \"authorId\": \"2322070046\",\\n            \"name\": \"Linxin Song\"\\n          },\\n          {\\n            \"authorId\": \"2309191644\",\\n            \"name\": \"Jieyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2311315868\",\\n            \"name\": \"Zheyuan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2269687536\",\\n            \"name\": \"Jingang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2309176938\",\\n            \"name\": \"Zhenyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2309202283\",\\n            \"name\": \"Jieyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2269470756\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"The use of large language models (LLMs) as judges, particularly in preference comparisons, has become widespread, but this reveals a notable bias towards longer responses, undermining the reliability of such evaluations. To better understand such bias, we propose to decompose the preference evaluation metric, specifically the win rate, into two key components: desirability and information mass, where the former is length-independent and related to trustworthiness such as correctness, toxicity, and consistency, and the latter is length-dependent and represents the amount of information in the response. We empirically demonstrated the decomposition through controlled experiments and found that response length impacts evaluations by influencing information mass. To derive a reliable evaluation metric that assesses content quality without being confounded by response length, we propose AdapAlpaca, a simple yet effective adjustment to win rate measurement. Specifically, AdapAlpaca ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\\n      },\\n      {\\n        \"paperId\": \"00ce8beee350a260395676490915d7ebfa7430d1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/00ce8beee350a260395676490915d7ebfa7430d1\",\\n        \"title\": \"Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Investigation of fine-tuned LLMs in D2T tasks in terms of model size reveals that increasing LLM size enhances readability andformativeness in D2T tasks, but larger LLMs may sacrifice \\\\\\\\textit{faithfulness}.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8038450\",\\n            \"name\": \"Joy Mahapatra\"\\n          },\\n          {\\n            \"authorId\": \"2312204876\",\\n            \"name\": \"U. Garain\"\\n          }\\n        ],\\n        \"abstract\": \"Data-to-text (D2T) generation aims to generate human-readable text from semi-structured data, such as tables and graphs. The recent success of D2T is largely attributed to advancements in LLMs. Despite the success of LLMs, no research has been conducted to illustrate the impact of model size on the performance of fine-tuned LLMs for D2T tasks. D2T model performance is typically assessed based on three key qualities: \\\\\\\\textit{readability} (indicates fluency and coherence), \\\\\\\\textit{informativeness} (measures content similarity), and \\\\\\\\textit{faithfulness} (assesses consistency of factual information). It is currently uncertain whether increasing the size of LLMs effectively improves performance in D2T tasks across these three qualities. The objective of this study is to investigate the performance of fine-tuned LLMs in D2T tasks in terms of model size. Through extensive comparative analysis, we aim to elucidate both the advantages and limitations of scaling model sizes across five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and twelve state-of-the-art LLMs with varying sizes from five different LLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all the three essential qualities of D2T models, we incorporate six widely recognized automatic metrics -- \\\\\\\\textsc{BLEU}, \\\\\\\\textsc{METEOR}, \\\\\\\\textsc{BERTScore}, \\\\\\\\textsc{MoverScore}, \\\\\\\\textsc{Parent}, and \\\\\\\\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance concerning model size in the presence of source-reference divergence, a critical aspect of D2T tasks. Our investigation reveals that increasing LLM size enhances \\\\\\\\textit{readability} and \\\\\\\\textit{informativeness} in D2T tasks, but larger (in terms of size) LLMs may sacrifice \\\\\\\\textit{faithfulness}. Moreover, small-sized LLMs show more resilience than larger ones when source-reference divergence is present.\"\\n      },\\n      {\\n        \"paperId\": \"74908bc543e77b8995a6eebe32ab13cf0837949b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/74908bc543e77b8995a6eebe32ab13cf0837949b\",\\n        \"title\": \"LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment and demonstrates that, when using GPT-4, the proposed metric achieves evaluation consistency close to that of radiologists.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2294387070\",\\n            \"name\": \"Zilong Wang\"\\n          },\\n          {\\n            \"authorId\": \"13289447\",\\n            \"name\": \"Xufang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2268347004\",\\n            \"name\": \"Xinyang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268313028\",\\n            \"name\": \"Dongsheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2160727304\",\\n            \"name\": \"Lili Qiu\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task\\'s clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.\"\\n      },\\n      {\\n        \"paperId\": \"f33991c02f8f0ab8794dad020c648b37ccc53365\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f33991c02f8f0ab8794dad020c648b37ccc53365\",\\n        \"title\": \"CitaLaw: Enhancing LLM with Citations in Legal Domain\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.14556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes CitaLaw, the first benchmark designed to evaluate LLMs\\' ability to produce legally sound responses with appropriate citations, and introduces syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290124325\",\\n            \"name\": \"Kepu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2118684861\",\\n            \"name\": \"Weijie Yu\"\\n          },\\n          {\\n            \"authorId\": \"2155892801\",\\n            \"name\": \"Sunhao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2274965731\",\\n            \"name\": \"Jun Xu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs\\' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.\"\\n      },\\n      {\\n        \"paperId\": \"0d5ccf0861b62223fe562e13369ab15746188251\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0d5ccf0861b62223fe562e13369ab15746188251\",\\n        \"title\": \"QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.05345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums, and applies QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2300845194\",\\n            \"name\": \"Varun Nagaraj Rao\"\\n          },\\n          {\\n            \"authorId\": \"2300370478\",\\n            \"name\": \"Eesha Agarwal\"\\n          },\\n          {\\n            \"authorId\": \"2300371225\",\\n            \"name\": \"Samantha Dalal\"\\n          },\\n          {\\n            \"authorId\": \"2265042713\",\\n            \"name\": \"Dan Calacci\"\\n          },\\n          {\\n            \"authorId\": \"2266397659\",\\n            \"name\": \"Andr\\'es Monroy-Hern\\'andez\"\\n          }\\n        ],\\n        \"abstract\": \"Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methodologies used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting and human evaluation methodology. We applied this framework to analyze over one million comments from two of Reddit\\'s rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.\"\\n      },\\n      {\\n        \"paperId\": \"e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\\n        \"title\": \"Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.13082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TREACLE is proposed, a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\\'s monetary cost and latency constraints, and provides the user with the ability to gracefully trade off accuracy for cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2157197701\",\\n            \"name\": \"Xuechen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2297821531\",\\n            \"name\": \"Zijian Huang\"\\n          },\\n          {\\n            \"authorId\": \"2297769735\",\\n            \"name\": \"Ege Onur Taga\"\\n          },\\n          {\\n            \"authorId\": \"1393650147\",\\n            \"name\": \"Carlee Joe-Wong\"\\n          },\\n          {\\n            \"authorId\": \"3103394\",\\n            \"name\": \"Samet Oymak\"\\n          },\\n          {\\n            \"authorId\": \"2281075331\",\\n            \"name\": \"Jiasi Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE ($\\\\\\\\underline{T}$hrifty $\\\\\\\\underline{Rea}$soning via $\\\\\\\\underline{C}$ontext-Aware $\\\\\\\\underline{L}$LM and Prompt S$\\\\\\\\underline{e}$lection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\\'s monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.\"\\n      },\\n      {\\n        \"paperId\": \"c01bbc439164002f2c7326748f7939783e306a94\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c01bbc439164002f2c7326748f7939783e306a94\",\\n        \"title\": \"A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.09972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that the order of presenting reasons and scores significantly influences LLMs\\' scoring, with a different level of rule understanding in the prompt, crucial for improving the accuracy and consistency of LLM-based evaluations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2264464750\",\\n            \"name\": \"Kuanchao Chu\"\\n          },\\n          {\\n            \"authorId\": \"2109381394\",\\n            \"name\": \"Yi-Pei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2301580436\",\\n            \"name\": \"Hideki Nakayama\"\\n          }\\n        ],\\n        \"abstract\": \"This research investigates prompt designs of evaluating generated texts using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for open-ended text evaluation remains challenging due to model sensitivity and subjectivity in evaluation of text generation. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs\\' scoring, with a different level of rule understanding in the prompt. An additional optimization may enhance scoring alignment if sufficient data is available. This insight is crucial for improving the accuracy and consistency of LLM-based evaluations.\"\\n      },\\n      {\\n        \"paperId\": \"a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\\n        \"title\": \"How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment\",\\n        \"citationCount\": 135,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.14049\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Insight into novice learners\\\\u2019 use of AI code generators in a self-paced learning environment is offered, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3136345\",\\n            \"name\": \"Majeed Kazemitabaar\"\\n          },\\n          {\\n            \"authorId\": \"2112801172\",\\n            \"name\": \"Xinying Hou\"\\n          },\\n          {\\n            \"authorId\": \"2063979470\",\\n            \"name\": \"A. Henley\"\\n          },\\n          {\\n            \"authorId\": \"20937525\",\\n            \"name\": \"B. Ericson\"\\n          },\\n          {\\n            \"authorId\": \"2862077\",\\n            \"name\": \"David Weintrop\"\\n          },\\n          {\\n            \"authorId\": \"2666589\",\\n            \"name\": \"Tovi Grossman\"\\n          }\\n        ],\\n        \"abstract\": \"As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners\\\\u2019 utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners\\\\u2019 use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\\n      },\\n      {\\n        \"paperId\": \"221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\\n        \"title\": \"PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes to comprehensively address all subtasks together, and suggests employing a unique prompt-based in-context learning strategy to enhance error correction and error detection performance in medical systems where prediction errors can have grave consequences.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9122885\",\\n            \"name\": \"Kesav Gundabathula\"\\n          },\\n          {\\n            \"authorId\": \"2301202434\",\\n            \"name\": \"Sriram R Kolar\"\\n          }\\n        ],\\n        \"abstract\": \"This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals. This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it. Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information. We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy. We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge. In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='cdaed8ce-2456-42d3-90da-95687861e4d1')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ee237047-156b-46ff-b2d5-4c6075ddc235'), AIMessage(content='{\\n  \"research_question\": \"How can prompts be dynamically adapted to maintain coherence and engagement in extended LLM interactions?\",\\n  \"problem_domain\": \"natural language processing\",\\n  \"methodology_keywords\": [\\n    \"prompt engineering\",\\n    \"contextual analysis\",\\n    \"natural language generation\"\\n  ],\\n  \"key_concepts\": [\\n    \"dynamic prompt adaptation\",\\n    \"thematic consistency\",\\n    \"coherence\",\\n    \"iterative context update\",\\n    \"adaptive prompt generation\",\\n    \"reader engagement\",\\n    \"LLM interaction\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation method\",\\n    \"three-phase approach (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update)\",\\n    \"maintaining coherence and adaptability across LLM interactions\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='d4be8654-b546-4e18-ba54-8feb3a5492be'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation LLM\",\\n      \"rationale\": \"Directly targets the core concept of dynamically adapting prompts for LLMs, a key aspect of the research question.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"coherence in extended LLM\",\\n      \"rationale\": \"Focuses on the problem domain of maintaining coherence, which is central to the research question\\'s goal in long interactions.\",\\n      \"priority_concept\": \"coherence, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Highlights the methodology of generating prompts in an adaptive manner, aligning with the proposed novel approach.\",\\n      \"priority_concept\": \"adaptive prompt generation, prompt engineering\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Addresses the \\'Iterative Context Update\\' phase of the proposed method, crucial for maintaining state in extended interactions.\",\\n      \"priority_concept\": \"iterative context update, contextual analysis\"\\n    },\\n    {\\n      \"query_string\": \"thematic consistency LLM\",\\n      \"rationale\": \"Searches for work related to maintaining thematic consistency, a specific aspect of coherence in the context of LLM interactions.\",\\n      \"priority_concept\": \"thematic consistency, LLM interaction\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='df3e3d38-4dc6-473a-bd42-348c81dbc69c'), AIMessage(content='{\\n  \"dynamic prompt adaptation LLM\": {\\n    \"total\": 467,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"cf95279b1da9de1aad9e7c651f5048f69af295ed\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf95279b1da9de1aad9e7c651f5048f69af295ed\",\\n        \"title\": \"AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents\",\\n        \"citationCount\": 76,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.13352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2175939276\",\\n            \"name\": \"Edoardo Debenedetti\"\\n          },\\n          {\\n            \"authorId\": \"2299061721\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2138580250\",\\n            \"name\": \"Mislav Balunovi\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2150869869\",\\n            \"name\": \"Luca Beurer-Kellner\"\\n          },\\n          {\\n            \"authorId\": \"2307472727\",\\n            \"name\": \"Marc Fischer\"\\n          },\\n          {\\n            \"authorId\": \"2267733649\",\\n            \"name\": \"Florian Tram\\\\u00e8r\"\\n          }\\n        ],\\n        \"abstract\": \"AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.\"\\n      },\\n      {\\n        \"paperId\": \"3967189742efab8859da542ce3953d4c72957aca\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3967189742efab8859da542ce3953d4c72957aca\",\\n        \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.23904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI), designed to design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275054108\",\\n            \"name\": \"Qinqian Lei\"\\n          },\\n          {\\n            \"authorId\": \"2313081973\",\\n            \"name\": \"Bo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256998291\",\\n            \"name\": \"Robby T. Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\"\\n      },\\n      {\\n        \"paperId\": \"659e0b3303caa860348dee52f41476e3fddc9573\",\\n        \"url\": \"https://www.semanticscholar.org/paper/659e0b3303caa860348dee52f41476e3fddc9573\",\\n        \"title\": \"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method is introduced, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages and allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2237800256\",\\n            \"name\": \"Qichen Fu\"\\n          },\\n          {\\n            \"authorId\": \"2237803694\",\\n            \"name\": \"Minsik Cho\"\\n          },\\n          {\\n            \"authorId\": \"2178316365\",\\n            \"name\": \"Thomas Merth\"\\n          },\\n          {\\n            \"authorId\": \"2256998189\",\\n            \"name\": \"Sachin Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2284683934\",\\n            \"name\": \"Mohammad Rastegari\"\\n          },\\n          {\\n            \"authorId\": \"40465379\",\\n            \"name\": \"Mahyar Najibi\"\\n          }\\n        ],\\n        \"abstract\": \"The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.\"\\n      },\\n      {\\n        \"paperId\": \"00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\\n        \"title\": \"From ideas to ventures: building entrepreneurship knowledge with LLM, prompt engineering, and conversational agents\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10639-024-12775-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10639-024-12775-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Large Language Models like the Generative Pre-trained Transformer 4 (GPT-4), recognized for their exceptional performance on public datasets, are examined in this study for their potential adaptability and interactivity nature, which align well with the dynamic nature of entrepreneurship learning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1424283019\",\\n            \"name\": \"Marsela Thanasi-Bo\\\\u00e7e\"\\n          },\\n          {\\n            \"authorId\": \"2290624379\",\\n            \"name\": \"Julian Hoxha\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"b67a6181fad5c5838945583ccdc7f39187e29332\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b67a6181fad5c5838945583ccdc7f39187e29332\",\\n        \"title\": \"Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.20911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Mantis is a defensive framework that exploits LLMs\\' susceptibility to adversarial inputs to undermine malicious operations, leading the attacker\\'s LLM to disrupt their own operations or even compromise the attacker\\'s machine.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50610174\",\\n            \"name\": \"Dario Pasquini\"\\n          },\\n          {\\n            \"authorId\": \"2762279\",\\n            \"name\": \"Evgenios M. Kornaropoulos\"\\n          },\\n          {\\n            \"authorId\": \"1700850\",\\n            \"name\": \"G. Ateniese\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs\\' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker\\'s LLM to disrupt their own operations (passive defense) or even compromise the attacker\\'s machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker\\'s LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project_mantis\"\\n      },\\n      {\\n        \"paperId\": \"2f274db9aa447a13c019114e327057d4b161b6d5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f274db9aa447a13c019114e327057d4b161b6d5\",\\n        \"title\": \"LLM-controller: Dynamic robot control adaptation using large language models\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.robot.2024.104913?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.robot.2024.104913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"90182090\",\\n            \"name\": \"Rasoul Zahedifar\"\\n          },\\n          {\\n            \"authorId\": \"1799503\",\\n            \"name\": \"M. Baghshah\"\\n          },\\n          {\\n            \"authorId\": \"2273939584\",\\n            \"name\": \"Alireza Taheri\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\\n        \"url\": \"https://www.semanticscholar.org/paper/47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\\n        \"title\": \"HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.19878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE is proposed, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-09-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2211429378\",\\n            \"name\": \"Bingshen Mu\"\\n          },\\n          {\\n            \"authorId\": \"2299944267\",\\n            \"name\": \"Kun Wei\"\\n          },\\n          {\\n            \"authorId\": \"2061559378\",\\n            \"name\": \"Qijie Shao\"\\n          },\\n          {\\n            \"authorId\": \"2323714781\",\\n            \"name\": \"Yong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2249732546\",\\n            \"name\": \"Lei Xie\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.\"\\n      },\\n      {\\n        \"paperId\": \"bcac614f9774488447221ebb4f16f05e3975ec1e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bcac614f9774488447221ebb4f16f05e3975ec1e\",\\n        \"title\": \"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\",\\n        \"citationCount\": 75,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.04669\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A well-designed visual tokenizer is introduced to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read, which empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2239056878\",\\n            \"name\": \"Yang Jin\"\\n          },\\n          {\\n            \"authorId\": \"2266735757\",\\n            \"name\": \"Kun Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266735757\",\\n            \"name\": \"Kun Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266419021\",\\n            \"name\": \"Liwei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2239059653\",\\n            \"name\": \"Chao Liao\"\\n          },\\n          {\\n            \"authorId\": \"2239091862\",\\n            \"name\": \"Jianchao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2007771781\",\\n            \"name\": \"Quzhe Huang\"\\n          },\\n          {\\n            \"authorId\": \"2230906921\",\\n            \"name\": \"Bin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2366079231\",\\n            \"name\": \"Chenyi Lei\"\\n          },\\n          {\\n            \"authorId\": \"2239069665\",\\n            \"name\": \"An Liu\"\\n          },\\n          {\\n            \"authorId\": \"2241686105\",\\n            \"name\": \"Chengru Song\"\\n          },\\n          {\\n            \"authorId\": \"2238955477\",\\n            \"name\": \"Xiaoqiang Lei\"\\n          },\\n          {\\n            \"authorId\": \"2228125963\",\\n            \"name\": \"Di Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2238953778\",\\n            \"name\": \"Wenwu Ou\"\\n          },\\n          {\\n            \"authorId\": \"2238953242\",\\n            \"name\": \"Kun Gai\"\\n          },\\n          {\\n            \"authorId\": \"2238953689\",\\n            \"name\": \"Yadong Mu\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model\\'s potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.\"\\n      },\\n      {\\n        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"58700f3740105e3422eb030305372b6d8bc44986\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58700f3740105e3422eb030305372b6d8bc44986\",\\n        \"title\": \"Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours, and can serve as an orthogonal optimization for synergistic integration with existing speculative decoding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2242179319\",\\n            \"name\": \"H. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2303652170\",\\n            \"name\": \"Wayne Luk\"\\n          },\\n          {\\n            \"authorId\": \"2301711440\",\\n            \"name\": \"Ka-Fai Cedric Yiu\"\\n          },\\n          {\\n            \"authorId\": \"2152153064\",\\n            \"name\": \"Rui Li\"\\n          },\\n          {\\n            \"authorId\": \"2303652428\",\\n            \"name\": \"Konstantin Mishchenko\"\\n          },\\n          {\\n            \"authorId\": \"2115955596\",\\n            \"name\": \"Stylianos I. Venieris\"\\n          },\\n          {\\n            \"authorId\": \"10001427\",\\n            \"name\": \"Hongxiang Fan\"\\n          }\\n        ],\\n        \"abstract\": \"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\\\\\\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\\\\\\\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.\"\\n      },\\n      {\\n        \"paperId\": \"427c3a502d132b6e1cea2d5565460d284db6e3f7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/427c3a502d132b6e1cea2d5565460d284db6e3f7\",\\n        \"title\": \"Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2507.00601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies and introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-06-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2345186311\",\\n            \"name\": \"Shuangquan Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2353085449\",\\n            \"name\": \"Yingnan Deng\"\\n          },\\n          {\\n            \"authorId\": \"2372425942\",\\n            \"name\": \"Guiran Liu\"\\n          },\\n          {\\n            \"authorId\": \"2374351029\",\\n            \"name\": \"Zhen Qi\"\\n          },\\n          {\\n            \"authorId\": \"2372322790\",\\n            \"name\": \"Ruotong Wang\"\\n          }\\n        ],\\n        \"abstract\": \"This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model\\'s original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method\\'s applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.\"\\n      },\\n      {\\n        \"paperId\": \"c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"title\": \"Dynamic and Adaptive Feature Generation with LLM\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research introduces a novel approach adopting large language models and feature-generating prompts to address challenges of explainability, applicability, and inflexible strategy and proposes a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130031226\",\\n            \"name\": \"XinHao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108045855\",\\n            \"name\": \"Jinghan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1966492\",\\n            \"name\": \"Banafsheh Rekabdar\"\\n          },\\n          {\\n            \"authorId\": \"2145108199\",\\n            \"name\": \"Yuanchun Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2301248160\",\\n            \"name\": \"Pengfei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293571072\",\\n            \"name\": \"Kunpeng Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\\n        \"title\": \"Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.04295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process and demonstrates measurable performance improvements compared to content-only optimization methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2344555074\",\\n            \"name\": \"Yuanye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257094139\",\\n            \"name\": \"Jiahang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2274195530\",\\n            \"name\": \"L. Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2344193091\",\\n            \"name\": \"Qi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2341721557\",\\n            \"name\": \"Xuan Feng\"\\n          },\\n          {\\n            \"authorId\": \"2344520491\",\\n            \"name\": \"Yang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2339241318\",\\n            \"name\": \"Zhongxin Guo\"\\n          },\\n          {\\n            \"authorId\": \"2344097630\",\\n            \"name\": \"Yuqing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2296029993\",\\n            \"name\": \"Peng Cheng\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 130,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"e3e3aa5576de899b755100db211501bb405aba3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3e3aa5576de899b755100db211501bb405aba3e\",\\n        \"title\": \"Firewalls to Secure Dynamic LLM Agentic Networks\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.01822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work identifies required properties for agent communication: proactivity, adaptability, privacy, privacy, and security, and proposes a practical design and protocol inspired by network security principles.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2343752567\",\\n            \"name\": \"Sahar Abdelnabi\"\\n          },\\n          {\\n            \"authorId\": \"2249532110\",\\n            \"name\": \"Amr Gomaa\"\\n          },\\n          {\\n            \"authorId\": \"36103467\",\\n            \"name\": \"Eugene Bagdasarian\"\\n          },\\n          {\\n            \"authorId\": \"2237674591\",\\n            \"name\": \"P. O. Kristensson\"\\n          },\\n          {\\n            \"authorId\": \"2346834097\",\\n            \"name\": \"Reza Shokri\"\\n          }\\n        ],\\n        \"abstract\": \"LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.\"\\n      },\\n      {\\n        \"paperId\": \"ee552989a03693a441863af4c29dc594bfcd1ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ee552989a03693a441863af4c29dc594bfcd1ab5\",\\n        \"title\": \"AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2503.18891?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2503.18891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316787389\",\\n            \"name\": \"Zhexuan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2306069252\",\\n            \"name\": \"Yutong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256344322\",\\n            \"name\": \"Xuebo Liu\"\\n          },\\n          {\\n            \"authorId\": \"46573238\",\\n            \"name\": \"Liang Ding\"\\n          },\\n          {\\n            \"authorId\": \"2187384924\",\\n            \"name\": \"Miao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2348727938\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2346352158\",\\n            \"name\": \"Min Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents\\\\u2019 communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout , which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yufeng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"273b2c64d675edd522cd6f679891756ad5207296\",\\n        \"url\": \"https://www.semanticscholar.org/paper/273b2c64d675edd522cd6f679891756ad5207296\",\\n        \"title\": \"Exploring the Effectiveness of LLM Domain Adaptation for Business IT Machine Translation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://aclanthology.org/2024.eamt-1.51, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that while LLMs can translate on-par with SAP\\\\u2019s MT models on general domain data, it is difficult to close the gap on SAP\\\\u2019s domain-specific data, even with extensive training and carefully curated data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2322393313\",\\n            \"name\": \"Johannes Eschbach-Dymanus\"\\n          },\\n          {\\n            \"authorId\": \"2322400027\",\\n            \"name\": \"Frank Essenberger\"\\n          },\\n          {\\n            \"authorId\": \"1403814959\",\\n            \"name\": \"Bianka Buschbeck-Wolf\"\\n          },\\n          {\\n            \"authorId\": \"70124681\",\\n            \"name\": \"Miriam Exel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\\n        \"title\": \"ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities\",\\n        \"citationCount\": 81,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283831590\",\\n            \"name\": \"Jiarui Lu\"\\n          },\\n          {\\n            \"authorId\": \"2315811087\",\\n            \"name\": \"Thomas Holleis\"\\n          },\\n          {\\n            \"authorId\": \"2313695880\",\\n            \"name\": \"Yizhe Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2315810003\",\\n            \"name\": \"Bernhard Aumayer\"\\n          },\\n          {\\n            \"authorId\": \"2313640225\",\\n            \"name\": \"Feng Nan\"\\n          },\\n          {\\n            \"authorId\": \"2313910532\",\\n            \"name\": \"Felix Bai\"\\n          },\\n          {\\n            \"authorId\": \"2313694040\",\\n            \"name\": \"Shuang Ma\"\\n          },\\n          {\\n            \"authorId\": \"2313694042\",\\n            \"name\": \"Shen Ma\"\\n          },\\n          {\\n            \"authorId\": \"2315946702\",\\n            \"name\": \"Mengyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2293171017\",\\n            \"name\": \"Guoli Yin\"\\n          },\\n          {\\n            \"authorId\": \"2313671930\",\\n            \"name\": \"Zirui Wang\"\\n          },\\n          {\\n            \"authorId\": \"2238621132\",\\n            \"name\": \"Ruoming Pang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at https://github.com/apple/ToolSandbox\"\\n      },\\n      {\\n        \"paperId\": \"44b0d2e884efa5344e50424dbe2edf616981f201\",\\n        \"url\": \"https://www.semanticscholar.org/paper/44b0d2e884efa5344e50424dbe2edf616981f201\",\\n        \"title\": \"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.00807\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.00807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work develops and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply and achieves substantially lower latency than standard reranking methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2127522115\",\\n            \"name\": \"Jon Saad-Falcon\"\\n          },\\n          {\\n            \"authorId\": \"144112155\",\\n            \"name\": \"O. Khattab\"\\n          },\\n          {\\n            \"authorId\": \"50818255\",\\n            \"name\": \"Keshav Santhanam\"\\n          },\\n          {\\n            \"authorId\": \"1707117\",\\n            \"name\": \"Radu Florian\"\\n          },\\n          {\\n            \"authorId\": \"39038065\",\\n            \"name\": \"M. Franz\"\\n          },\\n          {\\n            \"authorId\": \"1781292\",\\n            \"name\": \"S. Roukos\"\\n          },\\n          {\\n            \"authorId\": \"2707234\",\\n            \"name\": \"Avirup Sil\"\\n          },\\n          {\\n            \"authorId\": \"2937809\",\\n            \"name\": \"Md Arafat Sultan\"\\n          },\\n          {\\n            \"authorId\": \"144922861\",\\n            \"name\": \"Christopher Potts\"\\n          }\\n        ],\\n        \"abstract\": \"Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.\"\\n      },\\n      {\\n        \"paperId\": \"9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\\n        \"title\": \"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\",\\n        \"citationCount\": 212,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.02490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing, is introduced, which effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2181120463\",\\n            \"name\": \"Huiqiang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"1527099159\",\\n            \"name\": \"Yucheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2284970741\",\\n            \"name\": \"Chengruidong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108728536\",\\n            \"name\": \"Qianhui Wu\"\\n          },\\n          {\\n            \"authorId\": \"13289447\",\\n            \"name\": \"Xufang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2309738728\",\\n            \"name\": \"Surin Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2281867465\",\\n            \"name\": \"Zhenhua Han\"\\n          },\\n          {\\n            \"authorId\": \"2309244780\",\\n            \"name\": \"Amir H. Abdi\"\\n          },\\n          {\\n            \"authorId\": \"2305587638\",\\n            \"name\": \"Dongsheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2257359863\",\\n            \"name\": \"Chin-Yew Lin\"\\n          },\\n          {\\n            \"authorId\": \"2125051198\",\\n            \"name\": \"Yuqing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2160727304\",\\n            \"name\": \"Lili Qiu\"\\n          }\\n        ],\\n        \"abstract\": \"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\"\\n      },\\n      {\\n        \"paperId\": \"34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\\n        \"title\": \"GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting\",\\n        \"citationCount\": 91,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/30383/32447\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i21.30383?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i21.30383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously simultaneously, named GPT4MTS, and proposes a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2293923697\",\\n            \"name\": \"Kevin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2260272787\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.\"\\n      },\\n      {\\n        \"paperId\": \"c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\\n        \"title\": \"Do LLMs Understand Visual Anomalies? Uncovering LLM\\'s Capabilities in Zero-shot Anomaly Detection\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.09654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM), enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108678212\",\\n            \"name\": \"Jiaqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"51459472\",\\n            \"name\": \"Shaofeng Cai\"\\n          },\\n          {\\n            \"authorId\": \"2276607267\",\\n            \"name\": \"Fang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2188240935\",\\n            \"name\": \"Bengchin Ooi\"\\n          },\\n          {\\n            \"authorId\": \"2296743990\",\\n            \"name\": \"Junran Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA\\'s effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.\"\\n      },\\n      {\\n        \"paperId\": \"8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"title\": \"FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.15813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2247164321\",\\n            \"name\": \"Yu Lu\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"3446334\",\\n            \"name\": \"Hehe Fan\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n      },\\n      {\\n        \"paperId\": \"162f33c7799683ca9b0f193275fe7eec5a0b973f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/162f33c7799683ca9b0f193275fe7eec5a0b973f\",\\n        \"title\": \"EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.09618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EasyRef is introduced, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt and surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1571400317\",\\n            \"name\": \"Zhuofan Zong\"\\n          },\\n          {\\n            \"authorId\": \"2293242031\",\\n            \"name\": \"Dongzhi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2261489892\",\\n            \"name\": \"Bingqi Ma\"\\n          },\\n          {\\n            \"authorId\": \"12920342\",\\n            \"name\": \"Guanglu Song\"\\n          },\\n          {\\n            \"authorId\": \"2075457131\",\\n            \"name\": \"Hao Shao\"\\n          },\\n          {\\n            \"authorId\": \"2292263397\",\\n            \"name\": \"Dazhong Shen\"\\n          },\\n          {\\n            \"authorId\": \"2292207974\",\\n            \"name\": \"Yu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2261394248\",\\n            \"name\": \"Hongsheng Li\"\\n          }\\n        ],\\n        \"abstract\": \"Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although the tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent elements within multiple images through the training process, it necessitates specific finetuning for each distinct image group. This paper introduces EasyRef, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM\\'s representations into the diffusion process through adapters can easily generalize to unseen domains, mining the consistent visual elements within unseen data. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\\n      },\\n      {\\n        \"paperId\": \"ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"title\": \"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.10462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation, is proposed and experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2241468593\",\\n            \"name\": \"Hossein Rajabzadeh\"\\n          },\\n          {\\n            \"authorId\": \"9200111\",\\n            \"name\": \"Mojtaba Valipour\"\\n          },\\n          {\\n            \"authorId\": \"2284643707\",\\n            \"name\": \"Tianshu Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1996315\",\\n            \"name\": \"Marzieh S. Tahaei\"\\n          },\\n          {\\n            \"authorId\": \"2241480742\",\\n            \"name\": \"Hyock Ju Kwon\"\\n          },\\n          {\\n            \"authorId\": \"2237425782\",\\n            \"name\": \"Ali Ghodsi\"\\n          },\\n          {\\n            \"authorId\": \"2237517964\",\\n            \"name\": \"Boxing Chen\"\\n          },\\n          {\\n            \"authorId\": \"2066076226\",\\n            \"name\": \"Mehdi Rezagholizadeh\"\\n          }\\n        ],\\n        \"abstract\": \"Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n      },\\n      {\\n        \"paperId\": \"9a73effed8775962c86587feb0f9ef841fa2ff4c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a73effed8775962c86587feb0f9ef841fa2ff4c\",\\n        \"title\": \"LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258550477\",\\n            \"name\": \"Yash Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2258751660\",\\n            \"name\": \"Wenchang Gao\"\\n          },\\n          {\\n            \"authorId\": \"3379438\",\\n            \"name\": \"Vasanth Sarathy\"\\n          },\\n          {\\n            \"authorId\": \"2258715054\",\\n            \"name\": \"Alvaro Velasquez\"\\n          },\\n          {\\n            \"authorId\": \"2258551993\",\\n            \"name\": \"Robert Wright\"\\n          },\\n          {\\n            \"authorId\": \"1715858\",\\n            \"name\": \"Jivko Sinapov\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a propreitary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the LLM proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.\"\\n      },\\n      {\\n        \"paperId\": \"108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\\n        \"url\": \"https://www.semanticscholar.org/paper/108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\\n        \"title\": \"TimeRAG: Boosting LLM Time Series Forecasting via Retrieval-Augmented Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.16643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TimeRAG is proposed, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2206558646\",\\n            \"name\": \"Si-Nan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2337389407\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2336916957\",\\n            \"name\": \"Haoqi Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2336884221\",\\n            \"name\": \"Ruochun Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.\"\\n      },\\n      {\\n        \"paperId\": \"1e6325865e809670765bea9dadd3c40b2014eb6d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e6325865e809670765bea9dadd3c40b2014eb6d\",\\n        \"title\": \"VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The performance contribution of different prompt components is explored, the vision for future improvement in visual accessibility, and the way for LLMs in video anomaly detection and vision-language understanding is paved.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256768778\",\\n            \"name\": \"Hao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292146470\",\\n            \"name\": \"Jiayou Qin\"\\n          },\\n          {\\n            \"authorId\": \"2260725391\",\\n            \"name\": \"Ashish Bastola\"\\n          },\\n          {\\n            \"authorId\": \"2024833342\",\\n            \"name\": \"Xiwen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2292183597\",\\n            \"name\": \"John Suchanek\"\\n          },\\n          {\\n            \"authorId\": \"2292143727\",\\n            \"name\": \"Zihao Gong\"\\n          },\\n          {\\n            \"authorId\": \"2064311884\",\\n            \"name\": \"Abolfazl Razi\"\\n          }\\n        ],\\n        \"abstract\": \"This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.\"\\n      },\\n      {\\n        \"paperId\": \"7f96bb27a8fca35b1f7d02ee319a64be04114809\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7f96bb27a8fca35b1f7d02ee319a64be04114809\",\\n        \"title\": \"LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.08337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties, and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265927745\",\\n            \"name\": \"Maonan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2219695355\",\\n            \"name\": \"Aoyu Pang\"\\n          },\\n          {\\n            \"authorId\": \"7592365\",\\n            \"name\": \"Yuheng Kan\"\\n          },\\n          {\\n            \"authorId\": \"144305489\",\\n            \"name\": \"Man-On Pun\"\\n          },\\n          {\\n            \"authorId\": \"2292117616\",\\n            \"name\": \"Chung Shue Chen\"\\n          },\\n          {\\n            \"authorId\": \"2291077490\",\\n            \"name\": \"Bo Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system\\'s adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\\\\\\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.\"\\n      },\\n      {\\n        \"paperId\": \"650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"title\": \"Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.acl-srw.90?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.acl-srw.90, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Time-LlaMA framework achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2346152667\",\\n            \"name\": \"Juyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334602368\",\\n            \"name\": \"Jiechao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2374971695\",\\n            \"name\": \"Wenwen Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2334718086\",\\n            \"name\": \"Wei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2321408906\",\\n            \"name\": \"Hui Yi Leong\"\\n          }\\n        ],\\n        \"abstract\": \"Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model\\\\u2019s predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 212,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\\n        \"title\": \"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair\",\\n        \"citationCount\": 188,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2176865575\",\\n            \"name\": \"Islem Bouzenia\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"2260683361\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": \"Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent\\'s effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI\\'s GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\\n      },\\n      {\\n        \"paperId\": \"3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\\n        \"title\": \"Token-Budget-Aware LLM Reasoning\",\\n        \"citationCount\": 114,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem is proposed, offering a practical solution to balance efficiency and accuracy in LLM reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170360833\",\\n            \"name\": \"Tingxu Han\"\\n          },\\n          {\\n            \"authorId\": \"2154723145\",\\n            \"name\": \"Zhenting Wang\"\\n          },\\n          {\\n            \"authorId\": \"2239197945\",\\n            \"name\": \"Chunrong Fang\"\\n          },\\n          {\\n            \"authorId\": \"2110773055\",\\n            \"name\": \"Shiyun Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2333472479\",\\n            \"name\": \"Shiqing Ma\"\\n          },\\n          {\\n            \"authorId\": \"2238950128\",\\n            \"name\": \"Zhenyu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE\"\\n      },\\n      {\\n        \"paperId\": \"20843eaa59db5e2af416d7db47d51d0aab3de230\",\\n        \"url\": \"https://www.semanticscholar.org/paper/20843eaa59db5e2af416d7db47d51d0aab3de230\",\\n        \"title\": \"Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue Resolution\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.11709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper analyzes 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution and identifies key textual and code-related heuristics\\\\u2014Specificity, Contextual Richness, and Clarity\\\\u2014that are associated with successful issue closure and help assess prompt quality.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2147154362\",\\n            \"name\": \"Ramtin Ehsani\"\\n          },\\n          {\\n            \"authorId\": \"2341336471\",\\n            \"name\": \"Sakshi Pathak\"\\n          },\\n          {\\n            \"authorId\": \"9728244\",\\n            \"name\": \"Preetha Chatterjee\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have become essential in software development, especially for issue resolution. However, despite their widespread use, significant challenges persist in the quality of LLM responses to issue resolution queries. LLM interactions often yield incorrect, incomplete, or ambiguous information, largely due to knowledge gaps in prompt design, which can lead to unproductive exchanges and reduced developer productivity.In this paper, we analyze 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution. We identify four main knowledge gaps in developer prompts: Missing Context, Missing Specifications, Multiple Context, and Unclear Instructions. Assuming that conversations within closed issues contributed to successful resolutions while those in open issues did not, we find that ineffective conversations contain knowledge gaps in $44.6 \\\\\\\\%$ of prompts, compared to only $12.6 \\\\\\\\%$ in effective ones. Additionally, we observe seven distinct conversational styles, with Directive Prompting, Chain of Thought, and Responsive Feedback being the most prevalent. We find that knowledge gaps are present in all styles of conversations, with Missing Context being the most repeated challenge developers face in issue-resolution conversations.Based on our analysis, we identify key textual and code-related heuristics\\\\u2014Specificity, Contextual Richness, and Clarity\\\\u2014that are associated with successful issue closure and help assess prompt quality. These heuristics lay the foundation for an automated tool that can dynamically flag unclear prompts and suggest structured improvements. To test feasibility, we developed a lightweight browser extension prototype for detecting prompt gaps, that can be easily adapted to other tools within developer workflows.\"\\n      },\\n      {\\n        \"paperId\": \"25cee84e3a1541697a7c97443d7526574127c344\",\\n        \"url\": \"https://www.semanticscholar.org/paper/25cee84e3a1541697a7c97443d7526574127c344\",\\n        \"title\": \"Don\\'t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\",\\n        \"citationCount\": 158,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.00367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs, and proposes two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2114887261\",\\n            \"name\": \"Shangbin Feng\"\\n          },\\n          {\\n            \"authorId\": \"2254168375\",\\n            \"name\": \"Weijia Shi\"\\n          },\\n          {\\n            \"authorId\": \"2108853330\",\\n            \"name\": \"Yike Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282214127\",\\n            \"name\": \"Wenxuan Ding\"\\n          },\\n          {\\n            \"authorId\": \"143820870\",\\n            \"name\": \"Vidhisha Balachandran\"\\n          },\\n          {\\n            \"authorId\": \"2249583325\",\\n            \"name\": \"Yulia Tsvetkov\"\\n          }\\n        ],\\n        \"abstract\": \"Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"d5342fce96175f83550cfae471a0a46d16401481\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d5342fce96175f83550cfae471a0a46d16401481\",\\n        \"title\": \"ST-LLM: Large Language Models Are Effective Temporal Learners\",\\n        \"citationCount\": 120,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM, and develops a dynamic masking strategy with tailor-made training objectives to address the overhead and stability issues introduced by uncompressed video tokens within LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"12287885\",\\n            \"name\": \"Ruyang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256784925\",\\n            \"name\": \"Chen Li\"\\n          },\\n          {\\n            \"authorId\": \"2294629231\",\\n            \"name\": \"Haoran Tang\"\\n          },\\n          {\\n            \"authorId\": \"152988335\",\\n            \"name\": \"Yixiao Ge\"\\n          },\\n          {\\n            \"authorId\": \"2265579883\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2294517847\",\\n            \"name\": \"Ge Li\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at https://github.com/TencentARC/ST-LLM.\"\\n      },\\n      {\\n        \"paperId\": \"12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\\n        \"title\": \"Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.17282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with, and performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218970025\",\\n            \"name\": \"Yi-Kai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2274937712\",\\n            \"name\": \"De-Chuan Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2151459740\",\\n            \"name\": \"Han-Jia Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"edfff0e15449f438a13a7341290c008bf6486afc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edfff0e15449f438a13a7341290c008bf6486afc\",\\n        \"title\": \"MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.18035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism, which significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2327293591\",\\n            \"name\": \"Jingfan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2322997112\",\\n            \"name\": \"Yi Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2327694040\",\\n            \"name\": \"Dan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2293390999\",\\n            \"name\": \"Xing Tian\"\\n          },\\n          {\\n            \"authorId\": \"2179528564\",\\n            \"name\": \"Huanran Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2322888603\",\\n            \"name\": \"Wei Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 203,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"ab4800a924508f49d644ced8ba236ec92f54f566\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ab4800a924508f49d644ced8ba236ec92f54f566\",\\n        \"title\": \"Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2505.16552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach and enhances CoLaR through reinforcement learning (RL) that leverages the latent head\\'s non-deterministic nature to explore diverse reasoning paths and exploit more compact ones.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218557953\",\\n            \"name\": \"Wenhui Tan\"\\n          },\\n          {\\n            \"authorId\": \"2362865102\",\\n            \"name\": \"Jiaze Li\"\\n          },\\n          {\\n            \"authorId\": \"2317982861\",\\n            \"name\": \"Jianzhong Ju\"\\n          },\\n          {\\n            \"authorId\": \"2363405807\",\\n            \"name\": \"Zhenbo Luo\"\\n          },\\n          {\\n            \"authorId\": \"2317980688\",\\n            \"name\": \"Jian Luan\"\\n          },\\n          {\\n            \"authorId\": \"2290923147\",\\n            \"name\": \"Ruihua Song\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head\\'s non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance.\"\\n      },\\n      {\\n        \"paperId\": \"268e28f8d5235031dcd7bfae0f857439e27e8564\",\\n        \"url\": \"https://www.semanticscholar.org/paper/268e28f8d5235031dcd7bfae0f857439e27e8564\",\\n        \"title\": \"SteP: Stacked LLM Policies for Web Actions\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.03720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Stacked LLM Policies for Web Actions is proposed, an approach to dynamically compose policies to solve a diverse set of web tasks and is competitive with prior works while using significantly less data.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-10-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2254267756\",\\n            \"name\": \"Paloma Sodhi\"\\n          },\\n          {\\n            \"authorId\": \"1741598\",\\n            \"name\": \"S. Branavan\"\\n          },\\n          {\\n            \"authorId\": \"2066324938\",\\n            \"name\": \"Yoav Artzi\"\\n          },\\n          {\\n            \"authorId\": \"2254260284\",\\n            \"name\": \"Ryan McDonald\"\\n          }\\n        ],\\n        \"abstract\": \"Performing tasks on the web presents fundamental challenges to large language models (LLMs), including combinatorially large open-world tasks and variations across web interfaces. Simply specifying a large prompt to handle all possible behaviors and states is extremely complex, and results in behavior leaks between unrelated behaviors. Decomposition to distinct policies can address this challenge, but requires carefully handing off control between policies. We propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically compose policies to solve a diverse set of web tasks. SteP defines a Markov Decision Process where the state is a stack of policies representing the control state, i.e., the chain of policy calls. Unlike traditional methods that are restricted to static hierarchies, SteP enables dynamic control that adapts to the complexity of the task. We evaluate SteP against multiple baselines and web environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP improves (14.9\\\\\\\\% to 33.5\\\\\\\\%) over SOTA that use GPT-4 policies, while on MiniWob++, SteP is competitive with prior works while using significantly less data. Our code and data are available at https://asappresearch.github.io/webagents-step.\"\\n      },\\n      {\\n        \"paperId\": \"1f9822022f586e375461660db792f23e891c7123\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1f9822022f586e375461660db792f23e891c7123\",\\n        \"title\": \"Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems\",\\n        \"citationCount\": 62,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2307.06187\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.06187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models, grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2658311\",\\n            \"name\": \"N. Nascimento\"\\n          },\\n          {\\n            \"authorId\": \"40761174\",\\n            \"name\": \"Paulo Alencar\"\\n          },\\n          {\\n            \"authorId\": \"2149928782\",\\n            \"name\": \"Donald D. Cowan\"\\n          }\\n        ],\\n        \"abstract\": \"The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs\\' capabilities and indicating further research opportunities to assess LLMs\\' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.\"\\n      },\\n      {\\n        \"paperId\": \"3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\\n        \"title\": \"Bridging the Gulf of Envisioning: Cognitive Challenges in Prompt Based Interactions with LLMs\",\\n        \"citationCount\": 91,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642754\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work theorizes how end-users \\\\u2018envision\\\\u2019 translating their goals into clear intentions and craft prompts to obtain the desired LLM response.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2301051721\",\\n            \"name\": \"Hariharan Subramonyam\"\\n          },\\n          {\\n            \"authorId\": \"2246886383\",\\n            \"name\": \"Roy D. Pea\"\\n          },\\n          {\\n            \"authorId\": \"1825757380\",\\n            \"name\": \"Christopher Pondoc\"\\n          },\\n          {\\n            \"authorId\": \"1820412\",\\n            \"name\": \"Maneesh Agrawala\"\\n          },\\n          {\\n            \"authorId\": \"2289103973\",\\n            \"name\": \"Colleen M. Seifert\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman\\\\u2019s gulfs of execution and evaluation. To address this gap, we theorize how end-users \\\\u2018envision\\\\u2019 translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments on not knowing: (1) what the task should be, (2) how to instruct the LLM to do the task, and (3) what to expect for the LLM\\\\u2019s output in meeting the goal. Finally, we make recommendations to narrow the gulf of envisioning in human-LLM interactions.\"\\n      },\\n      {\\n        \"paperId\": \"28d6411019f448f54834c2a5cff723cd350345b5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/28d6411019f448f54834c2a5cff723cd350345b5\",\\n        \"title\": \"Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC) involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2289252454\",\\n            \"name\": \"Haokun Liu\"\\n          },\\n          {\\n            \"authorId\": \"8247318\",\\n            \"name\": \"Yaonan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2110285301\",\\n            \"name\": \"Kenji Kato\"\\n          },\\n          {\\n            \"authorId\": \"2307380233\",\\n            \"name\": \"Atsushi Tsukahara\"\\n          },\\n          {\\n            \"authorId\": \"2282115338\",\\n            \"name\": \"Izumi Kondo\"\\n          },\\n          {\\n            \"authorId\": \"1752849\",\\n            \"name\": \"T. Aoyama\"\\n          },\\n          {\\n            \"authorId\": \"2237520520\",\\n            \"name\": \"Yasuhisa Hasegawa\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This letter proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.\"\\n      },\\n      {\\n        \"paperId\": \"77a9c310df0d7896d297da90fc4a1131819c341e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/77a9c310df0d7896d297da90fc4a1131819c341e\",\\n        \"title\": \"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding, and proposes LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293615241\",\\n            \"name\": \"Mingxing Peng\"\\n          },\\n          {\\n            \"authorId\": \"2293665950\",\\n            \"name\": \"Xusen Guo\"\\n          },\\n          {\\n            \"authorId\": \"2146413818\",\\n            \"name\": \"Xianda Chen\"\\n          },\\n          {\\n            \"authorId\": \"2241024418\",\\n            \"name\": \"Meixin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2267078966\",\\n            \"name\": \"Kehua Chen\"\\n          },\\n          {\\n            \"authorId\": \"2293775145\",\\n            \"name\": \"Hao Yang\"\\n          },\\n          {\\n            \"authorId\": \"2258778041\",\\n            \"name\": \"Xuesong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2258755636\",\\n            \"name\": \"Yinhai Wang\"\\n          }\\n        ],\\n        \"abstract\": \"To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.\"\\n      }\\n    ]\\n  },\\n  \"coherence in extended LLM\": {\\n    \"total\": 23,\\n    \"offset\": 0,\\n    \"data\": [\\n      {\\n        \"paperId\": \"d0da372b4b6f422e13556ce272595a0c9002fe90\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d0da372b4b6f422e13556ce272595a0c9002fe90\",\\n        \"title\": \"GPT-4 in Education: Evaluating Aptness, Reliability, and Loss of Coherence in Solving Calculus Problems and Grading Submissions\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s40593-024-00403-3.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40593-024-00403-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40593-024-00403-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256969876\",\\n            \"name\": \"Alberto Gandolfi\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we initially investigate the capabilities of GPT-3 5 and GPT-4 in solving college-level calculus problems, an essential segment of mathematics that remains under-explored so far. Although improving upon earlier versions, GPT-4 attains approximately 65% accuracy for standard problems and decreases to 20% for competition-like scenarios. Overall, the models prove to be unreliable due to common arithmetic errors. Our primary contribution lies then in examining the use of ChatGPT for grading solutions to calculus exercises. Our objectives are to probe an in-context learning task with less emphasis over direct calculations; recognize positive applications of ChatGPT in educational contexts; highlight a potentially emerging facet of AI that could necessitate oversight; and introduce unconventional AI benchmarks, for which models like GPT are untrained. Pertaining to the latter, we uncover a tendency for loss of coherence in extended contexts. Our findings suggest that while the current ChatGPT exhibits comprehension of the grading task and often provides relevant outputs, the consistency of grading is marred by occasional loss of coherence and hallucinations. Intriguingly, GPT-4\\'s overall scores, delivered in mere moments, align closely with human graders, although its detailed accuracy remains suboptimal. This work suggests that, when appropriately orchestrated, collaboration between human graders and LLMs like GPT-4 might combine their unique strengths while mitigating their respective shortcomings In this direction, it is imperative to consider implementing transparency, fairness, and appropriate regulations in the near future.\"\\n      },\\n      {\\n        \"paperId\": \"49b7bd275a0386392769f5b33028500754dbc69d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/49b7bd275a0386392769f5b33028500754dbc69d\",\\n        \"title\": \"AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE), and reveals that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2317040111\",\\n            \"name\": \"Abhay Gupta\"\\n          },\\n          {\\n            \"authorId\": \"2317010916\",\\n            \"name\": \"Philip Meng\"\\n          },\\n          {\\n            \"authorId\": \"2317007185\",\\n            \"name\": \"Ece Yurtseven\"\\n          },\\n          {\\n            \"authorId\": \"2241351144\",\\n            \"name\": \"Sean O\\'Brien\"\\n          },\\n          {\\n            \"authorId\": \"2312105716\",\\n            \"name\": \"Kevin Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting biases in natural language understanding (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language processing (NLP) systems. To address dialect-induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE). AAVENUE builds upon and extends existing benchmarks like VALUE, replacing deterministic syntactic and morphological transformations with a more flexible methodology leveraging LLM-based translation with few-shot prompting, improving performance across our evaluation metrics when translating key tasks from the GLUE and SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics including fluency, BARTScore, quality, coherence, and understandability. Additionally, we recruit fluent AAVE speakers to validate our translations for authenticity. Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\\n      },\\n      {\\n        \"paperId\": \"e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\\n        \"title\": \"DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.findings-emnlp.606.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME), which proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2110546424\",\\n            \"name\": \"Weijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261645232\",\\n            \"name\": \"Wenxiang Hu\"\\n          },\\n          {\\n            \"authorId\": \"2261413304\",\\n            \"name\": \"Fanyou Wu\"\\n          },\\n          {\\n            \"authorId\": \"1757518\",\\n            \"name\": \"Srinivasan H. Sengamedu\"\\n          }\\n        ],\\n        \"abstract\": \"In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME\\'s potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\\n      },\\n      {\\n        \"paperId\": \"0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\\n        \"title\": \"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An evaluation framework for mental health chatbots is validated, proving its effectiveness in improving safety and reliability and highlighting the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-08-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2315914561\",\\n            \"name\": \"Jung In Park\"\\n          },\\n          {\\n            \"authorId\": \"2182148069\",\\n            \"name\": \"Mahyar Abbasian\"\\n          },\\n          {\\n            \"authorId\": \"2241201441\",\\n            \"name\": \"Iman Azimi\"\\n          },\\n          {\\n            \"authorId\": \"2315810363\",\\n            \"name\": \"Dawn Bounds\"\\n          },\\n          {\\n            \"authorId\": \"2315810053\",\\n            \"name\": \"Angela Jun\"\\n          },\\n          {\\n            \"authorId\": \"2315889398\",\\n            \"name\": \"Jaesu Han\"\\n          },\\n          {\\n            \"authorId\": \"2315811390\",\\n            \"name\": \"Robert McCarron\"\\n          },\\n          {\\n            \"authorId\": \"2297708916\",\\n            \"name\": \"Jessica Borelli\"\\n          },\\n          {\\n            \"authorId\": \"2348273518\",\\n            \"name\": \"Parmida Safavi\"\\n          },\\n          {\\n            \"authorId\": \"2348305728\",\\n            \"name\": \"Sanaz Mirbaha\"\\n          },\\n          {\\n            \"authorId\": \"2315875066\",\\n            \"name\": \"Jia Li\"\\n          },\\n          {\\n            \"authorId\": \"2315811652\",\\n            \"name\": \"Mona Mahmoudi\"\\n          },\\n          {\\n            \"authorId\": \"2315811354\",\\n            \"name\": \"Carmen Wiedenhoeft\"\\n          },\\n          {\\n            \"authorId\": \"2311169857\",\\n            \"name\": \"Amir M. Rahmani\"\\n          }\\n        ],\\n        \"abstract\": \"Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Materials and Methods: We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. Results: The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Discussion: Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. Conclusion: The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.\"\\n      },\\n      {\\n        \"paperId\": \"09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\\n        \"url\": \"https://www.semanticscholar.org/paper/09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\\n        \"title\": \"Asynchronous LLM Function Calling\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.07017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An in-context protocol for function calls and interrupts is designed, an fine-tuning strategy is provided to adapt LLMs to the interrupt semantics, and mechanisms to asynchronously notify the LLM in-flight when function calls return are implemented.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265756791\",\\n            \"name\": \"In Gim\"\\n          },\\n          {\\n            \"authorId\": \"2118065368\",\\n            \"name\": \"Seung-seob Lee\"\\n          },\\n          {\\n            \"authorId\": \"2323908057\",\\n            \"name\": \"Lin Zhong\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM\\'s operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call\\'s completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.\"\\n      },\\n      {\\n        \"paperId\": \"3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\\n        \"title\": \"Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-024-56309-6.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10925587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies, and demonstrating a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260831947\",\\n            \"name\": \"Juho Jung\"\\n          },\\n          {\\n            \"authorId\": \"2152569226\",\\n            \"name\": \"Jinyoung Han\"\\n          },\\n          {\\n            \"authorId\": \"6777367\",\\n            \"name\": \"J. Han\"\\n          },\\n          {\\n            \"authorId\": \"2088247339\",\\n            \"name\": \"Junseo Ko\"\\n          },\\n          {\\n            \"authorId\": \"1677558107\",\\n            \"name\": \"Jeewoo Yoon\"\\n          },\\n          {\\n            \"authorId\": \"39548326\",\\n            \"name\": \"J. Hwang\"\\n          },\\n          {\\n            \"authorId\": \"2265721342\",\\n            \"name\": \"Ji In Park\"\\n          },\\n          {\\n            \"authorId\": \"2054436718\",\\n            \"name\": \"Gyudeok Hwang\"\\n          },\\n          {\\n            \"authorId\": \"2290973453\",\\n            \"name\": \"Jae Ho Jung\"\\n          },\\n          {\\n            \"authorId\": \"2003794778\",\\n            \"name\": \"Daniel Duck-Jin Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"Neovascular age-related macular degeneration (nAMD) can result in blindness if left untreated, and patients often require repeated anti-vascular endothelial growth factor injections. Although, the treat-and-extend method is becoming popular to reduce vision loss attributed to recurrence, it may pose a risk of overtreatment. This study aimed to develop a deep learning model based on DenseNet201 to predict nAMD recurrence within 3 months after confirming dry-up 1 month following three loading injections in treatment-na\\\\u00efve patients. A dataset of 1076 spectral domain optical coherence tomography (OCT) images from 269 patients diagnosed with nAMD was used. The performance of the model was compared with that of 6 ophthalmologists, using 100 randomly selected samples. The DenseNet201-based model achieved 53.0% accuracy in predicting nAMD recurrence using a single pre-injection image and 60.2% accuracy after viewing all the images immediately after the 1st, 2nd, and 3rd injections. The model outperformed experienced ophthalmologists, with an average accuracy of 52.17% using a single pre-injection image and 53.3% after examining four images before and after three loading injections. In conclusion, the artificial intelligence model demonstrated a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists. These findings suggest that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies.\"\\n      },\\n      {\\n        \"paperId\": \"b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\\n        \"title\": \"Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.03275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a hybrid representation of the reasoning process, where the initial reasoning steps are partially abstracted away using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2325888815\",\\n            \"name\": \"DiJia Su\"\\n          },\\n          {\\n            \"authorId\": \"2255310892\",\\n            \"name\": \"Hanlin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2269737738\",\\n            \"name\": \"Yingchen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          },\\n          {\\n            \"authorId\": \"2285362895\",\\n            \"name\": \"Yuandong Tian\"\\n          },\\n          {\\n            \"authorId\": \"2326106870\",\\n            \"name\": \"Qinqing Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\\n        \"title\": \"13.4 A 48GB 16-High 1280GB/s HBM3E DRAM with All-Around Power TSV and a 6-Phase RDQS Scheme for TSV Area Optimization\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISSCC49657.2024.10454440?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISSCC49657.2024.10454440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287764300\",\\n            \"name\": \"Jinhyung Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287959684\",\\n            \"name\": \"Kyungjun Cho\"\\n          },\\n          {\\n            \"authorId\": \"2109114249\",\\n            \"name\": \"C. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287956252\",\\n            \"name\": \"Yeonho Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287119443\",\\n            \"name\": \"Jae-Hyung Park\"\\n          },\\n          {\\n            \"authorId\": \"2286899300\",\\n            \"name\": \"Su-Hyun Oh\"\\n          },\\n          {\\n            \"authorId\": \"1640393542\",\\n            \"name\": \"Yucheon Ju\"\\n          },\\n          {\\n            \"authorId\": \"3365606\",\\n            \"name\": \"Chunseok Jeong\"\\n          },\\n          {\\n            \"authorId\": \"123947284\",\\n            \"name\": \"H. Cho\"\\n          },\\n          {\\n            \"authorId\": \"2198615241\",\\n            \"name\": \"Jaeseung Lee\"\\n          },\\n          {\\n            \"authorId\": \"3376046\",\\n            \"name\": \"T. Yun\"\\n          },\\n          {\\n            \"authorId\": \"2292215366\",\\n            \"name\": \"Jin Hee Cho\"\\n          },\\n          {\\n            \"authorId\": \"3375969\",\\n            \"name\": \"Sangmuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"1640397693\",\\n            \"name\": \"J. Moon\"\\n          },\\n          {\\n            \"authorId\": \"2110426052\",\\n            \"name\": \"Y. Park\"\\n          },\\n          {\\n            \"authorId\": \"2291083018\",\\n            \"name\": \"Hong-Seok Choi\"\\n          },\\n          {\\n            \"authorId\": \"2159532637\",\\n            \"name\": \"In-Keun Kim\"\\n          },\\n          {\\n            \"authorId\": \"2286904054\",\\n            \"name\": \"Seung Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"2286933095\",\\n            \"name\": \"Sun-Yeol Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291078685\",\\n            \"name\": \"Jaemin Jang\"\\n          },\\n          {\\n            \"authorId\": \"2292140319\",\\n            \"name\": \"Jinwook Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108644317\",\\n            \"name\": \"S. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2286906698\",\\n            \"name\": \"Younghyun Jeon\"\\n          },\\n          {\\n            \"authorId\": \"2292172289\",\\n            \"name\": \"Juhyung Park\"\\n          },\\n          {\\n            \"authorId\": \"2159571158\",\\n            \"name\": \"Tae-Kyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"82375019\",\\n            \"name\": \"D. Ka\"\\n          },\\n          {\\n            \"authorId\": \"2159321700\",\\n            \"name\": \"Sanghoon Oh\"\\n          },\\n          {\\n            \"authorId\": \"2292143044\",\\n            \"name\": \"Jinse Kim\"\\n          },\\n          {\\n            \"authorId\": \"2159244890\",\\n            \"name\": \"Junyeol Jeon\"\\n          },\\n          {\\n            \"authorId\": \"2292173125\",\\n            \"name\": \"Seonhong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291075551\",\\n            \"name\": \"Kyeong Tae Kim\"\\n          },\\n          {\\n            \"authorId\": \"2154855420\",\\n            \"name\": \"Tae-Hwan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291083403\",\\n            \"name\": \"Hyeonjin Yang\"\\n          },\\n          {\\n            \"authorId\": \"2291072681\",\\n            \"name\": \"Dongju Yang\"\\n          },\\n          {\\n            \"authorId\": \"2291084224\",\\n            \"name\": \"Minseop Lee\"\\n          },\\n          {\\n            \"authorId\": \"30684992\",\\n            \"name\": \"Heewoong Song\"\\n          },\\n          {\\n            \"authorId\": \"2291069292\",\\n            \"name\": \"Dongwook Jang\"\\n          },\\n          {\\n            \"authorId\": \"2287120235\",\\n            \"name\": \"Junghyun Shin\"\\n          },\\n          {\\n            \"authorId\": \"2287261607\",\\n            \"name\": \"Hyunsik Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291041862\",\\n            \"name\": \"Changki Baek\"\\n          },\\n          {\\n            \"authorId\": \"2291023817\",\\n            \"name\": \"Hajun Jeong\"\\n          },\\n          {\\n            \"authorId\": \"2291081668\",\\n            \"name\": \"Jongchan Yoon\"\\n          },\\n          {\\n            \"authorId\": \"1641325386\",\\n            \"name\": \"SeungGyeon Lim\"\\n          },\\n          {\\n            \"authorId\": \"2110630984\",\\n            \"name\": \"Kyo Yun Lee\"\\n          },\\n          {\\n            \"authorId\": \"2159499427\",\\n            \"name\": \"Young Jun Koo\"\\n          },\\n          {\\n            \"authorId\": \"2287107051\",\\n            \"name\": \"Myeong-Jae Park\"\\n          },\\n          {\\n            \"authorId\": \"2510417\",\\n            \"name\": \"Joohwan Cho\"\\n          },\\n          {\\n            \"authorId\": \"2291056079\",\\n            \"name\": \"Jonghwan Kim\"\\n          }\\n        ],\\n        \"abstract\": \"With the emergence of large-language models (LLM) and generative AI, which require an enormous amount of model parameters, the required memory bandwidth and capacity for high-end systems is on an unprecedented increase. To meet this need, we present an extended version of the high-bandwidth memory-3 (HBM3 DRAM), HBM3E, which achieves a 1280GB/s bandwidth with a cube density of 48GB. New design schemes and features, such as all-around power-through-silicon via (TSV), a 6-phase read-data-strobe (RDQS) scheme, a byte-mapping swap scheme, and a voltage-drift compensator for write data strobe (WDQS), are implemented to achieve extended bandwidth and capacity with enhanced reliability. The overall architecture and specifications, such as bump map footprint, the number of channel and I/Os, and the operation voltage, are identical to the latest HBM3 [1, 2]; therefore, backward compatibility is provided, avoiding system modification.\"\\n      },\\n      {\\n        \"paperId\": \"76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\\n        \"title\": \"NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper details the design and construction of NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35504092\",\\n            \"name\": \"Cunxiang Wang\"\\n          },\\n          {\\n            \"authorId\": \"30819687\",\\n            \"name\": \"Ruoxi Ning\"\\n          },\\n          {\\n            \"authorId\": \"2292184774\",\\n            \"name\": \"Boqi Pan\"\\n          },\\n          {\\n            \"authorId\": \"2292208424\",\\n            \"name\": \"Tonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"3187768\",\\n            \"name\": \"Qipeng Guo\"\\n          },\\n          {\\n            \"authorId\": \"2292147095\",\\n            \"name\": \"Cheng Deng\"\\n          },\\n          {\\n            \"authorId\": \"1993226927\",\\n            \"name\": \"Guangsheng Bao\"\\n          },\\n          {\\n            \"authorId\": \"2292261580\",\\n            \"name\": \"Qian Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261496744\",\\n            \"name\": \"Yue Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models\\' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.\"\\n      },\\n      {\\n        \"paperId\": \"dda8031682684655744c7001374e6cb88c9503bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dda8031682684655744c7001374e6cb88c9503bd\",\\n        \"title\": \"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.10902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Soda-Eval is introduced, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, and the performance of several open-access instruction-tuned LLMs is studied, finding that dialogue evaluation remains challenging.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2007581062\",\\n            \"name\": \"John Mendon\\\\u00e7a\"\\n          },\\n          {\\n            \"authorId\": \"2268558660\",\\n            \"name\": \"Isabel Trancoso\"\\n          },\\n          {\\n            \"authorId\": \"1784914\",\\n            \"name\": \"A. Lavie\"\\n          }\\n        ],\\n        \"abstract\": \"Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.\"\\n      },\\n      {\\n        \"paperId\": \"2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"title\": \"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\",\\n        \"citationCount\": 100,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.07542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155795167\",\\n            \"name\": \"Chengzu Li\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2339967968\",\\n            \"name\": \"Huanyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2273419590\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2294850817\",\\n            \"name\": \"Li Dong\"\\n          },\\n          {\\n            \"authorId\": \"2339667880\",\\n            \"name\": \"Ivan Vuli\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"78875987dc674fc556873df037cf114f04932e80\",\\n        \"url\": \"https://www.semanticscholar.org/paper/78875987dc674fc556873df037cf114f04932e80\",\\n        \"title\": \"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\",\\n        \"citationCount\": 83,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2401.07764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.07764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A split learning system for LLM agents in 6G networks is proposed, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1454018677\",\\n            \"name\": \"Minrui Xu\"\\n          },\\n          {\\n            \"authorId\": \"1713586\",\\n            \"name\": \"D. Niyato\"\\n          },\\n          {\\n            \"authorId\": \"2261731446\",\\n            \"name\": \"Jiawen Kang\"\\n          },\\n          {\\n            \"authorId\": \"2943819\",\\n            \"name\": \"Zehui Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2237802924\",\\n            \"name\": \"Shiwen Mao\"\\n          },\\n          {\\n            \"authorId\": \"2264568786\",\\n            \"name\": \"Zhu Han\"\\n          },\\n          {\\n            \"authorId\": \"2228302663\",\\n            \"name\": \"Dong In Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269994509\",\\n            \"name\": \"K. B. Letaief\"\\n          }\\n        ],\\n        \"abstract\": \"AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.\"\\n      },\\n      {\\n        \"paperId\": \"807ba70b6dc5ce8104268ef8e579d6ff67051230\",\\n        \"url\": \"https://www.semanticscholar.org/paper/807ba70b6dc5ce8104268ef8e579d6ff67051230\",\\n        \"title\": \"A Survey on Post-training of Large Language Models\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2187039492\",\\n            \"name\": \"Guiyao Tie\"\\n          },\\n          {\\n            \"authorId\": \"2349394246\",\\n            \"name\": \"Zeli Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2347232612\",\\n            \"name\": \"Dingjie Song\"\\n          },\\n          {\\n            \"authorId\": \"2349375946\",\\n            \"name\": \"Fuyang Wei\"\\n          },\\n          {\\n            \"authorId\": \"2349949677\",\\n            \"name\": \"Rong Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2349402229\",\\n            \"name\": \"Yurou Dai\"\\n          },\\n          {\\n            \"authorId\": \"2349357634\",\\n            \"name\": \"Wen Yin\"\\n          },\\n          {\\n            \"authorId\": \"121937496\",\\n            \"name\": \"Zhejian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2349498657\",\\n            \"name\": \"Jiangyue Yan\"\\n          },\\n          {\\n            \"authorId\": \"2342866931\",\\n            \"name\": \"Yao Su\"\\n          },\\n          {\\n            \"authorId\": \"2349400490\",\\n            \"name\": \"Zhenhan Dai\"\\n          },\\n          {\\n            \"authorId\": \"2324567791\",\\n            \"name\": \"Yifeng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2211165440\",\\n            \"name\": \"Yihan Cao\"\\n          },\\n          {\\n            \"authorId\": \"2301109277\",\\n            \"name\": \"Lichao Sun\"\\n          },\\n          {\\n            \"authorId\": \"2221116622\",\\n            \"name\": \"Pan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2254874151\",\\n            \"name\": \"Lifang He\"\\n          },\\n          {\\n            \"authorId\": \"2280102292\",\\n            \"name\": \"Hechang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2349483665\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2284983420\",\\n            \"name\": \"Qingsong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2348862207\",\\n            \"name\": \"Tianming Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249536787\",\\n            \"name\": \"Neil Zhenqiang Gong\"\\n          },\\n          {\\n            \"authorId\": \"2279062891\",\\n            \"name\": \"Jiliang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2265549448\",\\n            \"name\": \"Caiming Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2349551882\",\\n            \"name\": \"Heng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2293777434\",\\n            \"name\": \"Philip S. Yu\"\\n          },\\n          {\\n            \"authorId\": \"2288029761\",\\n            \"name\": \"Jianfeng Gao\"\\n          }\\n        ],\\n        \"abstract\": \"The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT\\'s alignment strategies to DeepSeek-R1\\'s innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.\"\\n      },\\n      {\\n        \"paperId\": \"ecb1002427e476ec76463e0a8b5a453471a1931f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ecb1002427e476ec76463e0a8b5a453471a1931f\",\\n        \"title\": \"The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks are illustrated, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283841922\",\\n            \"name\": \"Ayo Adedeji\"\\n          },\\n          {\\n            \"authorId\": \"2283935118\",\\n            \"name\": \"Sarita Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2283841541\",\\n            \"name\": \"Brendan Doohan\"\\n          }\\n        ],\\n        \"abstract\": \"In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\\n      },\\n      {\\n        \"paperId\": \"a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\\n        \"title\": \"Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.03151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A thorough and up-to-date comparison of reasoning techniques in both textual and multimodal LLMs is offered, highlighting practical methods for post-training optimization and test-time inference.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2066776633\",\\n            \"name\": \"Jing Bi\"\\n          },\\n          {\\n            \"authorId\": \"2153545235\",\\n            \"name\": \"Susan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2322454494\",\\n            \"name\": \"Xiaofei Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2279760245\",\\n            \"name\": \"Pinxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2331365304\",\\n            \"name\": \"Junjia Guo\"\\n          },\\n          {\\n            \"authorId\": \"2119309562\",\\n            \"name\": \"Yunlong Tang\"\\n          },\\n          {\\n            \"authorId\": \"2242154602\",\\n            \"name\": \"Luchuan Song\"\\n          },\\n          {\\n            \"authorId\": \"2161012966\",\\n            \"name\": \"Chao Huang\"\\n          },\\n          {\\n            \"authorId\": \"2350866278\",\\n            \"name\": \"Guangyu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2350999609\",\\n            \"name\": \"Jinxi He\"\\n          },\\n          {\\n            \"authorId\": \"2350428628\",\\n            \"name\": \"Jiarui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2354168235\",\\n            \"name\": \"Shu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2266412651\",\\n            \"name\": \"Daoan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2350433568\",\\n            \"name\": \"Chen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2152129821\",\\n            \"name\": \"L. Wen\"\\n          },\\n          {\\n            \"authorId\": \"2337241442\",\\n            \"name\": \"Zhang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2320811177\",\\n            \"name\": \"Jiebo Luo\"\\n          },\\n          {\\n            \"authorId\": \"2293314762\",\\n            \"name\": \"Chenliang Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.\"\\n      },\\n      {\\n        \"paperId\": \"5550118041a89121e9d7274f83aef420cd9ed487\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5550118041a89121e9d7274f83aef420cd9ed487\",\\n        \"title\": \"Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.11849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311451390\",\\n            \"name\": \"Yinghao Aaron Li\"\\n          },\\n          {\\n            \"authorId\": \"2243118841\",\\n            \"name\": \"Xilin Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2162961620\",\\n            \"name\": \"Jordan Darefsky\"\\n          },\\n          {\\n            \"authorId\": \"2316835793\",\\n            \"name\": \"Ge Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1686269\",\\n            \"name\": \"N. Mesgarani\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end speech-to-speech conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and speech styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the speech, which is then played back to the user. While the response speech is being played, the input speech undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input speech. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and speech-to-speech baselines in terms of both dialogue naturalness and coherence while being more than 50% faster.\"\\n      },\\n      {\\n        \"paperId\": \"afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\\n        \"title\": \"What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This survey summarizes existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual, and proposes a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-08-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2302816545\",\\n            \"name\": \"Dingyi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2317013378\",\\n            \"name\": \"Qin Jin\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories. Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations.\"\\n      },\\n      {\\n        \"paperId\": \"bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\\n        \"title\": \"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.19846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Quest is introduced, a query-centric data synthesis method aggregating semantically relevant yet diverse documents that achieves superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152644192\",\\n            \"name\": \"Chaochen Gao\"\\n          },\\n          {\\n            \"authorId\": \"2155226596\",\\n            \"name\": \"Xing Wu\"\\n          },\\n          {\\n            \"authorId\": \"2176771084\",\\n            \"name\": \"Qingfang Fu\"\\n          },\\n          {\\n            \"authorId\": \"2257376973\",\\n            \"name\": \"Songlin Hu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest\\'s superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\\n      },\\n      {\\n        \"paperId\": \"1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\\n        \"title\": \"When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.13476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AnchorAttention is developed, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training, and reduces training time by over 50% compared to standard full attention mechanisms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2267866973\",\\n            \"name\": \"Haonan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284062049\",\\n            \"name\": \"Qian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2325201427\",\\n            \"name\": \"Chao Du\"\\n          },\\n          {\\n            \"authorId\": \"2291015783\",\\n            \"name\": \"Tongyao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2325980280\",\\n            \"name\": \"Cunxiao Du\"\\n          },\\n          {\\n            \"authorId\": \"2256995496\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"19201674\",\\n            \"name\": \"Tianyu Pang\"\\n          }\\n        ],\\n        \"abstract\": \"Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16\\'s limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\\\\\\\% compared to standard full attention mechanisms, while preserving the original LLM\\'s capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.\"\\n      },\\n      {\\n        \"paperId\": \"38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\\n        \"title\": \"Ultrafast imaging of coherent polariton propagation and interactions\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2143435931\",\\n            \"name\": \"Ding Xu\"\\n          },\\n          {\\n            \"authorId\": \"13643895\",\\n            \"name\": \"Arkajit Mandal\"\\n          },\\n          {\\n            \"authorId\": \"2053177998\",\\n            \"name\": \"James M. Baxter\"\\n          },\\n          {\\n            \"authorId\": \"2004406278\",\\n            \"name\": \"Shangjun Cheng\"\\n          },\\n          {\\n            \"authorId\": \"49805255\",\\n            \"name\": \"I. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1849913502\",\\n            \"name\": \"Haowen (Vicky) Su\"\\n          },\\n          {\\n            \"authorId\": \"2144363449\",\\n            \"name\": \"Song Liu\"\\n          },\\n          {\\n            \"authorId\": \"6834462\",\\n            \"name\": \"D. Reichman\"\\n          },\\n          {\\n            \"authorId\": \"3895968\",\\n            \"name\": \"Milan Delor\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\\n        \"title\": \"Extrapolation-Based Video Retargeting With Backward Warping Using an Image-to-Warping Vector Generation Network\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LSP.2020.2977206?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LSP.2020.2977206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46445100\",\\n            \"name\": \"Sung In Cho\"\\n          },\\n          {\\n            \"authorId\": \"1739537\",\\n            \"name\": \"Suk-ju Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Video retargeting is a technique used to transform a given video to a target aspect ratio. Current methods often cause severe visual distortion due to frequent temporal incoherence during the retargeting. In this study, we propose a new extrapolation-based video retargeting method using an image-to-warping vector generation network to maintain temporal coherence and prevent deformation of an input frame by extending the side area of an input frame. Backward warping-based extrapolation is performed using a displacement vector (DV) that is generated by a proposed convolutional neural network (CNN). The DV is defined as the displacement between the current hole to be filled in the extended area and a pixel in the input frame used to fill the hole. We also propose a technique to efficiently train the CNN including a method for ground-truth DV generation. After the extrapolation, we propose a technique for the maintenance of temporal coherence of the extended region and a distortion suppression scheme (DSC) for minimizing visual artifacts. The simulation results demonstrated that the proposed method improved bidirectional similarity (BDS) up to 3.69, which is a measure of the quality of video retargeting, compared with existing video retargeting methods.\"\\n      },\\n      {\\n        \"paperId\": \"0df45f6ab09cc6ddcaf6829c131c777732a73731\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0df45f6ab09cc6ddcaf6829c131c777732a73731\",\\n        \"title\": \"Temporal Incoherence-Free Video Retargeting Using Foreground Aware Extrapolation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2020.2977171?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2020.2977171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46445100\",\\n            \"name\": \"Sung In Cho\"\\n          },\\n          {\\n            \"authorId\": \"1739537\",\\n            \"name\": \"Suk-ju Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Video retargeting is a method of adjusting the aspect ratio of a given video to the target aspect ratio. However, temporal incoherence of video contents, which can occur frequently by video retargeting, is the most dominant factor that degrades the quality of retargeted videos. Current methods to maintain temporal coherence use the entire frames of the input videos; however, these methods cannot be implemented as on-time systems because of their tremendous computational complexity. As far as we know, there is no existing on-time video retargeting method that can avoid spatial distortion while perfectly maintaining temporal coherence. In this paper, we propose a novel on-time video retargeting method that can perfectly maintain temporal coherence and prevent the spatial distortion by using only two consecutive input frames. In our method, the maximum a posteriori-based foreground aware-block matching is used for the extrapolation that extends the side area of a given video to adjust its aspect ratio to the target. To maintain the temporal coherence of the extended area, the result of block matching for backward warping-based extrapolation of the start frame after the scene change occurs, is reused for the other frames until the next scene change occurs. In addition, we propose a scene scenario-adaptive fallback scheme to prevent severe distortions that can occur with reusing block matching results or extrapolation-based side extension. The simulation results showed that the proposed method greatly improved the bidirectional similarity value, which can measure the quality of video retargeting, by up to 10.26 compared with the existing on-time video retargeting methods.\"\\n      },\\n      {\\n        \"paperId\": \"cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\\n        \"title\": \"Posterior-GRPO: Rewarding Reasoning Processes in Code Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2508.05170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a unified framework that can effectively incorporate the quality of the reasoning process during RL and introduces Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-08-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310403349\",\\n            \"name\": \"Lishui Fan\"\\n          },\\n          {\\n            \"authorId\": \"2375147757\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2125101083\",\\n            \"name\": \"Mouxiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2331676092\",\\n            \"name\": \"Zhongxin Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model\\'s internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.\"\\n      }\\n    ]\\n  },\\n  \"adaptive prompt generation\": {\\n    \"total\": 13544,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"title\": \"Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions, which is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which the authors call flocking.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2171964328\",\\n            \"name\": \"Harry Dong\"\\n          },\\n          {\\n            \"authorId\": \"2282555057\",\\n            \"name\": \"Beidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284063779\",\\n            \"name\": \"Yuejie Chi\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method\\'s simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model\\'s performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\\\\\times$ and 1.25$\\\\\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\"\\n      },\\n      {\\n        \"paperId\": \"bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"title\": \"Prompt Expansion for Adaptive Text-to-Image Generation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A human evaluation study shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2070966889\",\\n            \"name\": \"Siddhartha Datta\"\\n          },\\n          {\\n            \"authorId\": \"2276608298\",\\n            \"name\": \"Alexander Ku\"\\n          },\\n          {\\n            \"authorId\": \"2275054270\",\\n            \"name\": \"Deepak Ramachandran\"\\n          },\\n          {\\n            \"authorId\": \"2276610768\",\\n            \"name\": \"Peter Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\"\\n      },\\n      {\\n        \"paperId\": \"1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"title\": \"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\",\\n        \"citationCount\": 52,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.12761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights and incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13563486\",\\n            \"name\": \"Jaehong Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2164249715\",\\n            \"name\": \"Shoubin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2061083016\",\\n            \"name\": \"Vaidehi Patil\"\\n          },\\n          {\\n            \"authorId\": \"2267311471\",\\n            \"name\": \"Huaxiu Yao\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\"\\n      },\\n      {\\n        \"paperId\": \"a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"title\": \"Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.06401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2273557458\",\\n            \"name\": \"Hao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2273559489\",\\n            \"name\": \"Jun Li\"\\n          },\\n          {\\n            \"authorId\": \"2118764798\",\\n            \"name\": \"Yizhuang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2273589717\",\\n            \"name\": \"Jun Wan\"\\n          },\\n          {\\n            \"authorId\": \"2113457514\",\\n            \"name\": \"Zhen Lei\"\\n          },\\n          {\\n            \"authorId\": \"2274088311\",\\n            \"name\": \"Xiangyu Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\"\\n      },\\n      {\\n        \"paperId\": \"bc9f1025246efeed568650934b6e183415aca279\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bc9f1025246efeed568650934b6e183415aca279\",\\n        \"title\": \"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.03214\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation, outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on the authors\\' main metrics for each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221313264\",\\n            \"name\": \"Jonathan Pei\"\\n          },\\n          {\\n            \"authorId\": \"1410652795\",\\n            \"name\": \"Kevin Yang\"\\n          },\\n          {\\n            \"authorId\": \"38666915\",\\n            \"name\": \"D. Klein\"\\n          }\\n        ],\\n        \"abstract\": \"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\"\\n      },\\n      {\\n        \"paperId\": \"cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"title\": \"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\",\\n        \"citationCount\": 115,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds, and shows that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1443432623\",\\n            \"name\": \"Anselm Paulus\"\\n          },\\n          {\\n            \"authorId\": \"3461866\",\\n            \"name\": \"Arman Zharmagambetov\"\\n          },\\n          {\\n            \"authorId\": \"2298951327\",\\n            \"name\": \"Chuan Guo\"\\n          },\\n          {\\n            \"authorId\": \"2298758184\",\\n            \"name\": \"Brandon Amos\"\\n          },\\n          {\\n            \"authorId\": \"2253746559\",\\n            \"name\": \"Yuandong Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n      },\\n      {\\n        \"paperId\": \"1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"title\": \"WavLLM: Towards Robust and Adaptive Speech Large Language Model\",\\n        \"citationCount\": 104,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"WavLLM is introduced, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277450543\",\\n            \"name\": \"Shujie Hu\"\\n          },\\n          {\\n            \"authorId\": \"2135918679\",\\n            \"name\": \"Long Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2107983441\",\\n            \"name\": \"Shujie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107970655\",\\n            \"name\": \"Sanyuan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2294360053\",\\n            \"name\": \"Hongkun Hao\"\\n          },\\n          {\\n            \"authorId\": \"2258308585\",\\n            \"name\": \"Jing Pan\"\\n          },\\n          {\\n            \"authorId\": \"2274190703\",\\n            \"name\": \"Xunying Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280887661\",\\n            \"name\": \"Jinyu Li\"\\n          },\\n          {\\n            \"authorId\": \"9075412\",\\n            \"name\": \"S. Sivasankaran\"\\n          },\\n          {\\n            \"authorId\": \"2294832157\",\\n            \"name\": \"Linquan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277299355\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker\\'s identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\\\\\url{aka.ms/wavllm}.\"\\n      },\\n      {\\n        \"paperId\": \"5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"title\": \"OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.18866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniAvatar is introduced, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements, and introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293272991\",\\n            \"name\": \"Qijun Gan\"\\n          },\\n          {\\n            \"authorId\": \"2294510159\",\\n            \"name\": \"Ruizi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2314648986\",\\n            \"name\": \"Jianke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2370937157\",\\n            \"name\": \"Shaofei Xue\"\\n          },\\n          {\\n            \"authorId\": \"2370937932\",\\n            \"name\": \"Steven Hoi\"\\n          }\\n        ],\\n        \"abstract\": \"Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"title\": \"AdaPlanner: Adaptive Planning from Feedback with Language Models\",\\n        \"citationCount\": 175,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16653\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118180896\",\\n            \"name\": \"Haotian Sun\"\\n          },\\n          {\\n            \"authorId\": \"8103389\",\\n            \"name\": \"Yuchen Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2865034\",\\n            \"name\": \"Lingkai Kong\"\\n          },\\n          {\\n            \"authorId\": \"2218437288\",\\n            \"name\": \"Bo Dai\"\\n          },\\n          {\\n            \"authorId\": \"145657504\",\\n            \"name\": \"Chao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"title\": \"Adaptive Machine Translation with Large Language Models\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.13294\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.13294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9400076\",\\n            \"name\": \"Yasmin Moslem\"\\n          },\\n          {\\n            \"authorId\": \"1748844\",\\n            \"name\": \"Rejwanul Haque\"\\n          },\\n          {\\n            \"authorId\": \"144315616\",\\n            \"name\": \"Andy Way\"\\n          }\\n        ],\\n        \"abstract\": \"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\\n      },\\n      {\\n        \"paperId\": \"7665642af9e682e012bec045102a4d009421067c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7665642af9e682e012bec045102a4d009421067c\",\\n        \"title\": \"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\",\\n        \"citationCount\": 98,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.17061\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An efficient yet effective framework that generates high-quality 3D humans with fine-grained geometry and realistic appearance, based on the insight that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257708193\",\\n            \"name\": \"Xian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2260342453\",\\n            \"name\": \"Xiaohang Zhan\"\\n          },\\n          {\\n            \"authorId\": \"1397711601\",\\n            \"name\": \"Jiaxiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2260340529\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2247995148\",\\n            \"name\": \"Gang Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2258618427\",\\n            \"name\": \"Dahua Lin\"\\n          },\\n          {\\n            \"authorId\": \"2257370021\",\\n            \"name\": \"Xihui Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249080787\",\\n            \"name\": \"Ziwei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"title\": \"ChatUniTest: a ChatGPT-based automated unit test generation tool\",\\n        \"citationCount\": 87,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.04764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.04764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.04764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework that outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and generates assertions while utilizing mock objects and re\\\\ufb02ection to achieve test objectives.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1387638000\",\\n            \"name\": \"Zhuo-Qi Xie\"\\n          },\\n          {\\n            \"authorId\": \"2300176046\",\\n            \"name\": \"Yinghao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2064478633\",\\n            \"name\": \"Chen Zhi\"\\n          },\\n          {\\n            \"authorId\": \"145590434\",\\n            \"name\": \"Shuiguang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2116398505\",\\n            \"name\": \"Jianwei Yin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"836e3069a83f455f916114e7265e00187e511838\",\\n        \"url\": \"https://www.semanticscholar.org/paper/836e3069a83f455f916114e7265e00187e511838\",\\n        \"title\": \"Locally Differentially Private Document Generation Using Zero Shot Prompting\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.16111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1512255229\",\\n            \"name\": \"Saiteja Utpala\"\\n          },\\n          {\\n            \"authorId\": \"2261493078\",\\n            \"name\": \"Sara Hooker\"\\n          },\\n          {\\n            \"authorId\": \"2261697074\",\\n            \"name\": \"Pin Yu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\\\\\% reduction in author identification F1 score against static attackers and a 26\\\\\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\"\\n      },\\n      {\\n        \"paperId\": \"f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"title\": \"Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"153003087\",\\n            \"name\": \"Xiangteng He\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"143753918\",\\n            \"name\": \"Yuxin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\"\\n      },\\n      {\\n        \"paperId\": \"107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"title\": \"The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.09576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1847858\",\\n            \"name\": \"Subhankar Maity\"\\n          },\\n          {\\n            \"authorId\": \"2144085844\",\\n            \"name\": \"Aniket Deroy\"\\n          }\\n        ],\\n        \"abstract\": \"In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n      },\\n      {\\n        \"paperId\": \"b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"title\": \"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.00330?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.00330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"StyleCrafter is introduced, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image, and designs a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269171464\",\\n            \"name\": \"Gongye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257035878\",\\n            \"name\": \"Menghan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149052351\",\\n            \"name\": \"Haoxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2087273800\",\\n            \"name\": \"Jinbo Xing\"\\n          },\\n          {\\n            \"authorId\": \"2253795356\",\\n            \"name\": \"Xintao Wang\"\\n          },\\n          {\\n            \"authorId\": \"3001727\",\\n            \"name\": \"Yujiu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257019659\",\\n            \"name\": \"Ying Shan\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0f78249a5ff64441cc51e55bbe5b97e28f801240\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f78249a5ff64441cc51e55bbe5b97e28f801240\",\\n        \"title\": \"Prompt Tuning for Generative Multimodal Pretrained Models\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2208.02532\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2208.02532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work implements prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks, with a focus on generative multimodal pretrained models, instead of contrastive ones.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-08-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50841357\",\\n            \"name\": \"Han Yang\"\\n          },\\n          {\\n            \"authorId\": \"35996608\",\\n            \"name\": \"Junyang Lin\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"An Yang\"\\n          },\\n          {\\n            \"authorId\": \"2155302144\",\\n            \"name\": \"Peng Wang\"\\n          },\\n          {\\n            \"authorId\": \"144161025\",\\n            \"name\": \"Chang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"38385080\",\\n            \"name\": \"Hongxia Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at \\\\\\\\url{https://github.com/OFA-Sys/OFA}\"\\n      },\\n      {\\n        \"paperId\": \"192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"title\": \"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2312.01663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.01663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269468811\",\\n            \"name\": \"Runze He\"\\n          },\\n          {\\n            \"authorId\": \"2052151521\",\\n            \"name\": \"Shaofei Huang\"\\n          },\\n          {\\n            \"authorId\": \"2269461105\",\\n            \"name\": \"Xuecheng Nie\"\\n          },\\n          {\\n            \"authorId\": \"151475424\",\\n            \"name\": \"Tianrui Hui\"\\n          },\\n          {\\n            \"authorId\": \"1776665\",\\n            \"name\": \"Luoqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2108984\",\\n            \"name\": \"Jiao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2269685669\",\\n            \"name\": \"Jizhong Han\"\\n          },\\n          {\\n            \"authorId\": \"2269748083\",\\n            \"name\": \"Guanbin Li\"\\n          },\\n          {\\n            \"authorId\": \"2269687302\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\"\\n      },\\n      {\\n        \"paperId\": \"817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\\n        \"url\": \"https://www.semanticscholar.org/paper/817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\\n        \"title\": \"TaskCraft: Automated Generation of Agentic Tasks\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.10055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories, and empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2367198602\",\\n            \"name\": \"Dingfeng Shi\"\\n          },\\n          {\\n            \"authorId\": \"2366609463\",\\n            \"name\": \"Jingyi Cao\"\\n          },\\n          {\\n            \"authorId\": \"2368654631\",\\n            \"name\": \"Qianben Chen\"\\n          },\\n          {\\n            \"authorId\": \"2367090248\",\\n            \"name\": \"Weichen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2366569457\",\\n            \"name\": \"Weizhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2366571583\",\\n            \"name\": \"Hongxuan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2366522296\",\\n            \"name\": \"Fangchen Dong\"\\n          },\\n          {\\n            \"authorId\": \"2366567233\",\\n            \"name\": \"Tianrui Qin\"\\n          },\\n          {\\n            \"authorId\": \"2368705239\",\\n            \"name\": \"King Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2283080546\",\\n            \"name\": \"Minghao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2366695720\",\\n            \"name\": \"Jian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2366560952\",\\n            \"name\": \"Ge Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2182423032\",\\n            \"name\": \"Jiaheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2351712181\",\\n            \"name\": \"Changwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2366603044\",\\n            \"name\": \"Jun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2134457930\",\\n            \"name\": \"Y. Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2284803168\",\\n            \"name\": \"Wangchunshu Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce \\\\\\\\textsc{TaskCraft}, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.\"\\n      },\\n      {\\n        \"paperId\": \"f373c5569b45bf580b7502729a83761a791ee209\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f373c5569b45bf580b7502729a83761a791ee209\",\\n        \"title\": \"MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work is the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing, and Experimental results on four benchmarks suggest that the method outperforms the previous method on each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238430925\",\\n            \"name\": \"Haoyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2238207741\",\\n            \"name\": \"Tianyi Lu\"\\n          },\\n          {\\n            \"authorId\": \"2239092619\",\\n            \"name\": \"Jiaxi Gu\"\\n          },\\n          {\\n            \"authorId\": \"2238449354\",\\n            \"name\": \"Xing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2311476511\",\\n            \"name\": \"Qingping Zheng\"\\n          },\\n          {\\n            \"authorId\": \"3099139\",\\n            \"name\": \"Zuxuan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2258963974\",\\n            \"name\": \"Hang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238451522\",\\n            \"name\": \"Yu-Gang Jiang\"\\n          }\\n        ],\\n        \"abstract\": \"The diffusion model is widely leveraged for either video generation or video editing. As each field has its task-specific problems, it is difficult to merely develop a single diffusion for completing both tasks simultaneously. Video diffusion sorely relying on the text prompt can be adapted to unify the two tasks. However, it lacks a high capability of aligning heterogeneous modalities between text and image, leading to various misalignment problems. In this work, we are the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing. The proposed MagDiff introduces three types of alignments, including subject-driven alignment, adaptive prompts alignment, and high-fidelity alignment. Particularly, the subject-driven alignment is put forward to trade off the image and text prompts, serving as a unified foundation generative model for both tasks. The adaptive prompts alignment is introduced to emphasize different strengths of homogeneous and heterogeneous alignments by assigning different values of weights to the image and the text prompts. The high-fidelity alignment is developed to further enhance the fidelity of both video generation and editing by taking the subject image as an additional model input. Experimental results on four benchmarks suggest that our method outperforms the previous method on each task.\"\\n      },\\n      {\\n        \"paperId\": \"a17aee1261b56ee828f029b1caeef78033acea83\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a17aee1261b56ee828f029b1caeef78033acea83\",\\n        \"title\": \"Adaptive Ship Detection From Optical to SAR Images\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LGRS.2023.3317321?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LGRS.2023.3317321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243328855\",\\n            \"name\": \"Yuxuan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2221128858\",\\n            \"name\": \"Zhijie Rao\"\\n          },\\n          {\\n            \"authorId\": \"2232100687\",\\n            \"name\": \"Chuyang Lin\"\\n          },\\n          {\\n            \"authorId\": \"1950637\",\\n            \"name\": \"Yue Huang\"\\n          },\\n          {\\n            \"authorId\": \"2713947\",\\n            \"name\": \"Xinghao Ding\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in synthetic aperture radar (SAR) ship detection have witnessed remarkable success by using large-scale annotated datasets. However, the annotation of SAR images requires strong domain-specific expertise, significantly hindering the prompt adoption of modern object detectors in this regime. Compared to SAR data, optical data in geoscience are considerably easier to label. Motivated by this, we investigate a new and challenging problem\\\\u2014adaptive ship detection\\\\u2014with the goal of enhancing ship detection performance on SAR images by leveraging knowledge transferred from optical images. Considering the large distributional discrepancy between the source (optical) and target (SAR) domains, we present OmniAdapt, a novel framework that progressively narrows the distance between the two types of images at the pixel, feature, and classifier levels. Specifically, OmniAdapt consists of three main modules, target-like generation module (TLGM), multifeature alignment module (MFAM), and common specific decomposition module (CSDM). TLGM minimizes the visual disparity by infusing the target-domain style into the source domain. MFAM aligns local- and global-level feature representations in an adversarial manner. Finally, CSDM decomposes the classifier into two independent components, that is, the domain-common component and the domain-specific component, and promotes the recognition ability of the former via regularization learning. Experimental results demonstrate the effectiveness of the proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\\n        \"title\": \"Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.05256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate, is introduced, a reinforcement learning objective tailoring generation length to per-prompt solve rate that delivers higher accuracy on the hardest problems with higher cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274104658\",\\n            \"name\": \"Violet Xiang\"\\n          },\\n          {\\n            \"authorId\": \"2326300653\",\\n            \"name\": \"Chase Blagden\"\\n          },\\n          {\\n            \"authorId\": \"102801230\",\\n            \"name\": \"Rafael Rafailov\"\\n          },\\n          {\\n            \"authorId\": \"2283848553\",\\n            \"name\": \"nathan lile\"\\n          },\\n          {\\n            \"authorId\": \"2366009773\",\\n            \"name\": \"Sang Truong\"\\n          },\\n          {\\n            \"authorId\": \"2284774407\",\\n            \"name\": \"Chelsea Finn\"\\n          },\\n          {\\n            \"authorId\": \"2274104149\",\\n            \"name\": \"Nick Haber\"\\n          }\\n        ],\\n        \"abstract\": \"Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt\\'s online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\\\\\\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.\"\\n      },\\n      {\\n        \"paperId\": \"44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\\n        \"title\": \"OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.10825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2190109518\",\\n            \"name\": \"Dianbing Xi\"\\n          },\\n          {\\n            \"authorId\": \"2356794181\",\\n            \"name\": \"Jiepeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2337074199\",\\n            \"name\": \"Yuanzhi Liang\"\\n          },\\n          {\\n            \"authorId\": \"2336910859\",\\n            \"name\": \"Xi Qiu\"\\n          },\\n          {\\n            \"authorId\": \"3131188\",\\n            \"name\": \"Yuchi Huo\"\\n          },\\n          {\\n            \"authorId\": \"2325437281\",\\n            \"name\": \"Rui Wang\"\\n          },\\n          {\\n            \"authorId\": \"2336934367\",\\n            \"name\": \"Chi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2336880377\",\\n            \"name\": \"Xuelong Li\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff , aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. Our framework supports three key capabilities: (1) Text-conditioned video generation, where all modalities are jointly synthesized from a textual prompt; (2) Video understanding, where structural modalities are predicted from rgb inputs in a coherent manner; and (3) X-conditioned video generation, where video synthesis is guided by finegrained inputs such as depth, canny and segmentation. Extensive experiments demonstrate that OmniVDiff achieves state-of-the-art performance in video generation tasks and competitive results in video understanding. Its flexibility and scalability make it well-suited for downstream applications such as video-to-video translation, modality adaptation for visual tasks, and scene reconstruction.\"\\n      },\\n      {\\n        \"paperId\": \"42117d01d498eb9f8c21b788c3565bc6855d620b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/42117d01d498eb9f8c21b788c3565bc6855d620b\",\\n        \"title\": \"Learning to Transfer Prompts for Text Generation\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.01543\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.01543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper improves this technique and proposes a novel prompt-based method (PTG) for text generation in a transferable setting that learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2018027\",\\n            \"name\": \"Junyi Li\"\\n          },\\n          {\\n            \"authorId\": \"1997234792\",\\n            \"name\": \"Tianyi Tang\"\\n          },\\n          {\\n            \"authorId\": \"50204644\",\\n            \"name\": \"J. Nie\"\\n          },\\n          {\\n            \"authorId\": \"153693432\",\\n            \"name\": \"Ji-rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2542603\",\\n            \"name\": \"Wayne Xin Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.\"\\n      },\\n      {\\n        \"paperId\": \"7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\\n        \"title\": \"CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer\",\\n        \"citationCount\": 1175,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions, and develops an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2303231681\",\\n            \"name\": \"Zhuoyi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2238205354\",\\n            \"name\": \"Jiayan Teng\"\\n          },\\n          {\\n            \"authorId\": \"2163967642\",\\n            \"name\": \"Wendi Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2055623340\",\\n            \"name\": \"Ming Ding\"\\n          },\\n          {\\n            \"authorId\": \"2305795673\",\\n            \"name\": \"Shiyu Huang\"\\n          },\\n          {\\n            \"authorId\": \"2214082934\",\\n            \"name\": \"Jiazheng Xu\"\\n          },\\n          {\\n            \"authorId\": \"2315948290\",\\n            \"name\": \"Yuanming Yang\"\\n          },\\n          {\\n            \"authorId\": \"2105844599\",\\n            \"name\": \"Wenyi Hong\"\\n          },\\n          {\\n            \"authorId\": \"2268628279\",\\n            \"name\": \"Xiaohan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2307077651\",\\n            \"name\": \"Guanyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2307075814\",\\n            \"name\": \"Da Yin\"\\n          },\\n          {\\n            \"authorId\": \"2290625851\",\\n            \"name\": \"Xiaotao Gu\"\\n          },\\n          {\\n            \"authorId\": \"2316099643\",\\n            \"name\": \"Yuxuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265518149\",\\n            \"name\": \"Weihan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2306161782\",\\n            \"name\": \"Yean Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2315952736\",\\n            \"name\": \"Ting Liu\"\\n          },\\n          {\\n            \"authorId\": \"2288066971\",\\n            \"name\": \"Bin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2243402027\",\\n            \"name\": \"Yuxiao Dong\"\\n          },\\n          {\\n            \"authorId\": \"2238207092\",\\n            \"name\": \"Jie Tang\"\\n          }\\n        ],\\n        \"abstract\": \"We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.\"\\n      },\\n      {\\n        \"paperId\": \"f37a5c2bf4266d031533d5e029b74b00b48ef038\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f37a5c2bf4266d031533d5e029b74b00b48ef038\",\\n        \"title\": \"Latency-Aware Generative Semantic Communications With Pre-Trained Diffusion Models\",\\n        \"citationCount\": 62,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.17256\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A latency-aware semantic communications framework with pre-trained generative models that designs a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258983485\",\\n            \"name\": \"Li Qiao\"\\n          },\\n          {\\n            \"authorId\": \"3202702\",\\n            \"name\": \"Mahdi Boloursaz Mashhadi\"\\n          },\\n          {\\n            \"authorId\": \"2293693238\",\\n            \"name\": \"Zhen Gao\"\\n          },\\n          {\\n            \"authorId\": \"1690137\",\\n            \"name\": \"C. Foh\"\\n          },\\n          {\\n            \"authorId\": \"2293392561\",\\n            \"name\": \"Pei Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2279548875\",\\n            \"name\": \"Mehdi Bennis\"\\n          }\\n        ],\\n        \"abstract\": \"Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this letter, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.\"\\n      },\\n      {\\n        \"paperId\": \"759b95f7f90addc4c526cd92557e486ab143fbec\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b95f7f90addc4c526cd92557e486ab143fbec\",\\n        \"title\": \"Style Vectors for Steering Generative Large Language Models\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.01618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches, demonstrating the effectiveness of activation engineering using such style vectors.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2098959640\",\\n            \"name\": \"Kai Konen\"\\n          },\\n          {\\n            \"authorId\": \"2282467369\",\\n            \"name\": \"Sophie Jentzsch\"\\n          },\\n          {\\n            \"authorId\": \"2274662002\",\\n            \"name\": \"Diaoul\\\\u00e9 Diallo\"\\n          },\\n          {\\n            \"authorId\": \"2282467364\",\\n            \"name\": \"Peer Schutt\"\\n          },\\n          {\\n            \"authorId\": \"2282467405\",\\n            \"name\": \"Oliver Bensch\"\\n          },\\n          {\\n            \"authorId\": \"51185829\",\\n            \"name\": \"Roxanne El Baff\"\\n          },\\n          {\\n            \"authorId\": \"2282467346\",\\n            \"name\": \"Dominik Opitz\"\\n          },\\n          {\\n            \"authorId\": \"2282467403\",\\n            \"name\": \"Tobias Hecking\"\\n          }\\n        ],\\n        \"abstract\": \"This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.\"\\n      },\\n      {\\n        \"paperId\": \"621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\\n        \"title\": \"EdgeLLM: Fast On-Device LLM Inference With Speculative Decoding\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMC.2024.3513457?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMC.2024.3513457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238890503\",\\n            \"name\": \"Daliang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238575108\",\\n            \"name\": \"Wangsong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2306091156\",\\n            \"name\": \"Hao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239060901\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"2326531487\",\\n            \"name\": \"Ying Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2238910539\",\\n            \"name\": \"Shiyun Wei\"\\n          },\\n          {\\n            \"authorId\": \"2326083763\",\\n            \"name\": \"Mengwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2237080638\",\\n            \"name\": \"Xuanzhe Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device\\'s memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3\\\\u00d7 faster than existing engines.\"\\n      },\\n      {\\n        \"paperId\": \"411b16add23976ffcdf6422f932453f6ebcca119\",\\n        \"url\": \"https://www.semanticscholar.org/paper/411b16add23976ffcdf6422f932453f6ebcca119\",\\n        \"title\": \"EvoPrompting: Language Models for Code-Level Neural Architecture Search\",\\n        \"citationCount\": 118,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.14838\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.14838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13336152\",\\n            \"name\": \"Angelica Chen\"\\n          },\\n          {\\n            \"authorId\": \"35363891\",\\n            \"name\": \"David Dohan\"\\n          },\\n          {\\n            \"authorId\": \"48165870\",\\n            \"name\": \"David R. So\"\\n          }\\n        ],\\n        \"abstract\": \"Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\\n      },\\n      {\\n        \"paperId\": \"1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\\n        \"title\": \"Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.18950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Adaptive Guided Erasure (AGE) method is proposed, which selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2007884558\",\\n            \"name\": \"Anh-Vu Bui\"\\n          },\\n          {\\n            \"authorId\": \"2299801919\",\\n            \"name\": \"T. V\\\\u0169\"\\n          },\\n          {\\n            \"authorId\": \"67329496\",\\n            \"name\": \"Tung-Long Vuong\"\\n          },\\n          {\\n            \"authorId\": \"2249909946\",\\n            \"name\": \"Trung Le\"\\n          },\\n          {\\n            \"authorId\": \"2292198330\",\\n            \"name\": \"Paul Montague\"\\n          },\\n          {\\n            \"authorId\": \"2059248789\",\\n            \"name\": \"Tamas Abraham\"\\n          },\\n          {\\n            \"authorId\": \"2275034108\",\\n            \"name\": \"Junae Kim\"\\n          },\\n          {\\n            \"authorId\": \"1400659302\",\\n            \"name\": \"Dinh Q. Phung\"\\n          }\\n        ],\\n        \"abstract\": \"Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a neutral concept or just an empty text prompt. In this paper, we demonstrate that this fixed-target strategy is suboptimal, as it fails to account for the impact of erasing one concept on the others. To address this limitation, we model the concept space as a graph and empirically analyze the effects of erasing one concept on the remaining concepts. Our analysis uncovers intriguing geometric properties of the concept space, where the influence of erasing a concept is confined to a local region. Building on this insight, we propose the Adaptive Guided Erasure (AGE) method, which \\\\\\\\emph{dynamically} selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects. Experimental results show that AGE significantly outperforms state-of-the-art erasure methods on preserving unrelated concepts while maintaining effective erasure performance. Our code is published at {https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.\"\\n      },\\n      {\\n        \"paperId\": \"af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\\n        \"title\": \"StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.08503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A cross-modal Adaptive Instance Normalization mechanism is introduced for better integration of style and text features, enhancing alignment and a Style-based Classifier-Free Guidance approach is developed that enables selective control over stylistic elements, reducing irrelevant influences.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2334740500\",\\n            \"name\": \"Mingkun Lei\"\\n          },\\n          {\\n            \"authorId\": \"2334824597\",\\n            \"name\": \"Xue Song\"\\n          },\\n          {\\n            \"authorId\": \"2336265253\",\\n            \"name\": \"Beier Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2334818360\",\\n            \"name\": \"Hao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2334822819\",\\n            \"name\": \"Chi Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.\"\\n      },\\n      {\\n        \"paperId\": \"5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\\n        \"title\": \"TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.16425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information, and designs a Dynamic Map Scaling mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333781842\",\\n            \"name\": \"Linqing Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2105618628\",\\n            \"name\": \"Chen Gao\"\\n          },\\n          {\\n            \"authorId\": \"2264574237\",\\n            \"name\": \"Zihan Ding\"\\n          },\\n          {\\n            \"authorId\": \"2325825544\",\\n            \"name\": \"Yue Liao\"\\n          },\\n          {\\n            \"authorId\": \"2325537018\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM\\'s spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.\"\\n      },\\n      {\\n        \"paperId\": \"4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\\n        \"title\": \"A new rapid deflagration-to-detonation transition in a short smooth tube\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1063/5.0191500?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1063/5.0191500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"92832821\",\\n            \"name\": \"Wandong Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290316859\",\\n            \"name\": \"Caizhi Fan\"\\n          },\\n          {\\n            \"authorId\": \"1734241\",\\n            \"name\": \"R. Deiterding\"\\n          },\\n          {\\n            \"authorId\": \"2290237139\",\\n            \"name\": \"Xiaokang Li\"\\n          },\\n          {\\n            \"authorId\": \"36072040\",\\n            \"name\": \"Jianhan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290198596\",\\n            \"name\": \"Xiong Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Obtaining a rapid deflagration-to-detonation transition (DDT) within a short smooth tube is a challenging task. Here, an unconventional means of flame acceleration propagating upstream in subsonic and supersonic mixtures within a smooth tube was introduced to acquire a speedy DDT. The Navier\\\\u2013Stokes equations with an adaptive mesh refinement technique and a detailed hydrogen\\\\u2013air chemistry reaction mechanism of 11 species and 27 steps were utilized to resolve the entire DDT characteristics. The effect of the initial Mach number on flame acceleration and DDT mechanism was revealed comprehensively. The results demonstrated that a prompt oblique shock wave (SW) occurs when the flame propagates upstream along the boundary walls due to the boundary layer influence. An intense coupling between the SW and the leading flame front is enhanced by increasing the initial Mach number of the mixture. The speedy generation of the oblique SW is formed at the incipient stage, mainly produced by the boundary layer influence and the coalescences of the compression waves. Consequently, the run-up time to detonation is shortened accordingly through a fierce reflected SW due to the intense leading SW after it reflects from the confined wall. Furthermore, three kinds of DDT evolution are revealed from the obtained results: (1) localized ignition in the upper boundary wall after the reflected and transverse shock waves propagate in the upper wall regions; (2) autoignition is formed in the confined wall corner after the reflected SW; and (3) direct detonation transition occurs at the end wall behind a strongly reflected SW in the supersonic case.\"\\n      },\\n      {\\n        \"paperId\": \"815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\\n        \"title\": \"Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2405.14713\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Generative AI capabilities are introduced to assist educators in creating tutor interfaces that meet their needs while adhering to design principles and raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170697820\",\\n            \"name\": \"Tommaso Cal\\\\u00f2\"\\n          },\\n          {\\n            \"authorId\": \"2257237899\",\\n            \"name\": \"Christopher J. MacLellan\"\\n          }\\n        ],\\n        \"abstract\": \"Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\\n      },\\n      {\\n        \"paperId\": \"e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\\n        \"title\": \"Transformer-Based Variable-Rate Image Compression with Region-of-Interest Control\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.10807\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.10807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a transformer-based learned image compression system that is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality while confirming its superiority over the other competing methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2140736758\",\\n            \"name\": \"Chia-Hao Kao\"\\n          },\\n          {\\n            \"authorId\": \"1723619\",\\n            \"name\": \"Ying Weng\"\\n          },\\n          {\\n            \"authorId\": \"2116613919\",\\n            \"name\": \"Yi-Hsin Chen\"\\n          },\\n          {\\n            \"authorId\": \"37811787\",\\n            \"name\": \"Wei-Chen Chiu\"\\n          },\\n          {\\n            \"authorId\": \"123608804\",\\n            \"name\": \"Wenmin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"This paper proposes a transformer-based learned image compression system. It is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality. Inspired by prompt tuning, we introduce prompt generation networks to condition the transformer-based autoencoder of compression. Our prompt generation networks generate content-adaptive tokens according to the input image, an ROI mask, and a rate parameter. The separation of the ROI mask and the rate parameter allows an intuitive way to achieve variable-rate and ROI coding simultaneously. Extensive experiments validate the effectiveness of our proposed method and confirm its superiority over the other competing methods.\"\\n      },\\n      {\\n        \"paperId\": \"c7acf9250926100f531bcf46d63d7da06e73928e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c7acf9250926100f531bcf46d63d7da06e73928e\",\\n        \"title\": \"MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work identifies that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images, and advances the cross-modality relation learning between the prompt and the generated images to better align the prompt and image content.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238903147\",\\n            \"name\": \"Yupeng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"18119920\",\\n            \"name\": \"Daquan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2238887924\",\\n            \"name\": \"Zuo-Liang Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2238889090\",\\n            \"name\": \"Yaxing Wang\"\\n          },\\n          {\\n            \"authorId\": \"3298532\",\\n            \"name\": \"Qibin Hou\"\\n          },\\n          {\\n            \"authorId\": \"33221685\",\\n            \"name\": \"Jiashi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in diffusion models have showcased their impressive capacity to generate visually striking images. However, ensuring a close match between the generated image and the given prompt remains a persistent challenge. In this work, we identify that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images. To better align the prompt and image content, we advance the cross-attention with an adaptive mask, which is conditioned on the attention maps and the prompt embeddings, to dynamically adjust the contribution of each text token to the image features. This mechanism explicitly diminishes the ambiguity in the semantic information embedding of the text encoder, leading to a boost of text-to-image consistency in the synthesized images. Our method, termed MaskDiffusion, is training-free and hot-pluggable for popular pre-trained diffusion models. When applied to the latent diffusion models, our MaskDiffusion can largely enhance their capability to correctly generate objects and their attributes, with negligible computation overhead compared to the original diffusion models. Our project page is https://github.com/HVision-NKU/MaskDiffusion.\"\\n      },\\n      {\\n        \"paperId\": \"66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\\n        \"url\": \"https://www.semanticscholar.org/paper/66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\\n        \"title\": \"TEMPERA: Test-Time Prompting via Reinforcement Learning\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2211.11890\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.11890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work designs a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers, and achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-11-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1993655237\",\\n            \"name\": \"Tianjun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275277634\",\\n            \"name\": \"Xuezhi Wang\"\\n          },\\n          {\\n            \"authorId\": \"65855107\",\\n            \"name\": \"Denny Zhou\"\\n          },\\n          {\\n            \"authorId\": \"50319359\",\\n            \"name\": \"D. Schuurmans\"\\n          },\\n          {\\n            \"authorId\": \"49988044\",\\n            \"name\": \"Joseph E. Gonzalez\"\\n          }\\n        ],\\n        \"abstract\": \"Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.\"\\n      },\\n      {\\n        \"paperId\": \"88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\\n        \"title\": \"QA-CLIMS: Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.09883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2280039282\",\\n            \"name\": \"Songhe Deng\"\\n          },\\n          {\\n            \"authorId\": \"2279830536\",\\n            \"name\": \"Wei Zhuo\"\\n          },\\n          {\\n            \"authorId\": \"2220635949\",\\n            \"name\": \"Jinheng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2265520934\",\\n            \"name\": \"Linlin Shen\"\\n          }\\n        ],\\n        \"abstract\": \"Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model\\'s ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets.\"\\n      },\\n      {\\n        \"paperId\": \"d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\\n        \"title\": \"Immunosenescence, Inflammaging, and Lung Senescence in Asthma in the Elderly\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2218-273X/12/10/1456/pdf?version=1665574999\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9599177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Immunosenescence\\\\u2019s contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly is discussed and an overview of age-related features in the immune system and lung structure is presented.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47522049\",\\n            \"name\": \"T. Soma\"\\n          },\\n          {\\n            \"authorId\": \"46375440\",\\n            \"name\": \"M. Nagata\"\\n          }\\n        ],\\n        \"abstract\": \"Prevalence of asthma in older adults is growing along with increasing global life expectancy. Due to poor clinical consequences such as high mortality, advancement in understanding the pathophysiology of asthma in older patients has been sought to provide prompt treatment for them. Age-related alterations of functions in the immune system and lung parenchyma occur throughout life. Alterations with advancing age are promoted by various stimuli, including pathobionts, fungi, viruses, pollutants, and damage-associated molecular patterns derived from impaired cells, abandoned cell debris, and senescent cells. Age-related changes in the innate and adaptive immune response, termed immunosenescence, includes impairment of phagocytosis and antigen presentation, enhancement of proinflammatory mediator generation, and production of senescence-associated secretory phenotype. Immnunosenescence could promote inflammaging (chronic low-grade inflammation) and contribute to late-onset adult asthma and asthma in the elderly, along with age-related pulmonary disease, such as chronic obstructive pulmonary disease and pulmonary fibrosis, due to lung parenchyma senescence. Aged patients with asthma exhibit local and systemic type 2 and non-type 2 inflammation, associated with clinical manifestations. Here, we discuss immunosenescence\\\\u2019s contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly and present an overview of age-related features in the immune system and lung structure.\"\\n      },\\n      {\\n        \"paperId\": \"94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\\n        \"title\": \"DAFT-E: Feature-Based Multivariate and Multi-Step-Ahead Wind Power Forecasting\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tste.2021.3130949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tste.2021.3130949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1978743481\",\\n            \"name\": \"F. De Caro\"\\n          },\\n          {\\n            \"authorId\": \"2142416821\",\\n            \"name\": \"Jacopo De Stefani\"\\n          },\\n          {\\n            \"authorId\": \"33858225\",\\n            \"name\": \"A. Vaccaro\"\\n          },\\n          {\\n            \"authorId\": \"1772497\",\\n            \"name\": \"Gianluca Bontempi\"\\n          }\\n        ],\\n        \"abstract\": \"Wind energy is one of the most promising resources for the mitigation of greenhouse gas emissions that contribute to anthropogenic global warming. However, the large proliferation of wind power generators is causing several critical issues in power systems due to their variable power generated profiles. For this reason, a large number of learning techniques, e.g. integrating Vector Auto-Regressive and Neural Network-based models, were proposed in the literature for mitigating wind power uncertainty issues. Unfortunately, these methodologies show several limitations, e.g. the huge number of parameters and/or the heavy computational cost, which hinder their deployment in modern power system operation, where prompt and reliable wide-area wind power generation forecasts are requested for supporting time-critical decision making on several time horizons. To try addressing this issue, this paper proposes the Dynamic Adaptive Feature-based Temporal Ensemble (DAFT-E) forecasting approach, which relies on an extensive feature engineering, a fast feature selection step and an ensemble of computationally inexpensive models to reduce the computational complexity of the forecasting task, while still preserving predictive accuracy. The experimental results, which benchmark DAFT-E against multivariate (VAR and deep learning) alternatives on two real case studies, show that the proposed approach outperforms state-of-the-art and representation learning models according to several forecasting accuracy metrics.\"\\n      },\\n      {\\n        \"paperId\": \"f6023556221018f779a63a60874973195aea8352\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6023556221018f779a63a60874973195aea8352\",\\n        \"title\": \"Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.09857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel framework named Attention-aware Self-adaptive Prompt (ASP), which prevents overfitting on base task and does not require enormous data in few-shot incremental tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275745354\",\\n            \"name\": \"Chenxi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2254008335\",\\n            \"name\": \"Zhenyi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2249155683\",\\n            \"name\": \"Tianyi Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2262968852\",\\n            \"name\": \"Ruibo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2254326623\",\\n            \"name\": \"Yihan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2275765198\",\\n            \"name\": \"Junfeng Guo\"\\n          },\\n          {\\n            \"authorId\": \"2261394090\",\\n            \"name\": \"Heng Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensive experiments on three benchmark datasets validate that ASP consistently outperforms state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning new classes and mitigating forgetting.\"\\n      },\\n      {\\n        \"paperId\": \"c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"title\": \"Soft Prompt Generation for Domain Generalization\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG), which consists of a two-stage training phase and an inference phase, aiming to incorporate the generative model domain knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2290728057\",\\n            \"name\": \"Yuedi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\"\\n      },\\n      {\\n        \"paperId\": \"37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"title\": \"Adapting to Distribution Shift by Visual Domain Prompt Generation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.02797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction and outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35793956\",\\n            \"name\": \"Zhixiang Chi\"\\n          },\\n          {\\n            \"authorId\": \"2300096585\",\\n            \"name\": \"Li Gu\"\\n          },\\n          {\\n            \"authorId\": \"2300089295\",\\n            \"name\": \"Tao Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2277793919\",\\n            \"name\": \"Huan Liu\"\\n          },\\n          {\\n            \"authorId\": \"1787848\",\\n            \"name\": \"Yuanhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2277598061\",\\n            \"name\": \"Konstantinos N. Plataniotis\"\\n          },\\n          {\\n            \"authorId\": \"2277695392\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n      },\\n      {\\n        \"paperId\": \"fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\\n        \"title\": \"Query-Based Adversarial Prompt Generation\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.12329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work improves on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268494505\",\\n            \"name\": \"Jonathan Hayase\"\\n          },\\n          {\\n            \"authorId\": \"2284689404\",\\n            \"name\": \"Ema Borevkovic\"\\n          },\\n          {\\n            \"authorId\": \"2483738\",\\n            \"name\": \"Nicholas Carlini\"\\n          },\\n          {\\n            \"authorId\": \"2444919\",\\n            \"name\": \"Florian Tram\\\\u00e8r\"\\n          },\\n          {\\n            \"authorId\": \"3490923\",\\n            \"name\": \"Milad Nasr\"\\n          }\\n        ],\\n        \"abstract\": \"Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI\\'s safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.\"\\n      },\\n      {\\n        \"paperId\": \"91b6158978b248e9a0e65d0d588bc1054e72bc16\",\\n        \"url\": \"https://www.semanticscholar.org/paper/91b6158978b248e9a0e65d0d588bc1054e72bc16\",\\n        \"title\": \"MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.findings-emnlp.215.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP and proposes a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260750930\",\\n            \"name\": \"Yuyan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2260655625\",\\n            \"name\": \"Zhihao Wen\"\\n          },\\n          {\\n            \"authorId\": \"2260651904\",\\n            \"name\": \"Ge Fan\"\\n          },\\n          {\\n            \"authorId\": \"2273721608\",\\n            \"name\": \"Zhengyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2273816877\",\\n            \"name\": \"Wei Wu\"\\n          },\\n          {\\n            \"authorId\": \"2260908086\",\\n            \"name\": \"Dayiheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2243457917\",\\n            \"name\": \"Zhixu Li\"\\n          },\\n          {\\n            \"authorId\": \"2163832089\",\\n            \"name\": \"Bang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2265724350\",\\n            \"name\": \"Yanghua Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.\"\\n      },\\n      {\\n        \"paperId\": \"135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\\n        \"url\": \"https://www.semanticscholar.org/paper/135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\\n        \"title\": \"Federated Text-driven Prompt Generation for Vision-Language Models\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256983385\",\\n            \"name\": \"Chen Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2257324808\",\\n            \"name\": \"Xingyu Li\"\\n          },\\n          {\\n            \"authorId\": \"29359383\",\\n            \"name\": \"Chaithanya Kumar Mummadi\"\\n          },\\n          {\\n            \"authorId\": \"144487556\",\\n            \"name\": \"M. Ganesh\"\\n          },\\n          {\\n            \"authorId\": \"2257091754\",\\n            \"name\": \"Zhenzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2257130661\",\\n            \"name\": \"Lu Peng\"\\n          },\\n          {\\n            \"authorId\": \"2257132255\",\\n            \"name\": \"Wan-Yi Lin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"title\": \"EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2024.3482175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2024.3482175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145205189\",\\n            \"name\": \"Yinsong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266873357\",\\n            \"name\": \"Jiaqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2266387679\",\\n            \"name\": \"Aidong Men\"\\n          },\\n          {\\n            \"authorId\": \"2266583142\",\\n            \"name\": \"Qingchao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\"\\n      },\\n      {\\n        \"paperId\": \"1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"title\": \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\\n        \"citationCount\": 92,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new approach to code generation by LLMs is proposed, which is called AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72729733\",\\n            \"name\": \"T. Ridnik\"\\n          },\\n          {\\n            \"authorId\": \"2279758170\",\\n            \"name\": \"Dedy Kredo\"\\n          },\\n          {\\n            \"authorId\": \"49668367\",\\n            \"name\": \"Itamar Friedman\"\\n          }\\n        ],\\n        \"abstract\": \"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\"\\n      },\\n      {\\n        \"paperId\": \"211e915b2e1e0753ddd581f10362fc82f28cc606\",\\n        \"url\": \"https://www.semanticscholar.org/paper/211e915b2e1e0753ddd581f10362fc82f28cc606\",\\n        \"title\": \"DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DiTCtrl is proposed, a training-free multi-prompt video generation method under MM-DiT architectures for the first time that achieves state-of-the-art performance without additional training and presents MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2337083029\",\\n            \"name\": \"Minghong Cai\"\\n          },\\n          {\\n            \"authorId\": \"30176430\",\\n            \"name\": \"Xiaodong Cun\"\\n          },\\n          {\\n            \"authorId\": \"2257035102\",\\n            \"name\": \"Xiaoyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2308556703\",\\n            \"name\": \"Wenze Liu\"\\n          },\\n          {\\n            \"authorId\": \"2303078452\",\\n            \"name\": \"Zhaoyang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2268490605\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2316484241\",\\n            \"name\": \"Xiangyu Yue\"\\n          }\\n        ],\\n        \"abstract\": \"Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer (MM-DiT) architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT\\\\u2019s attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training. Code is available at https://github.com/TencentARC/DiTCtrl.\"\\n      },\\n      {\\n        \"paperId\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04095\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces the Prompt Auto-Editing (PAE) method, which employs an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148661301\",\\n            \"name\": \"Wenyi Mo\"\\n          },\\n          {\\n            \"authorId\": \"2146332319\",\\n            \"name\": \"Tianyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2281418241\",\\n            \"name\": \"Yalong Bai\"\\n          },\\n          {\\n            \"authorId\": \"2295513824\",\\n            \"name\": \"Bing Su\"\\n          },\\n          {\\n            \"authorId\": \"2293310016\",\\n            \"name\": \"Ji-Rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2281323801\",\\n            \"name\": \"Qing Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\"\\n      }\\n    ]\\n  },\\n  \"iterative context update\": {\\n    \"total\": 9219,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 288,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 211,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 40,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\', \\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"fca9a4508863025d95a581ead47032d497825053\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fca9a4508863025d95a581ead47032d497825053\",\\n        \"title\": \"VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.02186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"VideoICL is a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach, improving OOD video understanding performance by extending effective context length without incurring high costs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333526847\",\\n            \"name\": \"Kangsan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2307075970\",\\n            \"name\": \"Geon Park\"\\n          },\\n          {\\n            \"authorId\": \"3445691\",\\n            \"name\": \"Youngwan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2119578055\",\\n            \"name\": \"Woongyeong Yeo\"\\n          },\\n          {\\n            \"authorId\": \"2265627157\",\\n            \"name\": \"Sung Ju Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in video large multimodal models (LMMs) have significantly improved their video understanding and reasoning capabilities. However, their performance drops on out-of-distribution (OOD) tasks that are underrepresented in training data. Traditional methods like fine-tuning on OOD datasets are impractical due to high computational costs. While In-context learning (ICL) with demonstration examples has shown promising generalization performance in language tasks and image-language tasks without fine-tuning, applying ICL to video-language tasks faces challenges due to the limited context length in Video LMMs, as videos require longer token lengths. To address these issues, we propose VideoICL, a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach. This allows to select the most relevant examples and rank them based on similarity, to be used for inference. If the generated response has low confidence, our framework selects new examples and performs inference again, iteratively refining the results until a high-confidence response is obtained. This approach improves OOD video understanding performance by extending effective context length without incurring high costs. The experimental results on multiple benchmarks demonstrate significant performance gains, especially in domain-specific scenarios, laying the groundwork for broader video comprehension applications. Code is released at https://github.com/KangsanKim07/VideoICL\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 72,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\\n        \"title\": \"Single image deraining using scale constraint iterative update network\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2023.121339?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2023.121339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1390863714\",\\n            \"name\": \"Yitong Yang\"\\n          },\\n          {\\n            \"authorId\": \"1591131546\",\\n            \"name\": \"Yongjun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"150356192\",\\n            \"name\": \"Zhongwei Cui\"\\n          },\\n          {\\n            \"authorId\": \"2112674491\",\\n            \"name\": \"Haoliang Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2210993430\",\\n            \"name\": \"Ting Ouyang\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 188,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 366,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 330,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 48,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2891,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      }\\n    ]\\n  },\\n  \"thematic consistency LLM\": {\\n    \"total\": 887,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\\n        \"title\": \"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.02896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This experimental study seeks to lay the groundwork for the understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction?\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282535211\",\\n            \"name\": \"Ivar Frisch\"\\n          },\\n          {\\n            \"authorId\": \"24068173\",\\n            \"name\": \"Mario Giulianelli\"\\n          }\\n        ],\\n        \"abstract\": \"Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model (LLM) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction? We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.\"\\n      },\\n      {\\n        \"paperId\": \"477a16bdbb43589e5feac3881b3370e3a4ab5624\",\\n        \"url\": \"https://www.semanticscholar.org/paper/477a16bdbb43589e5feac3881b3370e3a4ab5624\",\\n        \"title\": \"Evaluating the Consistency of LLM Evaluators\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.00543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Examining the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models demonstrates that strong proprietary models are not necessarily consistent evaluators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2291076200\",\\n            \"name\": \"Noah Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290955335\",\\n            \"name\": \"Jiwoo Hong\"\\n          },\\n          {\\n            \"authorId\": \"2290905396\",\\n            \"name\": \"James Thorne\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have shown potential as general evaluators along with the evident benefits of speed and cost. While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators. In this paper, we conduct extensive studies on the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models. Our comprehensive analysis demonstrates that strong proprietary models are not necessarily consistent evaluators, highlighting the importance of considering consistency in assessing the capability of LLM evaluators.\"\\n      },\\n      {\\n        \"paperId\": \"75062b58398b6e9409e5fec855f6912534331eaf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/75062b58398b6e9409e5fec855f6912534331eaf\",\\n        \"title\": \"Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales, facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-08-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": \"Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths,but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority voting, enabling more informed choices on when to halt sampling and which rationale to select. Our comprehensive experiments across diverse question-answering datasets demonstrate that RASC outperforms existing methods, reducing sample usage by approximately 70% while maintaining accuracy. Moreover, RASC facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs. Our approach effectively addresses the efficiency-accuracy trade-off in LLM reasoning tasks, offering a new perspective for more nuanced, faithful, and effective utilization of LLMs in resource-constrained environments.\"\\n      },\\n      {\\n        \"paperId\": \"3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\\n        \"title\": \"Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1186/s12911-024-02656-3\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11382443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare is emphasized, aiming to improve clinical outcomes ethically and effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-09-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"145770274\",\\n            \"name\": \"Tala Mirzaei\"\\n          },\\n          {\\n            \"authorId\": \"2320339088\",\\n            \"name\": \"Leila Amini\"\\n          },\\n          {\\n            \"authorId\": \"2574575\",\\n            \"name\": \"Pouyan Esmaeilzadeh\"\\n          }\\n        ],\\n        \"abstract\": \"This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions. We analyzed 3049 posts and comments extracted from a self-identified clinician subreddit using unsupervised machine learning via Latent Dirichlet Allocation and a structured qualitative analysis methodology. Analysis uncovered 14 salient themes of ethical implications, which we further consolidated into 4 overarching domains reflecting ethical issues around various clinical applications of LLM in healthcare, LLM coding, algorithm, and data governance, LLM\\\\u2019s role in health equity and the distribution of public health services, and the relationship between users (human) and LLM systems (machine). Mapping themes to ethical frameworks in literature illustrated multifaceted issues covering transparent LLM decisions, fairness, privacy, access disparities, user experiences, and reliability. This study emphasizes the need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare, aiming to improve clinical outcomes ethically and effectively.\"\\n      },\\n      {\\n        \"paperId\": \"6b7c5fc0f6b401962153f68f8250951f75da929e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b7c5fc0f6b401962153f68f8250951f75da929e\",\\n        \"title\": \"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.06503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper analyzes two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and finds that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-04-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295732707\",\\n            \"name\": \"Nathan Brake\"\\n          },\\n          {\\n            \"authorId\": \"2295732451\",\\n            \"name\": \"Thomas Schaaf\"\\n          }\\n        ],\\n        \"abstract\": \"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\\n      },\\n      {\\n        \"paperId\": \"e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"title\": \"LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\",\\n        \"citationCount\": 110,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL), which yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3400291\",\\n            \"name\": \"Shih-Chieh Dai\"\\n          },\\n          {\\n            \"authorId\": \"2261362789\",\\n            \"name\": \"Aiping Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1746959\",\\n            \"name\": \"Lun-Wei Ku\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n      },\\n      {\\n        \"paperId\": \"32426b96ff3c680125bde3b835bfa931288b8ade\",\\n        \"url\": \"https://www.semanticscholar.org/paper/32426b96ff3c680125bde3b835bfa931288b8ade\",\\n        \"title\": \"Better Patching Using LLM Prompting, via Self-Consistency\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2306.00108\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.00108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an application of the $\\\\\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots, on the MODIT dataset.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language models (LLMs) can be induced to solve non-trivial problems with \\\\u201cfew-shot\\\\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \\\\u201cchain of thought\\\\u201d ($\\\\\\\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \\\\u201cexplained\\\\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\\\\\\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\\\\\\\mathcal{S}-C$ (or even $\\\\\\\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\\\\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.\"\\n      },\\n      {\\n        \"paperId\": \"e24424283c02fbe7f641e5b3490d7bb059f8355a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e24424283c02fbe7f641e5b3490d7bb059f8355a\",\\n        \"title\": \"A Survey on LLM-as-a-Judge\",\\n        \"citationCount\": 776,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.15594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built?\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-11-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216587705\",\\n            \"name\": \"Jiawei Gu\"\\n          },\\n          {\\n            \"authorId\": \"144267788\",\\n            \"name\": \"Xuhui Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2287881684\",\\n            \"name\": \"Zhichao Shi\"\\n          },\\n          {\\n            \"authorId\": \"2274159320\",\\n            \"name\": \"Hexiang Tan\"\\n          },\\n          {\\n            \"authorId\": \"2332093190\",\\n            \"name\": \"Xuehao Zhai\"\\n          },\\n          {\\n            \"authorId\": \"2250617116\",\\n            \"name\": \"Chengjin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2330714501\",\\n            \"name\": \"Wei Li\"\\n          },\\n          {\\n            \"authorId\": \"1944248313\",\\n            \"name\": \"Yinghan Shen\"\\n          },\\n          {\\n            \"authorId\": \"2311556497\",\\n            \"name\": \"Shengjie Ma\"\\n          },\\n          {\\n            \"authorId\": \"2332306096\",\\n            \"name\": \"Honghao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257058703\",\\n            \"name\": \"Yuanzhuo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284217200\",\\n            \"name\": \"Jian Guo\"\\n          }\\n        ],\\n        \"abstract\": \"Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of\\\\\"LLM-as-a-Judge,\\\\\"where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.\"\\n      },\\n      {\\n        \"paperId\": \"0bf3a1867f7245b8a702093901c66b08b518eafc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0bf3a1867f7245b8a702093901c66b08b518eafc\",\\n        \"title\": \"Evaluating Very Long-Term Conversational Memory of LLM Agents\",\\n        \"citationCount\": 166,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A machine-human pipeline is introduced to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs, and presents a comprehensive evaluation benchmark to measure long-term memory in models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8785371\",\\n            \"name\": \"Adyasha Maharana\"\\n          },\\n          {\\n            \"authorId\": \"2266803131\",\\n            \"name\": \"Dong-Ho Lee\"\\n          },\\n          {\\n            \"authorId\": \"145582202\",\\n            \"name\": \"S. Tulyakov\"\\n          },\\n          {\\n            \"authorId\": \"2285969697\",\\n            \"name\": \"Mohit Bansal\"\\n          },\\n          {\\n            \"authorId\": \"2266751000\",\\n            \"name\": \"Francesco Barbieri\"\\n          },\\n          {\\n            \"authorId\": \"2267220081\",\\n            \"name\": \"Yuwei Fang\"\\n          }\\n        ],\\n        \"abstract\": \"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.\"\\n      },\\n      {\\n        \"paperId\": \"7c04ab297b59d4fe29285f339350882a3120b27f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7c04ab297b59d4fe29285f339350882a3120b27f\",\\n        \"title\": \"CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs\",\\n        \"citationCount\": 205,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642773\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.11314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions, is developed, revealing four design considerations for future educational AI assistants.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3136345\",\\n            \"name\": \"Majeed Kazemitabaar\"\\n          },\\n          {\\n            \"authorId\": \"2184253123\",\\n            \"name\": \"Runlong Ye\"\\n          },\\n          {\\n            \"authorId\": \"2280281736\",\\n            \"name\": \"Xiaoning Wang\"\\n          },\\n          {\\n            \"authorId\": \"2280145055\",\\n            \"name\": \"Austin Z Henley\"\\n          },\\n          {\\n            \"authorId\": \"2243041721\",\\n            \"name\": \"Paul Denny\"\\n          },\\n          {\\n            \"authorId\": \"2280145218\",\\n            \"name\": \"Michelle Craig\"\\n          },\\n          {\\n            \"authorId\": \"2280146888\",\\n            \"name\": \"Tovi Grossman\"\\n          }\\n        ],\\n        \"abstract\": \"Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student\\\\u2019s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI\\\\u2019s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.\"\\n      },\\n      {\\n        \"paperId\": \"da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\\n        \"url\": \"https://www.semanticscholar.org/paper/da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\\n        \"title\": \"Exploring and Evaluating Hallucinations in LLM-Powered Code Generation\",\\n        \"citationCount\": 221,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation, and proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290969862\",\\n            \"name\": \"Fang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2294504414\",\\n            \"name\": \"Yang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2295165194\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2294528116\",\\n            \"name\": \"Houkun Huang\"\\n          },\\n          {\\n            \"authorId\": \"2294510508\",\\n            \"name\": \"Ruifeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2294664033\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290433096\",\\n            \"name\": \"Li Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users\\' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.\"\\n      },\\n      {\\n        \"paperId\": \"a06d3e9e90008c64c45a0029d580541d5f646771\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771\",\\n        \"title\": \"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.00812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An overview of the various benefits of integrating code into LLMs\\' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277527247\",\\n            \"name\": \"Ke Yang\"\\n          },\\n          {\\n            \"authorId\": \"33456794\",\\n            \"name\": \"Jiateng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277421308\",\\n            \"name\": \"John Wu\"\\n          },\\n          {\\n            \"authorId\": \"2277597831\",\\n            \"name\": \"Chaoqi Yang\"\\n          },\\n          {\\n            \"authorId\": \"51135899\",\\n            \"name\": \"Y. Fung\"\\n          },\\n          {\\n            \"authorId\": \"2262396117\",\\n            \"name\": \"Sha Li\"\\n          },\\n          {\\n            \"authorId\": \"2277416897\",\\n            \"name\": \"Zixuan Huang\"\\n          },\\n          {\\n            \"authorId\": \"2344961610\",\\n            \"name\": \"Xu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2144803999\",\\n            \"name\": \"Xingyao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2277247982\",\\n            \"name\": \"Yiquan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2277409745\",\\n            \"name\": \"Heng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2261082008\",\\n            \"name\": \"ChengXiang Zhai\"\\n          }\\n        ],\\n        \"abstract\": \"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs\\' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.\"\\n      },\\n      {\\n        \"paperId\": \"2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\\n        \"title\": \"PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.12532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The number of crucial keys and values that influence future generations decreases layer by layer and can be extracted by the consistency in attention weights, leading to PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2182432556\",\\n            \"name\": \"Dongjie Yang\"\\n          },\\n          {\\n            \"authorId\": \"2302683712\",\\n            \"name\": \"Xiaodong Han\"\\n          },\\n          {\\n            \"authorId\": \"2302558089\",\\n            \"name\": \"Yan Gao\"\\n          },\\n          {\\n            \"authorId\": \"2302556666\",\\n            \"name\": \"Yao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2302704855\",\\n            \"name\": \"Shilin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2302545224\",\\n            \"name\": \"Hai Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.\"\\n      },\\n      {\\n        \"paperId\": \"36b9dac525fc93100b18d8e489bd97460cd49a5e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/36b9dac525fc93100b18d8e489bd97460cd49a5e\",\\n        \"title\": \"Thematic-LM: A LLM-based Multi-agent System for Large-scale Thematic Analysis\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3696410.3714595?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3696410.3714595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study applies Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-04-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2343740149\",\\n            \"name\": \"Tingrui Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2343749049\",\\n            \"name\": \"Caroline Walker\"\\n          },\\n          {\\n            \"authorId\": \"2343746435\",\\n            \"name\": \"Chris Cunningham\"\\n          },\\n          {\\n            \"authorId\": \"2310725786\",\\n            \"name\": \"Yun Sing Koh\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) is a widely used qualitative method for identifying underlying meanings within unstructured text. However, TA requires manual processes, which become increasingly labour-intensive and time-consuming as datasets grow. While large language models (LLMs) have been introduced to assist with TA on small-scale datasets, three key limitations hinder their effectiveness. First, current approaches often depend on interactions between an LLM agent and a human coder, a process that becomes challenging with larger datasets. Second, with feedback from the human coder, the LLM tends to mirror the human coder, which provides a narrower viewpoint of the data. Third, existing methods follow a sequential process, where codes are generated for individual samples without recalling previous codes and associated data, reducing the ability to analyse data holistically. To address these limitations, we propose Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis. Thematic-LM assigns specialised tasks to each agent, such as coding, aggregating codes, and maintaining and updating the codebook. We assign coder agents different identity perspectives to simulate the subjective nature of TA, fostering a more diverse interpretation of the data. We applied Thematic-LM to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change. We evaluate the resulting themes based on trustworthiness principles in qualitative research. Our study reveals insights such as assigning different identities to coder agents promotes divergence in codes and themes.\"\\n      },\\n      {\\n        \"paperId\": \"54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\\n        \"title\": \"Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation\",\\n        \"citationCount\": 88,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2285255408\",\\n            \"name\": \"Jiawei Wang\"\\n          },\\n          {\\n            \"authorId\": \"31279896\",\\n            \"name\": \"Renhe Jiang\"\\n          },\\n          {\\n            \"authorId\": \"46962297\",\\n            \"name\": \"Chuang Yang\"\\n          },\\n          {\\n            \"authorId\": \"2157765133\",\\n            \"name\": \"Zengqing Wu\"\\n          },\\n          {\\n            \"authorId\": \"2266396584\",\\n            \"name\": \"Makoto Onizuka\"\\n          },\\n          {\\n            \"authorId\": \"2239490643\",\\n            \"name\": \"Ryosuke Shibasaki\"\\n          },\\n          {\\n            \"authorId\": \"2284717877\",\\n            \"name\": \"Chuan Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\\n      },\\n      {\\n        \"paperId\": \"7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\\n        \"title\": \"ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification\",\\n        \"citationCount\": 75,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3660810?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3660810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions, and can effectively facilitate the practical application of LLMs in real-world development environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1994579604\",\\n            \"name\": \"Fangwen Mu\"\\n          },\\n          {\\n            \"authorId\": \"2305416699\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2259571951\",\\n            \"name\": \"Song Wang\"\\n          },\\n          {\\n            \"authorId\": \"2259613131\",\\n            \"name\": \"Zhuohao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2259874187\",\\n            \"name\": \"Binquan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2259824656\",\\n            \"name\": \"ChenXue Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260294248\",\\n            \"name\": \"Shichao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2157214565\",\\n            \"name\": \"Qing Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.\"\\n      },\\n      {\\n        \"paperId\": \"630c47372317164fc367153f938903e1d5b76059\",\\n        \"url\": \"https://www.semanticscholar.org/paper/630c47372317164fc367153f938903e1d5b76059\",\\n        \"title\": \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52202/079017-1077?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52202/079017-1077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1602820179\",\\n            \"name\": \"Gaurang Sriramanan\"\\n          },\\n          {\\n            \"authorId\": \"2344249306\",\\n            \"name\": \"Siddhant Bharti\"\\n          },\\n          {\\n            \"authorId\": \"150333898\",\\n            \"name\": \"Vinu Sankar Sadasivan\"\\n          },\\n          {\\n            \"authorId\": \"152623528\",\\n            \"name\": \"Shoumik Saha\"\\n          },\\n          {\\n            \"authorId\": \"2305809801\",\\n            \"name\": \"Priyatham Kattakinda\"\\n          },\\n          {\\n            \"authorId\": \"34389431\",\\n            \"name\": \"S. Feizi\"\\n          }\\n        ],\\n        \"abstract\": \"While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations\\\\u2014 outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.\"\\n      },\\n      {\\n        \"paperId\": \"038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\\n        \"title\": \"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt), which eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1966961\",\\n            \"name\": \"Yanshen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2239274607\",\\n            \"name\": \"Jianfeng He\"\\n          },\\n          {\\n            \"authorId\": \"2293779433\",\\n            \"name\": \"Limeng Cui\"\\n          },\\n          {\\n            \"authorId\": \"3433489\",\\n            \"name\": \"Shuo Lei\"\\n          },\\n          {\\n            \"authorId\": \"2249846863\",\\n            \"name\": \"Chang-Tien Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.\"\\n      },\\n      {\\n        \"paperId\": \"f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\\n        \"title\": \"Reasoning Runtime Behavior of a Program with LLM: How Far are We?\",\\n        \"citationCount\": 49,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a framework, namely $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution, and utilizes existing code benchmarks and adapt them to new benchmarks within this framework.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293350098\",\\n            \"name\": \"Junkai Chen\"\\n          },\\n          {\\n            \"authorId\": \"2276184077\",\\n            \"name\": \"Zhiyuan Pan\"\\n          },\\n          {\\n            \"authorId\": \"2110049191\",\\n            \"name\": \"Xing Hu\"\\n          },\\n          {\\n            \"authorId\": \"2293350478\",\\n            \"name\": \"Zhenhao Li\"\\n          },\\n          {\\n            \"authorId\": \"2286413567\",\\n            \"name\": \"Ge Li\"\\n          },\\n          {\\n            \"authorId\": \"2265241871\",\\n            \"name\": \"Xin Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs (i.e., predicting code execution behaviors such as program output and execution path), but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs. Our code, data and $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$ leaderboard are available at https://r-eval.github.io.\"\\n      },\\n      {\\n        \"paperId\": \"8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\\n        \"title\": \"Don\\'t Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper leverages the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics, they can be prompted to autoformalize informal mathematical statements into formal Isabelle code -- which provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287813125\",\\n            \"name\": \"Jin Peng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2144884927\",\\n            \"name\": \"Charles Staats\"\\n          },\\n          {\\n            \"authorId\": \"2293653101\",\\n            \"name\": \"Wenda Li\"\\n          },\\n          {\\n            \"authorId\": \"2574060\",\\n            \"name\": \"Christian Szegedy\"\\n          },\\n          {\\n            \"authorId\": \"7446832\",\\n            \"name\": \"Kilian Q. Weinberger\"\\n          },\\n          {\\n            \"authorId\": \"2287780080\",\\n            \"name\": \"Yuhuai Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLM), such as Google\\'s Minerva and OpenAI\\'s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.\"\\n      },\\n      {\\n        \"paperId\": \"1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\\n        \"title\": \"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.17578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MM-Eval is introduced, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages designed with multilingual-specific challenges in mind and finds that evaluators are unfair and inconsistent when evaluating lower-resourced languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2321452295\",\\n            \"name\": \"Guijin Son\"\\n          },\\n          {\\n            \"authorId\": \"29830817\",\\n            \"name\": \"Dongkeun Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2299329316\",\\n            \"name\": \"Juyoung Suk\"\\n          },\\n          {\\n            \"authorId\": \"2301578911\",\\n            \"name\": \"Javier Aula-Blasco\"\\n          },\\n          {\\n            \"authorId\": \"2327215494\",\\n            \"name\": \"Mano Aslan\"\\n          },\\n          {\\n            \"authorId\": \"2327216625\",\\n            \"name\": \"Vu Trong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2232783785\",\\n            \"name\": \"Shayekh Bin Islam\"\\n          },\\n          {\\n            \"authorId\": \"2327215436\",\\n            \"name\": \"Jaume Prats-Cristi\\\\u00e0\"\\n          },\\n          {\\n            \"authorId\": \"2327217057\",\\n            \"name\": \"Luc\\\\u00eda Tormo-Ba\\\\u00f1uelos\"\\n          },\\n          {\\n            \"authorId\": \"2184037220\",\\n            \"name\": \"Seungone Kim\"\\n          }\\n        ],\\n        \"abstract\": \"As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\\\\\"meta-evaluation benchmarks\\\\\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.\"\\n      },\\n      {\\n        \"paperId\": \"f1366e505de4f1d0e901903e3c17471033758a96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f1366e505de4f1d0e901903e3c17471033758a96\",\\n        \"title\": \"Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based Chatbots\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.11876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Rescriber is designed, built, and evaluated, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts, presenting a promising approach to address the privacy and trust challenges of AI.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2326261436\",\\n            \"name\": \"Jijie Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2326116321\",\\n            \"name\": \"Eryue Xu\"\\n          },\\n          {\\n            \"authorId\": \"2326262935\",\\n            \"name\": \"Yaoyao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2326228781\",\\n            \"name\": \"Tianshi Li\"\\n          }\\n        ],\\n        \"abstract\": \"The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users\\\\u2019 personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=Rescriber) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users\\\\u2019 subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4o. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users\\\\u2019 trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.\"\\n      },\\n      {\\n        \"paperId\": \"3ec06fe8d8764123490544ab5dc956143e84b443\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3ec06fe8d8764123490544ab5dc956143e84b443\",\\n        \"title\": \"Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.11977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise and the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310565909\",\\n            \"name\": \"Guangzhi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2089532795\",\\n            \"name\": \"Xiao Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2275248675\",\\n            \"name\": \"Jose Such\"\\n          }\\n        ],\\n        \"abstract\": \"The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\\n      },\\n      {\\n        \"paperId\": \"dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\\n        \"title\": \"Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.07791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating position bias in LLM judges across pairwise and list-wise comparison settings confirms that position bias is not due to random chance and varies significantly across judges and tasks, and provides insights into the distribution of judging difficulty across the dataset.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305925735\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2262963382\",\\n            \"name\": \"Chiyu Ma\"\\n          },\\n          {\\n            \"authorId\": \"2330065663\",\\n            \"name\": \"Wenhua Liang\"\\n          },\\n          {\\n            \"authorId\": \"2227771\",\\n            \"name\": \"Weicheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"1918441\",\\n            \"name\": \"Soroush Vosoughi\"\\n          }\\n        ],\\n        \"abstract\": \"LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases - particularly position bias, the tendency to favor solutions based on their position within the prompt - compromise its reliability. This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 15 LLM judges across MTBench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.\"\\n      },\\n      {\\n        \"paperId\": \"638d8d1f3865ebf065605535a7aa50727d5ffabe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/638d8d1f3865ebf065605535a7aa50727d5ffabe\",\\n        \"title\": \"TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.18919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation, significantly improves semantic and contextual consistency in synthesized images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2298945483\",\\n            \"name\": \"Junhao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2298032416\",\\n            \"name\": \"Baiqiao Yin\"\\n          },\\n          {\\n            \"authorId\": \"2229014859\",\\n            \"name\": \"Kaixin Cai\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2276604489\",\\n            \"name\": \"Hanhui Li\"\\n          },\\n          {\\n            \"authorId\": \"2299161673\",\\n            \"name\": \"Yuxin He\"\\n          },\\n          {\\n            \"authorId\": \"2298943419\",\\n            \"name\": \"Xi Lu\"\\n          },\\n          {\\n            \"authorId\": \"2298043252\",\\n            \"name\": \"Yue Li\"\\n          },\\n          {\\n            \"authorId\": \"2298926852\",\\n            \"name\": \"Yifei Li\"\\n          },\\n          {\\n            \"authorId\": \"2298644153\",\\n            \"name\": \"Yuhao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"144880586\",\\n            \"name\": \"Yiqiang Yan\"\\n          },\\n          {\\n            \"authorId\": \"2291389227\",\\n            \"name\": \"Xiaodan Liang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a\\\\\"Screenwriter\\\\\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the\\\\\"Rehearsal\\\\\". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the\\\\\"Final Performance\\\\\". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.\"\\n      },\\n      {\\n        \"paperId\": \"4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"title\": \"Improving Text-to-Image Consistency via Automatic Prompt Optimization\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models and paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1796269096\",\\n            \"name\": \"Oscar Ma\\\\u00f1as\"\\n          },\\n          {\\n            \"authorId\": \"2274101827\",\\n            \"name\": \"Pietro Astolfi\"\\n          },\\n          {\\n            \"authorId\": \"2293590162\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"2256372432\",\\n            \"name\": \"Candace Ross\"\\n          },\\n          {\\n            \"authorId\": \"39219656\",\\n            \"name\": \"Jack Urbanek\"\\n          },\\n          {\\n            \"authorId\": \"2293907712\",\\n            \"name\": \"Adina Williams\"\\n          },\\n          {\\n            \"authorId\": \"2801949\",\\n            \"name\": \"Aishwarya Agrawal\"\\n          },\\n          {\\n            \"authorId\": \"1456285042\",\\n            \"name\": \"Adriana Romero-Soriano\"\\n          },\\n          {\\n            \"authorId\": \"3325894\",\\n            \"name\": \"M. Drozdzal\"\\n          }\\n        ],\\n        \"abstract\": \"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"275a3955a83867dd36a3683788e0e053e00f8a89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/275a3955a83867dd36a3683788e0e053e00f8a89\",\\n        \"title\": \"Collaborative Enhancement of Consistency and Accuracy in US Diagnosis of Thyroid Nodules Using Large Language Models.\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1148/radiol.232255?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1148/radiol.232255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging and ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1678966811\",\\n            \"name\": \"Shaohong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2272038393\",\\n            \"name\": \"Wen-juan Tong\"\\n          },\\n          {\\n            \"authorId\": \"2127991969\",\\n            \"name\": \"Ming-De Li\"\\n          },\\n          {\\n            \"authorId\": \"28890237\",\\n            \"name\": \"Hang-tong Hu\"\\n          },\\n          {\\n            \"authorId\": \"2187182790\",\\n            \"name\": \"Xiao-zhou Lu\"\\n          },\\n          {\\n            \"authorId\": \"2185218332\",\\n            \"name\": \"Ze-Rong Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290915880\",\\n            \"name\": \"Xin-Xin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290897544\",\\n            \"name\": \"Ruifang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2261915754\",\\n            \"name\": \"Ming-De Lu\"\\n          },\\n          {\\n            \"authorId\": \"6457299\",\\n            \"name\": \"Li-da Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290866279\",\\n            \"name\": \"Wei Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Background Large language models (LLMs) hold substantial promise for medical imaging interpretation. However, there is a lack of studies on their feasibility in handling reasoning questions associated with medical diagnosis. Purpose To investigate the viability of leveraging three publicly available LLMs to enhance consistency and diagnostic accuracy in medical imaging based on standardized reporting, with pathology as the reference standard. Materials and Methods US images of thyroid nodules with pathologic results were retrospectively collected from a tertiary referral hospital between July 2022 and December 2022 and used to evaluate malignancy diagnoses generated by three LLMs-OpenAI\\'s ChatGPT 3.5, ChatGPT 4.0, and Google\\'s Bard. Inter- and intra-LLM agreement of diagnosis were evaluated. Then, diagnostic performance, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), was evaluated and compared for the LLMs and three interactive approaches: human reader combined with LLMs, image-to-text model combined with LLMs, and an end-to-end convolutional neural network model. Results A total of 1161 US images of thyroid nodules (498 benign, 663 malignant) from 725 patients (mean age, 42.2 years \\\\u00b1 14.1 [SD]; 516 women) were evaluated. ChatGPT 4.0 and Bard displayed substantial to almost perfect intra-LLM agreement (\\\\u03ba range, 0.65-0.86 [95% CI: 0.64, 0.86]), while ChatGPT 3.5 showed fair to substantial agreement (\\\\u03ba range, 0.36-0.68 [95% CI: 0.36, 0.68]). ChatGPT 4.0 had an accuracy of 78%-86% (95% CI: 76%, 88%) and sensitivity of 86%-95% (95% CI: 83%, 96%), compared with 74%-86% (95% CI: 71%, 88%) and 74%-91% (95% CI: 71%, 93%), respectively, for Bard. Moreover, with ChatGPT 4.0, the image-to-text-LLM strategy exhibited an AUC (0.83 [95% CI: 0.80, 0.85]) and accuracy (84% [95% CI: 82%, 86%]) comparable to those of the human-LLM interaction strategy with two senior readers and one junior reader and exceeding those of the human-LLM interaction strategy with one junior reader. Conclusion LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging. ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5. \\\\u00a9 RSNA, 2024 Supplemental material is available for this article.\"\\n      },\\n      {\\n        \"paperId\": \"b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\\n        \"title\": \"Mitigating LLM Hallucinations via Conformal Abstention\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.01563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1388837087\",\\n            \"name\": \"Yasin Abbasi-Yadkori\"\\n          },\\n          {\\n            \"authorId\": \"3150458\",\\n            \"name\": \"Ilja Kuzborskij\"\\n          },\\n          {\\n            \"authorId\": \"2298902427\",\\n            \"name\": \"David Stutz\"\\n          },\\n          {\\n            \"authorId\": \"2305592785\",\\n            \"name\": \"Andr\\\\u00e1s Gy\\\\u00f6rgy\"\\n          },\\n          {\\n            \"authorId\": \"2299943725\",\\n            \"name\": \"Adam Fisch\"\\n          },\\n          {\\n            \"authorId\": \"2299943677\",\\n            \"name\": \"Arnaud Doucet\"\\n          },\\n          {\\n            \"authorId\": \"2299943992\",\\n            \"name\": \"Iuliya Beloshapka\"\\n          },\\n          {\\n            \"authorId\": \"2239098855\",\\n            \"name\": \"Wei-Hung Weng\"\\n          },\\n          {\\n            \"authorId\": \"2300022831\",\\n            \"name\": \"Yao-Yuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257346986\",\\n            \"name\": \"Csaba Szepesv\\'ari\"\\n          },\\n          {\\n            \"authorId\": \"9235290\",\\n            \"name\": \"Ali Taylan Cemgil\"\\n          },\\n          {\\n            \"authorId\": \"2359197879\",\\n            \"name\": \"Nenad Tomasev\"\\n          }\\n        ],\\n        \"abstract\": \"We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying\\\\\"I don\\'t know\\\\\") in a general domain, instead of resorting to possibly\\\\\"hallucinating\\\\\"a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.\"\\n      },\\n      {\\n        \"paperId\": \"023a98af94a3e7e8e538a6183da8ec05024fec56\",\\n        \"url\": \"https://www.semanticscholar.org/paper/023a98af94a3e7e8e538a6183da8ec05024fec56\",\\n        \"title\": \"How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work prompts different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions, and finds that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2307916998\",\\n            \"name\": \"Julia Kharchenko\"\\n          },\\n          {\\n            \"authorId\": \"2284066307\",\\n            \"name\": \"Tanya Roosta\"\\n          },\\n          {\\n            \"authorId\": \"2284065969\",\\n            \"name\": \"Aman Chadha\"\\n          },\\n          {\\n            \"authorId\": \"2234352974\",\\n            \"name\": \"Chirag Shah\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user\\'s known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs\\'cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"a1849a77644ff411a03833b5aa7a65ff57158c50\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1849a77644ff411a03833b5aa7a65ff57158c50\",\\n        \"title\": \"CLLMs: Consistency Large Language Models\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.00835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work develops a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory by refining the target LLM to consistently predict the fixed point given any state as input.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258963117\",\\n            \"name\": \"Siqi Kou\"\\n          },\\n          {\\n            \"authorId\": \"2258334187\",\\n            \"name\": \"Lanxiang Hu\"\\n          },\\n          {\\n            \"authorId\": \"2116778591\",\\n            \"name\": \"Zhe He\"\\n          },\\n          {\\n            \"authorId\": \"2260296481\",\\n            \"name\": \"Zhijie Deng\"\\n          },\\n          {\\n            \"authorId\": \"2289837431\",\\n            \"name\": \"Hao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\\\\\\\times$ to 3.4$\\\\\\\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\\n        \"title\": \"PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children\\'s Collaborative Learning\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613905.3651008\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.14227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded and thus highlight potential design improvements and considerations for peer agents in both roles.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-03-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2296401615\",\\n            \"name\": \"Jiawen Liu\"\\n          },\\n          {\\n            \"authorId\": \"2292671914\",\\n            \"name\": \"Yuanyuan Yao\"\\n          },\\n          {\\n            \"authorId\": \"2283762773\",\\n            \"name\": \"Pengcheng An\"\\n          },\\n          {\\n            \"authorId\": \"2301178822\",\\n            \"name\": \"Qi Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In children\\\\u2019s collaborative learning, effective peer conversations can significantly enhance the quality of children\\\\u2019s collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children\\\\u2019s creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.\"\\n      },\\n      {\\n        \"paperId\": \"5e317746c0f38d1149f33a19807af47d513bdd27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5e317746c0f38d1149f33a19807af47d513bdd27\",\\n        \"title\": \"Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.12509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald\\'s omega, and demonstrates the limitations of fixed randomness and the importance of considering multiple samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2328309355\",\\n            \"name\": \"Kayla Schroeder\"\\n          },\\n          {\\n            \"authorId\": \"1411379613\",\\n            \"name\": \"Zach Wood-Doughty\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs. While deterministic settings can improve consistency, they do not guarantee reliability, as a single sample from the model\\'s probability distribution can still be misleading. Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald\\'s omega. We evaluate the reliability of LLMs when judging the outputs of other LLMs on standard single-turn and multi-turn benchmarks, simultaneously investigating the impact of temperature on reliability. By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implications for downstream applications. Our findings highlight the need for a nuanced understanding of LLM reliability and the potential risks associated with over-reliance on single-shot evaluations. This work provides a crucial step towards building more trustworthy and reliable LLM-based systems and applications.\"\\n      },\\n      {\\n        \"paperId\": \"235a8bd57a6b53ecab756780a45bce6e4743cecd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/235a8bd57a6b53ecab756780a45bce6e4743cecd\",\\n        \"title\": \"LLM-based Multi-Level Knowledge Generation for Few-shot Knowledge Graph Completion\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2024/236?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2024/236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a generation-based FKGC paradigm facilitated by LLM distillation, which achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284861197\",\\n            \"name\": \"Qian Li\"\\n          },\\n          {\\n            \"authorId\": \"2313599793\",\\n            \"name\": \"Zhuo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2052296239\",\\n            \"name\": \"Cheng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2313658619\",\\n            \"name\": \"Shiqi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2274552581\",\\n            \"name\": \"Jianxin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Knowledge Graphs (KGs) are pivotal in various NLP applications but often grapple with incompleteness, especially due to the long-tail problem where infrequent, unpopular relationships drastically reduce the KG completion performance. In this paper, we focus on Few-shot Knowledge Graph Completion (FKGC), a task addressing these gaps in long-tail scenarios. Amidst the rapid evolution of Large Language Models, we propose a generation-based FKGC paradigm facilitated by LLM distillation. Our MuKDC framework employs multi-level knowledge distillation for few-shot KG completion, generating supplementary knowledge to mitigate data scarcity in few-shot environments. MuKDC comprises two primary components: Multi-level Knowledge Generation, which enriches the KG at various levels, and Consistency Assessment, to ensure the coherence and reliability of the generated knowledge. Most notably, our method achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\\n      },\\n      {\\n        \"paperId\": \"9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\\n        \"title\": \"Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2144511530\",\\n            \"name\": \"Xuan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2265432385\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2302819855\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"2302765389\",\\n            \"name\": \"Haoyang Shang\"\\n          },\\n          {\\n            \"authorId\": \"2302927286\",\\n            \"name\": \"Chengxu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2302915902\",\\n            \"name\": \"Quanyan Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents\\' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents\\' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\"\\n      },\\n      {\\n        \"paperId\": \"086046d38b3a7066aa39e3d350905e8065c8f1b5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/086046d38b3a7066aa39e3d350905e8065c8f1b5\",\\n        \"title\": \"Efficiently Scaling LLM Reasoning with Certaindex\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.20993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy, so this work introduces Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-12-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282896192\",\\n            \"name\": \"Yichao Fu\"\\n          },\\n          {\\n            \"authorId\": \"2279862923\",\\n            \"name\": \"Junda Chen\"\\n          },\\n          {\\n            \"authorId\": \"2317134948\",\\n            \"name\": \"Siqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2337869460\",\\n            \"name\": \"Zheyu Fu\"\\n          },\\n          {\\n            \"authorId\": \"2351053054\",\\n            \"name\": \"Zhongdongming Dai\"\\n          },\\n          {\\n            \"authorId\": \"2152482391\",\\n            \"name\": \"Yonghao Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2363671676\",\\n            \"name\": \"Yian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2317112099\",\\n            \"name\": \"Aurick Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2379937619\",\\n            \"name\": \"Tajana Rosing\"\\n          },\\n          {\\n            \"authorId\": \"2344601177\",\\n            \"name\": \"Ion Stoica\"\\n          },\\n          {\\n            \"authorId\": \"2337807823\",\\n            \"name\": \"Hao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50% compute savings and 3.3x higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git\"\\n      },\\n      {\\n        \"paperId\": \"43fedc4430be030c083626b3e64b7093916b429a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43fedc4430be030c083626b3e64b7093916b429a\",\\n        \"title\": \"Enhancing clinical reasoning skills for medical students: a qualitative comparison of LLM-powered social robotic versus computer-based virtual patients within rheumatology\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1007/s00296-024-05731-0\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11618132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform and shows promise in training CR skills, communication, and adaptive thinking.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290951787\",\\n            \"name\": \"Alexander Borg\"\\n          },\\n          {\\n            \"authorId\": \"2326093338\",\\n            \"name\": \"Benjamin Jobs\"\\n          },\\n          {\\n            \"authorId\": \"2164303442\",\\n            \"name\": \"Viking Huss\"\\n          },\\n          {\\n            \"authorId\": \"23717264\",\\n            \"name\": \"C. Gentline\"\\n          },\\n          {\\n            \"authorId\": \"2326093767\",\\n            \"name\": \"Fabricio Espinosa\"\\n          },\\n          {\\n            \"authorId\": \"2290722393\",\\n            \"name\": \"Mini Ruiz\"\\n          },\\n          {\\n            \"authorId\": \"2758537\",\\n            \"name\": \"Samuel Edelbring\"\\n          },\\n          {\\n            \"authorId\": \"2326094583\",\\n            \"name\": \"Carina Georg\"\\n          },\\n          {\\n            \"authorId\": \"103081544\",\\n            \"name\": \"G. Skantze\"\\n          },\\n          {\\n            \"authorId\": \"8637952\",\\n            \"name\": \"Ioannis Parodis\"\\n          }\\n        ],\\n        \"abstract\": \"Virtual patients (VPs) are increasingly used in medical education to train clinical reasoning (CR) skills. However, optimal VP design for enhancing interactivity and authenticity remains unclear. Novel interactive modalities, such as large language model (LLM)-enhanced social robotic VPs might increase interactivity and authenticity in CR skill practice. To evaluate medical students\\\\u2019 perceptions of CR training using an LLM-enhanced social robotic VP platform compared with a conventional computer-based VP platform. A qualitative study involved 23 third-year medical students from Karolinska Institutet, who completed VP cases on an LLM-enhanced social robotic platform and a computer-based semi-linear platform. In-depth interviews assessed students\\\\u2019 self-perceived acquirement of CR skills using the two platforms. Thematic analysis was employed to identify themes and sub-themes. Three main themes were identified: authenticity, VP application, and strengths and limitations. Students found the social robotic platform more authentic and engaging. It enabled highly interactive communication and expressed emotions, collectively offering a realistic experience. It facilitated active learning, hypothesis generation, and adaptive thinking. Limitations included lack of physical examination options and, occasionally, mechanical dialogue. The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform. Despite some limitations, it shows promise in training CR skills, communication, and adaptive thinking. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. \\\\u2022An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. \\\\u2022Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. \\\\u2022Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. \\\\u2022An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. \\\\u2022Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. \\\\u2022Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations.\"\\n      },\\n      {\\n        \"paperId\": \"f04c25fcf3247ff4d8eca72d862b22090b884b75\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f04c25fcf3247ff4d8eca72d862b22090b884b75\",\\n        \"title\": \"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.06461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49606614\",\\n            \"name\": \"Junlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282521448\",\\n            \"name\": \"Siddhartha Jain\"\\n          },\\n          {\\n            \"authorId\": \"2305691523\",\\n            \"name\": \"Dejiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2282366776\",\\n            \"name\": \"Baishakhi Ray\"\\n          },\\n          {\\n            \"authorId\": \"40574366\",\\n            \"name\": \"Varun Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2304481349\",\\n            \"name\": \"Ben Athiwaratkun\"\\n          }\\n        ],\\n        \"abstract\": \"A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don\\\\u2019t surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.\"\\n      },\\n      {\\n        \"paperId\": \"7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\\n        \"title\": \"What Did I Do Wrong? Quantifying LLMs\\' Sensitivity and Consistency to Prompt Engineering\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.12334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Two metrics for classification tasks, namely sensitivity and consistency, are introduced, which are complementary to task performance, and it is hoped that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2307085791\",\\n            \"name\": \"Federico Errica\"\\n          },\\n          {\\n            \"authorId\": \"2009237\",\\n            \"name\": \"G. Siracusano\"\\n          },\\n          {\\n            \"authorId\": \"3109801\",\\n            \"name\": \"D. Sanvito\"\\n          },\\n          {\\n            \"authorId\": \"2269460793\",\\n            \"name\": \"Roberto Bifulco\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs\\'inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely sensitivity and consistency, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\\n      },\\n      {\\n        \"paperId\": \"024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\\n        \"title\": \"PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2403.09732?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2403.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt representation, called reference-enhanced representation, is introduced, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries and proposes using cross-consistency across different LLMs rather than self-consistency within a particular LLM.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2262448530\",\\n            \"name\": \"Zhishuai Li\"\\n          },\\n          {\\n            \"authorId\": \"2292059965\",\\n            \"name\": \"Xiang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2291921433\",\\n            \"name\": \"Jingjing Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2262480162\",\\n            \"name\": \"Sun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2228059114\",\\n            \"name\": \"Guoqing Du\"\\n          },\\n          {\\n            \"authorId\": \"2267589674\",\\n            \"name\": \"Xiaoru Hu\"\\n          },\\n          {\\n            \"authorId\": \"2315291036\",\\n            \"name\": \"Bin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2276235755\",\\n            \"name\": \"Yuxiao Ye\"\\n          },\\n          {\\n            \"authorId\": \"2262543561\",\\n            \"name\": \"Ziyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2263456785\",\\n            \"name\": \"Rui Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2262446566\",\\n            \"name\": \"Hangyu Mao\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0e314ddbf28514d92f2405b73941242c162ae0ba\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0e314ddbf28514d92f2405b73941242c162ae0ba\",\\n        \"title\": \"AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.11073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AFaCTA (Automatic Factual Claim deTection Annotator) is introduced, a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs) and calibrates its annotation confidence with consistency along three predefined reasoning paths.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046974354\",\\n            \"name\": \"Jingwei Ni\"\\n          },\\n          {\\n            \"authorId\": \"2284947386\",\\n            \"name\": \"Minjing Shi\"\\n          },\\n          {\\n            \"authorId\": \"146552774\",\\n            \"name\": \"Dominik Stammbach\"\\n          },\\n          {\\n            \"authorId\": \"2790926\",\\n            \"name\": \"Mrinmaya Sachan\"\\n          },\\n          {\\n            \"authorId\": \"2261279066\",\\n            \"name\": \"Elliott Ash\"\\n          },\\n          {\\n            \"authorId\": \"3073566\",\\n            \"name\": \"Markus Leippold\"\\n          }\\n        ],\\n        \"abstract\": \"With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.\"\\n      },\\n      {\\n        \"paperId\": \"1c5a097b4e376897545f153370425cf7e0c2d8fd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1c5a097b4e376897545f153370425cf7e0c2d8fd\",\\n        \"title\": \"Explaining Length Bias in LLM-Based Preference Evaluations\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.01085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AdapAlpaca is proposed, a simple yet effective adjustment to win rate measurement that ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Zhengyu Hu\"\\n          },\\n          {\\n            \"authorId\": \"2322070046\",\\n            \"name\": \"Linxin Song\"\\n          },\\n          {\\n            \"authorId\": \"2309191644\",\\n            \"name\": \"Jieyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2311315868\",\\n            \"name\": \"Zheyuan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2269687536\",\\n            \"name\": \"Jingang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2309176938\",\\n            \"name\": \"Zhenyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2309202283\",\\n            \"name\": \"Jieyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2269470756\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"The use of large language models (LLMs) as judges, particularly in preference comparisons, has become widespread, but this reveals a notable bias towards longer responses, undermining the reliability of such evaluations. To better understand such bias, we propose to decompose the preference evaluation metric, specifically the win rate, into two key components: desirability and information mass, where the former is length-independent and related to trustworthiness such as correctness, toxicity, and consistency, and the latter is length-dependent and represents the amount of information in the response. We empirically demonstrated the decomposition through controlled experiments and found that response length impacts evaluations by influencing information mass. To derive a reliable evaluation metric that assesses content quality without being confounded by response length, we propose AdapAlpaca, a simple yet effective adjustment to win rate measurement. Specifically, AdapAlpaca ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\\n      },\\n      {\\n        \"paperId\": \"00ce8beee350a260395676490915d7ebfa7430d1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/00ce8beee350a260395676490915d7ebfa7430d1\",\\n        \"title\": \"Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Investigation of fine-tuned LLMs in D2T tasks in terms of model size reveals that increasing LLM size enhances readability andformativeness in D2T tasks, but larger LLMs may sacrifice \\\\\\\\textit{faithfulness}.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8038450\",\\n            \"name\": \"Joy Mahapatra\"\\n          },\\n          {\\n            \"authorId\": \"2312204876\",\\n            \"name\": \"U. Garain\"\\n          }\\n        ],\\n        \"abstract\": \"Data-to-text (D2T) generation aims to generate human-readable text from semi-structured data, such as tables and graphs. The recent success of D2T is largely attributed to advancements in LLMs. Despite the success of LLMs, no research has been conducted to illustrate the impact of model size on the performance of fine-tuned LLMs for D2T tasks. D2T model performance is typically assessed based on three key qualities: \\\\\\\\textit{readability} (indicates fluency and coherence), \\\\\\\\textit{informativeness} (measures content similarity), and \\\\\\\\textit{faithfulness} (assesses consistency of factual information). It is currently uncertain whether increasing the size of LLMs effectively improves performance in D2T tasks across these three qualities. The objective of this study is to investigate the performance of fine-tuned LLMs in D2T tasks in terms of model size. Through extensive comparative analysis, we aim to elucidate both the advantages and limitations of scaling model sizes across five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and twelve state-of-the-art LLMs with varying sizes from five different LLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all the three essential qualities of D2T models, we incorporate six widely recognized automatic metrics -- \\\\\\\\textsc{BLEU}, \\\\\\\\textsc{METEOR}, \\\\\\\\textsc{BERTScore}, \\\\\\\\textsc{MoverScore}, \\\\\\\\textsc{Parent}, and \\\\\\\\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance concerning model size in the presence of source-reference divergence, a critical aspect of D2T tasks. Our investigation reveals that increasing LLM size enhances \\\\\\\\textit{readability} and \\\\\\\\textit{informativeness} in D2T tasks, but larger (in terms of size) LLMs may sacrifice \\\\\\\\textit{faithfulness}. Moreover, small-sized LLMs show more resilience than larger ones when source-reference divergence is present.\"\\n      },\\n      {\\n        \"paperId\": \"74908bc543e77b8995a6eebe32ab13cf0837949b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/74908bc543e77b8995a6eebe32ab13cf0837949b\",\\n        \"title\": \"LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment and demonstrates that, when using GPT-4, the proposed metric achieves evaluation consistency close to that of radiologists.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2294387070\",\\n            \"name\": \"Zilong Wang\"\\n          },\\n          {\\n            \"authorId\": \"13289447\",\\n            \"name\": \"Xufang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2268347004\",\\n            \"name\": \"Xinyang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268313028\",\\n            \"name\": \"Dongsheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2160727304\",\\n            \"name\": \"Lili Qiu\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task\\'s clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.\"\\n      },\\n      {\\n        \"paperId\": \"f33991c02f8f0ab8794dad020c648b37ccc53365\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f33991c02f8f0ab8794dad020c648b37ccc53365\",\\n        \"title\": \"CitaLaw: Enhancing LLM with Citations in Legal Domain\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.14556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes CitaLaw, the first benchmark designed to evaluate LLMs\\' ability to produce legally sound responses with appropriate citations, and introduces syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290124325\",\\n            \"name\": \"Kepu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2118684861\",\\n            \"name\": \"Weijie Yu\"\\n          },\\n          {\\n            \"authorId\": \"2155892801\",\\n            \"name\": \"Sunhao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2274965731\",\\n            \"name\": \"Jun Xu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs\\' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.\"\\n      },\\n      {\\n        \"paperId\": \"0d5ccf0861b62223fe562e13369ab15746188251\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0d5ccf0861b62223fe562e13369ab15746188251\",\\n        \"title\": \"QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.05345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums, and applies QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2300845194\",\\n            \"name\": \"Varun Nagaraj Rao\"\\n          },\\n          {\\n            \"authorId\": \"2300370478\",\\n            \"name\": \"Eesha Agarwal\"\\n          },\\n          {\\n            \"authorId\": \"2300371225\",\\n            \"name\": \"Samantha Dalal\"\\n          },\\n          {\\n            \"authorId\": \"2265042713\",\\n            \"name\": \"Dan Calacci\"\\n          },\\n          {\\n            \"authorId\": \"2266397659\",\\n            \"name\": \"Andr\\'es Monroy-Hern\\'andez\"\\n          }\\n        ],\\n        \"abstract\": \"Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methodologies used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting and human evaluation methodology. We applied this framework to analyze over one million comments from two of Reddit\\'s rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.\"\\n      },\\n      {\\n        \"paperId\": \"e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\\n        \"title\": \"Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.13082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TREACLE is proposed, a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\\'s monetary cost and latency constraints, and provides the user with the ability to gracefully trade off accuracy for cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2157197701\",\\n            \"name\": \"Xuechen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2297821531\",\\n            \"name\": \"Zijian Huang\"\\n          },\\n          {\\n            \"authorId\": \"2297769735\",\\n            \"name\": \"Ege Onur Taga\"\\n          },\\n          {\\n            \"authorId\": \"1393650147\",\\n            \"name\": \"Carlee Joe-Wong\"\\n          },\\n          {\\n            \"authorId\": \"3103394\",\\n            \"name\": \"Samet Oymak\"\\n          },\\n          {\\n            \"authorId\": \"2281075331\",\\n            \"name\": \"Jiasi Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE ($\\\\\\\\underline{T}$hrifty $\\\\\\\\underline{Rea}$soning via $\\\\\\\\underline{C}$ontext-Aware $\\\\\\\\underline{L}$LM and Prompt S$\\\\\\\\underline{e}$lection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\\'s monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.\"\\n      },\\n      {\\n        \"paperId\": \"c01bbc439164002f2c7326748f7939783e306a94\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c01bbc439164002f2c7326748f7939783e306a94\",\\n        \"title\": \"A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.09972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that the order of presenting reasons and scores significantly influences LLMs\\' scoring, with a different level of rule understanding in the prompt, crucial for improving the accuracy and consistency of LLM-based evaluations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2264464750\",\\n            \"name\": \"Kuanchao Chu\"\\n          },\\n          {\\n            \"authorId\": \"2109381394\",\\n            \"name\": \"Yi-Pei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2301580436\",\\n            \"name\": \"Hideki Nakayama\"\\n          }\\n        ],\\n        \"abstract\": \"This research investigates prompt designs of evaluating generated texts using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for open-ended text evaluation remains challenging due to model sensitivity and subjectivity in evaluation of text generation. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs\\' scoring, with a different level of rule understanding in the prompt. An additional optimization may enhance scoring alignment if sufficient data is available. This insight is crucial for improving the accuracy and consistency of LLM-based evaluations.\"\\n      },\\n      {\\n        \"paperId\": \"a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\\n        \"title\": \"How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment\",\\n        \"citationCount\": 135,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.14049\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Insight into novice learners\\\\u2019 use of AI code generators in a self-paced learning environment is offered, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3136345\",\\n            \"name\": \"Majeed Kazemitabaar\"\\n          },\\n          {\\n            \"authorId\": \"2112801172\",\\n            \"name\": \"Xinying Hou\"\\n          },\\n          {\\n            \"authorId\": \"2063979470\",\\n            \"name\": \"A. Henley\"\\n          },\\n          {\\n            \"authorId\": \"20937525\",\\n            \"name\": \"B. Ericson\"\\n          },\\n          {\\n            \"authorId\": \"2862077\",\\n            \"name\": \"David Weintrop\"\\n          },\\n          {\\n            \"authorId\": \"2666589\",\\n            \"name\": \"Tovi Grossman\"\\n          }\\n        ],\\n        \"abstract\": \"As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners\\\\u2019 utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners\\\\u2019 use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\\n      },\\n      {\\n        \"paperId\": \"221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\\n        \"title\": \"PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes to comprehensively address all subtasks together, and suggests employing a unique prompt-based in-context learning strategy to enhance error correction and error detection performance in medical systems where prediction errors can have grave consequences.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9122885\",\\n            \"name\": \"Kesav Gundabathula\"\\n          },\\n          {\\n            \"authorId\": \"2301202434\",\\n            \"name\": \"Sriram R Kolar\"\\n          }\\n        ],\\n        \"abstract\": \"This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals. This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it. Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information. We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy. We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge. In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='cdaed8ce-2456-42d3-90da-95687861e4d1')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "graph = workflow.compile()\n",
    "graph.get_graph().draw_mermaid_png(output_file_path='story.png')\n",
    "\n",
    "result_llm = graph.invoke(\n",
    "    {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\":\n",
    "            \"\"\"Dynamic Prompt Adaptation:\n",
    "                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\n",
    "                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\n",
    "                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\n",
    "                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\n",
    "                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\"\"\"\n",
    "        }]\n",
    "    },\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3d43f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_papers = json.loads(result_llm[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9efed2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: dynamic prompt adaptation LLM\n",
      "Query: coherence in extended LLM\n",
      "Query: adaptive prompt generation\n",
      "Query: iterative context update\n",
      "Query: thematic consistency LLM\n"
     ]
    }
   ],
   "source": [
    "for query, papers in list_of_papers.items():\n",
    "    print(f\"Query: {query}\")\n",
    "    # for paper in papers['data']:\n",
    "    #     print(paper['title'], paper['citationCount'],paper['url'], paper['publicationDate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dynamic prompt adaptation LLM\": {\n",
      "    \"total\": 467,\n",
      "    \"offset\": 0,\n",
      "    \"next\": 50,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\n",
      "        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2221081346\",\n",
      "            \"name\": \"Yunbei Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2090707280\",\n",
      "            \"name\": \"Akshay Mehra\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261749247\",\n",
      "            \"name\": \"Shuaicheng Niu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4832622\",\n",
      "            \"name\": \"Jihun Hamm\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"cf95279b1da9de1aad9e7c651f5048f69af295ed\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/cf95279b1da9de1aad9e7c651f5048f69af295ed\",\n",
      "        \"title\": \"AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents\",\n",
      "        \"citationCount\": 76,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.13352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is found that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2175939276\",\n",
      "            \"name\": \"Edoardo Debenedetti\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299061721\",\n",
      "            \"name\": \"Jie Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2138580250\",\n",
      "            \"name\": \"Mislav Balunovi'c\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2150869869\",\n",
      "            \"name\": \"Luca Beurer-Kellner\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307472727\",\n",
      "            \"name\": \"Marc Fischer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267733649\",\n",
      "            \"name\": \"Florian Tramr\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3967189742efab8859da542ce3953d4c72957aca\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3967189742efab8859da542ce3953d4c72957aca\",\n",
      "        \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.23904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI), designed to design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2275054108\",\n",
      "            \"name\": \"Qinqian Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313081973\",\n",
      "            \"name\": \"Bo Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256998291\",\n",
      "            \"name\": \"Robby T. Tan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"659e0b3303caa860348dee52f41476e3fddc9573\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/659e0b3303caa860348dee52f41476e3fddc9573\",\n",
      "        \"title\": \"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference\",\n",
      "        \"citationCount\": 55,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel method is introduced, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages and allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2237800256\",\n",
      "            \"name\": \"Qichen Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237803694\",\n",
      "            \"name\": \"Minsik Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2178316365\",\n",
      "            \"name\": \"Thomas Merth\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256998189\",\n",
      "            \"name\": \"Sachin Mehta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284683934\",\n",
      "            \"name\": \"Mohammad Rastegari\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40465379\",\n",
      "            \"name\": \"Mahyar Najibi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\n",
      "        \"title\": \"From ideas to ventures: building entrepreneurship knowledge with LLM, prompt engineering, and conversational agents\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10639-024-12775-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10639-024-12775-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Large Language Models like the Generative Pre-trained Transformer 4 (GPT-4), recognized for their exceptional performance on public datasets, are examined in this study for their potential adaptability and interactivity nature, which align well with the dynamic nature of entrepreneurship learning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1424283019\",\n",
      "            \"name\": \"Marsela Thanasi-Boe\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290624379\",\n",
      "            \"name\": \"Julian Hoxha\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\n",
      "        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\n",
      "        \"citationCount\": 42,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2403.01439\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2238201928\",\n",
      "            \"name\": \"Xin Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"94882716\",\n",
      "            \"name\": \"Dingkang Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284804294\",\n",
      "            \"name\": \"Wei Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284636478\",\n",
      "            \"name\": \"Xingkui Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290029091\",\n",
      "            \"name\": \"Yihan Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287848763\",\n",
      "            \"name\": \"Zhikang Zou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238115894\",\n",
      "            \"name\": \"Xiang Bai\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b67a6181fad5c5838945583ccdc7f39187e29332\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b67a6181fad5c5838945583ccdc7f39187e29332\",\n",
      "        \"title\": \"Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.20911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Mantis is a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations, leading the attacker's LLM to disrupt their own operations or even compromise the attacker's machine.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"50610174\",\n",
      "            \"name\": \"Dario Pasquini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2762279\",\n",
      "            \"name\": \"Evgenios M. Kornaropoulos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1700850\",\n",
      "            \"name\": \"G. Ateniese\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker's LLM to disrupt their own operations (passive defense) or even compromise the attacker's machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker's LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project_mantis\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2f274db9aa447a13c019114e327057d4b161b6d5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/2f274db9aa447a13c019114e327057d4b161b6d5\",\n",
      "        \"title\": \"LLM-controller: Dynamic robot control adaptation using large language models\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.robot.2024.104913?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.robot.2024.104913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-01-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"90182090\",\n",
      "            \"name\": \"Rasoul Zahedifar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1799503\",\n",
      "            \"name\": \"M. Baghshah\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273939584\",\n",
      "            \"name\": \"Alireza Taheri\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\n",
      "        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1485402830\",\n",
      "            \"name\": \"Xin Jin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40093162\",\n",
      "            \"name\": \"Cuiling Lan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1634494276\",\n",
      "            \"name\": \"Wenjun Zeng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31482866\",\n",
      "            \"name\": \"Zhibo Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\n",
      "        \"title\": \"HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.19878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE is proposed, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-09-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2211429378\",\n",
      "            \"name\": \"Bingshen Mu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299944267\",\n",
      "            \"name\": \"Kun Wei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2061559378\",\n",
      "            \"name\": \"Qijie Shao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2323714781\",\n",
      "            \"name\": \"Yong Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249732546\",\n",
      "            \"name\": \"Lei Xie\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bcac614f9774488447221ebb4f16f05e3975ec1e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/bcac614f9774488447221ebb4f16f05e3975ec1e\",\n",
      "        \"title\": \"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\",\n",
      "        \"citationCount\": 75,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2309.04669\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A well-designed visual tokenizer is introduced to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read, which empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-09\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2239056878\",\n",
      "            \"name\": \"Yang Jin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266735757\",\n",
      "            \"name\": \"Kun Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266735757\",\n",
      "            \"name\": \"Kun Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266419021\",\n",
      "            \"name\": \"Liwei Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239059653\",\n",
      "            \"name\": \"Chao Liao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239091862\",\n",
      "            \"name\": \"Jianchao Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2007771781\",\n",
      "            \"name\": \"Quzhe Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2230906921\",\n",
      "            \"name\": \"Bin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366079231\",\n",
      "            \"name\": \"Chenyi Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239069665\",\n",
      "            \"name\": \"An Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241686105\",\n",
      "            \"name\": \"Chengru Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238955477\",\n",
      "            \"name\": \"Xiaoqiang Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2228125963\",\n",
      "            \"name\": \"Di Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238953778\",\n",
      "            \"name\": \"Wenwu Ou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238953242\",\n",
      "            \"name\": \"Kun Gai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238953689\",\n",
      "            \"name\": \"Yadong Mu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\n",
      "        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2284692979\",\n",
      "            \"name\": \"Guangya Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284699718\",\n",
      "            \"name\": \"Yuqi Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317120126\",\n",
      "            \"name\": \"Jie Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316974967\",\n",
      "            \"name\": \"Sheng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"58700f3740105e3422eb030305372b6d8bc44986\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/58700f3740105e3422eb030305372b6d8bc44986\",\n",
      "        \"title\": \"Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours, and can serve as an orthogonal optimization for synergistic integration with existing speculative decoding.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2242179319\",\n",
      "            \"name\": \"H. Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2303652170\",\n",
      "            \"name\": \"Wayne Luk\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301711440\",\n",
      "            \"name\": \"Ka-Fai Cedric Yiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152153064\",\n",
      "            \"name\": \"Rui Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2303652428\",\n",
      "            \"name\": \"Konstantin Mishchenko\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115955596\",\n",
      "            \"name\": \"Stylianos I. Venieris\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"10001427\",\n",
      "            \"name\": \"Hongxiang Fan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\\\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"427c3a502d132b6e1cea2d5565460d284db6e3f7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/427c3a502d132b6e1cea2d5565460d284db6e3f7\",\n",
      "        \"title\": \"Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2507.00601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies and introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation is proposed.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-06-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2345186311\",\n",
      "            \"name\": \"Shuangquan Lyu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2353085449\",\n",
      "            \"name\": \"Yingnan Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2372425942\",\n",
      "            \"name\": \"Guiran Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2374351029\",\n",
      "            \"name\": \"Zhen Qi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2372322790\",\n",
      "            \"name\": \"Ruotong Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model's original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method's applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c0e24f98323c7114b9229ac17b8b63581e3e5914\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c0e24f98323c7114b9229ac17b8b63581e3e5914\",\n",
      "        \"title\": \"Dynamic and Adaptive Feature Generation with LLM\",\n",
      "        \"citationCount\": 30,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This research introduces a novel approach adopting large language models and feature-generating prompts to address challenges of explainability, applicability, and inflexible strategy and proposes a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2130031226\",\n",
      "            \"name\": \"XinHao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108045855\",\n",
      "            \"name\": \"Jinghan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1966492\",\n",
      "            \"name\": \"Banafsheh Rekabdar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2145108199\",\n",
      "            \"name\": \"Yuanchun Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301248160\",\n",
      "            \"name\": \"Pengfei Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293571072\",\n",
      "            \"name\": \"Kunpeng Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\n",
      "        \"title\": \"Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.04295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process and demonstrates measurable performance improvements compared to content-only optimization methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-02-06\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2344555074\",\n",
      "            \"name\": \"Yuanye Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257094139\",\n",
      "            \"name\": \"Jiahang Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274195530\",\n",
      "            \"name\": \"L. Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344193091\",\n",
      "            \"name\": \"Qi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2341721557\",\n",
      "            \"name\": \"Xuan Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344520491\",\n",
      "            \"name\": \"Yang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2339241318\",\n",
      "            \"name\": \"Zhongxin Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344097630\",\n",
      "            \"name\": \"Yuqing Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2296029993\",\n",
      "            \"name\": \"Peng Cheng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\n",
      "        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\n",
      "        \"citationCount\": 130,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2212.04145\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-12-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2142286278\",\n",
      "            \"name\": \"Yulu Gan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1387903470\",\n",
      "            \"name\": \"Xianzheng Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9920529\",\n",
      "            \"name\": \"Yihang Lou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145079192\",\n",
      "            \"name\": \"Yan Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115713503\",\n",
      "            \"name\": \"Renrui Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2194622137\",\n",
      "            \"name\": \"Nian Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2194260623\",\n",
      "            \"name\": \"Lin Luo\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e3e3aa5576de899b755100db211501bb405aba3e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e3e3aa5576de899b755100db211501bb405aba3e\",\n",
      "        \"title\": \"Firewalls to Secure Dynamic LLM Agentic Networks\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.01822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work identifies required properties for agent communication: proactivity, adaptability, privacy, privacy, and security, and proposes a practical design and protocol inspired by network security principles.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-02-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2343752567\",\n",
      "            \"name\": \"Sahar Abdelnabi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249532110\",\n",
      "            \"name\": \"Amr Gomaa\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"36103467\",\n",
      "            \"name\": \"Eugene Bagdasarian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237674591\",\n",
      "            \"name\": \"P. O. Kristensson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2346834097\",\n",
      "            \"name\": \"Reza Shokri\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ee552989a03693a441863af4c29dc594bfcd1ab5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ee552989a03693a441863af4c29dc594bfcd1ab5\",\n",
      "        \"title\": \"AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration\",\n",
      "        \"citationCount\": 21,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2503.18891?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2503.18891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": null\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2316787389\",\n",
      "            \"name\": \"Zhexuan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2306069252\",\n",
      "            \"name\": \"Yutong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256344322\",\n",
      "            \"name\": \"Xuebo Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46573238\",\n",
      "            \"name\": \"Liang Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2187384924\",\n",
      "            \"name\": \"Miao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348727938\",\n",
      "            \"name\": \"Jie Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2346352158\",\n",
      "            \"name\": \"Min Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout , which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\n",
      "        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2229108213\",\n",
      "            \"name\": \"Ding-Chu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2149135777\",\n",
      "            \"name\": \"Zhi Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276808245\",\n",
      "            \"name\": \"Yufeng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"273b2c64d675edd522cd6f679891756ad5207296\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/273b2c64d675edd522cd6f679891756ad5207296\",\n",
      "        \"title\": \"Exploring the Effectiveness of LLM Domain Adaptation for Business IT Machine Translation\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/2024.eamt-1.51, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is observed that while LLMs can translate on-par with SAPs MT models on general domain data, it is difficult to close the gap on SAPs domain-specific data, even with extensive training and carefully curated data.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2322393313\",\n",
      "            \"name\": \"Johannes Eschbach-Dymanus\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322400027\",\n",
      "            \"name\": \"Frank Essenberger\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1403814959\",\n",
      "            \"name\": \"Bianka Buschbeck-Wolf\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"70124681\",\n",
      "            \"name\": \"Miriam Exel\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\n",
      "        \"title\": \"ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities\",\n",
      "        \"citationCount\": 81,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is shown that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2283831590\",\n",
      "            \"name\": \"Jiarui Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315811087\",\n",
      "            \"name\": \"Thomas Holleis\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313695880\",\n",
      "            \"name\": \"Yizhe Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315810003\",\n",
      "            \"name\": \"Bernhard Aumayer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313640225\",\n",
      "            \"name\": \"Feng Nan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313910532\",\n",
      "            \"name\": \"Felix Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313694040\",\n",
      "            \"name\": \"Shuang Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313694042\",\n",
      "            \"name\": \"Shen Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315946702\",\n",
      "            \"name\": \"Mengyu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293171017\",\n",
      "            \"name\": \"Guoli Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313671930\",\n",
      "            \"name\": \"Zirui Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238621132\",\n",
      "            \"name\": \"Ruoming Pang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at https://github.com/apple/ToolSandbox\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"44b0d2e884efa5344e50424dbe2edf616981f201\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/44b0d2e884efa5344e50424dbe2edf616981f201\",\n",
      "        \"title\": \"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers\",\n",
      "        \"citationCount\": 55,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2303.00807\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.00807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work develops and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply and achieves substantially lower latency than standard reranking methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-03-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2127522115\",\n",
      "            \"name\": \"Jon Saad-Falcon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144112155\",\n",
      "            \"name\": \"O. Khattab\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50818255\",\n",
      "            \"name\": \"Keshav Santhanam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1707117\",\n",
      "            \"name\": \"Radu Florian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39038065\",\n",
      "            \"name\": \"M. Franz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1781292\",\n",
      "            \"name\": \"S. Roukos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2707234\",\n",
      "            \"name\": \"Avirup Sil\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2937809\",\n",
      "            \"name\": \"Md Arafat Sultan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144922861\",\n",
      "            \"name\": \"Christopher Potts\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\n",
      "        \"title\": \"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\",\n",
      "        \"citationCount\": 212,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.02490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing, is introduced, which effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-02\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2181120463\",\n",
      "            \"name\": \"Huiqiang Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1527099159\",\n",
      "            \"name\": \"Yucheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284970741\",\n",
      "            \"name\": \"Chengruidong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108728536\",\n",
      "            \"name\": \"Qianhui Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"13289447\",\n",
      "            \"name\": \"Xufang Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309738728\",\n",
      "            \"name\": \"Surin Ahn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2281867465\",\n",
      "            \"name\": \"Zhenhua Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309244780\",\n",
      "            \"name\": \"Amir H. Abdi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305587638\",\n",
      "            \"name\": \"Dongsheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257359863\",\n",
      "            \"name\": \"Chin-Yew Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2125051198\",\n",
      "            \"name\": \"Yuqing Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2160727304\",\n",
      "            \"name\": \"Lili Qiu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\n",
      "        \"title\": \"GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting\",\n",
      "        \"citationCount\": 91,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/30383/32447\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i21.30383?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i21.30383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously simultaneously, named GPT4MTS, and proposes a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2256985863\",\n",
      "            \"name\": \"Furong Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293923697\",\n",
      "            \"name\": \"Kevin Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257061490\",\n",
      "            \"name\": \"Yixiang Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"120783624\",\n",
      "            \"name\": \"Defu Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260272787\",\n",
      "            \"name\": \"Yan Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\n",
      "        \"title\": \"Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in Zero-shot Anomaly Detection\",\n",
      "        \"citationCount\": 35,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.09654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM), enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2108678212\",\n",
      "            \"name\": \"Jiaqi Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51459472\",\n",
      "            \"name\": \"Shaofeng Cai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276607267\",\n",
      "            \"name\": \"Fang Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2188240935\",\n",
      "            \"name\": \"Bengchin Ooi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2296743990\",\n",
      "            \"name\": \"Junran Wu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\n",
      "        \"title\": \"FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.15813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-11-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2247164321\",\n",
      "            \"name\": \"Yu Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2948393\",\n",
      "            \"name\": \"Linchao Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3446334\",\n",
      "            \"name\": \"Hehe Fan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257587812\",\n",
      "            \"name\": \"Yi Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"162f33c7799683ca9b0f193275fe7eec5a0b973f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/162f33c7799683ca9b0f193275fe7eec5a0b973f\",\n",
      "        \"title\": \"EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.09618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"EasyRef is introduced, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt and surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1571400317\",\n",
      "            \"name\": \"Zhuofan Zong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293242031\",\n",
      "            \"name\": \"Dongzhi Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261489892\",\n",
      "            \"name\": \"Bingqi Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"12920342\",\n",
      "            \"name\": \"Guanglu Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2075457131\",\n",
      "            \"name\": \"Hao Shao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292263397\",\n",
      "            \"name\": \"Dazhong Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292207974\",\n",
      "            \"name\": \"Yu Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261394248\",\n",
      "            \"name\": \"Hongsheng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although the tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent elements within multiple images through the training process, it necessitates specific finetuning for each distinct image group. This paper introduces EasyRef, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM's representations into the diffusion process through adapters can easily generalize to unseen domains, mining the consistent visual elements within unseen data. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ca4f0d2c85cfe46b97ec42b38decda107780769d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ca4f0d2c85cfe46b97ec42b38decda107780769d\",\n",
      "        \"title\": \"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.10462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"DyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation, is proposed and experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2241468593\",\n",
      "            \"name\": \"Hossein Rajabzadeh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9200111\",\n",
      "            \"name\": \"Mojtaba Valipour\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284643707\",\n",
      "            \"name\": \"Tianshu Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1996315\",\n",
      "            \"name\": \"Marzieh S. Tahaei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241480742\",\n",
      "            \"name\": \"Hyock Ju Kwon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237425782\",\n",
      "            \"name\": \"Ali Ghodsi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237517964\",\n",
      "            \"name\": \"Boxing Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2066076226\",\n",
      "            \"name\": \"Mehdi Rezagholizadeh\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9a73effed8775962c86587feb0f9ef841fa2ff4c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9a73effed8775962c86587feb0f9ef841fa2ff4c\",\n",
      "        \"title\": \"LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2258550477\",\n",
      "            \"name\": \"Yash Shukla\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258751660\",\n",
      "            \"name\": \"Wenchang Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3379438\",\n",
      "            \"name\": \"Vasanth Sarathy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258715054\",\n",
      "            \"name\": \"Alvaro Velasquez\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258551993\",\n",
      "            \"name\": \"Robert Wright\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1715858\",\n",
      "            \"name\": \"Jivko Sinapov\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a propreitary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the LLM proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\n",
      "        \"title\": \"TimeRAG: Boosting LLM Time Series Forecasting via Retrieval-Augmented Generation\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.16643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"TimeRAG is proposed, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW).\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2206558646\",\n",
      "            \"name\": \"Si-Nan Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337389407\",\n",
      "            \"name\": \"Dong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336916957\",\n",
      "            \"name\": \"Haoqi Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336884221\",\n",
      "            \"name\": \"Ruochun Jin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1e6325865e809670765bea9dadd3c40b2014eb6d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1e6325865e809670765bea9dadd3c40b2014eb6d\",\n",
      "        \"title\": \"VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The performance contribution of different prompt components is explored, the vision for future improvement in visual accessibility, and the way for LLMs in video anomaly detection and vision-language understanding is paved.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2256768778\",\n",
      "            \"name\": \"Hao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292146470\",\n",
      "            \"name\": \"Jiayou Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260725391\",\n",
      "            \"name\": \"Ashish Bastola\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2024833342\",\n",
      "            \"name\": \"Xiwen Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292183597\",\n",
      "            \"name\": \"John Suchanek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292143727\",\n",
      "            \"name\": \"Zihao Gong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064311884\",\n",
      "            \"name\": \"Abolfazl Razi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7f96bb27a8fca35b1f7d02ee319a64be04114809\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7f96bb27a8fca35b1f7d02ee319a64be04114809\",\n",
      "        \"title\": \"LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments\",\n",
      "        \"citationCount\": 26,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.08337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties, and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-13\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2265927745\",\n",
      "            \"name\": \"Maonan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2219695355\",\n",
      "            \"name\": \"Aoyu Pang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7592365\",\n",
      "            \"name\": \"Yuheng Kan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144305489\",\n",
      "            \"name\": \"Man-On Pun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292117616\",\n",
      "            \"name\": \"Chung Shue Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291077490\",\n",
      "            \"name\": \"Bo Huang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"650a24da1702beca7eb70011a26f1f3238efad4b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/650a24da1702beca7eb70011a26f1f3238efad4b\",\n",
      "        \"title\": \"Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.acl-srw.90?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.acl-srw.90, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The proposed Time-LlaMA framework achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2346152667\",\n",
      "            \"name\": \"Juyuan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334602368\",\n",
      "            \"name\": \"Jiechao Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2374971695\",\n",
      "            \"name\": \"Wenwen Ouyang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334718086\",\n",
      "            \"name\": \"Wei Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2321408906\",\n",
      "            \"name\": \"Hui Yi Leong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the models predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\n",
      "        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\n",
      "        \"citationCount\": 31,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2163833353\",\n",
      "            \"name\": \"Jiapu Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2220259324\",\n",
      "            \"name\": \"Kai Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238130759\",\n",
      "            \"name\": \"Linhao Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302892265\",\n",
      "            \"name\": \"Wei Wei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2140879677\",\n",
      "            \"name\": \"Yongli Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243282363\",\n",
      "            \"name\": \"Alan Wee-Chung Liew\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294378361\",\n",
      "            \"name\": \"Shirui Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239088112\",\n",
      "            \"name\": \"Baocai Yin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\n",
      "        \"title\": \"Domain Adaptation via Prompt Learning\",\n",
      "        \"citationCount\": 212,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2202.06687\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-02-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2130368185\",\n",
      "            \"name\": \"Chunjiang Ge\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Rui Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112817811\",\n",
      "            \"name\": \"Mixue Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51451501\",\n",
      "            \"name\": \"Zihang Lai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1760750\",\n",
      "            \"name\": \"Shiji Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Shuang Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115218570\",\n",
      "            \"name\": \"Gao Huang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\n",
      "        \"title\": \"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair\",\n",
      "        \"citationCount\": 188,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2176865575\",\n",
      "            \"name\": \"Islem Bouzenia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"114875459\",\n",
      "            \"name\": \"Prem Devanbu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260683361\",\n",
      "            \"name\": \"Michael Pradel\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\n",
      "        \"title\": \"Token-Budget-Aware LLM Reasoning\",\n",
      "        \"citationCount\": 114,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem is proposed, offering a practical solution to balance efficiency and accuracy in LLM reasoning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2170360833\",\n",
      "            \"name\": \"Tingxu Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2154723145\",\n",
      "            \"name\": \"Zhenting Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239197945\",\n",
      "            \"name\": \"Chunrong Fang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110773055\",\n",
      "            \"name\": \"Shiyun Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2333472479\",\n",
      "            \"name\": \"Shiqing Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238950128\",\n",
      "            \"name\": \"Zhenyu Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"20843eaa59db5e2af416d7db47d51d0aab3de230\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/20843eaa59db5e2af416d7db47d51d0aab3de230\",\n",
      "        \"title\": \"Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue Resolution\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.11709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper analyzes 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution and identifies key textual and code-related heuristicsSpecificity, Contextual Richness, and Claritythat are associated with successful issue closure and help assess prompt quality.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-01-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2147154362\",\n",
      "            \"name\": \"Ramtin Ehsani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2341336471\",\n",
      "            \"name\": \"Sakshi Pathak\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9728244\",\n",
      "            \"name\": \"Preetha Chatterjee\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) have become essential in software development, especially for issue resolution. However, despite their widespread use, significant challenges persist in the quality of LLM responses to issue resolution queries. LLM interactions often yield incorrect, incomplete, or ambiguous information, largely due to knowledge gaps in prompt design, which can lead to unproductive exchanges and reduced developer productivity.In this paper, we analyze 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution. We identify four main knowledge gaps in developer prompts: Missing Context, Missing Specifications, Multiple Context, and Unclear Instructions. Assuming that conversations within closed issues contributed to successful resolutions while those in open issues did not, we find that ineffective conversations contain knowledge gaps in $44.6 \\\\%$ of prompts, compared to only $12.6 \\\\%$ in effective ones. Additionally, we observe seven distinct conversational styles, with Directive Prompting, Chain of Thought, and Responsive Feedback being the most prevalent. We find that knowledge gaps are present in all styles of conversations, with Missing Context being the most repeated challenge developers face in issue-resolution conversations.Based on our analysis, we identify key textual and code-related heuristicsSpecificity, Contextual Richness, and Claritythat are associated with successful issue closure and help assess prompt quality. These heuristics lay the foundation for an automated tool that can dynamically flag unclear prompts and suggest structured improvements. To test feasibility, we developed a lightweight browser extension prototype for detecting prompt gaps, that can be easily adapted to other tools within developer workflows.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"25cee84e3a1541697a7c97443d7526574127c344\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/25cee84e3a1541697a7c97443d7526574127c344\",\n",
      "        \"title\": \"Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\",\n",
      "        \"citationCount\": 158,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.00367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs, and proposes two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2114887261\",\n",
      "            \"name\": \"Shangbin Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254168375\",\n",
      "            \"name\": \"Weijia Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108853330\",\n",
      "            \"name\": \"Yike Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282214127\",\n",
      "            \"name\": \"Wenxuan Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143820870\",\n",
      "            \"name\": \"Vidhisha Balachandran\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249583325\",\n",
      "            \"name\": \"Yulia Tsvetkov\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d5342fce96175f83550cfae471a0a46d16401481\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d5342fce96175f83550cfae471a0a46d16401481\",\n",
      "        \"title\": \"ST-LLM: Large Language Models Are Effective Temporal Learners\",\n",
      "        \"citationCount\": 120,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM, and develops a dynamic masking strategy with tailor-made training objectives to address the overhead and stability issues introduced by uncompressed video tokens within LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"12287885\",\n",
      "            \"name\": \"Ruyang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256784925\",\n",
      "            \"name\": \"Chen Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294629231\",\n",
      "            \"name\": \"Haoran Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152988335\",\n",
      "            \"name\": \"Yixiao Ge\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265579883\",\n",
      "            \"name\": \"Ying Shan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294517847\",\n",
      "            \"name\": \"Ge Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at https://github.com/TencentARC/ST-LLM.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\n",
      "        \"title\": \"Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.17282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with, and performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-02-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2218970025\",\n",
      "            \"name\": \"Yi-Kai Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274937712\",\n",
      "            \"name\": \"De-Chuan Zhan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2151459740\",\n",
      "            \"name\": \"Han-Jia Ye\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"edfff0e15449f438a13a7341290c008bf6486afc\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/edfff0e15449f438a13a7341290c008bf6486afc\",\n",
      "        \"title\": \"MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning\",\n",
      "        \"citationCount\": 34,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.18035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism, which significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2327293591\",\n",
      "            \"name\": \"Jingfan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322997112\",\n",
      "            \"name\": \"Yi Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327694040\",\n",
      "            \"name\": \"Dan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293390999\",\n",
      "            \"name\": \"Xing Tian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2179528564\",\n",
      "            \"name\": \"Huanran Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322888603\",\n",
      "            \"name\": \"Wei Zhu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\n",
      "        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\n",
      "        \"citationCount\": 203,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2310.04948\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"120783624\",\n",
      "            \"name\": \"Defu Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256985863\",\n",
      "            \"name\": \"Furong Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2676352\",\n",
      "            \"name\": \"Sercan . Arik\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1945962\",\n",
      "            \"name\": \"Tomas Pfister\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257061490\",\n",
      "            \"name\": \"Yixiang Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256992266\",\n",
      "            \"name\": \"Wen Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257088730\",\n",
      "            \"name\": \"Yan Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ab4800a924508f49d644ced8ba236ec92f54f566\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ab4800a924508f49d644ced8ba236ec92f54f566\",\n",
      "        \"title\": \"Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2505.16552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach and enhances CoLaR through reinforcement learning (RL) that leverages the latent head's non-deterministic nature to explore diverse reasoning paths and exploit more compact ones.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-05-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2218557953\",\n",
      "            \"name\": \"Wenhui Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2362865102\",\n",
      "            \"name\": \"Jiaze Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317982861\",\n",
      "            \"name\": \"Jianzhong Ju\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2363405807\",\n",
      "            \"name\": \"Zhenbo Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317980688\",\n",
      "            \"name\": \"Jian Luan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290923147\",\n",
      "            \"name\": \"Ruihua Song\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head's non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"268e28f8d5235031dcd7bfae0f857439e27e8564\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/268e28f8d5235031dcd7bfae0f857439e27e8564\",\n",
      "        \"title\": \"SteP: Stacked LLM Policies for Web Actions\",\n",
      "        \"citationCount\": 44,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.03720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Stacked LLM Policies for Web Actions is proposed, an approach to dynamically compose policies to solve a diverse set of web tasks and is competitive with prior works while using significantly less data.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2023-10-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2254267756\",\n",
      "            \"name\": \"Paloma Sodhi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1741598\",\n",
      "            \"name\": \"S. Branavan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2066324938\",\n",
      "            \"name\": \"Yoav Artzi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254260284\",\n",
      "            \"name\": \"Ryan McDonald\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Performing tasks on the web presents fundamental challenges to large language models (LLMs), including combinatorially large open-world tasks and variations across web interfaces. Simply specifying a large prompt to handle all possible behaviors and states is extremely complex, and results in behavior leaks between unrelated behaviors. Decomposition to distinct policies can address this challenge, but requires carefully handing off control between policies. We propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically compose policies to solve a diverse set of web tasks. SteP defines a Markov Decision Process where the state is a stack of policies representing the control state, i.e., the chain of policy calls. Unlike traditional methods that are restricted to static hierarchies, SteP enables dynamic control that adapts to the complexity of the task. We evaluate SteP against multiple baselines and web environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP improves (14.9\\\\% to 33.5\\\\%) over SOTA that use GPT-4 policies, while on MiniWob++, SteP is competitive with prior works while using significantly less data. Our code and data are available at https://asappresearch.github.io/webagents-step.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1f9822022f586e375461660db792f23e891c7123\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1f9822022f586e375461660db792f23e891c7123\",\n",
      "        \"title\": \"Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems\",\n",
      "        \"citationCount\": 62,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2307.06187\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.06187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models, grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-07-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2658311\",\n",
      "            \"name\": \"N. Nascimento\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40761174\",\n",
      "            \"name\": \"Paulo Alencar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2149928782\",\n",
      "            \"name\": \"Donald D. Cowan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs' capabilities and indicating further research opportunities to assess LLMs' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\n",
      "        \"title\": \"Bridging the Gulf of Envisioning: Cognitive Challenges in Prompt Based Interactions with LLMs\",\n",
      "        \"citationCount\": 91,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642754\",\n",
      "          \"status\": \"BRONZE\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work theorizes how end-users envision translating their goals into clear intentions and craft prompts to obtain the desired LLM response.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2301051721\",\n",
      "            \"name\": \"Hariharan Subramonyam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2246886383\",\n",
      "            \"name\": \"Roy D. Pea\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1825757380\",\n",
      "            \"name\": \"Christopher Pondoc\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1820412\",\n",
      "            \"name\": \"Maneesh Agrawala\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289103973\",\n",
      "            \"name\": \"Colleen M. Seifert\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Normans gulfs of execution and evaluation. To address this gap, we theorize how end-users envision translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments on not knowing: (1) what the task should be, (2) how to instruct the LLM to do the task, and (3) what to expect for the LLMs output in meeting the goal. Finally, we make recommendations to narrow the gulf of envisioning in human-LLM interactions.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"28d6411019f448f54834c2a5cff723cd350345b5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/28d6411019f448f54834c2a5cff723cd350345b5\",\n",
      "        \"title\": \"Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration\",\n",
      "        \"citationCount\": 60,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC) involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2289252454\",\n",
      "            \"name\": \"Haokun Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"8247318\",\n",
      "            \"name\": \"Yaonan Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110285301\",\n",
      "            \"name\": \"Kenji Kato\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307380233\",\n",
      "            \"name\": \"Atsushi Tsukahara\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282115338\",\n",
      "            \"name\": \"Izumi Kondo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1752849\",\n",
      "            \"name\": \"T. Aoyama\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237520520\",\n",
      "            \"name\": \"Yasuhisa Hasegawa\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This letter proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"77a9c310df0d7896d297da90fc4a1131819c341e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/77a9c310df0d7896d297da90fc4a1131819c341e\",\n",
      "        \"title\": \"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models\",\n",
      "        \"citationCount\": 45,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding, and proposes LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2293615241\",\n",
      "            \"name\": \"Mingxing Peng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293665950\",\n",
      "            \"name\": \"Xusen Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146413818\",\n",
      "            \"name\": \"Xianda Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241024418\",\n",
      "            \"name\": \"Meixin Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267078966\",\n",
      "            \"name\": \"Kehua Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293775145\",\n",
      "            \"name\": \"Hao Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258778041\",\n",
      "            \"name\": \"Xuesong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258755636\",\n",
      "            \"name\": \"Yinhai Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"coherence in extended LLM\": {\n",
      "    \"total\": 23,\n",
      "    \"offset\": 0,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"d0da372b4b6f422e13556ce272595a0c9002fe90\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d0da372b4b6f422e13556ce272595a0c9002fe90\",\n",
      "        \"title\": \"GPT-4 in Education: Evaluating Aptness, Reliability, and Loss of Coherence in Solving Calculus Problems and Grading Submissions\",\n",
      "        \"citationCount\": 21,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://link.springer.com/content/pdf/10.1007/s40593-024-00403-3.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40593-024-00403-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40593-024-00403-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2256969876\",\n",
      "            \"name\": \"Alberto Gandolfi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we initially investigate the capabilities of GPT-3 5 and GPT-4 in solving college-level calculus problems, an essential segment of mathematics that remains under-explored so far. Although improving upon earlier versions, GPT-4 attains approximately 65% accuracy for standard problems and decreases to 20% for competition-like scenarios. Overall, the models prove to be unreliable due to common arithmetic errors. Our primary contribution lies then in examining the use of ChatGPT for grading solutions to calculus exercises. Our objectives are to probe an in-context learning task with less emphasis over direct calculations; recognize positive applications of ChatGPT in educational contexts; highlight a potentially emerging facet of AI that could necessitate oversight; and introduce unconventional AI benchmarks, for which models like GPT are untrained. Pertaining to the latter, we uncover a tendency for loss of coherence in extended contexts. Our findings suggest that while the current ChatGPT exhibits comprehension of the grading task and often provides relevant outputs, the consistency of grading is marred by occasional loss of coherence and hallucinations. Intriguingly, GPT-4's overall scores, delivered in mere moments, align closely with human graders, although its detailed accuracy remains suboptimal. This work suggests that, when appropriately orchestrated, collaboration between human graders and LLMs like GPT-4 might combine their unique strengths while mitigating their respective shortcomings In this direction, it is imperative to consider implementing transparency, fairness, and appropriate regulations in the near future.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"49b7bd275a0386392769f5b33028500754dbc69d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/49b7bd275a0386392769f5b33028500754dbc69d\",\n",
      "        \"title\": \"AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE), and reveals that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2317040111\",\n",
      "            \"name\": \"Abhay Gupta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317010916\",\n",
      "            \"name\": \"Philip Meng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317007185\",\n",
      "            \"name\": \"Ece Yurtseven\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241351144\",\n",
      "            \"name\": \"Sean O'Brien\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2312105716\",\n",
      "            \"name\": \"Kevin Zhu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Detecting biases in natural language understanding (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language processing (NLP) systems. To address dialect-induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE). AAVENUE builds upon and extends existing benchmarks like VALUE, replacing deterministic syntactic and morphological transformations with a more flexible methodology leveraging LLM-based translation with few-shot prompting, improving performance across our evaluation metrics when translating key tasks from the GLUE and SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics including fluency, BARTScore, quality, coherence, and understandability. Additionally, we recruit fluent AAVE speakers to validate our translations for authenticity. Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\n",
      "        \"title\": \"DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM\",\n",
      "        \"citationCount\": 30,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://aclanthology.org/2023.findings-emnlp.606.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study introduces a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME), which proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2110546424\",\n",
      "            \"name\": \"Weijie Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261645232\",\n",
      "            \"name\": \"Wenxiang Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261413304\",\n",
      "            \"name\": \"Fanyou Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1757518\",\n",
      "            \"name\": \"Srinivasan H. Sengamedu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME's potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\n",
      "        \"title\": \"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An evaluation framework for mental health chatbots is validated, proving its effectiveness in improving safety and reliability and highlighting the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-08-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2315914561\",\n",
      "            \"name\": \"Jung In Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2182148069\",\n",
      "            \"name\": \"Mahyar Abbasian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241201441\",\n",
      "            \"name\": \"Iman Azimi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315810363\",\n",
      "            \"name\": \"Dawn Bounds\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315810053\",\n",
      "            \"name\": \"Angela Jun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315889398\",\n",
      "            \"name\": \"Jaesu Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315811390\",\n",
      "            \"name\": \"Robert McCarron\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297708916\",\n",
      "            \"name\": \"Jessica Borelli\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348273518\",\n",
      "            \"name\": \"Parmida Safavi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348305728\",\n",
      "            \"name\": \"Sanaz Mirbaha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315875066\",\n",
      "            \"name\": \"Jia Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315811652\",\n",
      "            \"name\": \"Mona Mahmoudi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315811354\",\n",
      "            \"name\": \"Carmen Wiedenhoeft\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2311169857\",\n",
      "            \"name\": \"Amir M. Rahmani\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Materials and Methods: We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. Results: The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Discussion: Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. Conclusion: The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\n",
      "        \"title\": \"Asynchronous LLM Function Calling\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.07017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An in-context protocol for function calls and interrupts is designed, an fine-tuning strategy is provided to adapt LLMs to the interrupt semantics, and mechanisms to asynchronously notify the LLM in-flight when function calls return are implemented.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-09\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2265756791\",\n",
      "            \"name\": \"In Gim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118065368\",\n",
      "            \"name\": \"Seung-seob Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2323908057\",\n",
      "            \"name\": \"Lin Zhong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM's operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call's completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\n",
      "        \"title\": \"Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://www.nature.com/articles/s41598-024-56309-6.pdf\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10925587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is suggested that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies, and demonstrating a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2260831947\",\n",
      "            \"name\": \"Juho Jung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152569226\",\n",
      "            \"name\": \"Jinyoung Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6777367\",\n",
      "            \"name\": \"J. Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2088247339\",\n",
      "            \"name\": \"Junseo Ko\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1677558107\",\n",
      "            \"name\": \"Jeewoo Yoon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39548326\",\n",
      "            \"name\": \"J. Hwang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265721342\",\n",
      "            \"name\": \"Ji In Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2054436718\",\n",
      "            \"name\": \"Gyudeok Hwang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290973453\",\n",
      "            \"name\": \"Jae Ho Jung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2003794778\",\n",
      "            \"name\": \"Daniel Duck-Jin Hwang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Neovascular age-related macular degeneration (nAMD) can result in blindness if left untreated, and patients often require repeated anti-vascular endothelial growth factor injections. Although, the treat-and-extend method is becoming popular to reduce vision loss attributed to recurrence, it may pose a risk of overtreatment. This study aimed to develop a deep learning model based on DenseNet201 to predict nAMD recurrence within 3 months after confirming dry-up 1 month following three loading injections in treatment-nave patients. A dataset of 1076 spectral domain optical coherence tomography (OCT) images from 269 patients diagnosed with nAMD was used. The performance of the model was compared with that of 6 ophthalmologists, using 100 randomly selected samples. The DenseNet201-based model achieved 53.0% accuracy in predicting nAMD recurrence using a single pre-injection image and 60.2% accuracy after viewing all the images immediately after the 1st, 2nd, and 3rd injections. The model outperformed experienced ophthalmologists, with an average accuracy of 52.17% using a single pre-injection image and 53.3% after examining four images before and after three loading injections. In conclusion, the artificial intelligence model demonstrated a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists. These findings suggest that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\n",
      "        \"title\": \"Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning\",\n",
      "        \"citationCount\": 42,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.03275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a hybrid representation of the reasoning process, where the initial reasoning steps are partially abstracted away using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-02-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2325888815\",\n",
      "            \"name\": \"DiJia Su\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2255310892\",\n",
      "            \"name\": \"Hanlin Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269737738\",\n",
      "            \"name\": \"Yingchen Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258657022\",\n",
      "            \"name\": \"Jiantao Jiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2285362895\",\n",
      "            \"name\": \"Yuandong Tian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326106870\",\n",
      "            \"name\": \"Qinqing Zheng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\n",
      "        \"title\": \"13.4 A 48GB 16-High 1280GB/s HBM3E DRAM with All-Around Power TSV and a 6-Phase RDQS Scheme for TSV Area Optimization\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISSCC49657.2024.10454440?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISSCC49657.2024.10454440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2287764300\",\n",
      "            \"name\": \"Jinhyung Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287959684\",\n",
      "            \"name\": \"Kyungjun Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109114249\",\n",
      "            \"name\": \"C. Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287956252\",\n",
      "            \"name\": \"Yeonho Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287119443\",\n",
      "            \"name\": \"Jae-Hyung Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286899300\",\n",
      "            \"name\": \"Su-Hyun Oh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1640393542\",\n",
      "            \"name\": \"Yucheon Ju\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3365606\",\n",
      "            \"name\": \"Chunseok Jeong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"123947284\",\n",
      "            \"name\": \"H. Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2198615241\",\n",
      "            \"name\": \"Jaeseung Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3376046\",\n",
      "            \"name\": \"T. Yun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292215366\",\n",
      "            \"name\": \"Jin Hee Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3375969\",\n",
      "            \"name\": \"Sangmuk Oh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1640397693\",\n",
      "            \"name\": \"J. Moon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110426052\",\n",
      "            \"name\": \"Y. Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291083018\",\n",
      "            \"name\": \"Hong-Seok Choi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159532637\",\n",
      "            \"name\": \"In-Keun Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286904054\",\n",
      "            \"name\": \"Seung Min Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286933095\",\n",
      "            \"name\": \"Sun-Yeol Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291078685\",\n",
      "            \"name\": \"Jaemin Jang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292140319\",\n",
      "            \"name\": \"Jinwook Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108644317\",\n",
      "            \"name\": \"S. Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286906698\",\n",
      "            \"name\": \"Younghyun Jeon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292172289\",\n",
      "            \"name\": \"Juhyung Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159571158\",\n",
      "            \"name\": \"Tae-Kyun Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"82375019\",\n",
      "            \"name\": \"D. Ka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159321700\",\n",
      "            \"name\": \"Sanghoon Oh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292143044\",\n",
      "            \"name\": \"Jinse Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159244890\",\n",
      "            \"name\": \"Junyeol Jeon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292173125\",\n",
      "            \"name\": \"Seonhong Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291075551\",\n",
      "            \"name\": \"Kyeong Tae Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2154855420\",\n",
      "            \"name\": \"Tae-Hwan Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291083403\",\n",
      "            \"name\": \"Hyeonjin Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291072681\",\n",
      "            \"name\": \"Dongju Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291084224\",\n",
      "            \"name\": \"Minseop Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30684992\",\n",
      "            \"name\": \"Heewoong Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291069292\",\n",
      "            \"name\": \"Dongwook Jang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287120235\",\n",
      "            \"name\": \"Junghyun Shin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287261607\",\n",
      "            \"name\": \"Hyunsik Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291041862\",\n",
      "            \"name\": \"Changki Baek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291023817\",\n",
      "            \"name\": \"Hajun Jeong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291081668\",\n",
      "            \"name\": \"Jongchan Yoon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1641325386\",\n",
      "            \"name\": \"SeungGyeon Lim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110630984\",\n",
      "            \"name\": \"Kyo Yun Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159499427\",\n",
      "            \"name\": \"Young Jun Koo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287107051\",\n",
      "            \"name\": \"Myeong-Jae Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2510417\",\n",
      "            \"name\": \"Joohwan Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291056079\",\n",
      "            \"name\": \"Jonghwan Kim\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"With the emergence of large-language models (LLM) and generative AI, which require an enormous amount of model parameters, the required memory bandwidth and capacity for high-end systems is on an unprecedented increase. To meet this need, we present an extended version of the high-bandwidth memory-3 (HBM3 DRAM), HBM3E, which achieves a 1280GB/s bandwidth with a cube density of 48GB. New design schemes and features, such as all-around power-through-silicon via (TSV), a 6-phase read-data-strobe (RDQS) scheme, a byte-mapping swap scheme, and a voltage-drift compensator for write data strobe (WDQS), are implemented to achieve extended bandwidth and capacity with enhanced reliability. The overall architecture and specifications, such as bump map footprint, the number of channel and I/Os, and the operation voltage, are identical to the latest HBM3 [1, 2]; therefore, backward compatibility is provided, avoiding system modification.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\n",
      "        \"title\": \"NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper details the design and construction of NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"35504092\",\n",
      "            \"name\": \"Cunxiang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30819687\",\n",
      "            \"name\": \"Ruoxi Ning\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292184774\",\n",
      "            \"name\": \"Boqi Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292208424\",\n",
      "            \"name\": \"Tonghui Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3187768\",\n",
      "            \"name\": \"Qipeng Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292147095\",\n",
      "            \"name\": \"Cheng Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1993226927\",\n",
      "            \"name\": \"Guangsheng Bao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292261580\",\n",
      "            \"name\": \"Qian Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261496744\",\n",
      "            \"name\": \"Yue Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"dda8031682684655744c7001374e6cb88c9503bd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/dda8031682684655744c7001374e6cb88c9503bd\",\n",
      "        \"title\": \"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.10902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Soda-Eval is introduced, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, and the performance of several open-access instruction-tuned LLMs is studied, finding that dialogue evaluation remains challenging.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2007581062\",\n",
      "            \"name\": \"John Mendona\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268558660\",\n",
      "            \"name\": \"Isabel Trancoso\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1784914\",\n",
      "            \"name\": \"A. Lavie\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2d0b030d314a5aa8feaa03695e8471270130bdf9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9\",\n",
      "        \"title\": \"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\",\n",
      "        \"citationCount\": 100,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.07542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-01-13\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2155795167\",\n",
      "            \"name\": \"Chengzu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51198241\",\n",
      "            \"name\": \"Wenshan Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2339967968\",\n",
      "            \"name\": \"Huanyu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258547658\",\n",
      "            \"name\": \"Yan Xia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273419590\",\n",
      "            \"name\": \"Shaoguang Mao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294850817\",\n",
      "            \"name\": \"Li Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2339667880\",\n",
      "            \"name\": \"Ivan Vuli'c\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249539478\",\n",
      "            \"name\": \"Furu Wei\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"78875987dc674fc556873df037cf114f04932e80\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/78875987dc674fc556873df037cf114f04932e80\",\n",
      "        \"title\": \"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\",\n",
      "        \"citationCount\": 83,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2401.07764\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.07764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A split learning system for LLM agents in 6G networks is proposed, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1454018677\",\n",
      "            \"name\": \"Minrui Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1713586\",\n",
      "            \"name\": \"D. Niyato\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261731446\",\n",
      "            \"name\": \"Jiawen Kang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2943819\",\n",
      "            \"name\": \"Zehui Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237802924\",\n",
      "            \"name\": \"Shiwen Mao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2264568786\",\n",
      "            \"name\": \"Zhu Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2228302663\",\n",
      "            \"name\": \"Dong In Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269994509\",\n",
      "            \"name\": \"K. B. Letaief\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"807ba70b6dc5ce8104268ef8e579d6ff67051230\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/807ba70b6dc5ce8104268ef8e579d6ff67051230\",\n",
      "        \"title\": \"A Survey on Post-training of Large Language Models\",\n",
      "        \"citationCount\": 35,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-03-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2187039492\",\n",
      "            \"name\": \"Guiyao Tie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349394246\",\n",
      "            \"name\": \"Zeli Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2347232612\",\n",
      "            \"name\": \"Dingjie Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349375946\",\n",
      "            \"name\": \"Fuyang Wei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349949677\",\n",
      "            \"name\": \"Rong Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349402229\",\n",
      "            \"name\": \"Yurou Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349357634\",\n",
      "            \"name\": \"Wen Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"121937496\",\n",
      "            \"name\": \"Zhejian Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349498657\",\n",
      "            \"name\": \"Jiangyue Yan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2342866931\",\n",
      "            \"name\": \"Yao Su\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349400490\",\n",
      "            \"name\": \"Zhenhan Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2324567791\",\n",
      "            \"name\": \"Yifeng Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2211165440\",\n",
      "            \"name\": \"Yihan Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301109277\",\n",
      "            \"name\": \"Lichao Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2221116622\",\n",
      "            \"name\": \"Pan Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254874151\",\n",
      "            \"name\": \"Lifang He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280102292\",\n",
      "            \"name\": \"Hechang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349483665\",\n",
      "            \"name\": \"Yu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284983420\",\n",
      "            \"name\": \"Qingsong Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348862207\",\n",
      "            \"name\": \"Tianming Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249536787\",\n",
      "            \"name\": \"Neil Zhenqiang Gong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279062891\",\n",
      "            \"name\": \"Jiliang Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265549448\",\n",
      "            \"name\": \"Caiming Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349551882\",\n",
      "            \"name\": \"Heng Ji\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293777434\",\n",
      "            \"name\": \"Philip S. Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288029761\",\n",
      "            \"name\": \"Jianfeng Gao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT's alignment strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ecb1002427e476ec76463e0a8b5a453471a1931f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ecb1002427e476ec76463e0a8b5a453471a1931f\",\n",
      "        \"title\": \"The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks are illustrated, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2283841922\",\n",
      "            \"name\": \"Ayo Adedeji\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283935118\",\n",
      "            \"name\": \"Sarita Joshi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283841541\",\n",
      "            \"name\": \"Brendan Doohan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\n",
      "        \"title\": \"Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.03151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A thorough and up-to-date comparison of reasoning techniques in both textual and multimodal LLMs is offered, highlighting practical methods for post-training optimization and test-time inference.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-04-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2066776633\",\n",
      "            \"name\": \"Jing Bi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153545235\",\n",
      "            \"name\": \"Susan Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322454494\",\n",
      "            \"name\": \"Xiaofei Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279760245\",\n",
      "            \"name\": \"Pinxin Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2331365304\",\n",
      "            \"name\": \"Junjia Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2119309562\",\n",
      "            \"name\": \"Yunlong Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2242154602\",\n",
      "            \"name\": \"Luchuan Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2161012966\",\n",
      "            \"name\": \"Chao Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350866278\",\n",
      "            \"name\": \"Guangyu Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350999609\",\n",
      "            \"name\": \"Jinxi He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350428628\",\n",
      "            \"name\": \"Jiarui Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2354168235\",\n",
      "            \"name\": \"Shu Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266412651\",\n",
      "            \"name\": \"Daoan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350433568\",\n",
      "            \"name\": \"Chen Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152129821\",\n",
      "            \"name\": \"L. Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337241442\",\n",
      "            \"name\": \"Zhang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2320811177\",\n",
      "            \"name\": \"Jiebo Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293314762\",\n",
      "            \"name\": \"Chenliang Xu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5550118041a89121e9d7274f83aef420cd9ed487\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5550118041a89121e9d7274f83aef420cd9ed487\",\n",
      "        \"title\": \"Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.11849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": null\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-13\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2311451390\",\n",
      "            \"name\": \"Yinghao Aaron Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243118841\",\n",
      "            \"name\": \"Xilin Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2162961620\",\n",
      "            \"name\": \"Jordan Darefsky\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316835793\",\n",
      "            \"name\": \"Ge Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1686269\",\n",
      "            \"name\": \"N. Mesgarani\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end speech-to-speech conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and speech styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the speech, which is then played back to the user. While the response speech is being played, the input speech undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input speech. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and speech-to-speech baselines in terms of both dialogue naturalness and coherence while being more than 50% faster.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\n",
      "        \"title\": \"What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This survey summarizes existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual, and proposes a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2302816545\",\n",
      "            \"name\": \"Dingyi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317013378\",\n",
      "            \"name\": \"Qin Jin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories. Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\n",
      "        \"title\": \"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.19846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Quest is introduced, a query-centric data synthesis method aggregating semantically relevant yet diverse documents that achieves superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2152644192\",\n",
      "            \"name\": \"Chaochen Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155226596\",\n",
      "            \"name\": \"Xing Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2176771084\",\n",
      "            \"name\": \"Qingfang Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257376973\",\n",
      "            \"name\": \"Songlin Hu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest's superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\n",
      "        \"title\": \"When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.13476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AnchorAttention is developed, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training, and reduces training time by over 50% compared to standard full attention mechanisms.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-11-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2267866973\",\n",
      "            \"name\": \"Haonan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284062049\",\n",
      "            \"name\": \"Qian Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325201427\",\n",
      "            \"name\": \"Chao Du\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291015783\",\n",
      "            \"name\": \"Tongyao Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325980280\",\n",
      "            \"name\": \"Cunxiao Du\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256995496\",\n",
      "            \"name\": \"Kenji Kawaguchi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"19201674\",\n",
      "            \"name\": \"Tianyu Pang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16's limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\\\% compared to standard full attention mechanisms, while preserving the original LLM's capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\n",
      "        \"title\": \"Ultrafast imaging of coherent polariton propagation and interactions\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2143435931\",\n",
      "            \"name\": \"Ding Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"13643895\",\n",
      "            \"name\": \"Arkajit Mandal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2053177998\",\n",
      "            \"name\": \"James M. Baxter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2004406278\",\n",
      "            \"name\": \"Shangjun Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49805255\",\n",
      "            \"name\": \"I. Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1849913502\",\n",
      "            \"name\": \"Haowen (Vicky) Su\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144363449\",\n",
      "            \"name\": \"Song Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6834462\",\n",
      "            \"name\": \"D. Reichman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3895968\",\n",
      "            \"name\": \"Milan Delor\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\n",
      "        \"title\": \"Extrapolation-Based Video Retargeting With Backward Warping Using an Image-to-Warping Vector Generation Network\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LSP.2020.2977206?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LSP.2020.2977206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-02-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"46445100\",\n",
      "            \"name\": \"Sung In Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1739537\",\n",
      "            \"name\": \"Suk-ju Kang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Video retargeting is a technique used to transform a given video to a target aspect ratio. Current methods often cause severe visual distortion due to frequent temporal incoherence during the retargeting. In this study, we propose a new extrapolation-based video retargeting method using an image-to-warping vector generation network to maintain temporal coherence and prevent deformation of an input frame by extending the side area of an input frame. Backward warping-based extrapolation is performed using a displacement vector (DV) that is generated by a proposed convolutional neural network (CNN). The DV is defined as the displacement between the current hole to be filled in the extended area and a pixel in the input frame used to fill the hole. We also propose a technique to efficiently train the CNN including a method for ground-truth DV generation. After the extrapolation, we propose a technique for the maintenance of temporal coherence of the extended region and a distortion suppression scheme (DSC) for minimizing visual artifacts. The simulation results demonstrated that the proposed method improved bidirectional similarity (BDS) up to 3.69, which is a measure of the quality of video retargeting, compared with existing video retargeting methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0df45f6ab09cc6ddcaf6829c131c777732a73731\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0df45f6ab09cc6ddcaf6829c131c777732a73731\",\n",
      "        \"title\": \"Temporal Incoherence-Free Video Retargeting Using Foreground Aware Extrapolation\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2020.2977171?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2020.2977171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-03-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"46445100\",\n",
      "            \"name\": \"Sung In Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1739537\",\n",
      "            \"name\": \"Suk-ju Kang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Video retargeting is a method of adjusting the aspect ratio of a given video to the target aspect ratio. However, temporal incoherence of video contents, which can occur frequently by video retargeting, is the most dominant factor that degrades the quality of retargeted videos. Current methods to maintain temporal coherence use the entire frames of the input videos; however, these methods cannot be implemented as on-time systems because of their tremendous computational complexity. As far as we know, there is no existing on-time video retargeting method that can avoid spatial distortion while perfectly maintaining temporal coherence. In this paper, we propose a novel on-time video retargeting method that can perfectly maintain temporal coherence and prevent the spatial distortion by using only two consecutive input frames. In our method, the maximum a posteriori-based foreground aware-block matching is used for the extrapolation that extends the side area of a given video to adjust its aspect ratio to the target. To maintain the temporal coherence of the extended area, the result of block matching for backward warping-based extrapolation of the start frame after the scene change occurs, is reused for the other frames until the next scene change occurs. In addition, we propose a scene scenario-adaptive fallback scheme to prevent severe distortions that can occur with reusing block matching results or extrapolation-based side extension. The simulation results showed that the proposed method greatly improved the bidirectional similarity value, which can measure the quality of video retargeting, by up to 10.26 compared with the existing on-time video retargeting methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\n",
      "        \"title\": \"Posterior-GRPO: Rewarding Reasoning Processes in Code Generation\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2508.05170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces a unified framework that can effectively incorporate the quality of the reasoning process during RL and introduces Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-08-07\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2310403349\",\n",
      "            \"name\": \"Lishui Fan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2375147757\",\n",
      "            \"name\": \"Yu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2125101083\",\n",
      "            \"name\": \"Mouxiang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2331676092\",\n",
      "            \"name\": \"Zhongxin Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model's internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"adaptive prompt generation\": {\n",
      "    \"total\": 13544,\n",
      "    \"offset\": 0,\n",
      "    \"next\": 50,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"ef399ed62fcc00e73b02f286012080351652693c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ef399ed62fcc00e73b02f286012080351652693c\",\n",
      "        \"title\": \"Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions, which is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which the authors call flocking.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2171964328\",\n",
      "            \"name\": \"Harry Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282555057\",\n",
      "            \"name\": \"Beidi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284063779\",\n",
      "            \"name\": \"Yuejie Chi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\times$ and 1.25$\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\n",
      "        \"title\": \"Prompt Expansion for Adaptive Text-to-Image Generation\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A human evaluation study shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-12-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2070966889\",\n",
      "            \"name\": \"Siddhartha Datta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276608298\",\n",
      "            \"name\": \"Alexander Ku\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275054270\",\n",
      "            \"name\": \"Deepak Ramachandran\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276610768\",\n",
      "            \"name\": \"Peter Anderson\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1d954600f17a7c3f16aa726a5eaa902d6851e808\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1d954600f17a7c3f16aa726a5eaa902d6851e808\",\n",
      "        \"title\": \"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\",\n",
      "        \"citationCount\": 52,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.12761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model's weights and incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"13563486\",\n",
      "            \"name\": \"Jaehong Yoon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2164249715\",\n",
      "            \"name\": \"Shoubin Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2061083016\",\n",
      "            \"name\": \"Vaidehi Patil\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267311471\",\n",
      "            \"name\": \"Huaxiu Yao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276608813\",\n",
      "            \"name\": \"Mohit Bansal\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model's weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a05b33072a8330413947eab0833ed2fd21de4963\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a05b33072a8330413947eab0833ed2fd21de4963\",\n",
      "        \"title\": \"Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.06401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-12-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2273557458\",\n",
      "            \"name\": \"Hao Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273559489\",\n",
      "            \"name\": \"Jun Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118764798\",\n",
      "            \"name\": \"Yizhuang Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273589717\",\n",
      "            \"name\": \"Jun Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2113457514\",\n",
      "            \"name\": \"Zhen Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274088311\",\n",
      "            \"name\": \"Xiangyu Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bc9f1025246efeed568650934b6e183415aca279\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/bc9f1025246efeed568650934b6e183415aca279\",\n",
      "        \"title\": \"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\",\n",
      "        \"citationCount\": 29,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2307.03214\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The proposed Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation, outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on the authors' main metrics for each task.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-07-06\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2221313264\",\n",
      "            \"name\": \"Jonathan Pei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1410652795\",\n",
      "            \"name\": \"Kevin Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"38666915\",\n",
      "            \"name\": \"D. Klein\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\n",
      "        \"title\": \"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\",\n",
      "        \"citationCount\": 115,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds, and shows that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1443432623\",\n",
      "            \"name\": \"Anselm Paulus\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3461866\",\n",
      "            \"name\": \"Arman Zharmagambetov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298951327\",\n",
      "            \"name\": \"Chuan Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298758184\",\n",
      "            \"name\": \"Brandon Amos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253746559\",\n",
      "            \"name\": \"Yuandong Tian\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\n",
      "        \"title\": \"WavLLM: Towards Robust and Adaptive Speech Large Language Model\",\n",
      "        \"citationCount\": 104,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"WavLLM is introduced, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2277450543\",\n",
      "            \"name\": \"Shujie Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2135918679\",\n",
      "            \"name\": \"Long Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107983441\",\n",
      "            \"name\": \"Shujie Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107970655\",\n",
      "            \"name\": \"Sanyuan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294360053\",\n",
      "            \"name\": \"Hongkun Hao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258308585\",\n",
      "            \"name\": \"Jing Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274190703\",\n",
      "            \"name\": \"Xunying Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280887661\",\n",
      "            \"name\": \"Jinyu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9075412\",\n",
      "            \"name\": \"S. Sivasankaran\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294832157\",\n",
      "            \"name\": \"Linquan Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277299355\",\n",
      "            \"name\": \"Furu Wei\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\url{aka.ms/wavllm}.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5cacb35e7bd86e64e5aca126e5011b64630007d8\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5cacb35e7bd86e64e5aca126e5011b64630007d8\",\n",
      "        \"title\": \"OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.18866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"OmniAvatar is introduced, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements, and introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-06-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2293272991\",\n",
      "            \"name\": \"Qijun Gan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294510159\",\n",
      "            \"name\": \"Ruizi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2314648986\",\n",
      "            \"name\": \"Jianke Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2370937157\",\n",
      "            \"name\": \"Shaofei Xue\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2370937932\",\n",
      "            \"name\": \"Steven Hoi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8e37dc1215681aa153a51c07078ba8befd6a6e01\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8e37dc1215681aa153a51c07078ba8befd6a6e01\",\n",
      "        \"title\": \"AdaPlanner: Adaptive Planning from Feedback with Language Models\",\n",
      "        \"citationCount\": 175,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.16653\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2118180896\",\n",
      "            \"name\": \"Haotian Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"8103389\",\n",
      "            \"name\": \"Yuchen Zhuang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2865034\",\n",
      "            \"name\": \"Lingkai Kong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2218437288\",\n",
      "            \"name\": \"Bo Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145657504\",\n",
      "            \"name\": \"Chao Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9689acb6cb760e8bc21c16f368368b37dee977f9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9689acb6cb760e8bc21c16f368368b37dee977f9\",\n",
      "        \"title\": \"Adaptive Machine Translation with Large Language Models\",\n",
      "        \"citationCount\": 106,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2301.13294\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.13294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is observed that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-01-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"9400076\",\n",
      "            \"name\": \"Yasmin Moslem\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1748844\",\n",
      "            \"name\": \"Rejwanul Haque\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144315616\",\n",
      "            \"name\": \"Andy Way\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7665642af9e682e012bec045102a4d009421067c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7665642af9e682e012bec045102a4d009421067c\",\n",
      "        \"title\": \"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\",\n",
      "        \"citationCount\": 98,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2311.17061\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An efficient yet effective framework that generates high-quality 3D humans with fine-grained geometry and realistic appearance, based on the insight that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-11-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2257708193\",\n",
      "            \"name\": \"Xian Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260342453\",\n",
      "            \"name\": \"Xiaohang Zhan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1397711601\",\n",
      "            \"name\": \"Jiaxiang Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260340529\",\n",
      "            \"name\": \"Ying Shan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2247995148\",\n",
      "            \"name\": \"Gang Zeng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258618427\",\n",
      "            \"name\": \"Dahua Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257370021\",\n",
      "            \"name\": \"Xihui Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249080787\",\n",
      "            \"name\": \"Ziwei Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5888f7aba5c601a668c290bf57addf79cc1518f1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1\",\n",
      "        \"title\": \"ChatUniTest: a ChatGPT-based automated unit test generation tool\",\n",
      "        \"citationCount\": 87,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.04764\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.04764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.04764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework that outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and generates assertions while utilizing mock objects and reection to achieve test objectives.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1387638000\",\n",
      "            \"name\": \"Zhuo-Qi Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300176046\",\n",
      "            \"name\": \"Yinghao Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064478633\",\n",
      "            \"name\": \"Chen Zhi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145590434\",\n",
      "            \"name\": \"Shuiguang Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2116398505\",\n",
      "            \"name\": \"Jianwei Yin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"836e3069a83f455f916114e7265e00187e511838\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/836e3069a83f455f916114e7265e00187e511838\",\n",
      "        \"title\": \"Locally Differentially Private Document Generation Using Zero Shot Prompting\",\n",
      "        \"citationCount\": 57,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.16111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1512255229\",\n",
      "            \"name\": \"Saiteja Utpala\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261493078\",\n",
      "            \"name\": \"Sara Hooker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261697074\",\n",
      "            \"name\": \"Pin Yu Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\% reduction in author identification F1 score against static attackers and a 26\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\n",
      "        \"title\": \"Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2261902040\",\n",
      "            \"name\": \"Hongbo Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153003087\",\n",
      "            \"name\": \"Xiangteng He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261798088\",\n",
      "            \"name\": \"Jiahuan Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143753918\",\n",
      "            \"name\": \"Yuxin Peng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"107dce4ffbdd8d83b75492216646269d8c037ab5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/107dce4ffbdd8d83b75492216646269d8c037ab5\",\n",
      "        \"title\": \"The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.09576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1847858\",\n",
      "            \"name\": \"Subhankar Maity\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144085844\",\n",
      "            \"name\": \"Aniket Deroy\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b013c9eb1284554ae696fba02bd4d7fc599890b6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b013c9eb1284554ae696fba02bd4d7fc599890b6\",\n",
      "        \"title\": \"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.00330?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.00330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"StyleCrafter is introduced, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image, and designs a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2269171464\",\n",
      "            \"name\": \"Gongye Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257035878\",\n",
      "            \"name\": \"Menghan Xia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257199953\",\n",
      "            \"name\": \"Yong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2149052351\",\n",
      "            \"name\": \"Haoxin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2087273800\",\n",
      "            \"name\": \"Jinbo Xing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253795356\",\n",
      "            \"name\": \"Xintao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3001727\",\n",
      "            \"name\": \"Yujiu Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257019659\",\n",
      "            \"name\": \"Ying Shan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0f78249a5ff64441cc51e55bbe5b97e28f801240\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0f78249a5ff64441cc51e55bbe5b97e28f801240\",\n",
      "        \"title\": \"Prompt Tuning for Generative Multimodal Pretrained Models\",\n",
      "        \"citationCount\": 37,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2208.02532\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2208.02532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work implements prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks, with a focus on generative multimodal pretrained models, instead of contrastive ones.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-08-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"50841357\",\n",
      "            \"name\": \"Han Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35996608\",\n",
      "            \"name\": \"Junyang Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"An Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155302144\",\n",
      "            \"name\": \"Peng Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144161025\",\n",
      "            \"name\": \"Chang Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"38385080\",\n",
      "            \"name\": \"Hongxia Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at \\\\url{https://github.com/OFA-Sys/OFA}\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"192b808eba1232ba3b1d1481230db122b22c97e4\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/192b808eba1232ba3b1d1481230db122b22c97e4\",\n",
      "        \"title\": \"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2312.01663\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.01663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-12-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2269468811\",\n",
      "            \"name\": \"Runze He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2052151521\",\n",
      "            \"name\": \"Shaofei Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269461105\",\n",
      "            \"name\": \"Xuecheng Nie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"151475424\",\n",
      "            \"name\": \"Tianrui Hui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1776665\",\n",
      "            \"name\": \"Luoqi Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108984\",\n",
      "            \"name\": \"Jiao Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269685669\",\n",
      "            \"name\": \"Jizhong Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269748083\",\n",
      "            \"name\": \"Guanbin Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269687302\",\n",
      "            \"name\": \"Si Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\n",
      "        \"title\": \"TaskCraft: Automated Generation of Agentic Tasks\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.10055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories, and empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-06-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2367198602\",\n",
      "            \"name\": \"Dingfeng Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366609463\",\n",
      "            \"name\": \"Jingyi Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2368654631\",\n",
      "            \"name\": \"Qianben Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2367090248\",\n",
      "            \"name\": \"Weichen Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366569457\",\n",
      "            \"name\": \"Weizhen Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366571583\",\n",
      "            \"name\": \"Hongxuan Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366522296\",\n",
      "            \"name\": \"Fangchen Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366567233\",\n",
      "            \"name\": \"Tianrui Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2368705239\",\n",
      "            \"name\": \"King Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283080546\",\n",
      "            \"name\": \"Minghao Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366695720\",\n",
      "            \"name\": \"Jian Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366560952\",\n",
      "            \"name\": \"Ge Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2182423032\",\n",
      "            \"name\": \"Jiaheng Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2351712181\",\n",
      "            \"name\": \"Changwang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366603044\",\n",
      "            \"name\": \"Jun Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2134457930\",\n",
      "            \"name\": \"Y. Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284803168\",\n",
      "            \"name\": \"Wangchunshu Zhou\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce \\\\textsc{TaskCraft}, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f373c5569b45bf580b7502729a83761a791ee209\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f373c5569b45bf580b7502729a83761a791ee209\",\n",
      "        \"title\": \"MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work is the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing, and Experimental results on four benchmarks suggest that the method outperforms the previous method on each task.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-11-29\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2238430925\",\n",
      "            \"name\": \"Haoyu Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238207741\",\n",
      "            \"name\": \"Tianyi Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239092619\",\n",
      "            \"name\": \"Jiaxi Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238449354\",\n",
      "            \"name\": \"Xing Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2311476511\",\n",
      "            \"name\": \"Qingping Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3099139\",\n",
      "            \"name\": \"Zuxuan Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258963974\",\n",
      "            \"name\": \"Hang Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238451522\",\n",
      "            \"name\": \"Yu-Gang Jiang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The diffusion model is widely leveraged for either video generation or video editing. As each field has its task-specific problems, it is difficult to merely develop a single diffusion for completing both tasks simultaneously. Video diffusion sorely relying on the text prompt can be adapted to unify the two tasks. However, it lacks a high capability of aligning heterogeneous modalities between text and image, leading to various misalignment problems. In this work, we are the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing. The proposed MagDiff introduces three types of alignments, including subject-driven alignment, adaptive prompts alignment, and high-fidelity alignment. Particularly, the subject-driven alignment is put forward to trade off the image and text prompts, serving as a unified foundation generative model for both tasks. The adaptive prompts alignment is introduced to emphasize different strengths of homogeneous and heterogeneous alignments by assigning different values of weights to the image and the text prompts. The high-fidelity alignment is developed to further enhance the fidelity of both video generation and editing by taking the subject image as an additional model input. Experimental results on four benchmarks suggest that our method outperforms the previous method on each task.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a17aee1261b56ee828f029b1caeef78033acea83\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a17aee1261b56ee828f029b1caeef78033acea83\",\n",
      "        \"title\": \"Adaptive Ship Detection From Optical to SAR Images\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LGRS.2023.3317321?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LGRS.2023.3317321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2243328855\",\n",
      "            \"name\": \"Yuxuan Yuan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2221128858\",\n",
      "            \"name\": \"Zhijie Rao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232100687\",\n",
      "            \"name\": \"Chuyang Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1950637\",\n",
      "            \"name\": \"Yue Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2713947\",\n",
      "            \"name\": \"Xinghao Ding\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advances in synthetic aperture radar (SAR) ship detection have witnessed remarkable success by using large-scale annotated datasets. However, the annotation of SAR images requires strong domain-specific expertise, significantly hindering the prompt adoption of modern object detectors in this regime. Compared to SAR data, optical data in geoscience are considerably easier to label. Motivated by this, we investigate a new and challenging problemadaptive ship detectionwith the goal of enhancing ship detection performance on SAR images by leveraging knowledge transferred from optical images. Considering the large distributional discrepancy between the source (optical) and target (SAR) domains, we present OmniAdapt, a novel framework that progressively narrows the distance between the two types of images at the pixel, feature, and classifier levels. Specifically, OmniAdapt consists of three main modules, target-like generation module (TLGM), multifeature alignment module (MFAM), and common specific decomposition module (CSDM). TLGM minimizes the visual disparity by infusing the target-domain style into the source domain. MFAM aligns local- and global-level feature representations in an adversarial manner. Finally, CSDM decomposes the classifier into two independent components, that is, the domain-common component and the domain-specific component, and promotes the recognition ability of the former via regularization learning. Experimental results demonstrate the effectiveness of the proposed method.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\n",
      "        \"title\": \"Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.05256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate, is introduced, a reinforcement learning objective tailoring generation length to per-prompt solve rate that delivers higher accuracy on the hardest problems with higher cost.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-06-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2274104658\",\n",
      "            \"name\": \"Violet Xiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326300653\",\n",
      "            \"name\": \"Chase Blagden\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"102801230\",\n",
      "            \"name\": \"Rafael Rafailov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283848553\",\n",
      "            \"name\": \"nathan lile\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366009773\",\n",
      "            \"name\": \"Sang Truong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284774407\",\n",
      "            \"name\": \"Chelsea Finn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274104149\",\n",
      "            \"name\": \"Nick Haber\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\n",
      "        \"title\": \"OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.10825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-04-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2190109518\",\n",
      "            \"name\": \"Dianbing Xi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2356794181\",\n",
      "            \"name\": \"Jiepeng Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337074199\",\n",
      "            \"name\": \"Yuanzhi Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336910859\",\n",
      "            \"name\": \"Xi Qiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3131188\",\n",
      "            \"name\": \"Yuchi Huo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325437281\",\n",
      "            \"name\": \"Rui Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336934367\",\n",
      "            \"name\": \"Chi Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336880377\",\n",
      "            \"name\": \"Xuelong Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff , aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. Our framework supports three key capabilities: (1) Text-conditioned video generation, where all modalities are jointly synthesized from a textual prompt; (2) Video understanding, where structural modalities are predicted from rgb inputs in a coherent manner; and (3) X-conditioned video generation, where video synthesis is guided by finegrained inputs such as depth, canny and segmentation. Extensive experiments demonstrate that OmniVDiff achieves state-of-the-art performance in video generation tasks and competitive results in video understanding. Its flexibility and scalability make it well-suited for downstream applications such as video-to-video translation, modality adaptation for visual tasks, and scene reconstruction.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"42117d01d498eb9f8c21b788c3565bc6855d620b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/42117d01d498eb9f8c21b788c3565bc6855d620b\",\n",
      "        \"title\": \"Learning to Transfer Prompts for Text Generation\",\n",
      "        \"citationCount\": 43,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2205.01543\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.01543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper improves this technique and proposes a novel prompt-based method (PTG) for text generation in a transferable setting that learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-05-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2018027\",\n",
      "            \"name\": \"Junyi Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1997234792\",\n",
      "            \"name\": \"Tianyi Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50204644\",\n",
      "            \"name\": \"J. Nie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153693432\",\n",
      "            \"name\": \"Ji-rong Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2542603\",\n",
      "            \"name\": \"Wayne Xin Zhao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\n",
      "        \"title\": \"CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer\",\n",
      "        \"citationCount\": 1175,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions, and develops an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2303231681\",\n",
      "            \"name\": \"Zhuoyi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238205354\",\n",
      "            \"name\": \"Jiayan Teng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2163967642\",\n",
      "            \"name\": \"Wendi Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2055623340\",\n",
      "            \"name\": \"Ming Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305795673\",\n",
      "            \"name\": \"Shiyu Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2214082934\",\n",
      "            \"name\": \"Jiazheng Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315948290\",\n",
      "            \"name\": \"Yuanming Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2105844599\",\n",
      "            \"name\": \"Wenyi Hong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268628279\",\n",
      "            \"name\": \"Xiaohan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307077651\",\n",
      "            \"name\": \"Guanyu Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307075814\",\n",
      "            \"name\": \"Da Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290625851\",\n",
      "            \"name\": \"Xiaotao Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316099643\",\n",
      "            \"name\": \"Yuxuan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265518149\",\n",
      "            \"name\": \"Weihan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2306161782\",\n",
      "            \"name\": \"Yean Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315952736\",\n",
      "            \"name\": \"Ting Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288066971\",\n",
      "            \"name\": \"Bin Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243402027\",\n",
      "            \"name\": \"Yuxiao Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238207092\",\n",
      "            \"name\": \"Jie Tang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f37a5c2bf4266d031533d5e029b74b00b48ef038\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f37a5c2bf4266d031533d5e029b74b00b48ef038\",\n",
      "        \"title\": \"Latency-Aware Generative Semantic Communications With Pre-Trained Diffusion Models\",\n",
      "        \"citationCount\": 62,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2403.17256\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A latency-aware semantic communications framework with pre-trained generative models that designs a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2258983485\",\n",
      "            \"name\": \"Li Qiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3202702\",\n",
      "            \"name\": \"Mahdi Boloursaz Mashhadi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293693238\",\n",
      "            \"name\": \"Zhen Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1690137\",\n",
      "            \"name\": \"C. Foh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293392561\",\n",
      "            \"name\": \"Pei Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279548875\",\n",
      "            \"name\": \"Mehdi Bennis\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this letter, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"759b95f7f90addc4c526cd92557e486ab143fbec\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/759b95f7f90addc4c526cd92557e486ab143fbec\",\n",
      "        \"title\": \"Style Vectors for Steering Generative Large Language Models\",\n",
      "        \"citationCount\": 39,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.01618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is shown that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches, demonstrating the effectiveness of activation engineering using such style vectors.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-02\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2098959640\",\n",
      "            \"name\": \"Kai Konen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467369\",\n",
      "            \"name\": \"Sophie Jentzsch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274662002\",\n",
      "            \"name\": \"Diaoul Diallo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467364\",\n",
      "            \"name\": \"Peer Schutt\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467405\",\n",
      "            \"name\": \"Oliver Bensch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51185829\",\n",
      "            \"name\": \"Roxanne El Baff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467346\",\n",
      "            \"name\": \"Dominik Opitz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467403\",\n",
      "            \"name\": \"Tobias Hecking\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\n",
      "        \"title\": \"EdgeLLM: Fast On-Device LLM Inference With Speculative Decoding\",\n",
      "        \"citationCount\": 22,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMC.2024.3513457?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMC.2024.3513457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2238890503\",\n",
      "            \"name\": \"Daliang Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238575108\",\n",
      "            \"name\": \"Wangsong Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2306091156\",\n",
      "            \"name\": \"Hao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239060901\",\n",
      "            \"name\": \"Xin Jin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326531487\",\n",
      "            \"name\": \"Ying Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238910539\",\n",
      "            \"name\": \"Shiyun Wei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326083763\",\n",
      "            \"name\": \"Mengwei Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237080638\",\n",
      "            \"name\": \"Xuanzhe Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device's memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3 faster than existing engines.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"411b16add23976ffcdf6422f932453f6ebcca119\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/411b16add23976ffcdf6422f932453f6ebcca119\",\n",
      "        \"title\": \"EvoPrompting: Language Models for Code-Level Neural Architecture Search\",\n",
      "        \"citationCount\": 118,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2302.14838\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.14838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-02-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"13336152\",\n",
      "            \"name\": \"Angelica Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35363891\",\n",
      "            \"name\": \"David Dohan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48165870\",\n",
      "            \"name\": \"David R. So\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\n",
      "        \"title\": \"Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.18950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The Adaptive Guided Erasure (AGE) method is proposed, which selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-01-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2007884558\",\n",
      "            \"name\": \"Anh-Vu Bui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299801919\",\n",
      "            \"name\": \"T. V\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"67329496\",\n",
      "            \"name\": \"Tung-Long Vuong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249909946\",\n",
      "            \"name\": \"Trung Le\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292198330\",\n",
      "            \"name\": \"Paul Montague\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2059248789\",\n",
      "            \"name\": \"Tamas Abraham\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275034108\",\n",
      "            \"name\": \"Junae Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1400659302\",\n",
      "            \"name\": \"Dinh Q. Phung\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a neutral concept or just an empty text prompt. In this paper, we demonstrate that this fixed-target strategy is suboptimal, as it fails to account for the impact of erasing one concept on the others. To address this limitation, we model the concept space as a graph and empirically analyze the effects of erasing one concept on the remaining concepts. Our analysis uncovers intriguing geometric properties of the concept space, where the influence of erasing a concept is confined to a local region. Building on this insight, we propose the Adaptive Guided Erasure (AGE) method, which \\\\emph{dynamically} selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects. Experimental results show that AGE significantly outperforms state-of-the-art erasure methods on preserving unrelated concepts while maintaining effective erasure performance. Our code is published at {https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\n",
      "        \"title\": \"StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.08503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A cross-modal Adaptive Instance Normalization mechanism is introduced for better integration of style and text features, enhancing alignment and a Style-based Classifier-Free Guidance approach is developed that enables selective control over stylistic elements, reducing irrelevant influences.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2334740500\",\n",
      "            \"name\": \"Mingkun Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334824597\",\n",
      "            \"name\": \"Xue Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336265253\",\n",
      "            \"name\": \"Beier Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334818360\",\n",
      "            \"name\": \"Hao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334822819\",\n",
      "            \"name\": \"Chi Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\n",
      "        \"title\": \"TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.16425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information, and designs a Dynamic Map Scaling mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-11-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2333781842\",\n",
      "            \"name\": \"Linqing Zhong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2105618628\",\n",
      "            \"name\": \"Chen Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2264574237\",\n",
      "            \"name\": \"Zihan Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325825544\",\n",
      "            \"name\": \"Yue Liao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325537018\",\n",
      "            \"name\": \"Si Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM's spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\n",
      "        \"title\": \"A new rapid deflagration-to-detonation transition in a short smooth tube\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1063/5.0191500?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1063/5.0191500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"92832821\",\n",
      "            \"name\": \"Wandong Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290316859\",\n",
      "            \"name\": \"Caizhi Fan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1734241\",\n",
      "            \"name\": \"R. Deiterding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290237139\",\n",
      "            \"name\": \"Xiaokang Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"36072040\",\n",
      "            \"name\": \"Jianhan Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290198596\",\n",
      "            \"name\": \"Xiong Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Obtaining a rapid deflagration-to-detonation transition (DDT) within a short smooth tube is a challenging task. Here, an unconventional means of flame acceleration propagating upstream in subsonic and supersonic mixtures within a smooth tube was introduced to acquire a speedy DDT. The NavierStokes equations with an adaptive mesh refinement technique and a detailed hydrogenair chemistry reaction mechanism of 11 species and 27 steps were utilized to resolve the entire DDT characteristics. The effect of the initial Mach number on flame acceleration and DDT mechanism was revealed comprehensively. The results demonstrated that a prompt oblique shock wave (SW) occurs when the flame propagates upstream along the boundary walls due to the boundary layer influence. An intense coupling between the SW and the leading flame front is enhanced by increasing the initial Mach number of the mixture. The speedy generation of the oblique SW is formed at the incipient stage, mainly produced by the boundary layer influence and the coalescences of the compression waves. Consequently, the run-up time to detonation is shortened accordingly through a fierce reflected SW due to the intense leading SW after it reflects from the confined wall. Furthermore, three kinds of DDT evolution are revealed from the obtained results: (1) localized ignition in the upper boundary wall after the reflected and transverse shock waves propagate in the upper wall regions; (2) autoignition is formed in the confined wall corner after the reflected SW; and (3) direct detonation transition occurs at the end wall behind a strongly reflected SW in the supersonic case.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\n",
      "        \"title\": \"Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2405.14713\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Generative AI capabilities are introduced to assist educators in creating tutor interfaces that meet their needs while adhering to design principles and raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2170697820\",\n",
      "            \"name\": \"Tommaso Cal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257237899\",\n",
      "            \"name\": \"Christopher J. MacLellan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\n",
      "        \"title\": \"Transformer-Based Variable-Rate Image Compression with Region-of-Interest Control\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.10807\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.10807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a transformer-based learned image compression system that is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality while confirming its superiority over the other competing methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2140736758\",\n",
      "            \"name\": \"Chia-Hao Kao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1723619\",\n",
      "            \"name\": \"Ying Weng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2116613919\",\n",
      "            \"name\": \"Yi-Hsin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"37811787\",\n",
      "            \"name\": \"Wei-Chen Chiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"123608804\",\n",
      "            \"name\": \"Wenmin Peng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper proposes a transformer-based learned image compression system. It is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality. Inspired by prompt tuning, we introduce prompt generation networks to condition the transformer-based autoencoder of compression. Our prompt generation networks generate content-adaptive tokens according to the input image, an ROI mask, and a rate parameter. The separation of the ROI mask and the rate parameter allows an intuitive way to achieve variable-rate and ROI coding simultaneously. Extensive experiments validate the effectiveness of our proposed method and confirm its superiority over the other competing methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c7acf9250926100f531bcf46d63d7da06e73928e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c7acf9250926100f531bcf46d63d7da06e73928e\",\n",
      "        \"title\": \"MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work identifies that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images, and advances the cross-modality relation learning between the prompt and the generated images to better align the prompt and image content.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2238903147\",\n",
      "            \"name\": \"Yupeng Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"18119920\",\n",
      "            \"name\": \"Daquan Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238887924\",\n",
      "            \"name\": \"Zuo-Liang Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238889090\",\n",
      "            \"name\": \"Yaxing Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3298532\",\n",
      "            \"name\": \"Qibin Hou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"33221685\",\n",
      "            \"name\": \"Jiashi Feng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in diffusion models have showcased their impressive capacity to generate visually striking images. However, ensuring a close match between the generated image and the given prompt remains a persistent challenge. In this work, we identify that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images. To better align the prompt and image content, we advance the cross-attention with an adaptive mask, which is conditioned on the attention maps and the prompt embeddings, to dynamically adjust the contribution of each text token to the image features. This mechanism explicitly diminishes the ambiguity in the semantic information embedding of the text encoder, leading to a boost of text-to-image consistency in the synthesized images. Our method, termed MaskDiffusion, is training-free and hot-pluggable for popular pre-trained diffusion models. When applied to the latent diffusion models, our MaskDiffusion can largely enhance their capability to correctly generate objects and their attributes, with negligible computation overhead compared to the original diffusion models. Our project page is https://github.com/HVision-NKU/MaskDiffusion.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\n",
      "        \"title\": \"TEMPERA: Test-Time Prompting via Reinforcement Learning\",\n",
      "        \"citationCount\": 45,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2211.11890\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.11890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work designs a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers, and achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-11-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1993655237\",\n",
      "            \"name\": \"Tianjun Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275277634\",\n",
      "            \"name\": \"Xuezhi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"65855107\",\n",
      "            \"name\": \"Denny Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50319359\",\n",
      "            \"name\": \"D. Schuurmans\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49988044\",\n",
      "            \"name\": \"Joseph E. Gonzalez\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\n",
      "        \"title\": \"QA-CLIMS: Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.09883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Book\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2280039282\",\n",
      "            \"name\": \"Songhe Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279830536\",\n",
      "            \"name\": \"Wei Zhuo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2220635949\",\n",
      "            \"name\": \"Jinheng Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265520934\",\n",
      "            \"name\": \"Linlin Shen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model's ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\n",
      "        \"title\": \"Immunosenescence, Inflammaging, and Lung Senescence in Asthma in the Elderly\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://www.mdpi.com/2218-273X/12/10/1456/pdf?version=1665574999\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9599177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Immunosenescences contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly is discussed and an overview of age-related features in the immune system and lung structure is presented.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Review\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-10-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"47522049\",\n",
      "            \"name\": \"T. Soma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46375440\",\n",
      "            \"name\": \"M. Nagata\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Prevalence of asthma in older adults is growing along with increasing global life expectancy. Due to poor clinical consequences such as high mortality, advancement in understanding the pathophysiology of asthma in older patients has been sought to provide prompt treatment for them. Age-related alterations of functions in the immune system and lung parenchyma occur throughout life. Alterations with advancing age are promoted by various stimuli, including pathobionts, fungi, viruses, pollutants, and damage-associated molecular patterns derived from impaired cells, abandoned cell debris, and senescent cells. Age-related changes in the innate and adaptive immune response, termed immunosenescence, includes impairment of phagocytosis and antigen presentation, enhancement of proinflammatory mediator generation, and production of senescence-associated secretory phenotype. Immnunosenescence could promote inflammaging (chronic low-grade inflammation) and contribute to late-onset adult asthma and asthma in the elderly, along with age-related pulmonary disease, such as chronic obstructive pulmonary disease and pulmonary fibrosis, due to lung parenchyma senescence. Aged patients with asthma exhibit local and systemic type 2 and non-type 2 inflammation, associated with clinical manifestations. Here, we discuss immunosenescences contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly and present an overview of age-related features in the immune system and lung structure.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\n",
      "        \"title\": \"DAFT-E: Feature-Based Multivariate and Multi-Step-Ahead Wind Power Forecasting\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tste.2021.3130949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tste.2021.3130949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2022-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1978743481\",\n",
      "            \"name\": \"F. De Caro\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2142416821\",\n",
      "            \"name\": \"Jacopo De Stefani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"33858225\",\n",
      "            \"name\": \"A. Vaccaro\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1772497\",\n",
      "            \"name\": \"Gianluca Bontempi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Wind energy is one of the most promising resources for the mitigation of greenhouse gas emissions that contribute to anthropogenic global warming. However, the large proliferation of wind power generators is causing several critical issues in power systems due to their variable power generated profiles. For this reason, a large number of learning techniques, e.g. integrating Vector Auto-Regressive and Neural Network-based models, were proposed in the literature for mitigating wind power uncertainty issues. Unfortunately, these methodologies show several limitations, e.g. the huge number of parameters and/or the heavy computational cost, which hinder their deployment in modern power system operation, where prompt and reliable wide-area wind power generation forecasts are requested for supporting time-critical decision making on several time horizons. To try addressing this issue, this paper proposes the Dynamic Adaptive Feature-based Temporal Ensemble (DAFT-E) forecasting approach, which relies on an extensive feature engineering, a fast feature selection step and an ensemble of computationally inexpensive models to reduce the computational complexity of the forecasting task, while still preserving predictive accuracy. The experimental results, which benchmark DAFT-E against multivariate (VAR and deep learning) alternatives on two real case studies, show that the proposed approach outperforms state-of-the-art and representation learning models according to several forecasting accuracy metrics.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f6023556221018f779a63a60874973195aea8352\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f6023556221018f779a63a60874973195aea8352\",\n",
      "        \"title\": \"Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.09857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a novel framework named Attention-aware Self-adaptive Prompt (ASP), which prevents overfitting on base task and does not require enormous data in few-shot incremental tasks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2275745354\",\n",
      "            \"name\": \"Chenxi Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254008335\",\n",
      "            \"name\": \"Zhenyi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249155683\",\n",
      "            \"name\": \"Tianyi Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262968852\",\n",
      "            \"name\": \"Ruibo Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254326623\",\n",
      "            \"name\": \"Yihan Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275765198\",\n",
      "            \"name\": \"Junfeng Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261394090\",\n",
      "            \"name\": \"Heng Huang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensive experiments on three benchmark datasets validate that ASP consistently outperforms state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning new classes and mitigating forgetting.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c36cb4a41369369d837ea170397f7818d02150dd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c36cb4a41369369d837ea170397f7818d02150dd\",\n",
      "        \"title\": \"Soft Prompt Generation for Domain Generalization\",\n",
      "        \"citationCount\": 25,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG), which consists of a two-stage training phase and an inference phase, aiming to incorporate the generative model domain knowledge.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2274938328\",\n",
      "            \"name\": \"Shuanghao Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290728057\",\n",
      "            \"name\": \"Yuedi Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275025483\",\n",
      "            \"name\": \"Wanqi Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7778036\",\n",
      "            \"name\": \"Zhirong Luan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275030348\",\n",
      "            \"name\": \"Badong Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\n",
      "        \"title\": \"Adapting to Distribution Shift by Visual Domain Prompt Generation\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.02797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction and outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"35793956\",\n",
      "            \"name\": \"Zhixiang Chi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300096585\",\n",
      "            \"name\": \"Li Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300089295\",\n",
      "            \"name\": \"Tao Zhong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277793919\",\n",
      "            \"name\": \"Huan Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1787848\",\n",
      "            \"name\": \"Yuanhao Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277598061\",\n",
      "            \"name\": \"Konstantinos N. Plataniotis\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277695392\",\n",
      "            \"name\": \"Yang Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\n",
      "        \"title\": \"Query-Based Adversarial Prompt Generation\",\n",
      "        \"citationCount\": 44,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.12329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work improves on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2268494505\",\n",
      "            \"name\": \"Jonathan Hayase\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284689404\",\n",
      "            \"name\": \"Ema Borevkovic\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2483738\",\n",
      "            \"name\": \"Nicholas Carlini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2444919\",\n",
      "            \"name\": \"Florian Tramr\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3490923\",\n",
      "            \"name\": \"Milad Nasr\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"91b6158978b248e9a0e65d0d588bc1054e72bc16\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/91b6158978b248e9a0e65d0d588bc1054e72bc16\",\n",
      "        \"title\": \"MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization\",\n",
      "        \"citationCount\": 28,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://aclanthology.org/2023.findings-emnlp.215.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP and proposes a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2260750930\",\n",
      "            \"name\": \"Yuyan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260655625\",\n",
      "            \"name\": \"Zhihao Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260651904\",\n",
      "            \"name\": \"Ge Fan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273721608\",\n",
      "            \"name\": \"Zhengyu Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273816877\",\n",
      "            \"name\": \"Wei Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260908086\",\n",
      "            \"name\": \"Dayiheng Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243457917\",\n",
      "            \"name\": \"Zhixu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2163832089\",\n",
      "            \"name\": \"Bang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265724350\",\n",
      "            \"name\": \"Yanghua Xiao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\n",
      "        \"title\": \"Federated Text-driven Prompt Generation for Vision-Language Models\",\n",
      "        \"citationCount\": 27,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2256983385\",\n",
      "            \"name\": \"Chen Qiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257324808\",\n",
      "            \"name\": \"Xingyu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"29359383\",\n",
      "            \"name\": \"Chaithanya Kumar Mummadi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144487556\",\n",
      "            \"name\": \"M. Ganesh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257091754\",\n",
      "            \"name\": \"Zhenzhen Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257130661\",\n",
      "            \"name\": \"Lu Peng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257132255\",\n",
      "            \"name\": \"Wan-Yi Lin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\n",
      "        \"title\": \"EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2024.3482175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2024.3482175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2145205189\",\n",
      "            \"name\": \"Yinsong Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266873357\",\n",
      "            \"name\": \"Jiaqi Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266387679\",\n",
      "            \"name\": \"Aidong Men\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266583142\",\n",
      "            \"name\": \"Qingchao Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1566d96346927ad4dced85de4d55356f6aee6fb6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1566d96346927ad4dced85de4d55356f6aee6fb6\",\n",
      "        \"title\": \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\n",
      "        \"citationCount\": 92,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A new approach to code generation by LLMs is proposed, which is called AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"72729733\",\n",
      "            \"name\": \"T. Ridnik\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279758170\",\n",
      "            \"name\": \"Dedy Kredo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49668367\",\n",
      "            \"name\": \"Itamar Friedman\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"211e915b2e1e0753ddd581f10362fc82f28cc606\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/211e915b2e1e0753ddd581f10362fc82f28cc606\",\n",
      "        \"title\": \"DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"DiTCtrl is proposed, a training-free multi-prompt video generation method under MM-DiT architectures for the first time that achieves state-of-the-art performance without additional training and presents MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2337083029\",\n",
      "            \"name\": \"Minghong Cai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30176430\",\n",
      "            \"name\": \"Xiaodong Cun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257035102\",\n",
      "            \"name\": \"Xiaoyu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2308556703\",\n",
      "            \"name\": \"Wenze Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2303078452\",\n",
      "            \"name\": \"Zhaoyang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257199953\",\n",
      "            \"name\": \"Yong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268490605\",\n",
      "            \"name\": \"Ying Shan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316484241\",\n",
      "            \"name\": \"Xiangyu Yue\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer (MM-DiT) architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiTs attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training. Code is available at https://github.com/TencentARC/DiTCtrl.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7be9a0708300e42fd3c376b4f4027dd530f240af\",\n",
      "        \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2404.04095\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces the Prompt Auto-Editing (PAE) method, which employs an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2148661301\",\n",
      "            \"name\": \"Wenyi Mo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146332319\",\n",
      "            \"name\": \"Tianyu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2281418241\",\n",
      "            \"name\": \"Yalong Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2295513824\",\n",
      "            \"name\": \"Bing Su\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293310016\",\n",
      "            \"name\": \"Ji-Rong Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2281323801\",\n",
      "            \"name\": \"Qing Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"iterative context update\": {\n",
      "    \"total\": 9219,\n",
      "    \"offset\": 0,\n",
      "    \"next\": 50,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\n",
      "        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\n",
      "        \"citationCount\": 51,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2004.02194\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-04-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"144713153\",\n",
      "            \"name\": \"Dan Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46507139\",\n",
      "            \"name\": \"Haibo Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"5462268\",\n",
      "            \"name\": \"Hanwang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143962510\",\n",
      "            \"name\": \"Zhengjun Zha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47446553\",\n",
      "            \"name\": \"Meng Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\n",
      "        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\n",
      "        \"citationCount\": 288,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2303.06615\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-03-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2158317969\",\n",
      "            \"name\": \"Gangwei Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107956900\",\n",
      "            \"name\": \"Xianqi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2117436225\",\n",
      "            \"name\": \"Xiao-Hua Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2150441002\",\n",
      "            \"name\": \"Xin Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\n",
      "        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-04-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"15569181\",\n",
      "            \"name\": \"S. Cipolla\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51043392\",\n",
      "            \"name\": \"J. Gondzio\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to showusing a new rearrangement of the Schur complement which exploits regularizationthat general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\n",
      "        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\n",
      "          \"status\": \"BRONZE\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2021-06-29\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1490486462\",\n",
      "            \"name\": \"Jie Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1518268141\",\n",
      "            \"name\": \"Laiyan Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1516136186\",\n",
      "            \"name\": \"Rui Huang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\n",
      "        \"title\": \"Iterative Privileged Learning\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2116383294\",\n",
      "            \"name\": \"Xue Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145728792\",\n",
      "            \"name\": \"Bo Du\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2760404\",\n",
      "            \"name\": \"Yipeng Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Chang Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143719920\",\n",
      "            \"name\": \"D. Tao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\n",
      "        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\n",
      "        \"citationCount\": 211,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2286959603\",\n",
      "            \"name\": \"Zhimin Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301232669\",\n",
      "            \"name\": \"Jianwei Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301456725\",\n",
      "            \"name\": \"Qin Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291439620\",\n",
      "            \"name\": \"Jiangfeng Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301272172\",\n",
      "            \"name\": \"Yanxin Long\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291203760\",\n",
      "            \"name\": \"Xinchi Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301231346\",\n",
      "            \"name\": \"Yingfang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2278003955\",\n",
      "            \"name\": \"Xingchao Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2162225343\",\n",
      "            \"name\": \"Minbin Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301268000\",\n",
      "            \"name\": \"Zedong Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301264655\",\n",
      "            \"name\": \"Dayou Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268375443\",\n",
      "            \"name\": \"Jiajun He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276668269\",\n",
      "            \"name\": \"Jiahao Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301265720\",\n",
      "            \"name\": \"Wenyue Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279868541\",\n",
      "            \"name\": \"Chen Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2194482010\",\n",
      "            \"name\": \"Rongwei Quan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301417687\",\n",
      "            \"name\": \"Jianxiang Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301265144\",\n",
      "            \"name\": \"Jiabin Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302916549\",\n",
      "            \"name\": \"Xiaoyan Yuan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282541108\",\n",
      "            \"name\": \"Xiao-Ting Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238393614\",\n",
      "            \"name\": \"Yixuan Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290433340\",\n",
      "            \"name\": \"Jihong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256776298\",\n",
      "            \"name\": \"Chao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279065846\",\n",
      "            \"name\": \"Mengxi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290446584\",\n",
      "            \"name\": \"Jie Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2180527302\",\n",
      "            \"name\": \"Zheng Fang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280852403\",\n",
      "            \"name\": \"Weiyan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2067731583\",\n",
      "            \"name\": \"J. Xue\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267016579\",\n",
      "            \"name\": \"Yang-Dan Tao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2141509708\",\n",
      "            \"name\": \"Jianchen Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2278809013\",\n",
      "            \"name\": \"Kai Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107932778\",\n",
      "            \"name\": \"Si-Da Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2373974424\",\n",
      "            \"name\": \"Yifu Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261466479\",\n",
      "            \"name\": \"Yun Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2195078271\",\n",
      "            \"name\": \"Dongdong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2393630397\",\n",
      "            \"name\": \"Mingtao Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297268600\",\n",
      "            \"name\": \"Zhichao Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290971075\",\n",
      "            \"name\": \"Xiao Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266017750\",\n",
      "            \"name\": \"Yan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265797902\",\n",
      "            \"name\": \"Yuhong Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283172530\",\n",
      "            \"name\": \"Wei Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256286591\",\n",
      "            \"name\": \"Dingyong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284490220\",\n",
      "            \"name\": \"Yong Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280399696\",\n",
      "            \"name\": \"Jie Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268722917\",\n",
      "            \"name\": \"Qinglin Lu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\n",
      "        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\n",
      "        \"citationCount\": 39,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2279774134\",\n",
      "            \"name\": \"Yanlin Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279774134\",\n",
      "            \"name\": \"Yanlin Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2314374754\",\n",
      "            \"name\": \"Daya Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254800142\",\n",
      "            \"name\": \"Jiachi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305869100\",\n",
      "            \"name\": \"Ruikai Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305694096\",\n",
      "            \"name\": \"Yuchi Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267902535\",\n",
      "            \"name\": \"Zibin Zheng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\n",
      "        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-03-17\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2257320765\",\n",
      "            \"name\": \"Weiyu Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2347655949\",\n",
      "            \"name\": \"Ziyang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2347552474\",\n",
      "            \"name\": \"Shaoguang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2347022035\",\n",
      "            \"name\": \"Jianxiang He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294806563\",\n",
      "            \"name\": \"Yijie Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348899671\",\n",
      "            \"name\": \"Jinhui Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257321422\",\n",
      "            \"name\": \"Ying Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2346984163\",\n",
      "            \"name\": \"Hui Xiong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\"finding a needle in a haystack.\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\n",
      "        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2310610834\",\n",
      "            \"name\": \"Alexey Kravets\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2310607749\",\n",
      "            \"name\": \"Vinay Namboodiri\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\n",
      "        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\n",
      "        \"citationCount\": 21,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2023-03-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2133908735\",\n",
      "            \"name\": \"B. M. Kessels\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"102993518\",\n",
      "            \"name\": \"R. Fey\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2246982557\",\n",
      "            \"name\": \"N. van de Wouw\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the systems entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\n",
      "        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\n",
      "        \"citationCount\": 31,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBYNCND\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-04-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"50823831\",\n",
      "            \"name\": \"Jonah Casebeer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"32125125\",\n",
      "            \"name\": \"Nicholas J. Bryan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1718742\",\n",
      "            \"name\": \"Paris Smaragdis\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against  all using a single general-purpose configuration of our approach.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\n",
      "        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"46532001\",\n",
      "            \"name\": \"Guohua Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30176488\",\n",
      "            \"name\": \"Qizhang Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153095510\",\n",
      "            \"name\": \"Yanqi Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2049670966\",\n",
      "            \"name\": \"Xinjiang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115387723\",\n",
      "            \"name\": \"Yang Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1731634\",\n",
      "            \"name\": \"W. Pedrycz\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\n",
      "        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\n",
      "        \"citationCount\": 28,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2021-02-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"26411201\",\n",
      "            \"name\": \"Xiaojun Mei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35250883\",\n",
      "            \"name\": \"Huafeng Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144922818\",\n",
      "            \"name\": \"J. Xian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150041356\",\n",
      "            \"name\": \"Bowen Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\n",
      "        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2445322\",\n",
      "            \"name\": \"Guoqiu Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"27066021\",\n",
      "            \"name\": \"Huanqian Yan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2769710\",\n",
      "            \"name\": \"Xingxing Wei\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\% on average.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\n",
      "        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-12-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2765914\",\n",
      "            \"name\": \"Zhen Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2158172225\",\n",
      "            \"name\": \"Peng Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"10684484\",\n",
      "            \"name\": \"Yunpu Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1742501819\",\n",
      "            \"name\": \"Volker Tresp\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\n",
      "        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-05-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"40370059\",\n",
      "            \"name\": \"Ashok Bandi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153604607\",\n",
      "            \"name\": \"Bhavani Shankar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1760292\",\n",
      "            \"name\": \"S. Chatzinotas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"102896981\",\n",
      "            \"name\": \"B. Ottersten\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\n",
      "        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\n",
      "        \"citationCount\": 16,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-02-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"97740251\",\n",
      "            \"name\": \"Vijay Kakani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2380060\",\n",
      "            \"name\": \"Hakil Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108780171\",\n",
      "            \"name\": \"Jongso Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34630628\",\n",
      "            \"name\": \"Choonwoo Ryu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"14653761\",\n",
      "            \"name\": \"Mahendar Kumbham\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\n",
      "        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-10-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"49916135\",\n",
      "            \"name\": \"Salman Habib\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"93037496\",\n",
      "            \"name\": \"Allison Beemer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064748249\",\n",
      "            \"name\": \"J. Kliewer\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\n",
      "        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\n",
      "        \"citationCount\": 40,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2170260327\",\n",
      "            \"name\": \"Zhangqian Bi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273871682\",\n",
      "            \"name\": \"Yao Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293395709\",\n",
      "            \"name\": \"Zheng Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273385018\",\n",
      "            \"name\": \"Hongyu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293312291\",\n",
      "            \"name\": \"Batu Guan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293769071\",\n",
      "            \"name\": \"Fangxin Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293399901\",\n",
      "            \"name\": \"Zili Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34296085\",\n",
      "            \"name\": \"Yulei Sui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1678835\",\n",
      "            \"name\": \"Xuanhua Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277587874\",\n",
      "            \"name\": \"Hai Jin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\n",
      "        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract', 'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"151101398\",\n",
      "            \"name\": \"Lanyun Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261909262\",\n",
      "            \"name\": \"Tianrun Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2510538\",\n",
      "            \"name\": \"Jianxiong Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2250849553\",\n",
      "            \"name\": \"Simon See\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2321778822\",\n",
      "            \"name\": \"Jun Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\n",
      "        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\n",
      "        \"citationCount\": 30,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2306.09869\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-06-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"153118937\",\n",
      "            \"name\": \"Geon Yeong Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109216792\",\n",
      "            \"name\": \"Jeongsol Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3332270\",\n",
      "            \"name\": \"Beomsu Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152577026\",\n",
      "            \"name\": \"Sang Wan Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30547794\",\n",
      "            \"name\": \"Jong-Chul Ye\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fca9a4508863025d95a581ead47032d497825053\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/fca9a4508863025d95a581ead47032d497825053\",\n",
      "        \"title\": \"VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.02186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"VideoICL is a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach, improving OOD video understanding performance by extending effective context length without incurring high costs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2333526847\",\n",
      "            \"name\": \"Kangsan Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307075970\",\n",
      "            \"name\": \"Geon Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3445691\",\n",
      "            \"name\": \"Youngwan Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2119578055\",\n",
      "            \"name\": \"Woongyeong Yeo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265627157\",\n",
      "            \"name\": \"Sung Ju Hwang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in video large multimodal models (LMMs) have significantly improved their video understanding and reasoning capabilities. However, their performance drops on out-of-distribution (OOD) tasks that are underrepresented in training data. Traditional methods like fine-tuning on OOD datasets are impractical due to high computational costs. While In-context learning (ICL) with demonstration examples has shown promising generalization performance in language tasks and image-language tasks without fine-tuning, applying ICL to video-language tasks faces challenges due to the limited context length in Video LMMs, as videos require longer token lengths. To address these issues, we propose VideoICL, a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach. This allows to select the most relevant examples and rank them based on similarity, to be used for inference. If the generated response has low confidence, our framework selects new examples and performs inference again, iteratively refining the results until a high-confidence response is obtained. This approach improves OOD video understanding performance by extending effective context length without incurring high costs. The experimental results on multiple benchmarks demonstrate significant performance gains, especially in domain-specific scenarios, laying the groundwork for broader video comprehension applications. Code is released at https://github.com/KangsanKim07/VideoICL\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\n",
      "        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\n",
      "        \"citationCount\": 72,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2084609980\",\n",
      "            \"name\": \"Chengwei Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258754564\",\n",
      "            \"name\": \"Aston Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1627060158\",\n",
      "            \"name\": \"Anirudh Dagar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258719488\",\n",
      "            \"name\": \"Wenming Ye\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\n",
      "        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.13016\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2135964855\",\n",
      "            \"name\": \"Jiaxi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"151471590\",\n",
      "            \"name\": \"Binyuan Hui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144399900\",\n",
      "            \"name\": \"Min Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"66200440\",\n",
      "            \"name\": \"Binhua Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2087380523\",\n",
      "            \"name\": \"Fei Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1527090216\",\n",
      "            \"name\": \"Yongbin Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\n",
      "        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2309.13701\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2245381886\",\n",
      "            \"name\": \"Hosein Hasanbeig\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"20013278\",\n",
      "            \"name\": \"Hiteshi Sharma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"15449757\",\n",
      "            \"name\": \"Leo Betthauser\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1844283112\",\n",
      "            \"name\": \"F. Frujeri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2248289111\",\n",
      "            \"name\": \"Ida Momennejad\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\n",
      "        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2245381886\",\n",
      "            \"name\": \"Hosein Hasanbeig\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258705787\",\n",
      "            \"name\": \"Microsoft Usa HITESHI SHARMA\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258705783\",\n",
      "            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258705381\",\n",
      "            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2248289111\",\n",
      "            \"name\": \"Ida Momennejad\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\n",
      "        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2118272992\",\n",
      "            \"name\": \"Zihan Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144986261\",\n",
      "            \"name\": \"D. Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2156506994\",\n",
      "            \"name\": \"Xinghuo Yu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\n",
      "        \"title\": \"Single image deraining using scale constraint iterative update network\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2023.121339?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2023.121339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1390863714\",\n",
      "            \"name\": \"Yitong Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1591131546\",\n",
      "            \"name\": \"Yongjun Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150356192\",\n",
      "            \"name\": \"Zhongwei Cui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112674491\",\n",
      "            \"name\": \"Haoliang Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2210993430\",\n",
      "            \"name\": \"Ting Ouyang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\n",
      "        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\n",
      "        \"citationCount\": 188,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2299483768\",\n",
      "            \"name\": \"Yuxi Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1996705\",\n",
      "            \"name\": \"Anirudh Goyal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289857220\",\n",
      "            \"name\": \"Wenyue Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257033898\",\n",
      "            \"name\": \"Min-Yen Kan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2542999\",\n",
      "            \"name\": \"T. Lillicrap\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266466003\",\n",
      "            \"name\": \"Kenji Kawaguchi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289844602\",\n",
      "            \"name\": \"Michael Shieh\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\%$ (+$5.9\\\\%$), $34.7\\\\%$ (+$5.8\\\\%$), and $76.4\\\\%$ (+$15.8\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\n",
      "        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\n",
      "        \"citationCount\": 366,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.15294\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"144485528\",\n",
      "            \"name\": \"Zhihong Shao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2171182\",\n",
      "            \"name\": \"Yeyun Gong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1752875\",\n",
      "            \"name\": \"Yelong Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1730108\",\n",
      "            \"name\": \"Minlie Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46429989\",\n",
      "            \"name\": \"Nan Duan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109136147\",\n",
      "            \"name\": \"Weizhu Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\n",
      "        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\n",
      "        \"citationCount\": 330,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2303.12570\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-03-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2158120018\",\n",
      "            \"name\": \"Fengji Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143876723\",\n",
      "            \"name\": \"B. Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2211964951\",\n",
      "            \"name\": \"Yue Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155352529\",\n",
      "            \"name\": \"Jin Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2134434187\",\n",
      "            \"name\": \"Daoguang Zan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145469202\",\n",
      "            \"name\": \"Yi Mao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153249455\",\n",
      "            \"name\": \"Jian-Guang Lou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109136147\",\n",
      "            \"name\": \"Weizhu Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\n",
      "        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2103.02022\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2021-03-02\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"104491233\",\n",
      "            \"name\": \"M. Flamarion\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1429030643\",\n",
      "            \"name\": \"R. Ribeiro-Jr\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB SchwarzChristoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\n",
      "        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\n",
      "        \"citationCount\": 37,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"38318287\",\n",
      "            \"name\": \"P. Amestoy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1760511\",\n",
      "            \"name\": \"A. Buttari\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1699285\",\n",
      "            \"name\": \"N. Higham\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1398545502\",\n",
      "            \"name\": \"J. LExcellent\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144615299\",\n",
      "            \"name\": \"Tho Mary\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1720746305\",\n",
      "            \"name\": \"Bastien Vieubl\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \". GMRES-based iterative renement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the ve-precision GMRES-based iterative renement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identies a small subset of relevant combinations. By choosing from within this subset one can achieve dierent levels of tradeo between cost and robustness, which allows for a ner choice of precisions depending on the problem diculty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\n",
      "        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\n",
      "        \"citationCount\": 70,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"47196237\",\n",
      "            \"name\": \"Zihao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"70097297\",\n",
      "            \"name\": \"Anji Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257447835\",\n",
      "            \"name\": \"Haowei Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290567624\",\n",
      "            \"name\": \"Jiaqi Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257636629\",\n",
      "            \"name\": \"Xiaojian Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257367774\",\n",
      "            \"name\": \"Yitao Liang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\n",
      "        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\n",
      "        \"citationCount\": 32,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"35361812\",\n",
      "            \"name\": \"Hongping Gan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260282205\",\n",
      "            \"name\": \"Xiaoyang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260279661\",\n",
      "            \"name\": \"Lijun He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260264613\",\n",
      "            \"name\": \"Jie Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\n",
      "        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2021-02-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1423730807\",\n",
      "            \"name\": \"Carsten Wiecher\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2909345\",\n",
      "            \"name\": \"Joel Greenyer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40500399\",\n",
      "            \"name\": \"Carsten Wolff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152617022\",\n",
      "            \"name\": \"H. Anacker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3261846\",\n",
      "            \"name\": \"R. Dumitrescu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\n",
      "        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": \"CCBYNC\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2021-02-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"31183250\",\n",
      "            \"name\": \"I. Klein\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1834987\",\n",
      "            \"name\": \"S. Suraci\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2125043152\",\n",
      "            \"name\": \"Leonardo Castro de Oliveira\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"72414897\",\n",
      "            \"name\": \"V. F. Rofatto\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2059636056\",\n",
      "            \"name\": \"M. T. Matsuoka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1691074\",\n",
      "            \"name\": \"S. Baselga\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\n",
      "        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\n",
      "        \"citationCount\": 16,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2046956775\",\n",
      "            \"name\": \"Benita Nortmann\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267944139\",\n",
      "            \"name\": \"Andrea Monti\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1764871\",\n",
      "            \"name\": \"M. Sassano\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2695624\",\n",
      "            \"name\": \"T. Mylvaganam\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other's performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving humanrobot interaction.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\n",
      "        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"5691344\",\n",
      "            \"name\": \"Yujiao Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39271955\",\n",
      "            \"name\": \"Zhiwen Fang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153431402\",\n",
      "            \"name\": \"Shaofeng Yuan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30380386\",\n",
      "            \"name\": \"C. Zhan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2072728505\",\n",
      "            \"name\": \"Yanyan Xing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"10638646\",\n",
      "            \"name\": \"Joey Tianyi Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115412895\",\n",
      "            \"name\": \"Feng Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\"bibr\\\" rid=\\\"ref1\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\"bibr\\\" rid=\\\"ref2\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\"bibr\\\" rid=\\\"ref3\\\">[3]</xref> and PH2 <xref ref-type=\\\"bibr\\\" rid=\\\"ref4\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\n",
      "        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\n",
      "        \"citationCount\": 44,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-09-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2158317969\",\n",
      "            \"name\": \"Gangwei Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289775621\",\n",
      "            \"name\": \"Xianqi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2319408912\",\n",
      "            \"name\": \"Zhaoxing Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2049033628\",\n",
      "            \"name\": \"Junda Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2319622516\",\n",
      "            \"name\": \"Chunyuan Liao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265575132\",\n",
      "            \"name\": \"Xin Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\n",
      "        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\n",
      "        \"citationCount\": 48,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-06\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2028213158\",\n",
      "            \"name\": \"Zhenrui Yue\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39371343\",\n",
      "            \"name\": \"Honglei Zhuang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2324782053\",\n",
      "            \"name\": \"Aijun Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261281337\",\n",
      "            \"name\": \"Kai Hui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1886219\",\n",
      "            \"name\": \"R. Jagerman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2324910979\",\n",
      "            \"name\": \"Hansi Zeng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2099586642\",\n",
      "            \"name\": \"Zhen Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325158655\",\n",
      "            \"name\": \"Dong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261356664\",\n",
      "            \"name\": \"Xuanhui Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1815447\",\n",
      "            \"name\": \"Michael Bendersky\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\n",
      "        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\n",
      "        \"citationCount\": 41,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-29\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"23111704\",\n",
      "            \"name\": \"Banghua Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273930444\",\n",
      "            \"name\": \"Michael I. Jordan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258657022\",\n",
      "            \"name\": \"Jiantao Jiao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\n",
      "        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2116395954\",\n",
      "            \"name\": \"Junbin Yuan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254058034\",\n",
      "            \"name\": \"Aiqing Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2575087\",\n",
      "            \"name\": \"Qingzhen Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2757120\",\n",
      "            \"name\": \"Kanoksak Wattanachote\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2240697074\",\n",
      "            \"name\": \"Yongyi Gong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\n",
      "        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Book\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2305310515\",\n",
      "            \"name\": \"Zhouyu Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273904059\",\n",
      "            \"name\": \"Mengshu Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2303947668\",\n",
      "            \"name\": \"Lei Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290024603\",\n",
      "            \"name\": \"Zhiqiang Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\n",
      "        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\n",
      "        \"citationCount\": 2891,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1557386977\",\n",
      "            \"name\": \"Machel Reid\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2417003\",\n",
      "            \"name\": \"Nikolay Savinov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3035073\",\n",
      "            \"name\": \"Denis Teplyashin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150077954\",\n",
      "            \"name\": \"Dmitry Lepikhin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2542999\",\n",
      "            \"name\": \"T. Lillicrap\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2285263\",\n",
      "            \"name\": \"Jean-Baptiste Alayrac\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1737285\",\n",
      "            \"name\": \"Radu Soricut\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2672644\",\n",
      "            \"name\": \"Angeliki Lazaridou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273534960\",\n",
      "            \"name\": \"Orhan Firat\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4337102\",\n",
      "            \"name\": \"Julian Schrittwieser\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2460849\",\n",
      "            \"name\": \"Ioannis Antonoglou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1508890387\",\n",
      "            \"name\": \"Rohan Anil\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"148016269\",\n",
      "            \"name\": \"Sebastian Borgeaud\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273563615\",\n",
      "            \"name\": \"Andrew M. Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143434227\",\n",
      "            \"name\": \"Katie Millican\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180676\",\n",
      "            \"name\": \"Ethan Dyer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143471164\",\n",
      "            \"name\": \"Mia Glaese\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2070364520\",\n",
      "            \"name\": \"Thibault Sottiaux\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275292292\",\n",
      "            \"name\": \"Ben-jamin Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484786\",\n",
      "            \"name\": \"Fabio Viola\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47447264\",\n",
      "            \"name\": \"Malcolm Reynolds\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2145139570\",\n",
      "            \"name\": \"Yuanzhong Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2065370007\",\n",
      "            \"name\": \"James Molloy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249566095\",\n",
      "            \"name\": \"Jilin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2090818\",\n",
      "            \"name\": \"M. Isard\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152399055\",\n",
      "            \"name\": \"P. Barham\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146532222\",\n",
      "            \"name\": \"Tom Hennigan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176102\",\n",
      "            \"name\": \"Ross Mcilroy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275525680\",\n",
      "            \"name\": \"Melvin Johnson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1698491\",\n",
      "            \"name\": \"J. Schalkwyk\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181648\",\n",
      "            \"name\": \"Eli Collins\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143538252\",\n",
      "            \"name\": \"Eliza Rutherford\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185558\",\n",
      "            \"name\": \"Erica Moreira\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34122449\",\n",
      "            \"name\": \"Kareem W. Ayoub\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186741\",\n",
      "            \"name\": \"Megha Goel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1406288863\",\n",
      "            \"name\": \"Clemens Meyer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2005813\",\n",
      "            \"name\": \"Gregory Thornton\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275729730\",\n",
      "            \"name\": \"Zhen Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47407464\",\n",
      "            \"name\": \"H. Michalewski\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185143\",\n",
      "            \"name\": \"Zaheer Abbas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"74530494\",\n",
      "            \"name\": \"Nathan Schucher\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"12679121\",\n",
      "            \"name\": \"Ankesh Anand\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185509\",\n",
      "            \"name\": \"Richard Ives\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2058168486\",\n",
      "            \"name\": \"James Keeling\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3257286\",\n",
      "            \"name\": \"Karel Lenc\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40269586\",\n",
      "            \"name\": \"S. Haykal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2944868\",\n",
      "            \"name\": \"Siamak Shakeri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"67311962\",\n",
      "            \"name\": \"Pranav Shyam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2841893\",\n",
      "            \"name\": \"A. Chowdhery\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"81387328\",\n",
      "            \"name\": \"Roman Ring\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2135383313\",\n",
      "            \"name\": \"Stephen Spencer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1413718981\",\n",
      "            \"name\": \"Eren Sezener\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289035179\",\n",
      "            \"name\": \"Luke Vilnis\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186577\",\n",
      "            \"name\": \"Os-car Chang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288269273\",\n",
      "            \"name\": \"Nobuyuki Morioka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275183383\",\n",
      "            \"name\": \"George Tucker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276211400\",\n",
      "            \"name\": \"Ce Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186839\",\n",
      "            \"name\": \"Oliver Woodman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"80930649\",\n",
      "            \"name\": \"Nithya Attaluri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2367821\",\n",
      "            \"name\": \"Toms Kocisk\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187189\",\n",
      "            \"name\": \"Evgenii Eltyshev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275535939\",\n",
      "            \"name\": \"Xi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188933\",\n",
      "            \"name\": \"Timothy Chung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1394635460\",\n",
      "            \"name\": \"Vittorio Selo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1791585\",\n",
      "            \"name\": \"Siddhartha Brahma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1737522\",\n",
      "            \"name\": \"Petko Georgiev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"133666998\",\n",
      "            \"name\": \"Ambrose Slone\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275539055\",\n",
      "            \"name\": \"Zhenkai Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266398107\",\n",
      "            \"name\": \"James Lottes\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275178766\",\n",
      "            \"name\": \"Siyuan Qiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484287\",\n",
      "            \"name\": \"Ben Caine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287841795\",\n",
      "            \"name\": \"Sebastian Riedel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176047\",\n",
      "            \"name\": \"Alex Tomala\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159545857\",\n",
      "            \"name\": \"Martin Chadwick\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253158807\",\n",
      "            \"name\": \"J Christopher Love\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2070068655\",\n",
      "            \"name\": \"Peter Choy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2073395505\",\n",
      "            \"name\": \"Sid Mittal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2815290\",\n",
      "            \"name\": \"N. Houlsby\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269752766\",\n",
      "            \"name\": \"Yunhao Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289446231\",\n",
      "            \"name\": \"Matthew Lamm\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275159462\",\n",
      "            \"name\": \"Libin Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2197671266\",\n",
      "            \"name\": \"Qiao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253917827\",\n",
      "            \"name\": \"Luheng He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275287219\",\n",
      "            \"name\": \"Yong Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186692\",\n",
      "            \"name\": \"Peter Humphreys\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275290025\",\n",
      "            \"name\": \"Yujia Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1786259\",\n",
      "            \"name\": \"Sergey Brin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51042571\",\n",
      "            \"name\": \"Albin Cassirer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283231534\",\n",
      "            \"name\": \"Ying-Qi Miao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1780245\",\n",
      "            \"name\": \"Luks Zilka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275189014\",\n",
      "            \"name\": \"Taylor Tobin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266735761\",\n",
      "            \"name\": \"Kelvin Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2161966573\",\n",
      "            \"name\": \"Lev Proleev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275175792\",\n",
      "            \"name\": \"Daniel Sohn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182383\",\n",
      "            \"name\": \"Al-berto Magni\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258347245\",\n",
      "            \"name\": \"L. Hendricks\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290513267\",\n",
      "            \"name\": \"Isabel Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2217756237\",\n",
      "            \"name\": \"Santiago Ontan'on\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275177720\",\n",
      "            \"name\": \"Oskar Bunyan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185667\",\n",
      "            \"name\": \"Nathan Byrd\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275537981\",\n",
      "            \"name\": \"Abhanshu Sharma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48335426\",\n",
      "            \"name\": \"Biao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290662953\",\n",
      "            \"name\": \"Mario Pinto\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275170606\",\n",
      "            \"name\": \"Rishika Sinha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"18138802\",\n",
      "            \"name\": \"Harsh Mehta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186531\",\n",
      "            \"name\": \"Dawei Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1413064976\",\n",
      "            \"name\": \"Sergi Caelles\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1991019030\",\n",
      "            \"name\": \"Albert Webson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275162692\",\n",
      "            \"name\": \"Alex Morris\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2080504963\",\n",
      "            \"name\": \"Becca Roelofs\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290634593\",\n",
      "            \"name\": \"Yifan Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"86898863\",\n",
      "            \"name\": \"Robin Strudel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193471\",\n",
      "            \"name\": \"Xuehan Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39687627\",\n",
      "            \"name\": \"Marvin Ritter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256989598\",\n",
      "            \"name\": \"Mostafa Dehghani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1706980\",\n",
      "            \"name\": \"R. Chaabouni\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2078909017\",\n",
      "            \"name\": \"Abhijit Karmarkar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484705\",\n",
      "            \"name\": \"Guangda Lai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3468078\",\n",
      "            \"name\": \"Fabian Mentzer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290664586\",\n",
      "            \"name\": \"Bibo Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261797906\",\n",
      "            \"name\": \"YaGuang Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275534739\",\n",
      "            \"name\": \"Yujing Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40470211\",\n",
      "            \"name\": \"T. Paine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40034895\",\n",
      "            \"name\": \"Alex Goldin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3007442\",\n",
      "            \"name\": \"Behnam Neyshabur\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1734809439\",\n",
      "            \"name\": \"Kate Baumli\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6639036\",\n",
      "            \"name\": \"Anselm Levskaya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274104519\",\n",
      "            \"name\": \"Michael Laskin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193644\",\n",
      "            \"name\": \"Wenhao Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34269227\",\n",
      "            \"name\": \"Jack W. Rae\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268673324\",\n",
      "            \"name\": \"Kefan Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485493\",\n",
      "            \"name\": \"Antoine He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487747\",\n",
      "            \"name\": \"Skye Giordano\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307454258\",\n",
      "            \"name\": \"Lakshman Yagati\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143783339\",\n",
      "            \"name\": \"Jean-Baptiste Lespiau\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"122704930\",\n",
      "            \"name\": \"Paul Natsev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185831\",\n",
      "            \"name\": \"Sanjay Ganapathy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144097210\",\n",
      "            \"name\": \"Fangyu Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487616\",\n",
      "            \"name\": \"Danilo Martins\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249840944\",\n",
      "            \"name\": \"Nanxin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275191526\",\n",
      "            \"name\": \"Yunhan Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180117\",\n",
      "            \"name\": \"Megan Barnes\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268760156\",\n",
      "            \"name\": \"Rhys May\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188533\",\n",
      "            \"name\": \"Arpi Vezer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275114643\",\n",
      "            \"name\": \"Junhyuk Oh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118834006\",\n",
      "            \"name\": \"Ken Franko\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273670422\",\n",
      "            \"name\": \"Sophie Bridgers\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275832693\",\n",
      "            \"name\": \"Ruizhe Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275291909\",\n",
      "            \"name\": \"Boxi Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40608942\",\n",
      "            \"name\": \"Basil Mustafa\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487447\",\n",
      "            \"name\": \"Sean Sechrist\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3166516\",\n",
      "            \"name\": \"Emilio Parisotto\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2598683\",\n",
      "            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487293\",\n",
      "            \"name\": \"Chris Larkin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275149073\",\n",
      "            \"name\": \"Chenjie Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186804\",\n",
      "            \"name\": \"Christina Sorokin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2048712\",\n",
      "            \"name\": \"M. Krikun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182203\",\n",
      "            \"name\": \"Alexey Guseynov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2065404873\",\n",
      "            \"name\": \"Jessica Landon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187147\",\n",
      "            \"name\": \"Romina Datta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1863250\",\n",
      "            \"name\": \"A. Pritzel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2151245633\",\n",
      "            \"name\": \"Phoebe Thacker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275801132\",\n",
      "            \"name\": \"Fan Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487874\",\n",
      "            \"name\": \"Kevin Hui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"119556335\",\n",
      "            \"name\": \"A.E. Hauth\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273556813\",\n",
      "            \"name\": \"C. Yeh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290481818\",\n",
      "            \"name\": \"David Barker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1423275766\",\n",
      "            \"name\": \"J. Mao-Jones\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2166051497\",\n",
      "            \"name\": \"Sophia Austin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307453241\",\n",
      "            \"name\": \"Hannah Sheahan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2620528\",\n",
      "            \"name\": \"Parker Schuh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188153\",\n",
      "            \"name\": \"James Svensson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193365\",\n",
      "            \"name\": \"Rohan Jain\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"96641652\",\n",
      "            \"name\": \"V. Ramasesh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185833\",\n",
      "            \"name\": \"Anton Briukhov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180366\",\n",
      "            \"name\": \"D. Chung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51029932\",\n",
      "            \"name\": \"Tamara von Glehn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275166845\",\n",
      "            \"name\": \"Christina Butterfield\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184551\",\n",
      "            \"name\": \"Priya Jhakra\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275252154\",\n",
      "            \"name\": \"Matt Wiethoff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193725\",\n",
      "            \"name\": \"Justin Frye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275175432\",\n",
      "            \"name\": \"Jordan Grimstad\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2158369306\",\n",
      "            \"name\": \"Beer Changpinyo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153892869\",\n",
      "            \"name\": \"Charline Le Lan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181572\",\n",
      "            \"name\": \"Anna Bortsova\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275892922\",\n",
      "            \"name\": \"Yonghui Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2767859\",\n",
      "            \"name\": \"P. Voigtlaender\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279918122\",\n",
      "            \"name\": \"Tara N. Sainath\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275575378\",\n",
      "            \"name\": \"Charlotte Smith\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2191689971\",\n",
      "            \"name\": \"Will Hawkins\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275191626\",\n",
      "            \"name\": \"Kris Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186515\",\n",
      "            \"name\": \"James Besley\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2059763226\",\n",
      "            \"name\": \"S. Srinivasan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3175815\",\n",
      "            \"name\": \"Mark Omernick\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2160887964\",\n",
      "            \"name\": \"Colin Gaffney\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1956049835\",\n",
      "            \"name\": \"G. Surita\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484991\",\n",
      "            \"name\": \"Ryan Burnell\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143374656\",\n",
      "            \"name\": \"Bogdan Damoc\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275220028\",\n",
      "            \"name\": \"Junwhan Ahn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2285740851\",\n",
      "            \"name\": \"Andrew Brock\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146532125\",\n",
      "            \"name\": \"Mantas Pajarskas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187155\",\n",
      "            \"name\": \"Anastasia Petrushkina\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30155667\",\n",
      "            \"name\": \"Seb Noury\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186192\",\n",
      "            \"name\": \"Lorenzo Blanco\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1754860\",\n",
      "            \"name\": \"Kevin Swersky\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185727\",\n",
      "            \"name\": \"Arun Ahuja\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261737895\",\n",
      "            \"name\": \"Thi Avrahami\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40055795\",\n",
      "            \"name\": \"Vedant Misra\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184736\",\n",
      "            \"name\": \"Raoul de Liedekerke\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181534\",\n",
      "            \"name\": \"Mariko Iinuma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144703404\",\n",
      "            \"name\": \"A. Polozov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143981350\",\n",
      "            \"name\": \"Sarah York\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47568983\",\n",
      "            \"name\": \"George van den Driessche\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185732\",\n",
      "            \"name\": \"Paul Michel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273650801\",\n",
      "            \"name\": \"Justin Chiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46901218\",\n",
      "            \"name\": \"Rory Blevins\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185661\",\n",
      "            \"name\": \"Zach Gleicher\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39257069\",\n",
      "            \"name\": \"Adri Recasens\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186093\",\n",
      "            \"name\": \"Alban Rrustemi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1980809\",\n",
      "            \"name\": \"E. Gribovskaya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275277736\",\n",
      "            \"name\": \"Au-rko Roy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487054\",\n",
      "            \"name\": \"Wiktor Gworek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186656\",\n",
      "            \"name\": \"Sbastien M. R. Arnold\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275291886\",\n",
      "            \"name\": \"Lisa Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267341862\",\n",
      "            \"name\": \"James Lee-Thorp\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2090812426\",\n",
      "            \"name\": \"M. Maggioni\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275183119\",\n",
      "            \"name\": \"Enrique Piqueras\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2051018967\",\n",
      "            \"name\": \"Kartikeya Badola\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2425230\",\n",
      "            \"name\": \"S. Vikram\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275585027\",\n",
      "            \"name\": \"Lucas Gonzalez\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186584\",\n",
      "            \"name\": \"Anirudh Baddepudi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268665228\",\n",
      "            \"name\": \"Evan Senter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261961752\",\n",
      "            \"name\": \"J. Devlin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47901308\",\n",
      "            \"name\": \"James Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275148073\",\n",
      "            \"name\": \"Michael Azzam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1994939814\",\n",
      "            \"name\": \"Maja Trebacz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35930544\",\n",
      "            \"name\": \"M. Polacek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485355\",\n",
      "            \"name\": \"Kashyap Krishnakumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193337\",\n",
      "            \"name\": \"Shuo-Yiin Chang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176212\",\n",
      "            \"name\": \"Matthew Tung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187196\",\n",
      "            \"name\": \"Ivo Penchev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258551072\",\n",
      "            \"name\": \"Rishabh Joshi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180557\",\n",
      "            \"name\": \"Kate Olszewska\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184985\",\n",
      "            \"name\": \"Carrie Muir\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185968\",\n",
      "            \"name\": \"Mateo Wirth\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184113\",\n",
      "            \"name\": \"A. Hartman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2160888100\",\n",
      "            \"name\": \"Joshua Newlan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2252586080\",\n",
      "            \"name\": \"S. Kashem\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2218882489\",\n",
      "            \"name\": \"Vijay Bolina\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484418\",\n",
      "            \"name\": \"Elahe Dabir\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3038326\",\n",
      "            \"name\": \"Joost R. van Amersfoort\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275130349\",\n",
      "            \"name\": \"Zafarali Ahmed\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185511\",\n",
      "            \"name\": \"James Cobon-Kerr\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269391198\",\n",
      "            \"name\": \"Aishwarya B Kamath\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259962018\",\n",
      "            \"name\": \"A. M. Hrafnkelsson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274787555\",\n",
      "            \"name\": \"Le Hou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485798\",\n",
      "            \"name\": \"Ian Mackinnon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2156930381\",\n",
      "            \"name\": \"Alexandre Frechette\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51210148\",\n",
      "            \"name\": \"Eric Noland\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182246\",\n",
      "            \"name\": \"Xi-ance Si\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2779842\",\n",
      "            \"name\": \"Emanuel Taropa\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350430090\",\n",
      "            \"name\": \"Dong Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275183277\",\n",
      "            \"name\": \"Phil Crone\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4478284\",\n",
      "            \"name\": \"Anmol Gulati\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180682\",\n",
      "            \"name\": \"S'ebastien Cevey\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275173231\",\n",
      "            \"name\": \"Jonas Adler\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275786213\",\n",
      "            \"name\": \"Ada Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185813\",\n",
      "            \"name\": \"David Silver\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"148152480\",\n",
      "            \"name\": \"Simon Tokumine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2067745837\",\n",
      "            \"name\": \"Richard Powell\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275280377\",\n",
      "            \"name\": \"Stephan Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275571997\",\n",
      "            \"name\": \"Michael B. Chang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275396476\",\n",
      "            \"name\": \"Samer Hassan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2007712128\",\n",
      "            \"name\": \"Diana Mincu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064599701\",\n",
      "            \"name\": \"Antoine Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153898744\",\n",
      "            \"name\": \"Nir Levine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186701\",\n",
      "            \"name\": \"Jenny Brennan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249764807\",\n",
      "            \"name\": \"Mingqiu Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265053608\",\n",
      "            \"name\": \"Sarah Hodkinson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144551262\",\n",
      "            \"name\": \"Jeffrey Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487597\",\n",
      "            \"name\": \"Josh Lipschultz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"20702300\",\n",
      "            \"name\": \"Aedan Pope\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275571997\",\n",
      "            \"name\": \"Michael B. Chang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275767067\",\n",
      "            \"name\": \"Cheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2121764\",\n",
      "            \"name\": \"Laurent El Shafey\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2264591527\",\n",
      "            \"name\": \"M. Paganini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269733876\",\n",
      "            \"name\": \"Sholto Douglas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266464503\",\n",
      "            \"name\": \"Bernd Bohnet\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274107421\",\n",
      "            \"name\": \"Fabio Pardo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182230\",\n",
      "            \"name\": \"Seth Odoom\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269541835\",\n",
      "            \"name\": \"Mihaela Roca\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267546965\",\n",
      "            \"name\": \"Cicero Nogueira dos Santos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185640\",\n",
      "            \"name\": \"Kedar Soparkar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35099444\",\n",
      "            \"name\": \"A. Guez\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187110\",\n",
      "            \"name\": \"Tom Hudson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188563\",\n",
      "            \"name\": \"Steven Hansen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50844587\",\n",
      "            \"name\": \"Chulayuth Asawaroengchai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"104000494\",\n",
      "            \"name\": \"Ravichandra Addanki\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290486855\",\n",
      "            \"name\": \"Tianhe Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3448463\",\n",
      "            \"name\": \"Wojciech Stokowiec\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258793616\",\n",
      "            \"name\": \"Mina Khan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243002880\",\n",
      "            \"name\": \"Justin Gilmer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253808003\",\n",
      "            \"name\": \"Jaehoon Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485108\",\n",
      "            \"name\": \"Carrie Grimes Bostock\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1996199677\",\n",
      "            \"name\": \"Keran Rong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2263289033\",\n",
      "            \"name\": \"Jonathan Caton\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184275\",\n",
      "            \"name\": \"Pedram Pejman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1696719\",\n",
      "            \"name\": \"Filip Pavetic\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259937157\",\n",
      "            \"name\": \"Geoff Brown\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290595918\",\n",
      "            \"name\": \"Vivek Sharma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2170162986\",\n",
      "            \"name\": \"Mario Luvci'c\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176205\",\n",
      "            \"name\": \"Rajku-mar Samuel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2941141\",\n",
      "            \"name\": \"J. Djolonga\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2063800905\",\n",
      "            \"name\": \"Amol Mandhane\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187845\",\n",
      "            \"name\": \"Lars Lowe Sjosund\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"118801223\",\n",
      "            \"name\": \"Elena Buchatskaya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275158927\",\n",
      "            \"name\": \"Elspeth White\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2201776471\",\n",
      "            \"name\": \"Natalie Clay\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260169185\",\n",
      "            \"name\": \"Jiepu Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275798209\",\n",
      "            \"name\": \"Hyeontaek Lim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"38637384\",\n",
      "            \"name\": \"Ross Hemsley\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184618\",\n",
      "            \"name\": \"Jane Labanowski\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"41019080\",\n",
      "            \"name\": \"Nicola De Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188258\",\n",
      "            \"name\": \"David Steiner\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3362306\",\n",
      "            \"name\": \"Sayed Hadi Hashemi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288056644\",\n",
      "            \"name\": \"Jacob Austin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2105841261\",\n",
      "            \"name\": \"Anita Gergely\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2221119859\",\n",
      "            \"name\": \"Tim Blyth\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275190309\",\n",
      "            \"name\": \"Joe Stanton\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2272718153\",\n",
      "            \"name\": \"K. Shivakumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9356387\",\n",
      "            \"name\": \"Aditya Siddhant\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39552848\",\n",
      "            \"name\": \"Anders Andreassen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279996944\",\n",
      "            \"name\": \"Carlos L. Araya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187415\",\n",
      "            \"name\": \"Nikhil Sethi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2934334\",\n",
      "            \"name\": \"Rakesh Shivanna\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275161833\",\n",
      "            \"name\": \"Steven Hand\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"12295226\",\n",
      "            \"name\": \"Ankur Bapna\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2402489\",\n",
      "            \"name\": \"A. Khodaei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"19200186\",\n",
      "            \"name\": \"Antoine Miech\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287809580\",\n",
      "            \"name\": \"Garrett Tanzer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1394189636\",\n",
      "            \"name\": \"Andy Swing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"41037204\",\n",
      "            \"name\": \"S. Thakoor\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291169360\",\n",
      "            \"name\": \"Zhufeng Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"81408931\",\n",
      "            \"name\": \"Zachary Nado\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2218062983\",\n",
      "            \"name\": \"Stephanie Winkler\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256337021\",\n",
      "            \"name\": \"Dian Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144413479\",\n",
      "            \"name\": \"Mohammad Saleh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"108173905\",\n",
      "            \"name\": \"Lorenzo Maggiore\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159207795\",\n",
      "            \"name\": \"Iain Barr\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187490\",\n",
      "            \"name\": \"Minh Giang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186582\",\n",
      "            \"name\": \"Thais Kagohara\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1841008\",\n",
      "            \"name\": \"Ivo Danihelka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176043\",\n",
      "            \"name\": \"Amit Marathe\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181199\",\n",
      "            \"name\": \"Vladimir Feinberg\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176049\",\n",
      "            \"name\": \"Mohamed Elhawaty\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3404697\",\n",
      "            \"name\": \"Nimesh Ghelani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48257711\",\n",
      "            \"name\": \"Dan Horgan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275121046\",\n",
      "            \"name\": \"Helen Miller\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184334\",\n",
      "            \"name\": \"Lexi Walker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1825728\",\n",
      "            \"name\": \"Richard Tanburn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180099\",\n",
      "            \"name\": \"Mukarram Tariq\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275113487\",\n",
      "            \"name\": \"Disha Shrivastava\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487337\",\n",
      "            \"name\": \"Fei Xia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284761701\",\n",
      "            \"name\": \"Chung-Cheng Chiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2333511945\",\n",
      "            \"name\": \"Zoe Ashwood\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290486431\",\n",
      "            \"name\": \"Khuslen Baatarsukh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2412073\",\n",
      "            \"name\": \"Sina Samangooei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275177971\",\n",
      "            \"name\": \"Fred Alcober\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2163521750\",\n",
      "            \"name\": \"Axel Stjerngren\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258235140\",\n",
      "            \"name\": \"P. Komarek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185589\",\n",
      "            \"name\": \"Katerina Tsihlas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"11167300\",\n",
      "            \"name\": \"Anudhyan Boral\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"89066101\",\n",
      "            \"name\": \"R. Comanescu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275275439\",\n",
      "            \"name\": \"Jeremy Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7247867\",\n",
      "            \"name\": \"Ruibo Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185808\",\n",
      "            \"name\": \"Dawn Bloxwich\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2182971260\",\n",
      "            \"name\": \"Charlie Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265240845\",\n",
      "            \"name\": \"Yanhua Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275173841\",\n",
      "            \"name\": \"Fangxi-aoyu Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2251517316\",\n",
      "            \"name\": \"M. Mauger\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1404332584\",\n",
      "            \"name\": \"Xerxes Dotiwalla\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297847306\",\n",
      "            \"name\": \"V. Hellendoorn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184531\",\n",
      "            \"name\": \"Michael Sharman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187038\",\n",
      "            \"name\": \"Ivy Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256873459\",\n",
      "            \"name\": \"Krishna Haridasan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1403998955\",\n",
      "            \"name\": \"Gabriel Barth-Maron\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181554\",\n",
      "            \"name\": \"Craig Swanson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184739\",\n",
      "            \"name\": \"Dominika Rogozi'nska\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290741315\",\n",
      "            \"name\": \"Alek Andreev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249760524\",\n",
      "            \"name\": \"P. Rubenstein\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275189194\",\n",
      "            \"name\": \"Ruoxin Sang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265528853\",\n",
      "            \"name\": \"Dan Hurt\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275189864\",\n",
      "            \"name\": \"Gamaleldin Elsayed\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290529512\",\n",
      "            \"name\": \"Ren-shen Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485332\",\n",
      "            \"name\": \"Dave Lacey\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279830514\",\n",
      "            \"name\": \"Anastasija Ili'c\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275112414\",\n",
      "            \"name\": \"Yao Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2215449616\",\n",
      "            \"name\": \"Woohyun Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257256357\",\n",
      "            \"name\": \"Lora Aroyo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275177173\",\n",
      "            \"name\": \"Chimezie Iwuanyanwu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48942032\",\n",
      "            \"name\": \"Vitaly Nikolaev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40627523\",\n",
      "            \"name\": \"Balaji Lakshminarayanan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484919\",\n",
      "            \"name\": \"Sadegh Jazayeri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31713635\",\n",
      "            \"name\": \"Raphael Lopez Kaufman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2150348369\",\n",
      "            \"name\": \"Mani Varadarajan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"118505443\",\n",
      "            \"name\": \"Chetan Tekur\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187305\",\n",
      "            \"name\": \"Doug Fritz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2140488873\",\n",
      "            \"name\": \"Misha Khalman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257286979\",\n",
      "            \"name\": \"David Reitter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487762\",\n",
      "            \"name\": \"Kingshuk Dasgupta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1658856741\",\n",
      "            \"name\": \"Shourya Sarcar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"103861813\",\n",
      "            \"name\": \"T. Ornduff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265527968\",\n",
      "            \"name\": \"Javier Snaider\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2174667321\",\n",
      "            \"name\": \"Fantine Huot\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275694953\",\n",
      "            \"name\": \"Johnson Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186180\",\n",
      "            \"name\": \"Rupert Kemp\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1702423\",\n",
      "            \"name\": \"Nejc Trdin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186554\",\n",
      "            \"name\": \"Anitha Vijayakumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290490486\",\n",
      "            \"name\": \"Lucy Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269460640\",\n",
      "            \"name\": \"Christof Angermueller\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485653\",\n",
      "            \"name\": \"Li Lao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275249023\",\n",
      "            \"name\": \"Tianqi Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290556567\",\n",
      "            \"name\": \"Haibin Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485958\",\n",
      "            \"name\": \"David Engel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275190069\",\n",
      "            \"name\": \"Somer Greene\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275169305\",\n",
      "            \"name\": \"Anais White\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290488307\",\n",
      "            \"name\": \"Jessica Austin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290666013\",\n",
      "            \"name\": \"Lilly Taylor\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181309\",\n",
      "            \"name\": \"Shereen Ashraf\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290539499\",\n",
      "            \"name\": \"Dangyi Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280669236\",\n",
      "            \"name\": \"Maria Georgaki\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485789\",\n",
      "            \"name\": \"Irene Cai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275177999\",\n",
      "            \"name\": \"Yana Kulizhskaya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2096063076\",\n",
      "            \"name\": \"Sonam Goenka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4125424\",\n",
      "            \"name\": \"Brennan Saeta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4529644\",\n",
      "            \"name\": \"Kiran Vodrahalli\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290488254\",\n",
      "            \"name\": \"Christian Frank\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47182967\",\n",
      "            \"name\": \"D. Cesare\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186837\",\n",
      "            \"name\": \"Brona Robenek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487825\",\n",
      "            \"name\": \"Harry Richardson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186342\",\n",
      "            \"name\": \"Mah-moud Alnahlawi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187959\",\n",
      "            \"name\": \"Christo-pher Yew\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181434\",\n",
      "            \"name\": \"Priya Ponnapalli\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1749128\",\n",
      "            \"name\": \"M. Tagliasacchi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188906\",\n",
      "            \"name\": \"Alex Korchemniy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275999038\",\n",
      "            \"name\": \"Yelin Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275195333\",\n",
      "            \"name\": \"Dinghua Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2080520726\",\n",
      "            \"name\": \"B. Rosgen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186260\",\n",
      "            \"name\": \"Kyle Levin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187022\",\n",
      "            \"name\": \"Jeremy Wiesner\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187506\",\n",
      "            \"name\": \"Praseem Banzal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182960\",\n",
      "            \"name\": \"Praveen Srinivasan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254035020\",\n",
      "            \"name\": \"Hongkun Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186671\",\n",
      "            \"name\": \"cCauglar Unlu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275179889\",\n",
      "            \"name\": \"David Reid\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9941702\",\n",
      "            \"name\": \"Zora Tung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2591720\",\n",
      "            \"name\": \"D. Finchelstein\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290629265\",\n",
      "            \"name\": \"Ravin Kumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288791213\",\n",
      "            \"name\": \"A. Elisseeff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290557316\",\n",
      "            \"name\": \"Jin Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290594698\",\n",
      "            \"name\": \"Ming Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2070271342\",\n",
      "            \"name\": \"Rui Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185644\",\n",
      "            \"name\": \"Ricardo Aguilar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181171\",\n",
      "            \"name\": \"Mai Gim'enez\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275552322\",\n",
      "            \"name\": \"Jiawei Xia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2770149\",\n",
      "            \"name\": \"Olivier Dousse\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145556052\",\n",
      "            \"name\": \"W. Gierke\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1735318\",\n",
      "            \"name\": \"S. Yeganeh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290486731\",\n",
      "            \"name\": \"Damion Yates\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153776147\",\n",
      "            \"name\": \"Komal Jalan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275716550\",\n",
      "            \"name\": \"Lu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275189350\",\n",
      "            \"name\": \"Eri Latorre-Chimoto\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112293680\",\n",
      "            \"name\": \"D. D. Nguyen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187515\",\n",
      "            \"name\": \"Ken Durden\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2561675\",\n",
      "            \"name\": \"Praveen Kallakuri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290524803\",\n",
      "            \"name\": \"Yaxin Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275221227\",\n",
      "            \"name\": \"Matthew Johnson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275175372\",\n",
      "            \"name\": \"Tomy Tsai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188417\",\n",
      "            \"name\": \"Alice Talbert\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275539011\",\n",
      "            \"name\": \"Jasmine Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40390373\",\n",
      "            \"name\": \"Alexander Neitz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2217508229\",\n",
      "            \"name\": \"C. Elkind\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269473701\",\n",
      "            \"name\": \"Marco Selvi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188903\",\n",
      "            \"name\": \"Mimi Jasarevic\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258550407\",\n",
      "            \"name\": \"Livio Baldini Soares\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7353832\",\n",
      "            \"name\": \"Livio Baldini Soares\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2164862499\",\n",
      "            \"name\": \"Pidong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290580195\",\n",
      "            \"name\": \"A. Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2181807096\",\n",
      "            \"name\": \"Xinyu Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2214770531\",\n",
      "            \"name\": \"Krystal Kallarackal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188993\",\n",
      "            \"name\": \"Lucia Loher\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290486901\",\n",
      "            \"name\": \"Hoi Lam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485721\",\n",
      "            \"name\": \"Josef Broder\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1404655176\",\n",
      "            \"name\": \"D. Holtmann-Rice\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275150753\",\n",
      "            \"name\": \"Nina Martin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257926827\",\n",
      "            \"name\": \"Bramandia Ramadhana\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1393948967\",\n",
      "            \"name\": \"Daniel Toyama\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290488378\",\n",
      "            \"name\": \"Mrinal Shukla\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266467648\",\n",
      "            \"name\": \"Sujoy Basu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290784246\",\n",
      "            \"name\": \"Abhi Mohan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\n",
      "        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"144621045\",\n",
      "            \"name\": \"Adam Binch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145785386\",\n",
      "            \"name\": \"Gautham P. Das\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3291831\",\n",
      "            \"name\": \"J. P. Fentanes\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1728609\",\n",
      "            \"name\": \"Marc Hanheide\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\"black box\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\n",
      "        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\n",
      "        \"citationCount\": 59,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-09-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2275119437\",\n",
      "            \"name\": \"Wei Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1382573171\",\n",
      "            \"name\": \"Chengshuai Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266463492\",\n",
      "            \"name\": \"Jiaming Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302798001\",\n",
      "            \"name\": \"Aviv Rosenberg\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266819166\",\n",
      "            \"name\": \"Zhen Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2439765\",\n",
      "            \"name\": \"Daniele Calandriello\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2140488873\",\n",
      "            \"name\": \"Misha Khalman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258551072\",\n",
      "            \"name\": \"Rishabh Joshi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1808897\",\n",
      "            \"name\": \"Bilal Piot\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316576401\",\n",
      "            \"name\": \"Mohammad Saleh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2319960151\",\n",
      "            \"name\": \"Chi Jin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301173100\",\n",
      "            \"name\": \"Tong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239381730\",\n",
      "            \"name\": \"Tianqi Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\n",
      "        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\n",
      "        \"citationCount\": 30,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2105550407\",\n",
      "            \"name\": \"Chen Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275119437\",\n",
      "            \"name\": \"Wei Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283877837\",\n",
      "            \"name\": \"Yuheng Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35279146\",\n",
      "            \"name\": \"Hanze Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275167899\",\n",
      "            \"name\": \"Nan Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260473361\",\n",
      "            \"name\": \"Tong Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\n",
      "        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-09-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2292169\",\n",
      "            \"name\": \"Ehab ElSalamouny\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1722055\",\n",
      "            \"name\": \"C. Palamidessi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\n",
      "        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2401.17842\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2218156728\",\n",
      "            \"name\": \"N. V. Stein\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"108119673\",\n",
      "            \"name\": \"Diederick Vermetten\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3160375\",\n",
      "            \"name\": \"Anna V. Kononova\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237990304\",\n",
      "            \"name\": \"T. Back\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"thematic consistency LLM\": {\n",
      "    \"total\": 887,\n",
      "    \"offset\": 0,\n",
      "    \"next\": 50,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\n",
      "        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2284692979\",\n",
      "            \"name\": \"Guangya Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284699718\",\n",
      "            \"name\": \"Yuqi Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317120126\",\n",
      "            \"name\": \"Jie Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316974967\",\n",
      "            \"name\": \"Sheng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\n",
      "        \"title\": \"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\",\n",
      "        \"citationCount\": 25,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.02896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This experimental study seeks to lay the groundwork for the understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction?\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2282535211\",\n",
      "            \"name\": \"Ivar Frisch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"24068173\",\n",
      "            \"name\": \"Mario Giulianelli\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model (LLM) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction? We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"477a16bdbb43589e5feac3881b3370e3a4ab5624\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/477a16bdbb43589e5feac3881b3370e3a4ab5624\",\n",
      "        \"title\": \"Evaluating the Consistency of LLM Evaluators\",\n",
      "        \"citationCount\": 22,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.00543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Examining the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models demonstrates that strong proprietary models are not necessarily consistent evaluators.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-11-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2291076200\",\n",
      "            \"name\": \"Noah Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290955335\",\n",
      "            \"name\": \"Jiwoo Hong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290905396\",\n",
      "            \"name\": \"James Thorne\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) have shown potential as general evaluators along with the evident benefits of speed and cost. While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators. In this paper, we conduct extensive studies on the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models. Our comprehensive analysis demonstrates that strong proprietary models are not necessarily consistent evaluators, highlighting the importance of considering consistency in assessing the capability of LLM evaluators.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"75062b58398b6e9409e5fec855f6912534331eaf\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/75062b58398b6e9409e5fec855f6912534331eaf\",\n",
      "        \"title\": \"Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales, facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-08-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2284692979\",\n",
      "            \"name\": \"Guangya Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284699718\",\n",
      "            \"name\": \"Yuqi Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317120126\",\n",
      "            \"name\": \"Jie Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316974967\",\n",
      "            \"name\": \"Sheng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths,but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority voting, enabling more informed choices on when to halt sampling and which rationale to select. Our comprehensive experiments across diverse question-answering datasets demonstrate that RASC outperforms existing methods, reducing sample usage by approximately 70% while maintaining accuracy. Moreover, RASC facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs. Our approach effectively addresses the efficiency-accuracy trade-off in LLM reasoning tasks, offering a new perspective for more nuanced, faithful, and effective utilization of LLMs in resource-constrained environments.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\n",
      "        \"title\": \"Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications\",\n",
      "        \"citationCount\": 37,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://doi.org/10.1186/s12911-024-02656-3\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11382443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare is emphasized, aiming to improve clinical outcomes ethically and effectively.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-09-09\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"145770274\",\n",
      "            \"name\": \"Tala Mirzaei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2320339088\",\n",
      "            \"name\": \"Leila Amini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2574575\",\n",
      "            \"name\": \"Pouyan Esmaeilzadeh\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions. We analyzed 3049 posts and comments extracted from a self-identified clinician subreddit using unsupervised machine learning via Latent Dirichlet Allocation and a structured qualitative analysis methodology. Analysis uncovered 14 salient themes of ethical implications, which we further consolidated into 4 overarching domains reflecting ethical issues around various clinical applications of LLM in healthcare, LLM coding, algorithm, and data governance, LLMs role in health equity and the distribution of public health services, and the relationship between users (human) and LLM systems (machine). Mapping themes to ethical frameworks in literature illustrated multifaceted issues covering transparent LLM decisions, fairness, privacy, access disparities, user experiences, and reliability. This study emphasizes the need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare, aiming to improve clinical outcomes ethically and effectively.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"6b7c5fc0f6b401962153f68f8250951f75da929e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/6b7c5fc0f6b401962153f68f8250951f75da929e\",\n",
      "        \"title\": \"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.06503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper analyzes two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and finds that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-09\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2295732707\",\n",
      "            \"name\": \"Nathan Brake\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2295732451\",\n",
      "            \"name\": \"Thomas Schaaf\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e1770838ec0667cad48729a81764ed9964d6a8e6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e1770838ec0667cad48729a81764ed9964d6a8e6\",\n",
      "        \"title\": \"LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\",\n",
      "        \"citationCount\": 110,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL), which yields similar coding quality to that of human coders but reduces TA's labor and time demands.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3400291\",\n",
      "            \"name\": \"Shih-Chieh Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261362789\",\n",
      "            \"name\": \"Aiping Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1746959\",\n",
      "            \"name\": \"Lun-Wei Ku\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA's labor and time demands.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"32426b96ff3c680125bde3b835bfa931288b8ade\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/32426b96ff3c680125bde3b835bfa931288b8ade\",\n",
      "        \"title\": \"Better Patching Using LLM Prompting, via Self-Consistency\",\n",
      "        \"citationCount\": 45,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2306.00108\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.00108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper describes an application of the $\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots, on the MODIT dataset.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3432275\",\n",
      "            \"name\": \"Toufique Ahmed\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"114875459\",\n",
      "            \"name\": \"Prem Devanbu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language models (LLMs) can be induced to solve non-trivial problems with few-shot prompts including illustrative problem-solution examples. Now if the few-shots also include chain of thought ($\\\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a explained solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\\\mathcal{S}-C$ (or even $\\\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e24424283c02fbe7f641e5b3490d7bb059f8355a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e24424283c02fbe7f641e5b3490d7bb059f8355a\",\n",
      "        \"title\": \"A Survey on LLM-as-a-Judge\",\n",
      "        \"citationCount\": 776,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.15594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built?\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-11-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2216587705\",\n",
      "            \"name\": \"Jiawei Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144267788\",\n",
      "            \"name\": \"Xuhui Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287881684\",\n",
      "            \"name\": \"Zhichao Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274159320\",\n",
      "            \"name\": \"Hexiang Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2332093190\",\n",
      "            \"name\": \"Xuehao Zhai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2250617116\",\n",
      "            \"name\": \"Chengjin Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2330714501\",\n",
      "            \"name\": \"Wei Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1944248313\",\n",
      "            \"name\": \"Yinghan Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2311556497\",\n",
      "            \"name\": \"Shengjie Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2332306096\",\n",
      "            \"name\": \"Honghao Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257058703\",\n",
      "            \"name\": \"Yuanzhuo Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284217200\",\n",
      "            \"name\": \"Jian Guo\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of\\\"LLM-as-a-Judge,\\\"where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0bf3a1867f7245b8a702093901c66b08b518eafc\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0bf3a1867f7245b8a702093901c66b08b518eafc\",\n",
      "        \"title\": \"Evaluating Very Long-Term Conversational Memory of LLM Agents\",\n",
      "        \"citationCount\": 166,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A machine-human pipeline is introduced to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs, and presents a comprehensive evaluation benchmark to measure long-term memory in models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"8785371\",\n",
      "            \"name\": \"Adyasha Maharana\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266803131\",\n",
      "            \"name\": \"Dong-Ho Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145582202\",\n",
      "            \"name\": \"S. Tulyakov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2285969697\",\n",
      "            \"name\": \"Mohit Bansal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266751000\",\n",
      "            \"name\": \"Francesco Barbieri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267220081\",\n",
      "            \"name\": \"Yuwei Fang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7c04ab297b59d4fe29285f339350882a3120b27f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7c04ab297b59d4fe29285f339350882a3120b27f\",\n",
      "        \"title\": \"CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs\",\n",
      "        \"citationCount\": 205,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642773\",\n",
      "          \"status\": \"BRONZE\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.11314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions, is developed, revealing four design considerations for future educational AI assistants.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3136345\",\n",
      "            \"name\": \"Majeed Kazemitabaar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2184253123\",\n",
      "            \"name\": \"Runlong Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280281736\",\n",
      "            \"name\": \"Xiaoning Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280145055\",\n",
      "            \"name\": \"Austin Z Henley\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243041721\",\n",
      "            \"name\": \"Paul Denny\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280145218\",\n",
      "            \"name\": \"Michelle Craig\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280146888\",\n",
      "            \"name\": \"Tovi Grossman\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates students incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AIs unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\n",
      "        \"title\": \"Exploring and Evaluating Hallucinations in LLM-Powered Code Generation\",\n",
      "        \"citationCount\": 221,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation, and proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2290969862\",\n",
      "            \"name\": \"Fang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294504414\",\n",
      "            \"name\": \"Yang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2295165194\",\n",
      "            \"name\": \"Lin Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294528116\",\n",
      "            \"name\": \"Houkun Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294510508\",\n",
      "            \"name\": \"Ruifeng Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294664033\",\n",
      "            \"name\": \"Zhen Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290433096\",\n",
      "            \"name\": \"Li Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a06d3e9e90008c64c45a0029d580541d5f646771\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771\",\n",
      "        \"title\": \"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents\",\n",
      "        \"citationCount\": 108,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.00812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An overview of the various benefits of integrating code into LLMs' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2277527247\",\n",
      "            \"name\": \"Ke Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"33456794\",\n",
      "            \"name\": \"Jiateng Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277421308\",\n",
      "            \"name\": \"John Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277597831\",\n",
      "            \"name\": \"Chaoqi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51135899\",\n",
      "            \"name\": \"Y. Fung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262396117\",\n",
      "            \"name\": \"Sha Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277416897\",\n",
      "            \"name\": \"Zixuan Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344961610\",\n",
      "            \"name\": \"Xu Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144803999\",\n",
      "            \"name\": \"Xingyao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277247982\",\n",
      "            \"name\": \"Yiquan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277409745\",\n",
      "            \"name\": \"Heng Ji\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261082008\",\n",
      "            \"name\": \"ChengXiang Zhai\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\n",
      "        \"title\": \"PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference\",\n",
      "        \"citationCount\": 106,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.12532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The number of crucial keys and values that influence future generations decreases layer by layer and can be extracted by the consistency in attention weights, leading to PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2182432556\",\n",
      "            \"name\": \"Dongjie Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302683712\",\n",
      "            \"name\": \"Xiaodong Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302558089\",\n",
      "            \"name\": \"Yan Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302556666\",\n",
      "            \"name\": \"Yao Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302704855\",\n",
      "            \"name\": \"Shilin Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302545224\",\n",
      "            \"name\": \"Hai Zhao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"36b9dac525fc93100b18d8e489bd97460cd49a5e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/36b9dac525fc93100b18d8e489bd97460cd49a5e\",\n",
      "        \"title\": \"Thematic-LM: A LLM-based Multi-agent System for Large-scale Thematic Analysis\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3696410.3714595?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3696410.3714595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study applies Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-04-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2343740149\",\n",
      "            \"name\": \"Tingrui Qiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2343749049\",\n",
      "            \"name\": \"Caroline Walker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2343746435\",\n",
      "            \"name\": \"Chris Cunningham\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2310725786\",\n",
      "            \"name\": \"Yun Sing Koh\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Thematic analysis (TA) is a widely used qualitative method for identifying underlying meanings within unstructured text. However, TA requires manual processes, which become increasingly labour-intensive and time-consuming as datasets grow. While large language models (LLMs) have been introduced to assist with TA on small-scale datasets, three key limitations hinder their effectiveness. First, current approaches often depend on interactions between an LLM agent and a human coder, a process that becomes challenging with larger datasets. Second, with feedback from the human coder, the LLM tends to mirror the human coder, which provides a narrower viewpoint of the data. Third, existing methods follow a sequential process, where codes are generated for individual samples without recalling previous codes and associated data, reducing the ability to analyse data holistically. To address these limitations, we propose Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis. Thematic-LM assigns specialised tasks to each agent, such as coding, aggregating codes, and maintaining and updating the codebook. We assign coder agents different identity perspectives to simulate the subjective nature of TA, fostering a more diverse interpretation of the data. We applied Thematic-LM to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change. We evaluate the resulting themes based on trustworthiness principles in qualitative research. Our study reveals insights such as assigning different identities to coder agents promotes divergence in codes and themes.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\n",
      "        \"title\": \"Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation\",\n",
      "        \"citationCount\": 88,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2285255408\",\n",
      "            \"name\": \"Jiawei Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31279896\",\n",
      "            \"name\": \"Renhe Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46962297\",\n",
      "            \"name\": \"Chuang Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2157765133\",\n",
      "            \"name\": \"Zengqing Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266396584\",\n",
      "            \"name\": \"Makoto Onizuka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239490643\",\n",
      "            \"name\": \"Ryosuke Shibasaki\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284717877\",\n",
      "            \"name\": \"Chuan Xiao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\n",
      "        \"title\": \"ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification\",\n",
      "        \"citationCount\": 75,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3660810?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3660810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions, and can effectively facilitate the practical application of LLMs in real-world development environments.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1994579604\",\n",
      "            \"name\": \"Fangwen Mu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305416699\",\n",
      "            \"name\": \"Lin Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259571951\",\n",
      "            \"name\": \"Song Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259613131\",\n",
      "            \"name\": \"Zhuohao Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259874187\",\n",
      "            \"name\": \"Binquan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259824656\",\n",
      "            \"name\": \"ChenXue Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260294248\",\n",
      "            \"name\": \"Shichao Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2157214565\",\n",
      "            \"name\": \"Qing Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"630c47372317164fc367153f938903e1d5b76059\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/630c47372317164fc367153f938903e1d5b76059\",\n",
      "        \"title\": \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\",\n",
      "        \"citationCount\": 50,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52202/079017-1077?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52202/079017-1077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work seeks to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1602820179\",\n",
      "            \"name\": \"Gaurang Sriramanan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344249306\",\n",
      "            \"name\": \"Siddhant Bharti\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150333898\",\n",
      "            \"name\": \"Vinu Sankar Sadasivan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152623528\",\n",
      "            \"name\": \"Shoumik Saha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305809801\",\n",
      "            \"name\": \"Priyatham Kattakinda\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34389431\",\n",
      "            \"name\": \"S. Feizi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\n",
      "        \"title\": \"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges\",\n",
      "        \"citationCount\": 43,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt), which eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1966961\",\n",
      "            \"name\": \"Yanshen Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239274607\",\n",
      "            \"name\": \"Jianfeng He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293779433\",\n",
      "            \"name\": \"Limeng Cui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3433489\",\n",
      "            \"name\": \"Shuo Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249846863\",\n",
      "            \"name\": \"Chang-Tien Lu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\n",
      "        \"title\": \"Reasoning Runtime Behavior of a Program with LLM: How Far are We?\",\n",
      "        \"citationCount\": 49,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a framework, namely $\\\\boldsymbol{\\\\mathcal{R}}\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution, and utilizes existing code benchmarks and adapt them to new benchmarks within this framework.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2293350098\",\n",
      "            \"name\": \"Junkai Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276184077\",\n",
      "            \"name\": \"Zhiyuan Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110049191\",\n",
      "            \"name\": \"Xing Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293350478\",\n",
      "            \"name\": \"Zhenhao Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286413567\",\n",
      "            \"name\": \"Ge Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265241871\",\n",
      "            \"name\": \"Xin Xia\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs (i.e., predicting code execution behaviors such as program output and execution path), but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely $\\\\boldsymbol{\\\\mathcal{R}}\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs. Our code, data and $\\\\boldsymbol{\\\\mathcal{R}}\\\\mathbf{Eval}$ leaderboard are available at https://r-eval.github.io.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\n",
      "        \"title\": \"Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization\",\n",
      "        \"citationCount\": 58,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper leverages the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics, they can be prompted to autoformalize informal mathematical statements into formal Isabelle code -- which provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2287813125\",\n",
      "            \"name\": \"Jin Peng Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144884927\",\n",
      "            \"name\": \"Charles Staats\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293653101\",\n",
      "            \"name\": \"Wenda Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2574060\",\n",
      "            \"name\": \"Christian Szegedy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7446832\",\n",
      "            \"name\": \"Kilian Q. Weinberger\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287780080\",\n",
      "            \"name\": \"Yuhuai Wu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\n",
      "        \"title\": \"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.17578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"MM-Eval is introduced, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages designed with multilingual-specific challenges in mind and finds that evaluators are unfair and inconsistent when evaluating lower-resourced languages.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2321452295\",\n",
      "            \"name\": \"Guijin Son\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"29830817\",\n",
      "            \"name\": \"Dongkeun Yoon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299329316\",\n",
      "            \"name\": \"Juyoung Suk\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301578911\",\n",
      "            \"name\": \"Javier Aula-Blasco\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327215494\",\n",
      "            \"name\": \"Mano Aslan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327216625\",\n",
      "            \"name\": \"Vu Trong Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232783785\",\n",
      "            \"name\": \"Shayekh Bin Islam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327215436\",\n",
      "            \"name\": \"Jaume Prats-Cristi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327217057\",\n",
      "            \"name\": \"Luca Tormo-Bauelos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2184037220\",\n",
      "            \"name\": \"Seungone Kim\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\\\"meta-evaluation benchmarks\\\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f1366e505de4f1d0e901903e3c17471033758a96\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f1366e505de4f1d0e901903e3c17471033758a96\",\n",
      "        \"title\": \"Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based Chatbots\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.11876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Rescriber is designed, built, and evaluated, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts, presenting a promising approach to address the privacy and trust challenges of AI.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2326261436\",\n",
      "            \"name\": \"Jijie Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326116321\",\n",
      "            \"name\": \"Eryue Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326262935\",\n",
      "            \"name\": \"Yaoyao Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326228781\",\n",
      "            \"name\": \"Tianshi Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=Rescriber) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4o. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3ec06fe8d8764123490544ab5dc956143e84b443\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3ec06fe8d8764123490544ab5dc956143e84b443\",\n",
      "        \"title\": \"Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents\",\n",
      "        \"citationCount\": 50,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.11977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise and the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Book\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2310565909\",\n",
      "            \"name\": \"Guangzhi Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2089532795\",\n",
      "            \"name\": \"Xiao Zhan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275248675\",\n",
      "            \"name\": \"Jose Such\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\n",
      "        \"title\": \"Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge\",\n",
      "        \"citationCount\": 39,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.07791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Evaluating position bias in LLM judges across pairwise and list-wise comparison settings confirms that position bias is not due to random chance and varies significantly across judges and tasks, and provides insights into the distribution of judging difficulty across the dataset.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-06-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2305925735\",\n",
      "            \"name\": \"Lin Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262963382\",\n",
      "            \"name\": \"Chiyu Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2330065663\",\n",
      "            \"name\": \"Wenhua Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2227771\",\n",
      "            \"name\": \"Weicheng Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1918441\",\n",
      "            \"name\": \"Soroush Vosoughi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases - particularly position bias, the tendency to favor solutions based on their position within the prompt - compromise its reliability. This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 15 LLM judges across MTBench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"638d8d1f3865ebf065605535a7aa50727d5ffabe\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/638d8d1f3865ebf065605535a7aa50727d5ffabe\",\n",
      "        \"title\": \"TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.18919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation, significantly improves semantic and contextual consistency in synthesized images.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-29\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2298945483\",\n",
      "            \"name\": \"Junhao Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298032416\",\n",
      "            \"name\": \"Baiqiao Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2229014859\",\n",
      "            \"name\": \"Kaixin Cai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2162225343\",\n",
      "            \"name\": \"Minbin Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276604489\",\n",
      "            \"name\": \"Hanhui Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299161673\",\n",
      "            \"name\": \"Yuxin He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298943419\",\n",
      "            \"name\": \"Xi Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298043252\",\n",
      "            \"name\": \"Yue Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298926852\",\n",
      "            \"name\": \"Yifei Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298644153\",\n",
      "            \"name\": \"Yuhao Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144880586\",\n",
      "            \"name\": \"Yiqiang Yan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291389227\",\n",
      "            \"name\": \"Xiaodan Liang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a\\\"Screenwriter\\\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the\\\"Rehearsal\\\". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the\\\"Final Performance\\\". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4777305738fd1aa30243f96a1687d57d8f70fa5d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/4777305738fd1aa30243f96a1687d57d8f70fa5d\",\n",
      "        \"title\": \"Improving Text-to-Image Consistency via Automatic Prompt Optimization\",\n",
      "        \"citationCount\": 59,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models and paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1796269096\",\n",
      "            \"name\": \"Oscar Maas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274101827\",\n",
      "            \"name\": \"Pietro Astolfi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293590162\",\n",
      "            \"name\": \"Melissa Hall\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256372432\",\n",
      "            \"name\": \"Candace Ross\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39219656\",\n",
      "            \"name\": \"Jack Urbanek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293907712\",\n",
      "            \"name\": \"Adina Williams\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2801949\",\n",
      "            \"name\": \"Aishwarya Agrawal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1456285042\",\n",
      "            \"name\": \"Adriana Romero-Soriano\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3325894\",\n",
      "            \"name\": \"M. Drozdzal\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"275a3955a83867dd36a3683788e0e053e00f8a89\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/275a3955a83867dd36a3683788e0e053e00f8a89\",\n",
      "        \"title\": \"Collaborative Enhancement of Consistency and Accuracy in US Diagnosis of Thyroid Nodules Using Large Language Models.\",\n",
      "        \"citationCount\": 60,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1148/radiol.232255?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1148/radiol.232255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging and ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1678966811\",\n",
      "            \"name\": \"Shaohong Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2272038393\",\n",
      "            \"name\": \"Wen-juan Tong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2127991969\",\n",
      "            \"name\": \"Ming-De Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"28890237\",\n",
      "            \"name\": \"Hang-tong Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2187182790\",\n",
      "            \"name\": \"Xiao-zhou Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2185218332\",\n",
      "            \"name\": \"Ze-Rong Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290915880\",\n",
      "            \"name\": \"Xin-Xin Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290897544\",\n",
      "            \"name\": \"Ruifang Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261915754\",\n",
      "            \"name\": \"Ming-De Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6457299\",\n",
      "            \"name\": \"Li-da Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290866279\",\n",
      "            \"name\": \"Wei Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Background Large language models (LLMs) hold substantial promise for medical imaging interpretation. However, there is a lack of studies on their feasibility in handling reasoning questions associated with medical diagnosis. Purpose To investigate the viability of leveraging three publicly available LLMs to enhance consistency and diagnostic accuracy in medical imaging based on standardized reporting, with pathology as the reference standard. Materials and Methods US images of thyroid nodules with pathologic results were retrospectively collected from a tertiary referral hospital between July 2022 and December 2022 and used to evaluate malignancy diagnoses generated by three LLMs-OpenAI's ChatGPT 3.5, ChatGPT 4.0, and Google's Bard. Inter- and intra-LLM agreement of diagnosis were evaluated. Then, diagnostic performance, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), was evaluated and compared for the LLMs and three interactive approaches: human reader combined with LLMs, image-to-text model combined with LLMs, and an end-to-end convolutional neural network model. Results A total of 1161 US images of thyroid nodules (498 benign, 663 malignant) from 725 patients (mean age, 42.2 years  14.1 [SD]; 516 women) were evaluated. ChatGPT 4.0 and Bard displayed substantial to almost perfect intra-LLM agreement ( range, 0.65-0.86 [95% CI: 0.64, 0.86]), while ChatGPT 3.5 showed fair to substantial agreement ( range, 0.36-0.68 [95% CI: 0.36, 0.68]). ChatGPT 4.0 had an accuracy of 78%-86% (95% CI: 76%, 88%) and sensitivity of 86%-95% (95% CI: 83%, 96%), compared with 74%-86% (95% CI: 71%, 88%) and 74%-91% (95% CI: 71%, 93%), respectively, for Bard. Moreover, with ChatGPT 4.0, the image-to-text-LLM strategy exhibited an AUC (0.83 [95% CI: 0.80, 0.85]) and accuracy (84% [95% CI: 82%, 86%]) comparable to those of the human-LLM interaction strategy with two senior readers and one junior reader and exceeding those of the human-LLM interaction strategy with one junior reader. Conclusion LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging. ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5.  RSNA, 2024 Supplemental material is available for this article.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\n",
      "        \"title\": \"Mitigating LLM Hallucinations via Conformal Abstention\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.01563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": null\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1388837087\",\n",
      "            \"name\": \"Yasin Abbasi-Yadkori\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3150458\",\n",
      "            \"name\": \"Ilja Kuzborskij\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298902427\",\n",
      "            \"name\": \"David Stutz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305592785\",\n",
      "            \"name\": \"Andrs Gyrgy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299943725\",\n",
      "            \"name\": \"Adam Fisch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299943677\",\n",
      "            \"name\": \"Arnaud Doucet\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299943992\",\n",
      "            \"name\": \"Iuliya Beloshapka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239098855\",\n",
      "            \"name\": \"Wei-Hung Weng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300022831\",\n",
      "            \"name\": \"Yao-Yuan Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257346986\",\n",
      "            \"name\": \"Csaba Szepesv'ari\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9235290\",\n",
      "            \"name\": \"Ali Taylan Cemgil\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2359197879\",\n",
      "            \"name\": \"Nenad Tomasev\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying\\\"I don't know\\\") in a general domain, instead of resorting to possibly\\\"hallucinating\\\"a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"023a98af94a3e7e8e538a6183da8ec05024fec56\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/023a98af94a3e7e8e538a6183da8ec05024fec56\",\n",
      "        \"title\": \"How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\",\n",
      "        \"citationCount\": 42,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work prompts different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions, and finds that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2307916998\",\n",
      "            \"name\": \"Julia Kharchenko\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284066307\",\n",
      "            \"name\": \"Tanya Roosta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284065969\",\n",
      "            \"name\": \"Aman Chadha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2234352974\",\n",
      "            \"name\": \"Chirag Shah\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs'cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a1849a77644ff411a03833b5aa7a65ff57158c50\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a1849a77644ff411a03833b5aa7a65ff57158c50\",\n",
      "        \"title\": \"CLLMs: Consistency Large Language Models\",\n",
      "        \"citationCount\": 50,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.00835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work develops a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory by refining the target LLM to consistently predict the fixed point given any state as input.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2258963117\",\n",
      "            \"name\": \"Siqi Kou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258334187\",\n",
      "            \"name\": \"Lanxiang Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2116778591\",\n",
      "            \"name\": \"Zhe He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260296481\",\n",
      "            \"name\": \"Zhijie Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289837431\",\n",
      "            \"name\": \"Hao Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\\\times$ to 3.4$\\\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\n",
      "        \"title\": \"PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning\",\n",
      "        \"citationCount\": 25,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613905.3651008\",\n",
      "          \"status\": \"BRONZE\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.14227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded and thus highlight potential design improvements and considerations for peer agents in both roles.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Book\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2296401615\",\n",
      "            \"name\": \"Jiawen Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292671914\",\n",
      "            \"name\": \"Yuanyuan Yao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283762773\",\n",
      "            \"name\": \"Pengcheng An\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301178822\",\n",
      "            \"name\": \"Qi Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In childrens collaborative learning, effective peer conversations can significantly enhance the quality of childrens collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster childrens creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5e317746c0f38d1149f33a19807af47d513bdd27\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5e317746c0f38d1149f33a19807af47d513bdd27\",\n",
      "        \"title\": \"Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge\",\n",
      "        \"citationCount\": 22,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.12509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald's omega, and demonstrates the limitations of fixed randomness and the importance of considering multiple samples.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-17\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2328309355\",\n",
      "            \"name\": \"Kayla Schroeder\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1411379613\",\n",
      "            \"name\": \"Zach Wood-Doughty\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs. While deterministic settings can improve consistency, they do not guarantee reliability, as a single sample from the model's probability distribution can still be misleading. Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald's omega. We evaluate the reliability of LLMs when judging the outputs of other LLMs on standard single-turn and multi-turn benchmarks, simultaneously investigating the impact of temperature on reliability. By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implications for downstream applications. Our findings highlight the need for a nuanced understanding of LLM reliability and the potential risks associated with over-reliance on single-shot evaluations. This work provides a crucial step towards building more trustworthy and reliable LLM-based systems and applications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"235a8bd57a6b53ecab756780a45bce6e4743cecd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/235a8bd57a6b53ecab756780a45bce6e4743cecd\",\n",
      "        \"title\": \"LLM-based Multi-Level Knowledge Generation for Few-shot Knowledge Graph Completion\",\n",
      "        \"citationCount\": 22,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2024/236?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2024/236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a generation-based FKGC paradigm facilitated by LLM distillation, which achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2284861197\",\n",
      "            \"name\": \"Qian Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313599793\",\n",
      "            \"name\": \"Zhuo Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2052296239\",\n",
      "            \"name\": \"Cheng Ji\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313658619\",\n",
      "            \"name\": \"Shiqi Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274552581\",\n",
      "            \"name\": \"Jianxin Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Knowledge Graphs (KGs) are pivotal in various NLP applications but often grapple with incompleteness, especially due to the long-tail problem where infrequent, unpopular relationships drastically reduce the KG completion performance. In this paper, we focus on Few-shot Knowledge Graph Completion (FKGC), a task addressing these gaps in long-tail scenarios. Amidst the rapid evolution of Large Language Models, we propose a generation-based FKGC paradigm facilitated by LLM distillation. Our MuKDC framework employs multi-level knowledge distillation for few-shot KG completion, generating supplementary knowledge to mitigate data scarcity in few-shot environments. MuKDC comprises two primary components: Multi-level Knowledge Generation, which enriches the KG at various levels, and Consistency Assessment, to ensure the coherence and reliability of the generated knowledge. Most notably, our method achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\n",
      "        \"title\": \"Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2144511530\",\n",
      "            \"name\": \"Xuan Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265432385\",\n",
      "            \"name\": \"Jie Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302819855\",\n",
      "            \"name\": \"Song Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302765389\",\n",
      "            \"name\": \"Haoyang Shang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302927286\",\n",
      "            \"name\": \"Chengxu Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302915902\",\n",
      "            \"name\": \"Quanyan Zhu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"086046d38b3a7066aa39e3d350905e8065c8f1b5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/086046d38b3a7066aa39e3d350905e8065c8f1b5\",\n",
      "        \"title\": \"Efficiently Scaling LLM Reasoning with Certaindex\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.20993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy, so this work introduces Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-12-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2282896192\",\n",
      "            \"name\": \"Yichao Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279862923\",\n",
      "            \"name\": \"Junda Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317134948\",\n",
      "            \"name\": \"Siqi Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337869460\",\n",
      "            \"name\": \"Zheyu Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2351053054\",\n",
      "            \"name\": \"Zhongdongming Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152482391\",\n",
      "            \"name\": \"Yonghao Zhuang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2363671676\",\n",
      "            \"name\": \"Yian Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317112099\",\n",
      "            \"name\": \"Aurick Qiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2379937619\",\n",
      "            \"name\": \"Tajana Rosing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344601177\",\n",
      "            \"name\": \"Ion Stoica\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337807823\",\n",
      "            \"name\": \"Hao Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50% compute savings and 3.3x higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"43fedc4430be030c083626b3e64b7093916b429a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/43fedc4430be030c083626b3e64b7093916b429a\",\n",
      "        \"title\": \"Enhancing clinical reasoning skills for medical students: a qualitative comparison of LLM-powered social robotic versus computer-based virtual patients within rheumatology\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://doi.org/10.1007/s00296-024-05731-0\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11618132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform and shows promise in training CR skills, communication, and adaptive thinking.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2290951787\",\n",
      "            \"name\": \"Alexander Borg\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326093338\",\n",
      "            \"name\": \"Benjamin Jobs\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2164303442\",\n",
      "            \"name\": \"Viking Huss\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"23717264\",\n",
      "            \"name\": \"C. Gentline\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326093767\",\n",
      "            \"name\": \"Fabricio Espinosa\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290722393\",\n",
      "            \"name\": \"Mini Ruiz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2758537\",\n",
      "            \"name\": \"Samuel Edelbring\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326094583\",\n",
      "            \"name\": \"Carina Georg\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"103081544\",\n",
      "            \"name\": \"G. Skantze\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"8637952\",\n",
      "            \"name\": \"Ioannis Parodis\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Virtual patients (VPs) are increasingly used in medical education to train clinical reasoning (CR) skills. However, optimal VP design for enhancing interactivity and authenticity remains unclear. Novel interactive modalities, such as large language model (LLM)-enhanced social robotic VPs might increase interactivity and authenticity in CR skill practice. To evaluate medical students perceptions of CR training using an LLM-enhanced social robotic VP platform compared with a conventional computer-based VP platform. A qualitative study involved 23 third-year medical students from Karolinska Institutet, who completed VP cases on an LLM-enhanced social robotic platform and a computer-based semi-linear platform. In-depth interviews assessed students self-perceived acquirement of CR skills using the two platforms. Thematic analysis was employed to identify themes and sub-themes. Three main themes were identified: authenticity, VP application, and strengths and limitations. Students found the social robotic platform more authentic and engaging. It enabled highly interactive communication and expressed emotions, collectively offering a realistic experience. It facilitated active learning, hypothesis generation, and adaptive thinking. Limitations included lack of physical examination options and, occasionally, mechanical dialogue. The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform. Despite some limitations, it shows promise in training CR skills, communication, and adaptive thinking. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f04c25fcf3247ff4d8eca72d862b22090b884b75\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f04c25fcf3247ff4d8eca72d862b22090b884b75\",\n",
      "        \"title\": \"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.06461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"49606614\",\n",
      "            \"name\": \"Junlin Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282521448\",\n",
      "            \"name\": \"Siddhartha Jain\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305691523\",\n",
      "            \"name\": \"Dejiao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282366776\",\n",
      "            \"name\": \"Baishakhi Ray\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40574366\",\n",
      "            \"name\": \"Varun Kumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2304481349\",\n",
      "            \"name\": \"Ben Athiwaratkun\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often dont surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\n",
      "        \"title\": \"What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering\",\n",
      "        \"citationCount\": 70,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.12334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Two metrics for classification tasks, namely sensitivity and consistency, are introduced, which are complementary to task performance, and it is hoped that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2307085791\",\n",
      "            \"name\": \"Federico Errica\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2009237\",\n",
      "            \"name\": \"G. Siracusano\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3109801\",\n",
      "            \"name\": \"D. Sanvito\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269460793\",\n",
      "            \"name\": \"Roberto Bifulco\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs'inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely sensitivity and consistency, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\n",
      "        \"title\": \"PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency\",\n",
      "        \"citationCount\": 37,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2403.09732?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2403.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel prompt representation, called reference-enhanced representation, is introduced, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries and proposes using cross-consistency across different LLMs rather than self-consistency within a particular LLM.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2262448530\",\n",
      "            \"name\": \"Zhishuai Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292059965\",\n",
      "            \"name\": \"Xiang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291921433\",\n",
      "            \"name\": \"Jingjing Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262480162\",\n",
      "            \"name\": \"Sun Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2228059114\",\n",
      "            \"name\": \"Guoqing Du\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267589674\",\n",
      "            \"name\": \"Xiaoru Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315291036\",\n",
      "            \"name\": \"Bin Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276235755\",\n",
      "            \"name\": \"Yuxiao Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262543561\",\n",
      "            \"name\": \"Ziyue Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2263456785\",\n",
      "            \"name\": \"Rui Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262446566\",\n",
      "            \"name\": \"Hangyu Mao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0e314ddbf28514d92f2405b73941242c162ae0ba\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0e314ddbf28514d92f2405b73941242c162ae0ba\",\n",
      "        \"title\": \"AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators\",\n",
      "        \"citationCount\": 28,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.11073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AFaCTA (Automatic Factual Claim deTection Annotator) is introduced, a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs) and calibrates its annotation confidence with consistency along three predefined reasoning paths.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2046974354\",\n",
      "            \"name\": \"Jingwei Ni\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284947386\",\n",
      "            \"name\": \"Minjing Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"146552774\",\n",
      "            \"name\": \"Dominik Stammbach\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2790926\",\n",
      "            \"name\": \"Mrinmaya Sachan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261279066\",\n",
      "            \"name\": \"Elliott Ash\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3073566\",\n",
      "            \"name\": \"Markus Leippold\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1c5a097b4e376897545f153370425cf7e0c2d8fd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1c5a097b4e376897545f153370425cf7e0c2d8fd\",\n",
      "        \"title\": \"Explaining Length Bias in LLM-Based Preference Evaluations\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.01085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AdapAlpaca is proposed, a simple yet effective adjustment to win rate measurement that ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-07-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Zhengyu Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322070046\",\n",
      "            \"name\": \"Linxin Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309191644\",\n",
      "            \"name\": \"Jieyu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2311315868\",\n",
      "            \"name\": \"Zheyuan Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269687536\",\n",
      "            \"name\": \"Jingang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309176938\",\n",
      "            \"name\": \"Zhenyu Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309202283\",\n",
      "            \"name\": \"Jieyu Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269470756\",\n",
      "            \"name\": \"Hui Xiong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The use of large language models (LLMs) as judges, particularly in preference comparisons, has become widespread, but this reveals a notable bias towards longer responses, undermining the reliability of such evaluations. To better understand such bias, we propose to decompose the preference evaluation metric, specifically the win rate, into two key components: desirability and information mass, where the former is length-independent and related to trustworthiness such as correctness, toxicity, and consistency, and the latter is length-dependent and represents the amount of information in the response. We empirically demonstrated the decomposition through controlled experiments and found that response length impacts evaluations by influencing information mass. To derive a reliable evaluation metric that assesses content quality without being confounded by response length, we propose AdapAlpaca, a simple yet effective adjustment to win rate measurement. Specifically, AdapAlpaca ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"00ce8beee350a260395676490915d7ebfa7430d1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/00ce8beee350a260395676490915d7ebfa7430d1\",\n",
      "        \"title\": \"Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation\",\n",
      "        \"citationCount\": 16,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Investigation of fine-tuned LLMs in D2T tasks in terms of model size reveals that increasing LLM size enhances readability andformativeness in D2T tasks, but larger LLMs may sacrifice \\\\textit{faithfulness}.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"8038450\",\n",
      "            \"name\": \"Joy Mahapatra\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2312204876\",\n",
      "            \"name\": \"U. Garain\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Data-to-text (D2T) generation aims to generate human-readable text from semi-structured data, such as tables and graphs. The recent success of D2T is largely attributed to advancements in LLMs. Despite the success of LLMs, no research has been conducted to illustrate the impact of model size on the performance of fine-tuned LLMs for D2T tasks. D2T model performance is typically assessed based on three key qualities: \\\\textit{readability} (indicates fluency and coherence), \\\\textit{informativeness} (measures content similarity), and \\\\textit{faithfulness} (assesses consistency of factual information). It is currently uncertain whether increasing the size of LLMs effectively improves performance in D2T tasks across these three qualities. The objective of this study is to investigate the performance of fine-tuned LLMs in D2T tasks in terms of model size. Through extensive comparative analysis, we aim to elucidate both the advantages and limitations of scaling model sizes across five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and twelve state-of-the-art LLMs with varying sizes from five different LLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all the three essential qualities of D2T models, we incorporate six widely recognized automatic metrics -- \\\\textsc{BLEU}, \\\\textsc{METEOR}, \\\\textsc{BERTScore}, \\\\textsc{MoverScore}, \\\\textsc{Parent}, and \\\\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance concerning model size in the presence of source-reference divergence, a critical aspect of D2T tasks. Our investigation reveals that increasing LLM size enhances \\\\textit{readability} and \\\\textit{informativeness} in D2T tasks, but larger (in terms of size) LLMs may sacrifice \\\\textit{faithfulness}. Moreover, small-sized LLMs show more resilience than larger ones when source-reference divergence is present.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"74908bc543e77b8995a6eebe32ab13cf0837949b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/74908bc543e77b8995a6eebe32ab13cf0837949b\",\n",
      "        \"title\": \"LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment and demonstrates that, when using GPT-4, the proposed metric achieves evaluation consistency close to that of radiologists.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2294387070\",\n",
      "            \"name\": \"Zilong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"13289447\",\n",
      "            \"name\": \"Xufang Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268347004\",\n",
      "            \"name\": \"Xinyang Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268313028\",\n",
      "            \"name\": \"Dongsheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2160727304\",\n",
      "            \"name\": \"Lili Qiu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f33991c02f8f0ab8794dad020c648b37ccc53365\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f33991c02f8f0ab8794dad020c648b37ccc53365\",\n",
      "        \"title\": \"CitaLaw: Enhancing LLM with Citations in Legal Domain\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.14556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes CitaLaw, the first benchmark designed to evaluate LLMs' ability to produce legally sound responses with appropriate citations, and introduces syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2290124325\",\n",
      "            \"name\": \"Kepu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118684861\",\n",
      "            \"name\": \"Weijie Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155892801\",\n",
      "            \"name\": \"Sunhao Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274965731\",\n",
      "            \"name\": \"Jun Xu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0d5ccf0861b62223fe562e13369ab15746188251\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0d5ccf0861b62223fe562e13369ab15746188251\",\n",
      "        \"title\": \"QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.05345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums, and applies QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2300845194\",\n",
      "            \"name\": \"Varun Nagaraj Rao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300370478\",\n",
      "            \"name\": \"Eesha Agarwal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300371225\",\n",
      "            \"name\": \"Samantha Dalal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265042713\",\n",
      "            \"name\": \"Dan Calacci\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266397659\",\n",
      "            \"name\": \"Andr'es Monroy-Hern'andez\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methodologies used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting and human evaluation methodology. We applied this framework to analyze over one million comments from two of Reddit's rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\n",
      "        \"title\": \"Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.13082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"TREACLE is proposed, a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints, and provides the user with the ability to gracefully trade off accuracy for cost.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-17\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2157197701\",\n",
      "            \"name\": \"Xuechen Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297821531\",\n",
      "            \"name\": \"Zijian Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297769735\",\n",
      "            \"name\": \"Ege Onur Taga\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1393650147\",\n",
      "            \"name\": \"Carlee Joe-Wong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3103394\",\n",
      "            \"name\": \"Samet Oymak\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2281075331\",\n",
      "            \"name\": \"Jiasi Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE ($\\\\underline{T}$hrifty $\\\\underline{Rea}$soning via $\\\\underline{C}$ontext-Aware $\\\\underline{L}$LM and Prompt S$\\\\underline{e}$lection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c01bbc439164002f2c7326748f7939783e306a94\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c01bbc439164002f2c7326748f7939783e306a94\",\n",
      "        \"title\": \"A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.09972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a different level of rule understanding in the prompt, crucial for improving the accuracy and consistency of LLM-based evaluations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2264464750\",\n",
      "            \"name\": \"Kuanchao Chu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109381394\",\n",
      "            \"name\": \"Yi-Pei Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301580436\",\n",
      "            \"name\": \"Hideki Nakayama\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This research investigates prompt designs of evaluating generated texts using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for open-ended text evaluation remains challenging due to model sensitivity and subjectivity in evaluation of text generation. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a different level of rule understanding in the prompt. An additional optimization may enhance scoring alignment if sufficient data is available. This insight is crucial for improving the accuracy and consistency of LLM-based evaluations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\n",
      "        \"title\": \"How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment\",\n",
      "        \"citationCount\": 135,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2309.14049\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Insight into novice learners use of AI code generators in a self-paced learning environment is offered, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3136345\",\n",
      "            \"name\": \"Majeed Kazemitabaar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112801172\",\n",
      "            \"name\": \"Xinying Hou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2063979470\",\n",
      "            \"name\": \"A. Henley\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"20937525\",\n",
      "            \"name\": \"B. Ericson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2862077\",\n",
      "            \"name\": \"David Weintrop\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2666589\",\n",
      "            \"name\": \"Tovi Grossman\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\n",
      "        \"title\": \"PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes to comprehensively address all subtasks together, and suggests employing a unique prompt-based in-context learning strategy to enhance error correction and error detection performance in medical systems where prediction errors can have grave consequences.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"9122885\",\n",
      "            \"name\": \"Kesav Gundabathula\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301202434\",\n",
      "            \"name\": \"Sriram R Kolar\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals. This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it. Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information. We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy. We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge. In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(list_of_papers, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d1b4c8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>methodology_overlap</th>\n",
       "      <th>problem_overlap</th>\n",
       "      <th>domain_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    methodology_overlap  problem_overlap  domain_overlap\n",
       "0                  0.85             0.85            0.70\n",
       "1                  0.84             0.82            0.78\n",
       "2                  0.83             0.82            0.80\n",
       "3                  0.70             0.70            0.75\n",
       "4                  0.78             0.75            0.76\n",
       "5                  0.72             0.68            0.65\n",
       "6                  0.80             0.80            0.80\n",
       "7                  0.72             0.70            0.70\n",
       "8                  0.66             0.66            0.70\n",
       "9                  0.68             0.70            0.72\n",
       "10                 0.81             0.82            0.79\n",
       "11                 0.73             0.74            0.72\n",
       "12                 0.77             0.76            0.75\n",
       "13                 0.80             0.78            0.79\n",
       "14                 0.72             0.74            0.72\n",
       "15                 0.70             0.70            0.75\n",
       "16                 0.64             0.66            0.68\n",
       "17                 0.66             0.66            0.70\n",
       "18                 0.58             0.62            0.60\n",
       "19                 0.68             0.70            0.72"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Parse the results\n",
    "result_content = json.loads(result_llm[\"messages\"][-1].content)\n",
    "\n",
    "# Create DataFrame\n",
    "papers_df = pd.DataFrame(result_content['papers'])\n",
    "\n",
    "papers_df[[\"methodology_overlap\",\"problem_overlap\",\"domain_overlap\"]]\n",
    "\n",
    "\n",
    "## NOTE\n",
    "## Wrong overlap calculation\n",
    "\n",
    "### Development Note\n",
    "## might be interesting to make the output of this papers analysis passed into \n",
    "## a GAN-like architecture (So not using overlapping score like the current one)\n",
    "## where one agent is argumenting why the idea is novel\n",
    "## and another agent is criticising why the idea is not novel, both based on the prior work\n",
    "## and then another agent act as a judge to decide which argument is stronger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c48acd",
   "metadata": {},
   "source": [
    "## Evaluation Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8839bf",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "-> since everything here will be based on the retrieved papers, we need to make sure that the papers are retrieved correctly.\n",
    "\n",
    "\n",
    "-> and also we need to limit the number of papers not to be too many that would make the system too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ead707",
   "metadata": {},
   "source": [
    "### Adversarial GAN-like Evaluation of Research Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ae5a3",
   "metadata": {},
   "source": [
    "#### Advocate Agent\n",
    "The goal of the advocate agent is to defend the research idea, grounded based on the retrieved papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d351c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "# Define the state for the adversarial debate\n",
    "\n",
    "class DebateState(TypedDict):\n",
    "    \"\"\"State for the adversarial debate graph\"\"\"\n",
    "    research_idea: str\n",
    "    retrieved_papers: str\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    debate_concluded: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialAgent(BaseModel):\n",
    "    \"\"\"\n",
    "    Base class for Adversarial Agents\n",
    "    All agents must provide evidence-based arguments citing retrieved papers.\n",
    "    \"\"\"\n",
    "    argument: str = Field(\n",
    "        description=\"The main argument presented by the agent\"\n",
    "    )\n",
    "    \n",
    "    supporting_papers: List[str] = Field(\n",
    "        description=\"List of paper IDs cited to support this argument\"\n",
    "    )\n",
    "    \n",
    "    key_points: List[str] = Field(\n",
    "        description=\"Main points made in this argument\"\n",
    "    )\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1be492cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "advocate_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are the ADVOCATE for the proposed research idea.\n",
    "Your goal is to defend the idea, highlight its novelty and feasibility.\n",
    "Use the provided retrieved papers to support your arguments.\n",
    "\n",
    "Focus on:\n",
    "1. Unique contributions\n",
    "2. How it improves upon existing methods (cite paper IDs)\n",
    "3. Why the potential impact outweighs risks\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Provide a strong, evidence-based argument citing specific papers.\n",
    "\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Automatically includes message history\n",
    "])\n",
    "\n",
    "# Create advocate agent that returns structured output\n",
    "class AdvocateResponse(AdversarialAgent):\n",
    "    \"\"\"Advocate's argument response\"\"\"\n",
    "    pass\n",
    "\n",
    "advocate_agent = advocate_prompt | llm.with_structured_output(AdvocateResponse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1797683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_advocate_agent(state: DebateState):\n",
    "    research_idea = state['research_idea']\n",
    "    retrieved_papers = state['retrieved_papers']\n",
    "    messages = state['messages']\n",
    "    \n",
    "    response = advocate_agent.invoke({\n",
    "        \"research_idea\": research_idea,\n",
    "        \"retrieved_papers\": retrieved_papers,\n",
    "        \"history\": messages  # Pass the messages list directly\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.argument)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad10e3",
   "metadata": {},
   "source": [
    "#### Critics Agent\n",
    "The goal of the critics agent is to challenge the idea and the advocate's argument by finding the gap or weakness in the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "skeptic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are the SKEPTIC of the proposed research idea.\n",
    "Your goal is to critique the idea, point out flaws, and question its novelty.\n",
    "Use the provided retrieved papers to show similarity to prior work or identify weaknesses.\n",
    "\n",
    "Focus on:\n",
    "1. Overlaps with existing work (cite specific papers).\n",
    "2. Potential technical challenges or flaws.\n",
    "3. Why the idea might not be as novel or impactful as claimed.\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Provide a critical, evidence-based counter-argument.\n",
    "\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Automatically includes message history\n",
    "])\n",
    "\n",
    "# Create skeptic agent that returns structured output\n",
    "class SkepticResponse(AdversarialAgent):\n",
    "    \"\"\"Skeptic's argument response\"\"\"\n",
    "    pass\n",
    "\n",
    "skeptic_agent = skeptic_prompt | llm.with_structured_output(SkepticResponse)\n",
    "\n",
    "def call_skeptic_agent(state: DebateState):\n",
    "    research_idea = state['research_idea']\n",
    "    retrieved_papers = state['retrieved_papers']\n",
    "    messages = state['messages']\n",
    "    \n",
    "    response = skeptic_agent.invoke({\n",
    "        \"research_idea\": research_idea,\n",
    "        \"retrieved_papers\": retrieved_papers,\n",
    "        \"history\": messages  # Pass the messages history directly\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.argument)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f307bae",
   "metadata": {},
   "source": [
    "#### Judge Agent\n",
    "The goal of the judge agent is to be a neutral and objective evaluator between the advocate and critic agents, and to find the final verdict of the research idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beea497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "moderator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are an EXPERT guiding a debate between an Advocate and a Skeptic about a research idea.\n",
    "Your goal is to synthesize the arguments, ask probing questions to clarify the idea, and ensure the discussion remains grounded in the literature.\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Current Iteration: {iteration} / {max_iterations}\n",
    "\n",
    "Task:\n",
    "1. Summarize the key points made by both sides so far.\n",
    "2. If the maximum iterations have been reached or if the discussion has converged, provide a FINAL VERDICT on the idea's novelty and feasibility. Start your response with \"VERDICT:\".\n",
    "3. If the discussion should continue, ask a specific, probing question to guide the next round of debate.\n",
    "\n",
    "OUTPUT (valid JSON only):\n",
    "\n",
    "\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Automatically includes message history\n",
    "])\n",
    "\n",
    "# Create moderator response that returns structured output\n",
    "class ModeratorResponse(AdversarialAgent):\n",
    "    \"\"\"Moderator's response\"\"\"\n",
    "    verdict: str = Field(\n",
    "        description=\"Final verdict if debate concluded (starts with 'VERDICT:')\",\n",
    "        default=None\n",
    "    )\n",
    "    next_question: Optional[str] = Field(\n",
    "        description=\"Question for next round if continuing\",\n",
    "        default=None\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "\n",
    "moderator_agent = moderator_prompt | llm.with_structured_output(ModeratorResponse)\n",
    "\n",
    "\n",
    "class VerdictResponse(BaseModel):\n",
    "    \"\"\"Final verdict structure\"\"\"\n",
    "    novelty_score: int = Field(description=\"Score 1-10 for novelty\", ge=1, le=10)\n",
    "    feasibility_score: int = Field(description=\"Score 1-10 for feasibility\", ge=1, le=10)\n",
    "    summary: str = Field(description=\"Summary of the debate\")\n",
    "    strengths: List[str] = Field(description=\"Key strengths identified\")\n",
    "    weaknesses: List[str] = Field(description=\"Key weaknesses identified\")\n",
    "    recommendation: str = Field(description=\"Accept/Revise/Reject with justification\")\n",
    "    \n",
    "    def to_verdict_text(self) -> str:\n",
    "        return f\"\"\"VERDICT: Research Idea Evaluation\n",
    "\n",
    "SCORES:\n",
    "- Novelty: {self.novelty_score}/10\n",
    "- Feasibility: {self.feasibility_score}/10\n",
    "\n",
    "SUMMARY:\n",
    "{self.summary}\n",
    "\n",
    "STRENGTHS:\n",
    "{chr(10).join(f'- {s}' for s in self.strengths)}\n",
    "\n",
    "WEAKNESSES:\n",
    "{chr(10).join(f'- {w}' for w in self.weaknesses)}\n",
    "\n",
    "RECOMMENDATION: {self.recommendation}\n",
    "\"\"\"\n",
    "\n",
    "# Update moderator to use VerdictResponse on final iteration\n",
    "def call_moderator_agent(state: DebateState):\n",
    "    research_idea = state['research_idea']\n",
    "    retrieved_papers = state['retrieved_papers']\n",
    "    messages = state['messages']\n",
    "    iteration = state.get('iteration', 0)\n",
    "    max_iterations = state.get('max_iterations', 3)\n",
    "    \n",
    "    \n",
    "    is_final_iteration = (iteration +1 >= max_iterations)\n",
    "    \n",
    "    if is_final_iteration:\n",
    "        # Create a verdict-specific prompt\n",
    "        verdict_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are the EXPERT providing the FINAL VERDICT for a research idea debate.\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Debate History:\n",
    "The Advocate and Skeptic have exchanged arguments over {iteration} rounds.\n",
    "\n",
    "YOUR TASK:\n",
    "Provide a comprehensive final verdict with structured scores.\n",
    "\n",
    "You MUST return a structured response with:\n",
    "- novelty_score: integer 1-10\n",
    "- feasibility_score: integer 1-10  \n",
    "- summary: string summarizing the debate\n",
    "- strengths: list of strings\n",
    "- weaknesses: list of strings\n",
    "- recommendation: \"Accept\" or \"Revise\" or \"Reject\" with justification\n",
    "\n",
    "Be decisive and evidence-based. \n",
    "START your response with \"VERDICT:\".\n",
    "\"\"\"\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "        ])\n",
    "        \n",
    "        # Use verdict-specific prompt and schema\n",
    "        verdict_agent = verdict_prompt | llm.with_structured_output(VerdictResponse)\n",
    "        response = verdict_agent.invoke({\n",
    "            \"research_idea\": research_idea,\n",
    "            \"retrieved_papers\": retrieved_papers,\n",
    "            \"iteration\": iteration,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"history\": messages\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=response.to_verdict_text())],\n",
    "            \"iteration\": iteration + 1\n",
    "        }\n",
    "    else:\n",
    "        # Regular moderator response\n",
    "        response = moderator_agent.invoke({\n",
    "            \"research_idea\": research_idea,\n",
    "            \"retrieved_papers\": retrieved_papers,\n",
    "            \"iteration\": iteration,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"history\": messages\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=response.argument)],\n",
    "            \"iteration\": iteration + 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a6653",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "\n",
    "\n",
    "# Define routing logic\n",
    "def should_continue(state: DebateState) -> str:\n",
    "    \"\"\"Determine if debate should continue or end\"\"\"\n",
    "    # Check if we've reached max iterations\n",
    "    if state['iteration'] >= state['max_iterations']:\n",
    "        return \"end\"\n",
    "    \n",
    "    # Check if moderator issued a verdict\n",
    "    last_message = state['messages'][-1].content\n",
    "    if isinstance(last_message, str) and last_message.startswith(\"VERDICT:\"):\n",
    "        return \"end\"\n",
    "    \n",
    "    # Continue the debate\n",
    "    return \"continue\"\n",
    "\n",
    "def route_after_moderator(state: DebateState) -> str:\n",
    "    \"\"\"Route after moderator's turn\"\"\"\n",
    "    result = should_continue(state)\n",
    "    if result == \"end\":\n",
    "        return END\n",
    "    return \"advocate\"\n",
    "\n",
    "# Build the debate workflow\n",
    "debate_workflow = StateGraph(DebateState)\n",
    "\n",
    "# Add nodes\n",
    "debate_workflow.add_node(\"advocate\", call_advocate_agent)\n",
    "debate_workflow.add_node(\"skeptic\", call_skeptic_agent)\n",
    "debate_workflow.add_node(\"moderator\", call_moderator_agent)\n",
    "\n",
    "# Add edges\n",
    "debate_workflow.add_edge(START, \"advocate\")\n",
    "debate_workflow.add_edge(\"advocate\", \"skeptic\")\n",
    "debate_workflow.add_edge(\"skeptic\", \"moderator\")\n",
    "\n",
    "# Add conditional edge from moderator\n",
    "debate_workflow.add_conditional_edges(\n",
    "    \"moderator\",\n",
    "    route_after_moderator,\n",
    "    {\n",
    "        \"advocate\": \"advocate\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "debate_graph = debate_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4cb2f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Visualize the debate graph\n",
    "display(Image(debate_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9255129e",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_backends/sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    208\u001b[39m     sock = socket.create_connection(\n\u001b[32m    209\u001b[39m         address,\n\u001b[32m    210\u001b[39m         timeout,\n\u001b[32m    211\u001b[39m         source_address=source_address,\n\u001b[32m    212\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Run the adversarial debate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m debate_result = \u001b[43mdebate_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearch_idea\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresearch_idea_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieved_papers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_papers_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miteration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_iterations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdebate_concluded\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Display the debate messages\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m debate_result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/main.py:3050\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3048\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3050\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/main.py:2633\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2632\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2643\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcall_advocate_agent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      3\u001b[39m retrieved_papers = state[\u001b[33m'\u001b[39m\u001b[33mretrieved_papers\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m messages = state[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43madvocate_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearch_idea\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresearch_idea\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieved_papers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_papers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhistory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the messages list directly\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [AIMessage(content=response.argument)]\n\u001b[32m     14\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/runnables/base.py:3129\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3127\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3129\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/runnables/base.py:5534\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5527\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5528\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5529\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5532\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5533\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5535\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5536\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5537\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_openai/chat_models/base.py:1356\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1355\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1359\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1360\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1361\u001b[39m ):\n\u001b[32m   1362\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_openai/chat_models/base.py:1324\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1321\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     raw_response = (\n\u001b[32m-> \u001b[39m\u001b[32m1324\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1327\u001b[39m     )\n\u001b[32m   1328\u001b[39m     response = raw_response.parse()\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/resources/chat/completions/completions.py:184\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    179\u001b[39m         response_format=response_format,\n\u001b[32m    180\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    181\u001b[39m         input_tools=chat_completion_tools,\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_base_client.py:1014\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1011\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1013\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\n\u001b[32m   1017\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1018\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     response.headers,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error.",
      "During task with name 'advocate' and id '08f38a9c-5c80-ce6b-5861-ad1e0f43b3d2'"
     ]
    }
   ],
   "source": [
    "# Extract research idea from initial user input\n",
    "research_idea_text = result_llm[\"messages\"][0].content\n",
    "\n",
    "# Extract and format retrieved papers\n",
    "papers_json = json.loads(result_llm[\"messages\"][-2].content)  # -2 because -1 is the analysis\n",
    "retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n",
    "\n",
    "# Run the adversarial debate\n",
    "debate_result = debate_graph.invoke({\n",
    "    \"research_idea\": research_idea_text,\n",
    "    \"retrieved_papers\": retrieved_papers_text,\n",
    "    \"messages\": [],\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": 4,\n",
    "    \"debate_concluded\": False\n",
    "})\n",
    "\n",
    "# Display the debate messages\n",
    "for msg in debate_result[\"messages\"]:\n",
    "    print(f\"\\n{'-'*80}\\n{msg.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561162f",
   "metadata": {},
   "source": [
    "##### Example Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a8720",
   "metadata": {},
   "source": [
    "gemini\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) operationalizes a three-phase, closed-loop prompting paradigm to directly address coherence and adaptability in long-form generation and multi-turn dialogue. Unlike fixed prompts or static few-shot prompts, DPA explicitly analyzes the evolving interaction history to extract themes, tone shifts, and narrative goals (Contextual Analysis), generates updated prompts that steer subsequent outputs toward new elements or clarified past responses (Adaptive Prompt Generation), and continually synthesizes the aggregate of prior exchanges to preserve global coherence (Iterative Context Update). This creates a lightweight, memory-efficient alternative to full fine-tuning while delivering sustained prompt-driven alignment across long sessions, multi-turn narratives, and dynamic topics. The approach is novel in its explicit three-phase cycle tailored for ongoing interactions, rather than single-shot adaptation, and it targets the core bottleneck of LLM coherence over time rather than surface-level accuracy alone. In addition, the framework is designed to be modular and cross-domain: it can pair with parameter-efficient adapters (e.g., DynaLoRA-style dynamics) to strike a favorable accuracy/compute trade-off as shown by dynamic-adapter+prompt-tuning work (Dynamic Adapter Meets Prompt Tuning) and Time-LlaMA-style dynamic adaptation, ensuring feasibility for real-world deployment with modest compute overhead. The plan also outlines concrete, ecologically valid experiments (storytelling and dynamic dialogue) using established coherence and engagement metrics, leveraging CTTA-inspired prompts to handle domain shifts and evolving contexts. This positions DPA as a practical, scalable solution with immediate impact for education, entertainment, and humanAI collaboration in creative writing and extended conversations.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) proposes a three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to maintain coherence and adaptability in extended LLM interactions. While appealing, the idea risks being incremental rather than novel: it essentially concatenates existing dynamic-prompt and prompt-tuning ideas (analysis-driven prompt updates, per-step prompt adaptation, and history-aware synthesis) that are already explored in the literature. Moreover, there are significant practical and evaluative risks (unstable prompts, drift over many turns, poor correlation of BLEU/ROUGE with narrative quality, data issues) that are not adequately addressed. The claimed noveltythree phases tailored for ongoing interactionsoverlaps substantially with prior dynamic-prompting and test-time adaptation work, and the experimental plan relies on metrics and datasets that have well-known limitations for evaluating long-form coherence and engagement. Without stronger theoretical grounding, rigorous baselines, and robust, human-centered evaluation, the proposal may yield modest, domain-limited impact despite substantial engineering effort.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Advocate argues that Dynamic Prompt Adaptation (DPA) offers a novel three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to sustain coherence and adaptability in long-form LLM interactions. It emphasizes explicit, history-aware prompting as a lightweight alternative to full fine-tuning, with modular compatibility alongside parameter-efficient methods (e.g., dynamic adapters, LoRA). Proponents claim novelty lies in the explicit three-phase cycle tailored for ongoing interactions and the potential for cross-domain impact in creative writing and extended conversations, supported by a concrete experiment plan and engagement metrics. Skeptic counterpoints stress that the core ideas resemble existing dynamic prompting and test-time adaptation paradigms, risk being incremental rather than groundbreaking, and rely on evaluation metrics (e.g., BLEU/ROUGE) and datasets (e.g., Reddit) that have known limitations for measuring long-range coherence and narrative quality. They caution about prompt drift, instability over many turns, lack of rigorous theoretical grounding, and potential biases in data and baselines. Overall, the debate centers on whether the proposed three-phase loop genuinely introduces a new methodological paradigm or re-packages established dynamic prompting concepts with potentially modest novelty and impact without stronger theoretical guarantees or robust human-centered evaluation.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) introduces a principled three-phase loopContextual Analysis, Adaptive Prompt Generation, and Iterative Context Updatethat actively maintains coherence and adaptability in long-form LLM interactions. This goes beyond fixed prompts, fixed few-shots, or single-turn CoT-style prompting by building a lightweight, memory-efficient mechanism that continually reasons about the evolving dialogue and fabricates targeted prompt updates to steer future generations. The novelty lies not just in prompting, but in the explicit three-phase orchestration that (i) extracts themes and tonal shifts from prior exchanges, (ii) generates updated prompts that integrate new elements or reframe past responses, and (iii) synthesizes the entire dialogue history to preserve global coherence over many turns. This closes the loop between memory-like context and prompt-driven control, enabling sustained narrative quality in story-telling, tutoring, conversational agents, and interactive media. Importantly, DPA remains highly feasible in practice by leveraging established parameter-efficient dynamics (e.g., dynamic adapters, LoRA-style modules, and memory-efficient prompt mechanisms) as shown in related work, ensuring deployment on commodity hardware without full fine-tuning.\n",
    "\n",
    "Key contributions and novelty beyond prior work:\n",
    "- Explicit three-phase loop tailored for ongoing interactions, not just one-shot adaptation or static prompting (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update). This targets long-range coherence and dynamic thematic evolution in dialogue and narrative tasks.\n",
    "- Modular, plugandplay design that can be paired with parameter-efficient adaptation techniques (e.g., Dynamic Adapters + Prompt Tuning; Time-LlaMAs dynamic LoRA selection) to balance accuracy and compute (see Dynamic Adapter Meets Prompt Tuning; Time-LlaMA) [papers cited].\n",
    "- Compatibility with multi-modal and cross-domain prompts via domain-aware prompting and meta-relabling strategies, enabling robust generalization across tasks and domains (DAPrompt and RADA-prompt style ideas) [papers cited].\n",
    "- A concrete experimental path that uses established storytelling and dialogue benchmarks (e.g., Story Cloze-like tasks and user-dialogue datasets) with multi-faceted metrics (engagement, coherence, user satisfaction, BLEU/ROUGE) to capture long-range narrative quality, moving beyond surface metrics.\n",
    "- A risk-mitigated deployment plan that embraces ensemble and adaptive prompting strategies to curb drift and overfitting, inspired by robust test-time prompting approaches.\n",
    "\n",
    "How DPA improves upon existing methods, with supporting evidence from the literature:\n",
    "- Dynamic prompt cores/CTTA-style adaptation: DPCore shows that dynamic prompt coresets, dedicated visual prompts, and a dynamic update mechanism can achieve robust continual test-time adaptation while drastically reducing trainable parameters and compute, illustrating the practicality and efficiency of dynamic prompting in changing contexts (DPCore; 246482d9758e93d0b349e2926996d887417174d8).\n",
    "- Parameter-efficient adaptation with prompts: Dynamic Adapter + Prompt Tuning (DAPT) demonstrates that freezing base models and adding dynamic adapters alongside internal prompts yields superior performance with dramatic reductions in trainable parameters and memory (6b533de65b3d2cee190415f5a2a2e6afe2ac7c78).\n",
    "- Dynamic, task-aware adaptation without full fine-tuning: Time-LlaMA introduces dynamic low-rank adaptation that selects LoRA modules per input to balance performance and inference efficiency, supporting the feasibility of input-dependent, prompt-related adaptation in large models (650a24da1702beca7eb70011a26f1f3238efad4b).\n",
    "- Prompt-based test-time adaptation and robust prompting: DAPrompt and related prompt-learning schemes show that ensembling and meta-prompting can mitigate biases and improve robustness during test-time adaptation, aligning with DPAs emphasis on robust, adaptive prompting during ongoing interactions (5db3cfc974c42bfa2d9518a8910762790; 759b5f58e58a76f79a7d845acd3169dc899d0ac2).\n",
    "- Retrieval-augmented and iterative prompting for long-horizon tasks: RAT and Iter-RetGen illustrate how iterative retrieval-generation loops can enhance reasoning and grounding in long-horizon tasks, informing DPAs design where prompts are updated based on retrieved dialogue history and prior prompts (dc256e179d4e8eff48879a40ddc414b15b0b2300; 8051818817a9a3815be6623a679d4a7f5a7b7964).\n",
    "- Encouraging coherent multi-turn generation with structured prompting: Works on narrative generation and multi-turn interactions (e.g., DialogGen, Agents Room, SCENECRAFT) demonstrate the value of structured, multi-agent or multi-stage prompting and planning to achieve coherence and narrative alignment; DPA operationalizes this mindset into a three-phase, repeatable loop suitable for dynamic contexts.\n",
    "\n",
    "Why the potential impact justifies the risks:\n",
    "- Long-form coherence is a central bottleneck in education, entertainment, and human-AI collaboration. By continuously aligning prompts with evolving context, DPA can sustain thematic consistency, maintain stylistic voice, and adapt to user feedback without costly fine-tuning (as demonstrated by dynamic prompting literature and test-time adaptation work cited above).\n",
    "- The approach is scalable and deployment-friendly. The literature shows that dynamic prompt mechanisms, domain prompts, and adaptive prompts can achieve notable gains with modest parameter overhead and without retraining core models (e.g., DAPrompt, DPCore, DAPT, Time-LlaMA) [papers cited].\n",
    "- Cross-domain applicability: The architecture supports applying adaptive prompting to a wide range of taskscreative writing, interactive gaming narratives, tutoring, and long-form content generationthrough its modular three-phase design and compatibility with dynamic adapters and memory-augmented prompts.\n",
    "- Risk management via ensemble and dynamic prompting strategies: The literature provides practical approaches to reduce overfitting and bias at test-time (e.g., ensemble prompts in robust TTA; meta-prompting in domain adaptation), which we can incorporate to mitigate drift and reliability concerns in extended interactions (ADAPROMPT; 5db3cfc9; 759b5f58).\n",
    "\n",
    "Feasibility and a concrete path forward:\n",
    "- Feasibility is supported by substantial precedent for dynamic prompting and adaptive prompting in large language models across modalities and domains, with concrete gains in efficiency and performance (see the cited papers). DPAs three-phase loop can be implemented with existing tooling: a lightweight contextual analyzer to extract themes/tones from the history, a prompt generator that produces updated prompts conditioned on the extracted signals, and a synthesis module that maintains an ongoing narrative memory to inform future updates.\n",
    "- Evaluation strategy is well-grounded: we will compare against static prompts and per-turn prompting baselines, using engagement and coherence metrics, human judgments, and standard text-generation metrics (BLEU, ROUGE), in datasets such as Story Cloze-like storytelling tasks and user-dialogue interactions on Reddit-like conversational data.\n",
    "- Additional safeguards: adopt ensemble prompting, confidence-based prompts, and selective memory buffers to curb drift, following best practices from robust test-time prompting literature (e.g., ensemble prompts in ADAPPROMPT; domain-prompt learning approaches; RAT-like iterative grounding). These measures address common failure modes in long-running prompts and support reliable deployment.\n",
    "\n",
    "In sum, DPA is a novel, feasible, and impactful framework that explicitly closes the loop between evolving dialogue context and prompt-level control, enabling robust coherence and dynamic adaptability across long interactions while leveraging and integrating proven, efficient prompting and adaptation techniques from the referenced literature.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "The Dynamic Prompt Adaptation (DPA) idea aims to continuously steer LLM outputs across long interactions by a threephase loop: Contextual Analysis, Adaptive Prompt Generation, and Iterative Context Update. While appealing, the proposal appears incremental and shows substantial overlap with a body of prior work on dynamic/adaptive prompting, continual test-time adaptation, and retrieval-grounded prompting. Several cited papers already demonstrate close ideas: dynamic prompts and memory-aware adaptation (DPCore), parameter-efficient prompt/adaptation (Dynamic Adapter Meets Prompt Tuning; Time-LlaMA), domain-aware and meta-prompting (Domain Prompt Tuning via Meta Relabeling; Decorate the Newcomers CTTA; ADAPROMPT family), and iterative grounding/retrieval loops (RAT, Iter-RetGen). Moreover, long-horizon coherence in narratives and dialogue has been explored via multi-agent/prompting pipelines (DialogGen, Agents Room, PANGeA, SCENECRAFT). Taken together, the core thrustaligning prompts to evolving context across turnshas already been explored in multiple orthogonal directions, making the three-phase framing less novel than claimed. The claim of a single, unified novel paradigm for dynamic, evolving prompts across domains is therefore questionable.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Advocate argued that Dynamic Prompt Adaptation (DPA) offers a novel three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to sustain coherence and adaptability in long-form LLM interactions, and that it can operate without full fine-tuning by leveraging dynamic prompts and modular adapters. Skeptic contended that the three-phase loop largely re-packages established dynamic prompting and test-time adaptation concepts, with substantial overlap to work on DPCore, Dynamic Adapter + Prompt Tuning, Time-LlaMA, and domain/prompts-based adaptation, raising concerns about novelty and impact. The subsequent moderator assessment reinforced the view that the core idea is incremental, citing multiple orthogonal lines of prior work (dynamic prompts, CTTA, retrieval-grounded prompting, and narrative/dialogue pipelines) and arguing that without stronger theoretical grounding or robust human-centric evaluation, the proposal risks limited novelty and practical payoff.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) offers a principled, three-phase loop that explicitly ties evolving dialogue context to prompt-level control, enabling sustained coherence and adaptability in long-form interactions without full model fine-tuning. The three phasesContextual Analysis (extracting themes, tonal shifts, and narrative goals from history), Adaptive Prompt Generation (producing updated prompts that extend past ideas or steer toward new elements), and Iterative Context Update (synthesizing prior exchanges to preserve global coherence)form a cohesive memory-prompting cycle that mirrors human creative and conversational workflows. This design yields several unique advantages:\n",
    "- Memory-efficient long-horizon coherence: by re-synthesizing context into targeted prompts rather than retraining or storing large state, DPA achieves durable coherence with modest compute overhead, which aligns with successful demonstrations of dynamic prompting and prompt-efficient adapters (e.g., Dynamic Adapter Meets Prompt Tuning; Time-LlaMA).\n",
    "- Modular, domain-agnostic applicability: the three-phase loop can be paired with parameter-efficient primitives (dynamic adapters, LoRA variants, internal prompts) to adapt to storytelling, tutoring, and interactive media without expensive finetuning (supported by DAPT and Time-LlaMA literature).\n",
    "- Robustness under domain shifts and evolving user feedback: the framework inherently accommodates shifting audience and themes, leveraging ideas from test-time adaptation and meta-prompting to maintain reliability across sessions (RAT-inspired grounding, ensemble prompting strategies in robust TTA).\n",
    "- Cross-modal and multi-turn potential: by integrating with domain prompts and meta-relabeling we can extend DPA to visual storytelling, interactive dialogue, and multimodal narrative generation, as supported by MaPLe and related multi-modal prompt work.\n",
    "- Concrete, testable impact on reader engagement and coherence: the proposed evaluation plan mirrors established storytelling and dialogue tasks and benefits from prior work showing that adaptive prompting yields improvements in coherence, engagement, and adaptability (RAT, Iter-RetGen, SCENECRAFT, DialogGen).\n",
    "Overall, DPA brings a coherent, scalable, and practically deployable paradigm that leverages the best of prompt-based adaptation while addressing the key pain point of long-context coherence in LLM-powered creative and conversational tasks.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "The Dynamic Prompt Adaptation (DPA) idea promises to maintain coherence across long interactions by a three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) and to do so without full fine-tuning. However, the contribution is largely incremental and overlaps with a broad set of prior dynamic/adaptive prompting and test-time adaptation works. There is insufficient theoretical grounding, and the evaluation plan relies on metrics and data that have well-known limitations for judging long-horizon coherence and engagement. The claimed noveltyan explicit three-phase cycle tailored for ongoing interactionsappears to be a re-packaging of existing concepts rather than a fundamentally new paradigm. Without stronger baselines, rigorous human evaluations, or a formal treatment of stability/guarantees over many turns, the work risks limited impact beyond narrow demonstrations in storytelling or dialogue tasks.\n",
    "\n",
    "Key overlaps include dynamic prompts and domain-adaptive prompting, dynamic adapters, and retrieval-grounded prompting from the literature (see DPCore; Dynamic Adapter Meets Prompt Tuning; Time-LlaMA; DAPrompt/ADT family; RAT/Iter-RetGen). The three-phase loop shares motifs with iterative grounding and prompt refinement seen in Retrieval-Augmented Thoughts (RAT), Iterative Retrieval-Generation (Iter-RetGen), and CTTA-style adaptation. In short, the core ideaprompts that adapt to evolving context across turnshas already been explored in multiple orthogonal directions; the proposed three-phase framing does not clearly establish a unique, cohesive methodological advance on top of these prior works.\n",
    "\n",
    "Supporting_papers IDs: [\"246482d9758e93d0b349e2926996d887417174d8\", \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\", \"650a24da1702beca7eb70011a26f1f3238efad4b\", \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\", \"5db3cfc974c42bfa2d9518a8910762790\", \"dc256e179d4e8eff48879a40ddc414b15b0b2300\", \"dc256e179d4e8eff48879a40ddc414b15b0b2300\", \"8051818817a9a3815be6623a679d4a7f5a7b7964\"]}\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Advocate argues Dynamic Prompt Adaptation (DPA) offers a novel three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to maintain coherence and adaptability across long interactions without full fine-tuning, with modular compatibility to existing parameter-efficient methods. Skeptic counters that the three-phase loop largely recycles established dynamic prompting and test-time adaptation ideas and overlaps with DPCore, dynamic adapters + prompt tuning, Time-LlaMA, RAT/Iter-RetGen, and Dialog/NPC-driven narrative pipelines, raising concerns about novelty and practical impact. The moderator notes substantial overlap across multiple orthogonal lines of prior work and emphasizes the need for stronger theoretical grounding and robust, human-centered evaluation to establish genuine novelty and generalizability beyond narrow storytelling tasks. The current stance favors caution: feasible to implement, but novelty and significance require stronger demonstration and clearer baselines.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) offers a principled threephase loopContextual Analysis, Adaptive Prompt Generation, and Iterative Context Updatethat continually aligns prompts with an evolving dialogue to sustain coherence and engagement without full model finetuning. The novelty lies in explicitly decoupling history understanding (Contextual Analysis) from prompt reconfiguration (Adaptive Prompt Generation) and global history synthesis (Iterative Context Update) into a repeatable, memoryefficient cycle that can operate across domains (storytelling, tutoring, conversational agents, interactive media). Crucially, DPA is designed to plug into existing parameterefficient pipelines (e.g., dynamic adapters and prompt tuning) to balance accuracy and compute, making it deployable on commodity hardware. The feasibility and impact are underpinned by a rich literature showing that dynamic prompts, adaptive prompts, and retrievalaugmented reasoning can yield gains with modest training or finetuning costs, and scale to longhorizon tasks:\n",
    "\n",
    "- Dynamic Prompt Coreset and continual testtime adaptation demonstrate robust performance across changing domains with dramatically reduced trainable parameters and compute (DPCore) (246482d9758e93d0b349e2926996d887417174d8).\n",
    "- Parameterefficient transfer with prompts and dynamic adapters shows substantial gains while freezing the backbone (Dynamic Adapter Meets Prompt Tuning) (6b533de65b3d2cee190415f5a2a2e6afe2ac7c78).\n",
    "- Dynamic lowrank adaptation selects task/instancespecific modules per input, enabling efficient, scalable adaptation without full finetuning (TimeLLaMA) (650a24da1702beca7eb70011a26f1f3238efad4b).\n",
    "- Domainaware and meta prompting approaches illustrate how prompts can embed domain semantics and adapt to shifts without retraining (Domain Prompt Tuning via Meta Relabeling; Decorate the Newcomers for CTTA) (eacb61136023a2f30c5a0313f222d50e5f63ac9b).\n",
    "- Robust testtime prompting and ensemble strategies mitigate biases and improve adaptation under distribution shift (ADAPROMPT family) (5db3cfc974c42bfa2d9518a8910762790b516037).\n",
    "- Retrievalaugmented thought and iterative retrievalgeneration loops provide grounding and improved reasoning over long horizons, aligning prompts with retrieved evidence (RAT; IterRetGen) (dc256e179d4e8eff48879a40ddc414b15b0b2300; 8051818817a9a3815be6623a679d4a7f5a7b7964).\n",
    "- Longform narrative and multiturn dialogue literature shows tangible gains from structured, multistage prompting and planning (DialogGen; SCENECRAFT; Agents Room; PANGeA).\n",
    "\n",
    "Together, these findings support the core premise of DPA: a lightweight, threephase, historyaware prompting protocol can markedly improve coherence and adaptability across long interactions while avoiding heavy finetuning, thereby delivering high impact with feasible resource requirements.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) proposes a three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to maintain coherence and adaptability across long LLM interactions without full fine-tuning. While appealing, the idea risks being incremental rather than revolutionary. The literature already shows extensive precedent for dynamic, memory-aware prompting and test-time adaptation, including explicit three-phase or iterative workflows, prompting strategies, and retrieval-grounded reasoning. Without stronger theoretical grounding, rigorous baselines, or robust human-centered evaluation, DPAs claimed novelty and practical impact remain uncertain and potentially domain-limited to demonstrations rather than generalizable gains across tasks and modalities.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "VERDICT: Research Idea Evaluation\n",
    "\n",
    "SCORES:\n",
    "- Novelty: 4/10\n",
    "- Feasibility: 7/10\n",
    "\n",
    "SUMMARY:\n",
    "The debate centers on the novelty and feasibility of Dynamic Prompt Adaptation (DPA), a proposed three-phase method (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to improve coherence in long-form LLM interactions. The Advocate positions DPA as a novel, lightweight, and modular framework that explicitly orchestrates prompt updates based on the evolving dialogue history, citing supporting evidence from literature on dynamic prompting and parameter-efficient adaptation. The Skeptic argues that DPA is incremental, repackaging existing concepts from continual test-time adaptation, retrieval-augmented generation, and other dynamic prompting techniques without offering a significant methodological leap. The Skeptic also raises concerns about the reliance on potentially weak evaluation metrics (BLEU/ROUGE) and the practical challenges of prompt drift and stability over many turns. The final verdict leans towards the Skeptic's view, scoring novelty low while acknowledging feasibility, and recommending a revision to better differentiate the work from the extensive prior art and to incorporate more robust, human-centered evaluation methods.\n",
    "\n",
    "STRENGTHS:\n",
    "- The proposed method directly addresses the critical challenge of maintaining long-term coherence in LLMs.\n",
    "- The iterative, three-phase loop is a structured and intuitive approach to dynamically adapting prompts based on conversational context.\n",
    "- The experiment plan is well-defined, with clear baselines and metrics for evaluation.\n",
    "- The idea of integrating with parameter-efficient methods like LoRA and dynamic adapters makes it computationally feasible.\n",
    "\n",
    "WEAKNESSES:\n",
    "- The novelty of the proposed method is questionable, as it appears to be a recombination of existing techniques in dynamic prompting and test-time adaptation.\n",
    "- The reliance on automated metrics like BLEU and ROUGE is a significant weakness, as these are known to correlate poorly with human judgments of narrative coherence and quality.\n",
    "- The proposal does not adequately address the risk of 'prompt drift' or error accumulation over long interactions, where the model could get stuck in repetitive loops or diverge thematically.\n",
    "- The datasets mentioned (Story Cloze, Reddit) may not be sufficient to robustly evaluate long-form narrative coherence and engagement.\n",
    "\n",
    "RECOMMENDATION: Revise. The proposal is feasible but the novelty is questionable given the extensive prior work on dynamic and adaptive prompting. The authors should clearly articulate what distinguishes their three-phase loop from existing iterative/adaptive prompting frameworks (e.g., retrieval-augmented generation, continual test-time adaptation). The experiment plan needs more robust, human-centered evaluation metrics beyond BLEU/ROUGE to convincingly demonstrate improved coherence and engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80423d",
   "metadata": {},
   "source": [
    "### AGENTIC AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07491a",
   "metadata": {},
   "source": [
    "ReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5356c7",
   "metadata": {},
   "source": [
    "#### SINGLE AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f6370bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ReAct Agent Evaluation with Pre-Retrieved Papers...\n",
      "Number of retrieved papers: 222\n",
      "{'proposal': \"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", 'retrieved_papers': 'Paper ID: 246482d9758e93d0b349e2926996d887417174d8\\n                                    Title: DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\\n                                    Abstract: Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\\n                                \\n\\n---\\n\\nPaper ID: cf95279b1da9de1aad9e7c651f5048f69af295ed\\n                                    Title: AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents\\n                                    Abstract: AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.\\n                                \\n\\n---\\n\\nPaper ID: 3967189742efab8859da542ce3953d4c72957aca\\n                                    Title: EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\\n                                    Abstract: Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\\n                                \\n\\n---\\n\\nPaper ID: 659e0b3303caa860348dee52f41476e3fddc9573\\n                                    Title: LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference\\n                                    Abstract: The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.\\n                                \\n\\n---\\n\\nPaper ID: 00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\\n                                    Title: From ideas to ventures: building entrepreneurship knowledge with LLM, prompt engineering, and conversational agents\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\\n                                    Title: Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\\n                                    Abstract: Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\\n                                \\n\\n---\\n\\nPaper ID: b67a6181fad5c5838945583ccdc7f39187e29332\\n                                    Title: Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks\\n                                    Abstract: Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs\\' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker\\'s LLM to disrupt their own operations (passive defense) or even compromise the attacker\\'s machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker\\'s LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project_mantis\\n                                \\n\\n---\\n\\nPaper ID: 2f274db9aa447a13c019114e327057d4b161b6d5\\n                                    Title: LLM-controller: Dynamic robot control adaptation using large language models\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: eacb61136023a2f30c5a0313f222d50e5f63ac9b\\n                                    Title: Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\\n                                    Abstract: Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\\n                                \\n\\n---\\n\\nPaper ID: 47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\\n                                    Title: HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models\\n                                    Abstract: Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.\\n                                \\n\\n---\\n\\nPaper ID: bcac614f9774488447221ebb4f16f05e3975ec1e\\n                                    Title: Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\\n                                    Abstract: Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model\\'s potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.\\n                                \\n\\n---\\n\\nPaper ID: 3bf29791bd995d17ef0b1e1b876260fa93f2a51b\\n                                    Title: Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 58700f3740105e3422eb030305372b6d8bc44986\\n                                    Title: Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference\\n                                    Abstract: The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\\\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.\\n                                \\n\\n---\\n\\nPaper ID: 427c3a502d132b6e1cea2d5565460d284db6e3f7\\n                                    Title: Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach\\n                                    Abstract: This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model\\'s original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method\\'s applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.\\n                                \\n\\n---\\n\\nPaper ID: c0e24f98323c7114b9229ac17b8b63581e3e5914\\n                                    Title: Dynamic and Adaptive Feature Generation with LLM\\n                                    Abstract: The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\\n                                \\n\\n---\\n\\nPaper ID: 4607a529dfb8b64a5767e53fd482bfccd23cfc20\\n                                    Title: Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization\\n                                    Abstract: Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.\\n                                \\n\\n---\\n\\nPaper ID: 165503c48e553a5559190ce74cda823f4e166b54\\n                                    Title: Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\\n                                    Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\\n                                \\n\\n---\\n\\nPaper ID: e3e3aa5576de899b755100db211501bb405aba3e\\n                                    Title: Firewalls to Secure Dynamic LLM Agentic Networks\\n                                    Abstract: LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.\\n                                \\n\\n---\\n\\nPaper ID: ee552989a03693a441863af4c29dc594bfcd1ab5\\n                                    Title: AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration\\n                                    Abstract: Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout , which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.\\n                                \\n\\n---\\n\\nPaper ID: 5db3cfc974c42bfa2d9518a8910762790b516037\\n                                    Title: Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\\n                                    Abstract: CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\\n                                \\n\\n---\\n\\nPaper ID: 273b2c64d675edd522cd6f679891756ad5207296\\n                                    Title: Exploring the Effectiveness of LLM Domain Adaptation for Business IT Machine Translation\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\\n                                    Title: ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities\\n                                    Abstract: Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at https://github.com/apple/ToolSandbox\\n                                \\n\\n---\\n\\nPaper ID: 44b0d2e884efa5344e50424dbe2edf616981f201\\n                                    Title: UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers\\n                                    Abstract: Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.\\n                                \\n\\n---\\n\\nPaper ID: 9803d83bbb28d02fb01f00e0e05aa3c192a87255\\n                                    Title: MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\\n                                    Abstract: The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\\n                                \\n\\n---\\n\\nPaper ID: 34eedbb011e45d80045cadebaf1d01b2ddec22a1\\n                                    Title: GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting\\n                                    Abstract: Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.\\n                                \\n\\n---\\n\\nPaper ID: c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\\n                                    Title: Do LLMs Understand Visual Anomalies? Uncovering LLM\\'s Capabilities in Zero-shot Anomaly Detection\\n                                    Abstract: Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA\\'s effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.\\n                                \\n\\n---\\n\\nPaper ID: 8feb33300c04fffa050e0dca59c3fdcafc920a3b\\n                                    Title: FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\\n                                    Abstract: Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\\n                                \\n\\n---\\n\\nPaper ID: 162f33c7799683ca9b0f193275fe7eec5a0b973f\\n                                    Title: EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM\\n                                    Abstract: Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although the tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent elements within multiple images through the training process, it necessitates specific finetuning for each distinct image group. This paper introduces EasyRef, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM\\'s representations into the diffusion process through adapters can easily generalize to unseen domains, mining the consistent visual elements within unseen data. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\\n                                \\n\\n---\\n\\nPaper ID: ca4f0d2c85cfe46b97ec42b38decda107780769d\\n                                    Title: QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\\n                                    Abstract: Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\\n                                \\n\\n---\\n\\nPaper ID: 9a73effed8775962c86587feb0f9ef841fa2ff4c\\n                                    Title: LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents\\n                                    Abstract: Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a propreitary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the LLM proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.\\n                                \\n\\n---\\n\\nPaper ID: 108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\\n                                    Title: TimeRAG: Boosting LLM Time Series Forecasting via Retrieval-Augmented Generation\\n                                    Abstract: Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.\\n                                \\n\\n---\\n\\nPaper ID: 1e6325865e809670765bea9dadd3c40b2014eb6d\\n                                    Title: VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation\\n                                    Abstract: This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.\\n                                \\n\\n---\\n\\nPaper ID: 7f96bb27a8fca35b1f7d02ee319a64be04114809\\n                                    Title: LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments\\n                                    Abstract: Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system\\'s adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.\\n                                \\n\\n---\\n\\nPaper ID: 650a24da1702beca7eb70011a26f1f3238efad4b\\n                                    Title: Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\\n                                    Abstract: Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the models predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\\n                                \\n\\n---\\n\\nPaper ID: 4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\\n                                    Title: Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\\n                                    Abstract: Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\\n                                \\n\\n---\\n\\nPaper ID: 759b5f58e58a76f79a7d845acd3169dc899d0ac2\\n                                    Title: Domain Adaptation via Prompt Learning\\n                                    Abstract: Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\\n                                \\n\\n---\\n\\nPaper ID: b7f46c9f01d9f649d18b709e2e88b3b97bebd016\\n                                    Title: RepairAgent: An Autonomous, LLM-Based Agent for Program Repair\\n                                    Abstract: Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent\\'s effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI\\'s GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\\n                                \\n\\n---\\n\\nPaper ID: 3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\\n                                    Title: Token-Budget-Aware LLM Reasoning\\n                                    Abstract: Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE\\n                                \\n\\n---\\n\\nPaper ID: 20843eaa59db5e2af416d7db47d51d0aab3de230\\n                                    Title: Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue Resolution\\n                                    Abstract: Large language models (LLMs) have become essential in software development, especially for issue resolution. However, despite their widespread use, significant challenges persist in the quality of LLM responses to issue resolution queries. LLM interactions often yield incorrect, incomplete, or ambiguous information, largely due to knowledge gaps in prompt design, which can lead to unproductive exchanges and reduced developer productivity.In this paper, we analyze 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution. We identify four main knowledge gaps in developer prompts: Missing Context, Missing Specifications, Multiple Context, and Unclear Instructions. Assuming that conversations within closed issues contributed to successful resolutions while those in open issues did not, we find that ineffective conversations contain knowledge gaps in $44.6 \\\\%$ of prompts, compared to only $12.6 \\\\%$ in effective ones. Additionally, we observe seven distinct conversational styles, with Directive Prompting, Chain of Thought, and Responsive Feedback being the most prevalent. We find that knowledge gaps are present in all styles of conversations, with Missing Context being the most repeated challenge developers face in issue-resolution conversations.Based on our analysis, we identify key textual and code-related heuristicsSpecificity, Contextual Richness, and Claritythat are associated with successful issue closure and help assess prompt quality. These heuristics lay the foundation for an automated tool that can dynamically flag unclear prompts and suggest structured improvements. To test feasibility, we developed a lightweight browser extension prototype for detecting prompt gaps, that can be easily adapted to other tools within developer workflows.\\n                                \\n\\n---\\n\\nPaper ID: 25cee84e3a1541697a7c97443d7526574127c344\\n                                    Title: Don\\'t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\\n                                    Abstract: Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.\\n                                \\n\\n---\\n\\nPaper ID: d5342fce96175f83550cfae471a0a46d16401481\\n                                    Title: ST-LLM: Large Language Models Are Effective Temporal Learners\\n                                    Abstract: Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at https://github.com/TencentARC/ST-LLM.\\n                                \\n\\n---\\n\\nPaper ID: 12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\\n                                    Title: Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing\\n                                    Abstract: Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\\n                                \\n\\n---\\n\\nPaper ID: edfff0e15449f438a13a7341290c008bf6486afc\\n                                    Title: MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning\\n                                    Abstract: Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\\n                                \\n\\n---\\n\\nPaper ID: 83ac79bb8e8695fb3c3c024be74790d862adea74\\n                                    Title: TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\\n                                    Abstract: The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\\n                                \\n\\n---\\n\\nPaper ID: ab4800a924508f49d644ced8ba236ec92f54f566\\n                                    Title: Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains\\n                                    Abstract: Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head\\'s non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance.\\n                                \\n\\n---\\n\\nPaper ID: 268e28f8d5235031dcd7bfae0f857439e27e8564\\n                                    Title: SteP: Stacked LLM Policies for Web Actions\\n                                    Abstract: Performing tasks on the web presents fundamental challenges to large language models (LLMs), including combinatorially large open-world tasks and variations across web interfaces. Simply specifying a large prompt to handle all possible behaviors and states is extremely complex, and results in behavior leaks between unrelated behaviors. Decomposition to distinct policies can address this challenge, but requires carefully handing off control between policies. We propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically compose policies to solve a diverse set of web tasks. SteP defines a Markov Decision Process where the state is a stack of policies representing the control state, i.e., the chain of policy calls. Unlike traditional methods that are restricted to static hierarchies, SteP enables dynamic control that adapts to the complexity of the task. We evaluate SteP against multiple baselines and web environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP improves (14.9\\\\% to 33.5\\\\%) over SOTA that use GPT-4 policies, while on MiniWob++, SteP is competitive with prior works while using significantly less data. Our code and data are available at https://asappresearch.github.io/webagents-step.\\n                                \\n\\n---\\n\\nPaper ID: 1f9822022f586e375461660db792f23e891c7123\\n                                    Title: Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems\\n                                    Abstract: The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs\\' capabilities and indicating further research opportunities to assess LLMs\\' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.\\n                                \\n\\n---\\n\\nPaper ID: 3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\\n                                    Title: Bridging the Gulf of Envisioning: Cognitive Challenges in Prompt Based Interactions with LLMs\\n                                    Abstract: Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Normans gulfs of execution and evaluation. To address this gap, we theorize how end-users envision translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments on not knowing: (1) what the task should be, (2) how to instruct the LLM to do the task, and (3) what to expect for the LLMs output in meeting the goal. Finally, we make recommendations to narrow the gulf of envisioning in human-LLM interactions.\\n                                \\n\\n---\\n\\nPaper ID: 28d6411019f448f54834c2a5cff723cd350345b5\\n                                    Title: Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration\\n                                    Abstract: Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This letter proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.\\n                                \\n\\n---\\n\\nPaper ID: 77a9c310df0d7896d297da90fc4a1131819c341e\\n                                    Title: LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models\\n                                    Abstract: To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.\\n                                \\n\\n---\\n\\nPaper ID: d0da372b4b6f422e13556ce272595a0c9002fe90\\n                                    Title: GPT-4 in Education: Evaluating Aptness, Reliability, and Loss of Coherence in Solving Calculus Problems and Grading Submissions\\n                                    Abstract: In this paper, we initially investigate the capabilities of GPT-3 5 and GPT-4 in solving college-level calculus problems, an essential segment of mathematics that remains under-explored so far. Although improving upon earlier versions, GPT-4 attains approximately 65% accuracy for standard problems and decreases to 20% for competition-like scenarios. Overall, the models prove to be unreliable due to common arithmetic errors. Our primary contribution lies then in examining the use of ChatGPT for grading solutions to calculus exercises. Our objectives are to probe an in-context learning task with less emphasis over direct calculations; recognize positive applications of ChatGPT in educational contexts; highlight a potentially emerging facet of AI that could necessitate oversight; and introduce unconventional AI benchmarks, for which models like GPT are untrained. Pertaining to the latter, we uncover a tendency for loss of coherence in extended contexts. Our findings suggest that while the current ChatGPT exhibits comprehension of the grading task and often provides relevant outputs, the consistency of grading is marred by occasional loss of coherence and hallucinations. Intriguingly, GPT-4\\'s overall scores, delivered in mere moments, align closely with human graders, although its detailed accuracy remains suboptimal. This work suggests that, when appropriately orchestrated, collaboration between human graders and LLMs like GPT-4 might combine their unique strengths while mitigating their respective shortcomings In this direction, it is imperative to consider implementing transparency, fairness, and appropriate regulations in the near future.\\n                                \\n\\n---\\n\\nPaper ID: 49b7bd275a0386392769f5b33028500754dbc69d\\n                                    Title: AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark\\n                                    Abstract: Detecting biases in natural language understanding (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language processing (NLP) systems. To address dialect-induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE). AAVENUE builds upon and extends existing benchmarks like VALUE, replacing deterministic syntactic and morphological transformations with a more flexible methodology leveraging LLM-based translation with few-shot prompting, improving performance across our evaluation metrics when translating key tasks from the GLUE and SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics including fluency, BARTScore, quality, coherence, and understandability. Additionally, we recruit fluent AAVE speakers to validate our translations for authenticity. Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\\n                                \\n\\n---\\n\\nPaper ID: e5968ff7af575e014a4cb76a75f1f0f4378d16d7\\n                                    Title: DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM\\n                                    Abstract: In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME\\'s potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\\n                                \\n\\n---\\n\\nPaper ID: 0b04776792c83ec1bf4b83078c4b7618d85bc76e\\n                                    Title: Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools\\n                                    Abstract: Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Materials and Methods: We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. Results: The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Discussion: Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. Conclusion: The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.\\n                                \\n\\n---\\n\\nPaper ID: 09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\\n                                    Title: Asynchronous LLM Function Calling\\n                                    Abstract: Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM\\'s operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call\\'s completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.\\n                                \\n\\n---\\n\\nPaper ID: 3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\\n                                    Title: Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network\\n                                    Abstract: Neovascular age-related macular degeneration (nAMD) can result in blindness if left untreated, and patients often require repeated anti-vascular endothelial growth factor injections. Although, the treat-and-extend method is becoming popular to reduce vision loss attributed to recurrence, it may pose a risk of overtreatment. This study aimed to develop a deep learning model based on DenseNet201 to predict nAMD recurrence within 3 months after confirming dry-up 1 month following three loading injections in treatment-nave patients. A dataset of 1076 spectral domain optical coherence tomography (OCT) images from 269 patients diagnosed with nAMD was used. The performance of the model was compared with that of 6 ophthalmologists, using 100 randomly selected samples. The DenseNet201-based model achieved 53.0% accuracy in predicting nAMD recurrence using a single pre-injection image and 60.2% accuracy after viewing all the images immediately after the 1st, 2nd, and 3rd injections. The model outperformed experienced ophthalmologists, with an average accuracy of 52.17% using a single pre-injection image and 53.3% after examining four images before and after three loading injections. In conclusion, the artificial intelligence model demonstrated a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists. These findings suggest that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies.\\n                                \\n\\n---\\n\\nPaper ID: b2fe504e9f15de9438d22e7b632e81e57bfbbc06\\n                                    Title: Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning\\n                                    Abstract: Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.\\n                                \\n\\n---\\n\\nPaper ID: 27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\\n                                    Title: 13.4 A 48GB 16-High 1280GB/s HBM3E DRAM with All-Around Power TSV and a 6-Phase RDQS Scheme for TSV Area Optimization\\n                                    Abstract: With the emergence of large-language models (LLM) and generative AI, which require an enormous amount of model parameters, the required memory bandwidth and capacity for high-end systems is on an unprecedented increase. To meet this need, we present an extended version of the high-bandwidth memory-3 (HBM3 DRAM), HBM3E, which achieves a 1280GB/s bandwidth with a cube density of 48GB. New design schemes and features, such as all-around power-through-silicon via (TSV), a 6-phase read-data-strobe (RDQS) scheme, a byte-mapping swap scheme, and a voltage-drift compensator for write data strobe (WDQS), are implemented to achieve extended bandwidth and capacity with enhanced reliability. The overall architecture and specifications, such as bump map footprint, the number of channel and I/Os, and the operation voltage, are identical to the latest HBM3 [1, 2]; therefore, backward compatibility is provided, avoiding system modification.\\n                                \\n\\n---\\n\\nPaper ID: 76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\\n                                    Title: NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens\\n                                    Abstract: Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models\\' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.\\n                                \\n\\n---\\n\\nPaper ID: dda8031682684655744c7001374e6cb88c9503bd\\n                                    Title: Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs\\n                                    Abstract: Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.\\n                                \\n\\n---\\n\\nPaper ID: 2d0b030d314a5aa8feaa03695e8471270130bdf9\\n                                    Title: Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\\n                                    Abstract: Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\\n                                \\n\\n---\\n\\nPaper ID: 78875987dc674fc556873df037cf114f04932e80\\n                                    Title: When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\\n                                    Abstract: AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.\\n                                \\n\\n---\\n\\nPaper ID: 807ba70b6dc5ce8104268ef8e579d6ff67051230\\n                                    Title: A Survey on Post-training of Large Language Models\\n                                    Abstract: The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT\\'s alignment strategies to DeepSeek-R1\\'s innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.\\n                                \\n\\n---\\n\\nPaper ID: ecb1002427e476ec76463e0a8b5a453471a1931f\\n                                    Title: The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models\\n                                    Abstract: In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\\n                                \\n\\n---\\n\\nPaper ID: a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\\n                                    Title: Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)\\n                                    Abstract: Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.\\n                                \\n\\n---\\n\\nPaper ID: 5550118041a89121e9d7274f83aef420cd9ed487\\n                                    Title: Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation\\n                                    Abstract: The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end speech-to-speech conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and speech styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the speech, which is then played back to the user. While the response speech is being played, the input speech undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input speech. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and speech-to-speech baselines in terms of both dialogue naturalness and coherence while being more than 50% faster.\\n                                \\n\\n---\\n\\nPaper ID: afdc9b9c86f06db2b10816fac916e9f72d7b04ff\\n                                    Title: What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation\\n                                    Abstract: With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories. Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations.\\n                                \\n\\n---\\n\\nPaper ID: bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\\n                                    Title: Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model\\n                                    Abstract: Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest\\'s superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\\n                                \\n\\n---\\n\\nPaper ID: 1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\\n                                    Title: When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training\\n                                    Abstract: Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16\\'s limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\\\% compared to standard full attention mechanisms, while preserving the original LLM\\'s capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.\\n                                \\n\\n---\\n\\nPaper ID: 38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\\n                                    Title: Ultrafast imaging of coherent polariton propagation and interactions\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\\n                                    Title: Extrapolation-Based Video Retargeting With Backward Warping Using an Image-to-Warping Vector Generation Network\\n                                    Abstract: Video retargeting is a technique used to transform a given video to a target aspect ratio. Current methods often cause severe visual distortion due to frequent temporal incoherence during the retargeting. In this study, we propose a new extrapolation-based video retargeting method using an image-to-warping vector generation network to maintain temporal coherence and prevent deformation of an input frame by extending the side area of an input frame. Backward warping-based extrapolation is performed using a displacement vector (DV) that is generated by a proposed convolutional neural network (CNN). The DV is defined as the displacement between the current hole to be filled in the extended area and a pixel in the input frame used to fill the hole. We also propose a technique to efficiently train the CNN including a method for ground-truth DV generation. After the extrapolation, we propose a technique for the maintenance of temporal coherence of the extended region and a distortion suppression scheme (DSC) for minimizing visual artifacts. The simulation results demonstrated that the proposed method improved bidirectional similarity (BDS) up to 3.69, which is a measure of the quality of video retargeting, compared with existing video retargeting methods.\\n                                \\n\\n---\\n\\nPaper ID: 0df45f6ab09cc6ddcaf6829c131c777732a73731\\n                                    Title: Temporal Incoherence-Free Video Retargeting Using Foreground Aware Extrapolation\\n                                    Abstract: Video retargeting is a method of adjusting the aspect ratio of a given video to the target aspect ratio. However, temporal incoherence of video contents, which can occur frequently by video retargeting, is the most dominant factor that degrades the quality of retargeted videos. Current methods to maintain temporal coherence use the entire frames of the input videos; however, these methods cannot be implemented as on-time systems because of their tremendous computational complexity. As far as we know, there is no existing on-time video retargeting method that can avoid spatial distortion while perfectly maintaining temporal coherence. In this paper, we propose a novel on-time video retargeting method that can perfectly maintain temporal coherence and prevent the spatial distortion by using only two consecutive input frames. In our method, the maximum a posteriori-based foreground aware-block matching is used for the extrapolation that extends the side area of a given video to adjust its aspect ratio to the target. To maintain the temporal coherence of the extended area, the result of block matching for backward warping-based extrapolation of the start frame after the scene change occurs, is reused for the other frames until the next scene change occurs. In addition, we propose a scene scenario-adaptive fallback scheme to prevent severe distortions that can occur with reusing block matching results or extrapolation-based side extension. The simulation results showed that the proposed method greatly improved the bidirectional similarity value, which can measure the quality of video retargeting, by up to 10.26 compared with the existing on-time video retargeting methods.\\n                                \\n\\n---\\n\\nPaper ID: cc35548ca7f8b797402e9a95ff901b642af0b2b3\\n                                    Title: Posterior-GRPO: Rewarding Reasoning Processes in Code Generation\\n                                    Abstract: Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model\\'s internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.\\n                                \\n\\n---\\n\\nPaper ID: ef399ed62fcc00e73b02f286012080351652693c\\n                                    Title: Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\\n                                    Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method\\'s simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model\\'s performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\times$ and 1.25$\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\\n                                \\n\\n---\\n\\nPaper ID: bde9fc752842a2a91c70933169fb8cee2b81f8b2\\n                                    Title: Prompt Expansion for Adaptive Text-to-Image Generation\\n                                    Abstract: Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\\n                                \\n\\n---\\n\\nPaper ID: 1d954600f17a7c3f16aa726a5eaa902d6851e808\\n                                    Title: SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\\n                                    Abstract: Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\\n                                \\n\\n---\\n\\nPaper ID: a05b33072a8330413947eab0833ed2fd21de4963\\n                                    Title: Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\\n                                    Abstract: Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\\n                                \\n\\n---\\n\\nPaper ID: bc9f1025246efeed568650934b6e183415aca279\\n                                    Title: PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\\n                                    Abstract: We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\\n                                \\n\\n---\\n\\nPaper ID: cf56a7b28fb27279b1c94fb920b5722cf50c8852\\n                                    Title: AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\\n                                    Abstract: Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\\n                                \\n\\n---\\n\\nPaper ID: 1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\\n                                    Title: WavLLM: Towards Robust and Adaptive Speech Large Language Model\\n                                    Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker\\'s identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\url{aka.ms/wavllm}.\\n                                \\n\\n---\\n\\nPaper ID: 5cacb35e7bd86e64e5aca126e5011b64630007d8\\n                                    Title: OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\\n                                    Abstract: Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\\n                                \\n\\n---\\n\\nPaper ID: 8e37dc1215681aa153a51c07078ba8befd6a6e01\\n                                    Title: AdaPlanner: Adaptive Planning from Feedback with Language Models\\n                                    Abstract: Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\\n                                \\n\\n---\\n\\nPaper ID: 9689acb6cb760e8bc21c16f368368b37dee977f9\\n                                    Title: Adaptive Machine Translation with Large Language Models\\n                                    Abstract: Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\\n                                \\n\\n---\\n\\nPaper ID: 7665642af9e682e012bec045102a4d009421067c\\n                                    Title: HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\\n                                    Abstract: Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\\n                                \\n\\n---\\n\\nPaper ID: 5888f7aba5c601a668c290bf57addf79cc1518f1\\n                                    Title: ChatUniTest: a ChatGPT-based automated unit test generation tool\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 836e3069a83f455f916114e7265e00187e511838\\n                                    Title: Locally Differentially Private Document Generation Using Zero Shot Prompting\\n                                    Abstract: Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\% reduction in author identification F1 score against static attackers and a 26\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\\n                                \\n\\n---\\n\\nPaper ID: f087d0175346f143e190fc39f4ec4ceb5b3cc093\\n                                    Title: Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\\n                                    Abstract: Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\\n                                \\n\\n---\\n\\nPaper ID: 107dce4ffbdd8d83b75492216646269d8c037ab5\\n                                    Title: The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\\n                                    Abstract: In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\\n                                \\n\\n---\\n\\nPaper ID: b013c9eb1284554ae696fba02bd4d7fc599890b6\\n                                    Title: StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 0f78249a5ff64441cc51e55bbe5b97e28f801240\\n                                    Title: Prompt Tuning for Generative Multimodal Pretrained Models\\n                                    Abstract: Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at \\\\url{https://github.com/OFA-Sys/OFA}\\n                                \\n\\n---\\n\\nPaper ID: 192b808eba1232ba3b1d1481230db122b22c97e4\\n                                    Title: Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\\n                                    Abstract: In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\\n                                \\n\\n---\\n\\nPaper ID: 817ff4cfbcd5d6c870947fa8129ee5598f03a765\\n                                    Title: TaskCraft: Automated Generation of Agentic Tasks\\n                                    Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce \\\\textsc{TaskCraft}, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.\\n                                \\n\\n---\\n\\nPaper ID: f373c5569b45bf580b7502729a83761a791ee209\\n                                    Title: MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing\\n                                    Abstract: The diffusion model is widely leveraged for either video generation or video editing. As each field has its task-specific problems, it is difficult to merely develop a single diffusion for completing both tasks simultaneously. Video diffusion sorely relying on the text prompt can be adapted to unify the two tasks. However, it lacks a high capability of aligning heterogeneous modalities between text and image, leading to various misalignment problems. In this work, we are the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing. The proposed MagDiff introduces three types of alignments, including subject-driven alignment, adaptive prompts alignment, and high-fidelity alignment. Particularly, the subject-driven alignment is put forward to trade off the image and text prompts, serving as a unified foundation generative model for both tasks. The adaptive prompts alignment is introduced to emphasize different strengths of homogeneous and heterogeneous alignments by assigning different values of weights to the image and the text prompts. The high-fidelity alignment is developed to further enhance the fidelity of both video generation and editing by taking the subject image as an additional model input. Experimental results on four benchmarks suggest that our method outperforms the previous method on each task.\\n                                \\n\\n---\\n\\nPaper ID: a17aee1261b56ee828f029b1caeef78033acea83\\n                                    Title: Adaptive Ship Detection From Optical to SAR Images\\n                                    Abstract: Recent advances in synthetic aperture radar (SAR) ship detection have witnessed remarkable success by using large-scale annotated datasets. However, the annotation of SAR images requires strong domain-specific expertise, significantly hindering the prompt adoption of modern object detectors in this regime. Compared to SAR data, optical data in geoscience are considerably easier to label. Motivated by this, we investigate a new and challenging problemadaptive ship detectionwith the goal of enhancing ship detection performance on SAR images by leveraging knowledge transferred from optical images. Considering the large distributional discrepancy between the source (optical) and target (SAR) domains, we present OmniAdapt, a novel framework that progressively narrows the distance between the two types of images at the pixel, feature, and classifier levels. Specifically, OmniAdapt consists of three main modules, target-like generation module (TLGM), multifeature alignment module (MFAM), and common specific decomposition module (CSDM). TLGM minimizes the visual disparity by infusing the target-domain style into the source domain. MFAM aligns local- and global-level feature representations in an adversarial manner. Finally, CSDM decomposes the classifier into two independent components, that is, the domain-common component and the domain-specific component, and promotes the recognition ability of the former via regularization learning. Experimental results demonstrate the effectiveness of the proposed method.\\n                                \\n\\n---\\n\\nPaper ID: 94bbe80766824ca4163b2578e19daf6b9f2b1fd6\\n                                    Title: Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning\\n                                    Abstract: Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt\\'s online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.\\n                                \\n\\n---\\n\\nPaper ID: 44d6f053a43a8bc5c45fe807066656d8e34d3d27\\n                                    Title: OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding\\n                                    Abstract: In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff , aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. Our framework supports three key capabilities: (1) Text-conditioned video generation, where all modalities are jointly synthesized from a textual prompt; (2) Video understanding, where structural modalities are predicted from rgb inputs in a coherent manner; and (3) X-conditioned video generation, where video synthesis is guided by finegrained inputs such as depth, canny and segmentation. Extensive experiments demonstrate that OmniVDiff achieves state-of-the-art performance in video generation tasks and competitive results in video understanding. Its flexibility and scalability make it well-suited for downstream applications such as video-to-video translation, modality adaptation for visual tasks, and scene reconstruction.\\n                                \\n\\n---\\n\\nPaper ID: 42117d01d498eb9f8c21b788c3565bc6855d620b\\n                                    Title: Learning to Transfer Prompts for Text Generation\\n                                    Abstract: Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.\\n                                \\n\\n---\\n\\nPaper ID: 7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\\n                                    Title: CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer\\n                                    Abstract: We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.\\n                                \\n\\n---\\n\\nPaper ID: f37a5c2bf4266d031533d5e029b74b00b48ef038\\n                                    Title: Latency-Aware Generative Semantic Communications With Pre-Trained Diffusion Models\\n                                    Abstract: Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this letter, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.\\n                                \\n\\n---\\n\\nPaper ID: 759b95f7f90addc4c526cd92557e486ab143fbec\\n                                    Title: Style Vectors for Steering Generative Large Language Models\\n                                    Abstract: This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.\\n                                \\n\\n---\\n\\nPaper ID: 621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\\n                                    Title: EdgeLLM: Fast On-Device LLM Inference With Speculative Decoding\\n                                    Abstract: Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device\\'s memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3 faster than existing engines.\\n                                \\n\\n---\\n\\nPaper ID: 411b16add23976ffcdf6422f932453f6ebcca119\\n                                    Title: EvoPrompting: Language Models for Code-Level Neural Architecture Search\\n                                    Abstract: Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\\n                                \\n\\n---\\n\\nPaper ID: 1a907cda901dc8039e1d04838ff217fdbcacd2e6\\n                                    Title: Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them\\n                                    Abstract: Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a neutral concept or just an empty text prompt. In this paper, we demonstrate that this fixed-target strategy is suboptimal, as it fails to account for the impact of erasing one concept on the others. To address this limitation, we model the concept space as a graph and empirically analyze the effects of erasing one concept on the remaining concepts. Our analysis uncovers intriguing geometric properties of the concept space, where the influence of erasing a concept is confined to a local region. Building on this insight, we propose the Adaptive Guided Erasure (AGE) method, which \\\\emph{dynamically} selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects. Experimental results show that AGE significantly outperforms state-of-the-art erasure methods on preserving unrelated concepts while maintaining effective erasure performance. Our code is published at {https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.\\n                                \\n\\n---\\n\\nPaper ID: af06dc2be258419a3d7c5b38d446234e68b1a4b8\\n                                    Title: StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements\\n                                    Abstract: Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.\\n                                \\n\\n---\\n\\nPaper ID: 5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\\n                                    Title: TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation\\n                                    Abstract: The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM\\'s spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.\\n                                \\n\\n---\\n\\nPaper ID: 4e5c310f6b04c2122de108cb5f742b7a3b83af1d\\n                                    Title: A new rapid deflagration-to-detonation transition in a short smooth tube\\n                                    Abstract: Obtaining a rapid deflagration-to-detonation transition (DDT) within a short smooth tube is a challenging task. Here, an unconventional means of flame acceleration propagating upstream in subsonic and supersonic mixtures within a smooth tube was introduced to acquire a speedy DDT. The NavierStokes equations with an adaptive mesh refinement technique and a detailed hydrogenair chemistry reaction mechanism of 11 species and 27 steps were utilized to resolve the entire DDT characteristics. The effect of the initial Mach number on flame acceleration and DDT mechanism was revealed comprehensively. The results demonstrated that a prompt oblique shock wave (SW) occurs when the flame propagates upstream along the boundary walls due to the boundary layer influence. An intense coupling between the SW and the leading flame front is enhanced by increasing the initial Mach number of the mixture. The speedy generation of the oblique SW is formed at the incipient stage, mainly produced by the boundary layer influence and the coalescences of the compression waves. Consequently, the run-up time to detonation is shortened accordingly through a fierce reflected SW due to the intense leading SW after it reflects from the confined wall. Furthermore, three kinds of DDT evolution are revealed from the obtained results: (1) localized ignition in the upper boundary wall after the reflected and transverse shock waves propagate in the upper wall regions; (2) autoignition is formed in the confined wall corner after the reflected SW; and (3) direct detonation transition occurs at the end wall behind a strongly reflected SW in the supersonic case.\\n                                \\n\\n---\\n\\nPaper ID: 815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\\n                                    Title: Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces\\n                                    Abstract: Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\\n                                \\n\\n---\\n\\nPaper ID: e8e619d906f1e3a4f9b715a9e553d996a935b4f0\\n                                    Title: Transformer-Based Variable-Rate Image Compression with Region-of-Interest Control\\n                                    Abstract: This paper proposes a transformer-based learned image compression system. It is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality. Inspired by prompt tuning, we introduce prompt generation networks to condition the transformer-based autoencoder of compression. Our prompt generation networks generate content-adaptive tokens according to the input image, an ROI mask, and a rate parameter. The separation of the ROI mask and the rate parameter allows an intuitive way to achieve variable-rate and ROI coding simultaneously. Extensive experiments validate the effectiveness of our proposed method and confirm its superiority over the other competing methods.\\n                                \\n\\n---\\n\\nPaper ID: c7acf9250926100f531bcf46d63d7da06e73928e\\n                                    Title: MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask\\n                                    Abstract: Recent advancements in diffusion models have showcased their impressive capacity to generate visually striking images. However, ensuring a close match between the generated image and the given prompt remains a persistent challenge. In this work, we identify that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images. To better align the prompt and image content, we advance the cross-attention with an adaptive mask, which is conditioned on the attention maps and the prompt embeddings, to dynamically adjust the contribution of each text token to the image features. This mechanism explicitly diminishes the ambiguity in the semantic information embedding of the text encoder, leading to a boost of text-to-image consistency in the synthesized images. Our method, termed MaskDiffusion, is training-free and hot-pluggable for popular pre-trained diffusion models. When applied to the latent diffusion models, our MaskDiffusion can largely enhance their capability to correctly generate objects and their attributes, with negligible computation overhead compared to the original diffusion models. Our project page is https://github.com/HVision-NKU/MaskDiffusion.\\n                                \\n\\n---\\n\\nPaper ID: 66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\\n                                    Title: TEMPERA: Test-Time Prompting via Reinforcement Learning\\n                                    Abstract: Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.\\n                                \\n\\n---\\n\\nPaper ID: 88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\\n                                    Title: QA-CLIMS: Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation\\n                                    Abstract: Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model\\'s ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets.\\n                                \\n\\n---\\n\\nPaper ID: d3c81cc6e98a2ba22ef0a5139876e229db16f214\\n                                    Title: Immunosenescence, Inflammaging, and Lung Senescence in Asthma in the Elderly\\n                                    Abstract: Prevalence of asthma in older adults is growing along with increasing global life expectancy. Due to poor clinical consequences such as high mortality, advancement in understanding the pathophysiology of asthma in older patients has been sought to provide prompt treatment for them. Age-related alterations of functions in the immune system and lung parenchyma occur throughout life. Alterations with advancing age are promoted by various stimuli, including pathobionts, fungi, viruses, pollutants, and damage-associated molecular patterns derived from impaired cells, abandoned cell debris, and senescent cells. Age-related changes in the innate and adaptive immune response, termed immunosenescence, includes impairment of phagocytosis and antigen presentation, enhancement of proinflammatory mediator generation, and production of senescence-associated secretory phenotype. Immnunosenescence could promote inflammaging (chronic low-grade inflammation) and contribute to late-onset adult asthma and asthma in the elderly, along with age-related pulmonary disease, such as chronic obstructive pulmonary disease and pulmonary fibrosis, due to lung parenchyma senescence. Aged patients with asthma exhibit local and systemic type 2 and non-type 2 inflammation, associated with clinical manifestations. Here, we discuss immunosenescences contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly and present an overview of age-related features in the immune system and lung structure.\\n                                \\n\\n---\\n\\nPaper ID: 94d2f525eed0cd7c337bd692f833f006d62dc3ab\\n                                    Title: DAFT-E: Feature-Based Multivariate and Multi-Step-Ahead Wind Power Forecasting\\n                                    Abstract: Wind energy is one of the most promising resources for the mitigation of greenhouse gas emissions that contribute to anthropogenic global warming. However, the large proliferation of wind power generators is causing several critical issues in power systems due to their variable power generated profiles. For this reason, a large number of learning techniques, e.g. integrating Vector Auto-Regressive and Neural Network-based models, were proposed in the literature for mitigating wind power uncertainty issues. Unfortunately, these methodologies show several limitations, e.g. the huge number of parameters and/or the heavy computational cost, which hinder their deployment in modern power system operation, where prompt and reliable wide-area wind power generation forecasts are requested for supporting time-critical decision making on several time horizons. To try addressing this issue, this paper proposes the Dynamic Adaptive Feature-based Temporal Ensemble (DAFT-E) forecasting approach, which relies on an extensive feature engineering, a fast feature selection step and an ensemble of computationally inexpensive models to reduce the computational complexity of the forecasting task, while still preserving predictive accuracy. The experimental results, which benchmark DAFT-E against multivariate (VAR and deep learning) alternatives on two real case studies, show that the proposed approach outperforms state-of-the-art and representation learning models according to several forecasting accuracy metrics.\\n                                \\n\\n---\\n\\nPaper ID: f6023556221018f779a63a60874973195aea8352\\n                                    Title: Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt\\n                                    Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensive experiments on three benchmark datasets validate that ASP consistently outperforms state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning new classes and mitigating forgetting.\\n                                \\n\\n---\\n\\nPaper ID: c36cb4a41369369d837ea170397f7818d02150dd\\n                                    Title: Soft Prompt Generation for Domain Generalization\\n                                    Abstract: Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\\n                                \\n\\n---\\n\\nPaper ID: 37e1bc43d22da21d6be616522e3fe217bf7f3d8e\\n                                    Title: Adapting to Distribution Shift by Visual Domain Prompt Generation\\n                                    Abstract: In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\\n                                \\n\\n---\\n\\nPaper ID: fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\\n                                    Title: Query-Based Adversarial Prompt Generation\\n                                    Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI\\'s safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.\\n                                \\n\\n---\\n\\nPaper ID: 91b6158978b248e9a0e65d0d588bc1054e72bc16\\n                                    Title: MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization\\n                                    Abstract: Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.\\n                                \\n\\n---\\n\\nPaper ID: 135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\\n                                    Title: Federated Text-driven Prompt Generation for Vision-Language Models\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: ac4dc5ff917141f30b53314f765d30fb4b83c8b7\\n                                    Title: EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\\n                                    Abstract: Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\\n                                \\n\\n---\\n\\nPaper ID: 1566d96346927ad4dced85de4d55356f6aee6fb6\\n                                    Title: Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\\n                                    Abstract: Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\\n                                \\n\\n---\\n\\nPaper ID: 211e915b2e1e0753ddd581f10362fc82f28cc606\\n                                    Title: DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation\\n                                    Abstract: Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer (MM-DiT) architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiTs attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training. Code is available at https://github.com/TencentARC/DiTCtrl.\\n                                \\n\\n---\\n\\nPaper ID: 7be9a0708300e42fd3c376b4f4027dd530f240af\\n                                    Title: Dynamic Prompt Optimizing for Text-to-Image Generation\\n                                    Abstract: Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\\n                                \\n\\n---\\n\\nPaper ID: a0d2ea210c9bd21676605682a76cec1a4004320a\\n                                    Title: Iterative Context-Aware Graph Inference for Visual Dialog\\n                                    Abstract: Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\\n                                \\n\\n---\\n\\nPaper ID: 697e176d66a17c0b24613b8513ab951dc4112c34\\n                                    Title: Iterative Geometry Encoding Volume for Stereo Matching\\n                                    Abstract: Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\\n                                \\n\\n---\\n\\nPaper ID: 8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\\n                                    Title: Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\\n                                    Abstract: In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to showusing a new rearrangement of the Schur complement which exploits regularizationthat general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\\n                                \\n\\n---\\n\\nPaper ID: 8fd4d5762de7c6861c841bc54208ebeec76b6213\\n                                    Title: IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\\n                                    Abstract: 3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\\n                                \\n\\n---\\n\\nPaper ID: be089bc2e9983223068b4eb0d67b4b2f5fc30321\\n                                    Title: Iterative Privileged Learning\\n                                    Abstract: While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\\n                                \\n\\n---\\n\\nPaper ID: 22ae025a4f0b644369d05a3e13c0021f868a8372\\n                                    Title: Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\\n                                    Abstract: We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\\n                                \\n\\n---\\n\\nPaper ID: 7d2f92bfcded0fe63cb3926155d769263b9580c9\\n                                    Title: RLCoder: Reinforcement Learning for Repository-Level Code Completion\\n                                    Abstract: Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\\n                                \\n\\n---\\n\\nPaper ID: 69ec41a25a4ead52bab62ea220103fdde06f7126\\n                                    Title: Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\\n                                    Abstract: Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\"finding a needle in a haystack.\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\\n                                \\n\\n---\\n\\nPaper ID: c5dae02bb601106a51bec4497f0581affd6af8a7\\n                                    Title: Zero-Shot Class Unlearning in CLIP with Synthetic Samples\\n                                    Abstract: Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\\n                                \\n\\n---\\n\\nPaper ID: 096df636db478a4eb7ab9c74f8a6cae97efed149\\n                                    Title: Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\\n                                    Abstract: In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the systems entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\\n                                \\n\\n---\\n\\nPaper ID: 82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\\n                                    Title: Meta-AF: Meta-Learning for Adaptive Filters\\n                                    Abstract: Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against  all using a single general-purpose configuration of our approach.\\n                                \\n\\n---\\n\\nPaper ID: f55c7f2156ecc389132acf5c3fbaca4c3f832abf\\n                                    Title: Flexible Task Scheduling in Data Relay Satellite Networks\\n                                    Abstract: The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\\n                                \\n\\n---\\n\\nPaper ID: b12f40443c189e665520811f99f0ec2b47d90194\\n                                    Title: RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\\n                                    Abstract: Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\\n                                \\n\\n---\\n\\nPaper ID: c04e9454a696086ea69f37f59d45347f5c6d438c\\n                                    Title: Enhancing Transferability of Adversarial Examples with Spatial Momentum\\n                                    Abstract: Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\% on average.\\n                                \\n\\n---\\n\\nPaper ID: 8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\\n                                    Title: xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\\n                                    Abstract: Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\\n                                \\n\\n---\\n\\nPaper ID: 51458bf945a446271cf857016582d3fc881f8f9a\\n                                    Title: Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\\n                                    Abstract: This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\\n                                \\n\\n---\\n\\nPaper ID: 8c8595a050845a8926aa52b22998b2ca609a5d96\\n                                    Title: Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\\n                                    Abstract: The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\\n                                \\n\\n---\\n\\nPaper ID: 07503b6fa32459203e176ce880c2a5ba23f0f8e5\\n                                    Title: Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\\n                                    Abstract: We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\\n                                \\n\\n---\\n\\nPaper ID: 9aa6a885754a27fe42a87e4dfaed87d618fd8518\\n                                    Title: Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\\n                                    Abstract: Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\\n                                \\n\\n---\\n\\nPaper ID: 3e43429b24beb5b93513775a259c23ce3a133f67\\n                                    Title: Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 8745157f991013b23fbb79d300ba560f9005c8d4\\n                                    Title: Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\\n                                    Abstract: Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\\n                                \\n\\n---\\n\\nPaper ID: fca9a4508863025d95a581ead47032d497825053\\n                                    Title: VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding\\n                                    Abstract: Recent advancements in video large multimodal models (LMMs) have significantly improved their video understanding and reasoning capabilities. However, their performance drops on out-of-distribution (OOD) tasks that are underrepresented in training data. Traditional methods like fine-tuning on OOD datasets are impractical due to high computational costs. While In-context learning (ICL) with demonstration examples has shown promising generalization performance in language tasks and image-language tasks without fine-tuning, applying ICL to video-language tasks faces challenges due to the limited context length in Video LMMs, as videos require longer token lengths. To address these issues, we propose VideoICL, a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach. This allows to select the most relevant examples and rank them based on similarity, to be used for inference. If the generated response has low confidence, our framework selects new examples and performs inference again, iteratively refining the results until a high-confidence response is obtained. This approach improves OOD video understanding performance by extending effective context length without incurring high costs. The experimental results on multiple benchmarks demonstrate significant performance gains, especially in domain-specific scenarios, laying the groundwork for broader video comprehension applications. Code is released at https://github.com/KangsanKim07/VideoICL\\n                                \\n\\n---\\n\\nPaper ID: b217b6bc340af9a10bebbf8acc36ea30871769bd\\n                                    Title: In-Context Learning with Iterative Demonstration Selection\\n                                    Abstract: Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\\n                                \\n\\n---\\n\\nPaper ID: 43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\\n                                    Title: Iterative Forward Tuning Boosts In-context Learning in Language Models\\n                                    Abstract: Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\\n                                \\n\\n---\\n\\nPaper ID: fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\\n                                    Title: ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\\n                                    Abstract: From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\\n                                \\n\\n---\\n\\nPaper ID: 860bafe8a3aa0e3a981951f0757996272276b54e\\n                                    Title: ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\\n                                    Title: Enhancing Iterative Learning Control With Fractional Power Update Law\\n                                    Abstract: The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\\n                                \\n\\n---\\n\\nPaper ID: 16749df89bb5d09c570fccad0fac3bbe7fc41b6c\\n                                    Title: Single image deraining using scale constraint iterative update network\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 38333f6e8f0388968edc4b2ea7a683ce69677e69\\n                                    Title: Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\\n                                    Abstract: We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\%$ (+$5.9\\\\%$), $34.7\\\\%$ (+$5.8\\\\%$), and $76.4\\\\%$ (+$15.8\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\\n                                \\n\\n---\\n\\nPaper ID: a1675f47125aa409525c5f759b5e6bcc1c8831aa\\n                                    Title: Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\\n                                    Abstract: Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\\n                                \\n\\n---\\n\\nPaper ID: af5c7848417882012203ac21399977ebda695a2b\\n                                    Title: RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\\n                                    Abstract: The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\\n                                \\n\\n---\\n\\nPaper ID: cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\\n                                    Title: An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\\n                                    Abstract: An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB SchwarzChristoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\\n                                \\n\\n---\\n\\nPaper ID: edf23ce5689d0acf18cc7ff8556c696f9231644d\\n                                    Title: Five-Precision GMRES-Based Iterative Refinement\\n                                    Abstract: . GMRES-based iterative renement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the ve-precision GMRES-based iterative renement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identies a small subset of relevant combinations. By choosing from within this subset one can achieve dierent levels of tradeo between cost and robustness, which allows for a ner choice of precisions depending on the problem diculty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\\n                                \\n\\n---\\n\\nPaper ID: dc256e179d4e8eff48879a40ddc414b15b0b2300\\n                                    Title: RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\\n                                    Abstract: We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\\n                                \\n\\n---\\n\\nPaper ID: ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\\n                                    Title: Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\\n                                    Abstract: Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\\n                                \\n\\n---\\n\\nPaper ID: 53dcf71b6be11d543f1bc7753e8ff8e9945eff77\\n                                    Title: Iterative and Scenario-based Requirements Specification in a System of Systems Context\\n                                    Abstract: [Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\\n                                \\n\\n---\\n\\nPaper ID: f6ca551535a92b669ebd97b246ac582e73af30ee\\n                                    Title: An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\\n                                    Abstract: The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\\n                                \\n\\n---\\n\\nPaper ID: 2a5e4419c4bf16178a441c975b970fa33a9f5361\\n                                    Title: Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\\n                                    Abstract: Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving humanrobot interaction.\\n                                \\n\\n---\\n\\nPaper ID: d47ecf36e7e67d9349ecac29dfb981feaf229229\\n                                    Title: iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\\n                                    Abstract: Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\"bibr\" rid=\"ref1\">[1]</xref>, ISBI2017 <xref ref-type=\"bibr\" rid=\"ref2\">[2]</xref>, ISIC2018 <xref ref-type=\"bibr\" rid=\"ref3\">[3]</xref> and PH2 <xref ref-type=\"bibr\" rid=\"ref4\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\\n                                \\n\\n---\\n\\nPaper ID: 0019cb24ec04498836b8215e9495b968f2c01666\\n                                    Title: IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\\n                                    Abstract: Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\\n                                \\n\\n---\\n\\nPaper ID: 29528d8cb030a65f62a35b1237f1f5483077ad0a\\n                                    Title: Inference Scaling for Long-Context Retrieval Augmented Generation\\n                                    Abstract: The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\\n                                \\n\\n---\\n\\nPaper ID: ca47c63a97848e60389037c93a4feb2daf849c3e\\n                                    Title: Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\\n                                    Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\\n                                \\n\\n---\\n\\nPaper ID: 9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\\n                                    Title: CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\\n                                    Abstract: Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\\n                                \\n\\n---\\n\\nPaper ID: 8051818817a9a3815be6623a679d4a7f5a7b7964\\n                                    Title: Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\\n                                    Abstract: Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\\n                                \\n\\n---\\n\\nPaper ID: c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\\n                                    Title: Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\\n                                    Abstract: In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\\n                                \\n\\n---\\n\\nPaper ID: ac4c1c56a196523f42643f0722fe2fde776fd1fa\\n                                    Title: Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\\n                                    Abstract: Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \"black box\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\\n                                \\n\\n---\\n\\nPaper ID: 1d82f61ee52331a94141a50b59b186ccf105f6a9\\n                                    Title: Building Math Agents with Multi-Turn Iterative Preference Learning\\n                                    Abstract: Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\\n                                \\n\\n---\\n\\nPaper ID: a8176838a636651324b9bac1b3443c803b44e1b3\\n                                    Title: Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\\n                                    Abstract: We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\\n                                \\n\\n---\\n\\nPaper ID: 0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\\n                                    Title: Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\\n                                    Abstract: The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\\n                                \\n\\n---\\n\\nPaper ID: 21ed545a3b02bbcba6a62f5858bca948c2ed4641\\n                                    Title: Explainable Benchmarking for Iterative Optimization Heuristics\\n                                    Abstract: Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\\n                                \\n\\n---\\n\\nPaper ID: d2d96dc3bbf9d63c85f445e3fa08ad695457a532\\n                                    Title: LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\\n                                    Abstract: Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model (LLM) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction? We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.\\n                                \\n\\n---\\n\\nPaper ID: 477a16bdbb43589e5feac3881b3370e3a4ab5624\\n                                    Title: Evaluating the Consistency of LLM Evaluators\\n                                    Abstract: Large language models (LLMs) have shown potential as general evaluators along with the evident benefits of speed and cost. While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators. In this paper, we conduct extensive studies on the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models. Our comprehensive analysis demonstrates that strong proprietary models are not necessarily consistent evaluators, highlighting the importance of considering consistency in assessing the capability of LLM evaluators.\\n                                \\n\\n---\\n\\nPaper ID: 75062b58398b6e9409e5fec855f6912534331eaf\\n                                    Title: Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\\n                                    Abstract: Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths,but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority voting, enabling more informed choices on when to halt sampling and which rationale to select. Our comprehensive experiments across diverse question-answering datasets demonstrate that RASC outperforms existing methods, reducing sample usage by approximately 70% while maintaining accuracy. Moreover, RASC facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs. Our approach effectively addresses the efficiency-accuracy trade-off in LLM reasoning tasks, offering a new perspective for more nuanced, faithful, and effective utilization of LLMs in resource-constrained environments.\\n                                \\n\\n---\\n\\nPaper ID: 3c38af66ef9254df723f99bb50e6fa20f479e0ef\\n                                    Title: Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications\\n                                    Abstract: This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions. We analyzed 3049 posts and comments extracted from a self-identified clinician subreddit using unsupervised machine learning via Latent Dirichlet Allocation and a structured qualitative analysis methodology. Analysis uncovered 14 salient themes of ethical implications, which we further consolidated into 4 overarching domains reflecting ethical issues around various clinical applications of LLM in healthcare, LLM coding, algorithm, and data governance, LLMs role in health equity and the distribution of public health services, and the relationship between users (human) and LLM systems (machine). Mapping themes to ethical frameworks in literature illustrated multifaceted issues covering transparent LLM decisions, fairness, privacy, access disparities, user experiences, and reliability. This study emphasizes the need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare, aiming to improve clinical outcomes ethically and effectively.\\n                                \\n\\n---\\n\\nPaper ID: 6b7c5fc0f6b401962153f68f8250951f75da929e\\n                                    Title: Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?\\n                                    Abstract: Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\\n                                \\n\\n---\\n\\nPaper ID: e1770838ec0667cad48729a81764ed9964d6a8e6\\n                                    Title: LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\\n                                    Abstract: Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\\n                                \\n\\n---\\n\\nPaper ID: 32426b96ff3c680125bde3b835bfa931288b8ade\\n                                    Title: Better Patching Using LLM Prompting, via Self-Consistency\\n                                    Abstract: Large Language models (LLMs) can be induced to solve non-trivial problems with few-shot prompts including illustrative problem-solution examples. Now if the few-shots also include chain of thought ($\\\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a explained solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\\\mathcal{S}-C$ (or even $\\\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.\\n                                \\n\\n---\\n\\nPaper ID: e24424283c02fbe7f641e5b3490d7bb059f8355a\\n                                    Title: A Survey on LLM-as-a-Judge\\n                                    Abstract: Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of\"LLM-as-a-Judge,\"where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.\\n                                \\n\\n---\\n\\nPaper ID: 0bf3a1867f7245b8a702093901c66b08b518eafc\\n                                    Title: Evaluating Very Long-Term Conversational Memory of LLM Agents\\n                                    Abstract: Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.\\n                                \\n\\n---\\n\\nPaper ID: 7c04ab297b59d4fe29285f339350882a3120b27f\\n                                    Title: CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs\\n                                    Abstract: Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates students incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AIs unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.\\n                                \\n\\n---\\n\\nPaper ID: da9b51050a8574c4e03e5eb8e9e17cb640e27324\\n                                    Title: Exploring and Evaluating Hallucinations in LLM-Powered Code Generation\\n                                    Abstract: The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users\\' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.\\n                                \\n\\n---\\n\\nPaper ID: a06d3e9e90008c64c45a0029d580541d5f646771\\n                                    Title: If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents\\n                                    Abstract: The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs\\' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.\\n                                \\n\\n---\\n\\nPaper ID: 2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\\n                                    Title: PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference\\n                                    Abstract: Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.\\n                                \\n\\n---\\n\\nPaper ID: 36b9dac525fc93100b18d8e489bd97460cd49a5e\\n                                    Title: Thematic-LM: A LLM-based Multi-agent System for Large-scale Thematic Analysis\\n                                    Abstract: Thematic analysis (TA) is a widely used qualitative method for identifying underlying meanings within unstructured text. However, TA requires manual processes, which become increasingly labour-intensive and time-consuming as datasets grow. While large language models (LLMs) have been introduced to assist with TA on small-scale datasets, three key limitations hinder their effectiveness. First, current approaches often depend on interactions between an LLM agent and a human coder, a process that becomes challenging with larger datasets. Second, with feedback from the human coder, the LLM tends to mirror the human coder, which provides a narrower viewpoint of the data. Third, existing methods follow a sequential process, where codes are generated for individual samples without recalling previous codes and associated data, reducing the ability to analyse data holistically. To address these limitations, we propose Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis. Thematic-LM assigns specialised tasks to each agent, such as coding, aggregating codes, and maintaining and updating the codebook. We assign coder agents different identity perspectives to simulate the subjective nature of TA, fostering a more diverse interpretation of the data. We applied Thematic-LM to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change. We evaluate the resulting themes based on trustworthiness principles in qualitative research. Our study reveals insights such as assigning different identities to coder agents promotes divergence in codes and themes.\\n                                \\n\\n---\\n\\nPaper ID: 54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\\n                                    Title: Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation\\n                                    Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\\n                                \\n\\n---\\n\\nPaper ID: 7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\\n                                    Title: ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification\\n                                    Abstract: Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.\\n                                \\n\\n---\\n\\nPaper ID: 630c47372317164fc367153f938903e1d5b76059\\n                                    Title: LLM-Check: Investigating Detection of Hallucinations in Large Language Models\\n                                    Abstract: While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.\\n                                \\n\\n---\\n\\nPaper ID: 038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\\n                                    Title: Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges\\n                                    Abstract: Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.\\n                                \\n\\n---\\n\\nPaper ID: f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\\n                                    Title: Reasoning Runtime Behavior of a Program with LLM: How Far are We?\\n                                    Abstract: Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs (i.e., predicting code execution behaviors such as program output and execution path), but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely $\\\\boldsymbol{\\\\mathcal{R}}\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs. Our code, data and $\\\\boldsymbol{\\\\mathcal{R}}\\\\mathbf{Eval}$ leaderboard are available at https://r-eval.github.io.\\n                                \\n\\n---\\n\\nPaper ID: 8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\\n                                    Title: Don\\'t Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization\\n                                    Abstract: Large language models (LLM), such as Google\\'s Minerva and OpenAI\\'s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.\\n                                \\n\\n---\\n\\nPaper ID: 1b256fb2f9ac9857db996fa4f881f16e1345f8b1\\n                                    Title: MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models\\n                                    Abstract: As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\"meta-evaluation benchmarks\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.\\n                                \\n\\n---\\n\\nPaper ID: f1366e505de4f1d0e901903e3c17471033758a96\\n                                    Title: Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based Chatbots\\n                                    Abstract: The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=Rescriber) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4o. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.\\n                                \\n\\n---\\n\\nPaper ID: 3ec06fe8d8764123490544ab5dc956143e84b443\\n                                    Title: Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents\\n                                    Abstract: The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\\n                                \\n\\n---\\n\\nPaper ID: dfbfe75ec8c2143e899897a3c054ee58d99ead43\\n                                    Title: Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge\\n                                    Abstract: LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases - particularly position bias, the tendency to favor solutions based on their position within the prompt - compromise its reliability. This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 15 LLM judges across MTBench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.\\n                                \\n\\n---\\n\\nPaper ID: 638d8d1f3865ebf065605535a7aa50727d5ffabe\\n                                    Title: TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation\\n                                    Abstract: Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a\"Screenwriter\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the\"Rehearsal\". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the\"Final Performance\". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.\\n                                \\n\\n---\\n\\nPaper ID: 4777305738fd1aa30243f96a1687d57d8f70fa5d\\n                                    Title: Improving Text-to-Image Consistency via Automatic Prompt Optimization\\n                                    Abstract: Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\\n                                \\n\\n---\\n\\nPaper ID: 275a3955a83867dd36a3683788e0e053e00f8a89\\n                                    Title: Collaborative Enhancement of Consistency and Accuracy in US Diagnosis of Thyroid Nodules Using Large Language Models.\\n                                    Abstract: Background Large language models (LLMs) hold substantial promise for medical imaging interpretation. However, there is a lack of studies on their feasibility in handling reasoning questions associated with medical diagnosis. Purpose To investigate the viability of leveraging three publicly available LLMs to enhance consistency and diagnostic accuracy in medical imaging based on standardized reporting, with pathology as the reference standard. Materials and Methods US images of thyroid nodules with pathologic results were retrospectively collected from a tertiary referral hospital between July 2022 and December 2022 and used to evaluate malignancy diagnoses generated by three LLMs-OpenAI\\'s ChatGPT 3.5, ChatGPT 4.0, and Google\\'s Bard. Inter- and intra-LLM agreement of diagnosis were evaluated. Then, diagnostic performance, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), was evaluated and compared for the LLMs and three interactive approaches: human reader combined with LLMs, image-to-text model combined with LLMs, and an end-to-end convolutional neural network model. Results A total of 1161 US images of thyroid nodules (498 benign, 663 malignant) from 725 patients (mean age, 42.2 years  14.1 [SD]; 516 women) were evaluated. ChatGPT 4.0 and Bard displayed substantial to almost perfect intra-LLM agreement ( range, 0.65-0.86 [95% CI: 0.64, 0.86]), while ChatGPT 3.5 showed fair to substantial agreement ( range, 0.36-0.68 [95% CI: 0.36, 0.68]). ChatGPT 4.0 had an accuracy of 78%-86% (95% CI: 76%, 88%) and sensitivity of 86%-95% (95% CI: 83%, 96%), compared with 74%-86% (95% CI: 71%, 88%) and 74%-91% (95% CI: 71%, 93%), respectively, for Bard. Moreover, with ChatGPT 4.0, the image-to-text-LLM strategy exhibited an AUC (0.83 [95% CI: 0.80, 0.85]) and accuracy (84% [95% CI: 82%, 86%]) comparable to those of the human-LLM interaction strategy with two senior readers and one junior reader and exceeding those of the human-LLM interaction strategy with one junior reader. Conclusion LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging. ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5.  RSNA, 2024 Supplemental material is available for this article.\\n                                \\n\\n---\\n\\nPaper ID: b5cd6bc53343f2ad8a7c7830555d8c744e626245\\n                                    Title: Mitigating LLM Hallucinations via Conformal Abstention\\n                                    Abstract: We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying\"I don\\'t know\") in a general domain, instead of resorting to possibly\"hallucinating\"a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.\\n                                \\n\\n---\\n\\nPaper ID: 023a98af94a3e7e8e538a6183da8ec05024fec56\\n                                    Title: How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\\n                                    Abstract: Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user\\'s known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs\\'cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.\\n                                \\n\\n---\\n\\nPaper ID: a1849a77644ff411a03833b5aa7a65ff57158c50\\n                                    Title: CLLMs: Consistency Large Language Models\\n                                    Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\\\times$ to 3.4$\\\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.\\n                                \\n\\n---\\n\\nPaper ID: acdb4b5a64c0872655379efd7889e692b5d7d7f6\\n                                    Title: PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children\\'s Collaborative Learning\\n                                    Abstract: In childrens collaborative learning, effective peer conversations can significantly enhance the quality of childrens collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster childrens creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.\\n                                \\n\\n---\\n\\nPaper ID: 5e317746c0f38d1149f33a19807af47d513bdd27\\n                                    Title: Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge\\n                                    Abstract: Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs. While deterministic settings can improve consistency, they do not guarantee reliability, as a single sample from the model\\'s probability distribution can still be misleading. Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald\\'s omega. We evaluate the reliability of LLMs when judging the outputs of other LLMs on standard single-turn and multi-turn benchmarks, simultaneously investigating the impact of temperature on reliability. By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implications for downstream applications. Our findings highlight the need for a nuanced understanding of LLM reliability and the potential risks associated with over-reliance on single-shot evaluations. This work provides a crucial step towards building more trustworthy and reliable LLM-based systems and applications.\\n                                \\n\\n---\\n\\nPaper ID: 235a8bd57a6b53ecab756780a45bce6e4743cecd\\n                                    Title: LLM-based Multi-Level Knowledge Generation for Few-shot Knowledge Graph Completion\\n                                    Abstract: Knowledge Graphs (KGs) are pivotal in various NLP applications but often grapple with incompleteness, especially due to the long-tail problem where infrequent, unpopular relationships drastically reduce the KG completion performance. In this paper, we focus on Few-shot Knowledge Graph Completion (FKGC), a task addressing these gaps in long-tail scenarios. Amidst the rapid evolution of Large Language Models, we propose a generation-based FKGC paradigm facilitated by LLM distillation. Our MuKDC framework employs multi-level knowledge distillation for few-shot KG completion, generating supplementary knowledge to mitigate data scarcity in few-shot environments. MuKDC comprises two primary components: Multi-level Knowledge Generation, which enriches the KG at various levels, and Consistency Assessment, to ensure the coherence and reliability of the generated knowledge. Most notably, our method achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\\n                                \\n\\n---\\n\\nPaper ID: 9f8da87ee4416d57a2cc044bdf8223c7728d74d7\\n                                    Title: Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View\\n                                    Abstract: Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents\\' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents\\' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\\n                                \\n\\n---\\n\\nPaper ID: 086046d38b3a7066aa39e3d350905e8065c8f1b5\\n                                    Title: Efficiently Scaling LLM Reasoning with Certaindex\\n                                    Abstract: Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50% compute savings and 3.3x higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git\\n                                \\n\\n---\\n\\nPaper ID: 43fedc4430be030c083626b3e64b7093916b429a\\n                                    Title: Enhancing clinical reasoning skills for medical students: a qualitative comparison of LLM-powered social robotic versus computer-based virtual patients within rheumatology\\n                                    Abstract: Virtual patients (VPs) are increasingly used in medical education to train clinical reasoning (CR) skills. However, optimal VP design for enhancing interactivity and authenticity remains unclear. Novel interactive modalities, such as large language model (LLM)-enhanced social robotic VPs might increase interactivity and authenticity in CR skill practice. To evaluate medical students perceptions of CR training using an LLM-enhanced social robotic VP platform compared with a conventional computer-based VP platform. A qualitative study involved 23 third-year medical students from Karolinska Institutet, who completed VP cases on an LLM-enhanced social robotic platform and a computer-based semi-linear platform. In-depth interviews assessed students self-perceived acquirement of CR skills using the two platforms. Thematic analysis was employed to identify themes and sub-themes. Three main themes were identified: authenticity, VP application, and strengths and limitations. Students found the social robotic platform more authentic and engaging. It enabled highly interactive communication and expressed emotions, collectively offering a realistic experience. It facilitated active learning, hypothesis generation, and adaptive thinking. Limitations included lack of physical examination options and, occasionally, mechanical dialogue. The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform. Despite some limitations, it shows promise in training CR skills, communication, and adaptive thinking. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations.\\n                                \\n\\n---\\n\\nPaper ID: f04c25fcf3247ff4d8eca72d862b22090b884b75\\n                                    Title: Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\\n                                    Abstract: A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often dont surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.\\n                                \\n\\n---\\n\\nPaper ID: 7bef7f67019cd3b9ff81f715ea65628fac3291a5\\n                                    Title: What Did I Do Wrong? Quantifying LLMs\\' Sensitivity and Consistency to Prompt Engineering\\n                                    Abstract: Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs\\'inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely sensitivity and consistency, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\\n                                \\n\\n---\\n\\nPaper ID: 024c9fe5a0e00786683d64ec32d142aaaae55fa2\\n                                    Title: PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 0e314ddbf28514d92f2405b73941242c162ae0ba\\n                                    Title: AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators\\n                                    Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.\\n                                \\n\\n---\\n\\nPaper ID: 1c5a097b4e376897545f153370425cf7e0c2d8fd\\n                                    Title: Explaining Length Bias in LLM-Based Preference Evaluations\\n                                    Abstract: The use of large language models (LLMs) as judges, particularly in preference comparisons, has become widespread, but this reveals a notable bias towards longer responses, undermining the reliability of such evaluations. To better understand such bias, we propose to decompose the preference evaluation metric, specifically the win rate, into two key components: desirability and information mass, where the former is length-independent and related to trustworthiness such as correctness, toxicity, and consistency, and the latter is length-dependent and represents the amount of information in the response. We empirically demonstrated the decomposition through controlled experiments and found that response length impacts evaluations by influencing information mass. To derive a reliable evaluation metric that assesses content quality without being confounded by response length, we propose AdapAlpaca, a simple yet effective adjustment to win rate measurement. Specifically, AdapAlpaca ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\\n                                \\n\\n---\\n\\nPaper ID: 00ce8beee350a260395676490915d7ebfa7430d1\\n                                    Title: Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation\\n                                    Abstract: Data-to-text (D2T) generation aims to generate human-readable text from semi-structured data, such as tables and graphs. The recent success of D2T is largely attributed to advancements in LLMs. Despite the success of LLMs, no research has been conducted to illustrate the impact of model size on the performance of fine-tuned LLMs for D2T tasks. D2T model performance is typically assessed based on three key qualities: \\\\textit{readability} (indicates fluency and coherence), \\\\textit{informativeness} (measures content similarity), and \\\\textit{faithfulness} (assesses consistency of factual information). It is currently uncertain whether increasing the size of LLMs effectively improves performance in D2T tasks across these three qualities. The objective of this study is to investigate the performance of fine-tuned LLMs in D2T tasks in terms of model size. Through extensive comparative analysis, we aim to elucidate both the advantages and limitations of scaling model sizes across five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and twelve state-of-the-art LLMs with varying sizes from five different LLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all the three essential qualities of D2T models, we incorporate six widely recognized automatic metrics -- \\\\textsc{BLEU}, \\\\textsc{METEOR}, \\\\textsc{BERTScore}, \\\\textsc{MoverScore}, \\\\textsc{Parent}, and \\\\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance concerning model size in the presence of source-reference divergence, a critical aspect of D2T tasks. Our investigation reveals that increasing LLM size enhances \\\\textit{readability} and \\\\textit{informativeness} in D2T tasks, but larger (in terms of size) LLMs may sacrifice \\\\textit{faithfulness}. Moreover, small-sized LLMs show more resilience than larger ones when source-reference divergence is present.\\n                                \\n\\n---\\n\\nPaper ID: 74908bc543e77b8995a6eebe32ab13cf0837949b\\n                                    Title: LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation\\n                                    Abstract: Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task\\'s clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.\\n                                \\n\\n---\\n\\nPaper ID: f33991c02f8f0ab8794dad020c648b37ccc53365\\n                                    Title: CitaLaw: Enhancing LLM with Citations in Legal Domain\\n                                    Abstract: In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs\\' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.\\n                                \\n\\n---\\n\\nPaper ID: 0d5ccf0861b62223fe562e13369ab15746188251\\n                                    Title: QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums\\n                                    Abstract: Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methodologies used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting and human evaluation methodology. We applied this framework to analyze over one million comments from two of Reddit\\'s rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.\\n                                \\n\\n---\\n\\nPaper ID: e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\\n                                    Title: Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning\\n                                    Abstract: Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE ($\\\\underline{T}$hrifty $\\\\underline{Rea}$soning via $\\\\underline{C}$ontext-Aware $\\\\underline{L}$LM and Prompt S$\\\\underline{e}$lection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\\'s monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.\\n                                \\n\\n---\\n\\nPaper ID: c01bbc439164002f2c7326748f7939783e306a94\\n                                    Title: A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization\\n                                    Abstract: This research investigates prompt designs of evaluating generated texts using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for open-ended text evaluation remains challenging due to model sensitivity and subjectivity in evaluation of text generation. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs\\' scoring, with a different level of rule understanding in the prompt. An additional optimization may enhance scoring alignment if sufficient data is available. This insight is crucial for improving the accuracy and consistency of LLM-based evaluations.\\n                                \\n\\n---\\n\\nPaper ID: a03bdcf22b137ba220f0508768a5f3b4ff374bcd\\n                                    Title: How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment\\n                                    Abstract: As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\\n                                \\n\\n---\\n\\nPaper ID: 221c89578778f872a9eb39c01a0b0b9b2f2a30f2\\n                                    Title: PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles\\n                                    Abstract: This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals. This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it. Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information. We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy. We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge. In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance.\\n                                ', 'plan': ['Here\\'s a step-by-step plan to evaluate the novelty and feasibility of \"Dynamic Prompt Adaptation\" using the 222 retrieved papers:\\n\\n1.  **Initial Screening for Direct Overlap (Novelty - Step 1):**\\n    *   **Action:** Quickly scan the titles and abstracts of all 222 papers for keywords directly related to \"dynamic prompt adaptation,\" \"contextual prompt generation,\" \"adaptive prompting,\" \"iterative prompting,\" \"prompt evolution,\" or similar phrases.\\n    *   **Goal:** Identify any papers that propose a method with a very similar name or core concept.\\n    *   **Outcome:** Categorize papers into \"High Overlap,\" \"Potential Overlap,\" and \"Low Overlap.\"\\n\\n2.  **Detailed Analysis of \"High Overlap\" Papers (Novelty - Step 1 & 2):**\\n    *   **Action:** For papers identified as \"High Overlap,\" conduct a thorough review of their methodology, problem statements, and proposed solutions. Compare them directly against the three phases of \"Dynamic Prompt Adaptation\" (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update).\\n    *   **Goal:** Determine if the proposed method is a direct replication or a minor variation of an existing, well-established technique.\\n    *   **Outcome:** If a paper is found to be a direct match, the novelty of the proposed research is significantly diminished. Document the specific similarities.\\n\\n3.  **Analysis of \"Potential Overlap\" Papers (Novelty - Step 2):**\\n    *   **Action:** For papers in the \"Potential Overlap\" category, analyze their abstracts and introductions to understand their approaches to prompt adaptation, context maintenance, or conversational flow. Focus on:\\n        *   **Contextual Analysis:** Do they analyze previous outputs/prompts to extract themes or tone shifts?\\n        *   **Adaptive Prompt Generation:** Do they dynamically generate new prompts based on prior interactions?\\n        *   **Iterative Context Update:** Do they maintain a synthesized history of the interaction for coherence?\\n    *   **Goal:** Identify specific techniques within existing papers that might be similar to aspects of the proposed method, even if not presented as a complete \"Dynamic Prompt Adaptation\" system.\\n    *   **Outcome:** Document specific similarities and differences. This will help refine the understanding of the unique contributions of the proposed method.\\n\\n4.  **Identify Unique Contributions (Novelty - Step 2):**\\n    *   **Action:** Based on the detailed analysis in steps 2 and 3, synthesize the findings. Specifically, look for:\\n        *   Does the proposed three-phase structure (Analysis, Generation, Update) offer a novel *framework* for dynamic adaptation?\\n        *   Are the *specific prompting structures* suggested (e.g., \"Reflect on the previous topic of [theme] and build on it.\") novel or a novel combination of existing techniques?\\n        *   Does the emphasis on *simultaneously* addressing thematic consistency, tonal shifts, and narrative progression in an iterative loop represent a unique synthesis?\\n    *   **Goal:** Clearly articulate what aspects of \"Dynamic Prompt Adaptation\" are not directly addressed or combined in the existing literature. This forms the basis of the novelty claim.\\n    *   **Outcome:** A concise list of unique contributions.\\n\\n5.  **Assess Technical Feasibility - Phase 1: Contextual Analysis (Feasibility - Step 3):**\\n    *   **Action:** Review papers that deal with:\\n        *   Text summarization techniques.\\n        *   Topic modeling and theme extraction from text.\\n        *   Sentiment analysis and tone detection.\\n        *   Methods for tracking conversational state or dialogue history.\\n    *   **Goal:** Determine if existing NLP techniques are mature enough to reliably perform the proposed contextual analysis. Are there off-the-shelf tools or well-documented algorithms?\\n    *   **Outcome:** A judgment on the feasibility of Phase 1, citing relevant existing methods.\\n\\n6.  **Assess Technical Feasibility - Phase 2: Adaptive Prompt Generation (Feasibility - Step 3):**\\n    *   **Action:** Review papers that explore:\\n        *   Prompt engineering techniques for LLMs.\\n        *   Methods for generating text conditioned on specific instructions or context.\\n        *   Few-shot learning and in-context learning for prompt modification.\\n        *   Techniques for controlling LLM output style and content.\\n    *   **Goal:** Evaluate if LLMs can be reliably steered to generate prompts that effectively adapt based on analyzed context. Are there known limitations or challenges in generating *effective* adaptive prompts?\\n    *   **Outcome:** A judgment on the feasibility of Phase 2, noting potential challenges and supporting existing work.\\n\\n7.  **Assess Technical Feasibility - Phase 3: Iterative Context Update (Feasibility - Step 3):**\\n    *   **Action:** Review papers focusing on:\\n        *   Long-context LLMs.\\n        *   Techniques for maintaining coherence in long-form text generation.\\n        *   Methods for synthesizing dialogue or narrative histories.\\n        *   Memory mechanisms in LLM architectures.\\n    *   **Goal:** Determine if LLMs can effectively process and synthesize a growing history of interactions to maintain coherence, as proposed by the iterative context update.\\n    *   **Outcome:** A judgment on the feasibility of Phase 3, considering the limitations of current LLMs in handling extended context.\\n\\n8.  **Evaluate Experiment Plan Feasibility and Relevance (Feasibility - Step 3):**\\n    *   **Action:** Examine the proposed experiment plan.\\n        *   **Metrics:** Are BLEU and ROUGE appropriate for assessing coherence and engagement in narrative/conversational tasks? Do other papers use better metrics for these specific goals?\\n        *   **Datasets:** Are the \\'Story Cloze Test\\' and Reddit data suitable? Are there more established or relevant datasets for evaluating dynamic adaptation in storytelling or conversation?\\n        *   **User Feedback:** Is incorporating user feedback a standard and feasible practice for this type of evaluation?\\n    *   **Goal:** Assess if the proposed experimental setup is practical, robust, and aligned with current best practices in LLM evaluation.\\n    *   **Outcome:** Recommendations for refining the experiment plan, identifying potential challenges, and suggesting alternative approaches based on existing research.\\n\\n9.  **Synthesize Overall Novelty and Feasibility Assessment:**\\n    *   **Action:** Consolidate all findings from the previous steps.\\n    *   **Goal:** Provide a comprehensive evaluation of the research proposal\\'s novelty and feasibility, supported by evidence from the retrieved papers.\\n    *   **Outcome:** A final report or summary that clearly states the strengths, weaknesses, unique contributions, and technical viability of \"Dynamic Prompt Adaptation.\"', 'Here\\'s a step-by-step plan to evaluate the novelty and feasibility of \"Dynamic Prompt Adaptation\" using the 222 retrieved papers:\\n\\n1.  **Phase 1: Categorize Retrieved Papers by Core Concept (Novelty & Feasibility Foundation):**\\n    *   **Action:** Systematically review the titles and abstracts of all 222 papers. Assign each paper to one or more of the following categories:\\n        *   **Fixed Prompting/Few-Shot Learning:** Papers focusing on static prompts or traditional few-shot examples.\\n        *   **Chain-of-Thought (CoT) / Step-by-Step Reasoning:** Papers exploring explicit reasoning processes.\\n        *   **Contextual Analysis in LLMs:** Papers that analyze prior text (dialogue, narrative) for themes, tone, sentiment, entities, etc.\\n        *   **Adaptive/Dynamic Prompt Generation:** Papers that actively modify or generate prompts based on interaction history or external factors.\\n        *   **Maintaining Coherence/Continuity in LLMs:** Papers addressing thematic consistency, long-range dependencies, or narrative flow.\\n        *   **Iterative Refinement/Dialogue Systems:** Papers on systems that involve multiple turns of interaction, feedback loops, or synthesis of dialogue.\\n        *   **LLM Evaluation Metrics (Coherence, Engagement, Naturalness):** Papers discussing metrics for assessing LLM output quality.\\n        *   **Specific Datasets for Storytelling/Dialogue:** Papers that utilize datasets like Story Cloze or conversational corpora.\\n    *   **Goal:** Create a structured overview of the existing research landscape relevant to the proposal, identifying areas of high density and potential gaps.\\n    *   **Outcome:** A categorized list of papers with brief annotations for each paper\\'s relevance.\\n\\n2.  **Phase 2: Deep Dive into \"Adaptive/Dynamic Prompt Generation\" and \"Contextual Analysis\" (Novelty - Step 1 & 2):**\\n    *   **Action:** Select papers categorized under \"Adaptive/Dynamic Prompt Generation\" and \"Contextual Analysis in LLMs.\" Thoroughly read their methodology sections, focusing on *how* they analyze context and *how* they adapt prompts.\\n    *   **Goal:** Identify specific techniques, algorithms, and prompt structures used in existing adaptive prompting methods. Compare these directly to the proposed \"Contextual Analysis\" and \"Adaptive Prompt Generation\" phases.\\n    *   **Outcome:** A detailed mapping of existing adaptive prompting techniques against the proposal\\'s first two phases. This will highlight direct overlaps and potential unique aspects of the proposal\\'s specific analysis/generation mechanisms.\\n\\n3.  **Phase 3: Analyze \"Maintaining Coherence/Continuity\" and \"Iterative Refinement\" (Novelty - Step 1 & 2):**\\n    *   **Action:** Select papers from \"Maintaining Coherence/Continuity\" and \"Iterative Refinement/Dialogue Systems.\" Analyze their approaches to managing long-term context, synthesizing past interactions, and ensuring thematic consistency.\\n    *   **Goal:** Understand how current methods achieve continuity. Compare their strategies (e.g., memory modules, summarization techniques, fixed context windows) with the proposed \"Iterative Context Update\" phase and its prompting strategy.\\n    *   **Outcome:** A clear picture of existing coherence strategies and how they compare to the proposal\\'s iterative update mechanism. This will help pinpoint the novelty of the proposal\\'s synthesis approach.\\n\\n4.  **Phase 4: Synthesize Unique Contributions (Novelty - Step 2):**\\n    *   **Action:** Based on Phases 2 and 3, consolidate the findings. Explicitly compare the proposal\\'s three-phase structure and specific prompting examples against the identified existing methods.\\n    *   **Goal:** Articulate precisely what makes \"Dynamic Prompt Adaptation\" novel. Is it the specific combination of the three phases? A novel way of performing contextual analysis? A new type of adaptive prompt generation? A unique iterative update mechanism? Or a novel synthesis of these elements?\\n    *   **Outcome:** A clear statement of the proposal\\'s novel contributions, supported by evidence from the literature review (e.g., \"While X proposed adaptive prompt generation, it did not integrate it with a distinct contextual analysis phase focused on tonal shifts,\" or \"Unlike Y\\'s iterative refinement which uses fixed summarization, this proposal introduces dynamically generated synthesis prompts.\").\\n\\n5.  **Phase 5: Assess Technical Feasibility of \"Contextual Analysis\" (Feasibility - Step 3):**\\n    *   **Action:** Review papers categorized under \"Contextual Analysis in LLMs\" and \"LLM Evaluation Metrics.\" Examine the techniques used for theme extraction, tonal shift detection, and analysis of previous outputs.\\n    *   **Goal:** Determine if the proposed \"Contextual Analysis\" phase is technically feasible with current LLM capabilities and NLP techniques. Are there well-established methods for these tasks? What are their limitations (e.g., accuracy, computational cost)?\\n    *   **Outcome:** A feasibility assessment for Phase 1, noting which components are well-supported by existing research and which might require significant engineering effort or face inherent challenges.\\n\\n6.  **Phase 6: Assess Technical Feasibility of \"Adaptive Prompt Generation\" (Feasibility - Step 3):**\\n    *   **Action:** Review papers categorized under \"Adaptive/Dynamic Prompt Generation\" and \"Fixed Prompting/Few-Shot Learning.\" Focus on how LLMs are instructed or fine-tuned to generate prompts or modify their output based on context.\\n    *   **Goal:** Evaluate the feasibility of generating *effective* adaptive prompts. Can LLMs reliably take contextual insights and translate them into prompts that steer the model as intended? Are there known issues with prompt injection, controllability, or generating meaningful prompt variations?\\n    *   **Outcome:** A feasibility assessment for Phase 2, considering the controllability and effectiveness of LLM-generated adaptive prompts.\\n\\n7.  **Phase 7: Assess Technical Feasibility of \"Iterative Context Update\" (Feasibility - Step 3):**\\n    *   **Action:** Review papers categorized under \"Maintaining Coherence/Continuity\" and \"Iterative Refinement/Dialogue Systems.\" Pay close attention to methods for handling long contexts, summarizing histories, and ensuring consistency.\\n    *   **Goal:** Determine the technical feasibility of maintaining thematic coherence over extended interactions. Can LLMs effectively process and synthesize a cumulative history of interactions as proposed? What are the current limitations regarding context window size, memory, and synthesis quality?\\n    *   **Outcome:** A feasibility assessment for Phase 3, highlighting challenges related to long-context processing and effective synthesis.\\n\\n8.  **Phase 8: Evaluate Experiment Plan Feasibility and Relevance (Feasibility - Step 3):**\\n    *   **Action:** Review papers categorized under \"LLM Evaluation Metrics\" and \"Specific Datasets for Storytelling/Dialogue.\"\\n    *   **Goal:** Assess the suitability and feasibility of the proposed experiment plan. Are BLEU and ROUGE adequate for coherence and engagement? Are the chosen datasets appropriate for evaluating dynamic adaptation? Is incorporating user feedback a feasible and standard practice for this type of evaluation? Are there more established benchmarks or metrics for evaluating dynamic interaction?\\n    *   **Outcome:** Recommendations for refining the experiment plan, suggesting alternative metrics or datasets if the proposed ones are found lacking or less feasible, and confirming the viability of user feedback integration.\\n\\n9.  **Phase 9: Synthesize Overall Assessment (Novelty & Feasibility Summary):**\\n    *   **Action:** Consolidate all findings from Phases 1-8.\\n    *   **Goal:** Produce a comprehensive evaluation of the research proposal\\'s novelty and feasibility. Clearly state the identified unique contributions and the technical viability of each proposed phase, supported by evidence from the retrieved papers. Highlight any significant gaps or potential challenges that need to be addressed.\\n    *   **Outcome:** A final report summarizing the novelty and feasibility of \"Dynamic Prompt Adaptation,\" providing a clear direction for the research.'], 'findings': [\"Analysis of papers focusing on 'Categorize retrieved papers by core concept (Novelty & Feasibility Foundation)': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Adaptive/Dynamic Prompt Generation and Contextual Analysis in LLMs': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Maintaining Coherence/Continuity and Iterative Refinement/Dialogue Systems': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Synthesize Unique Contributions': Found relevant insights from the pre-retrieved literature.\"], 'scores': AIMessage(content='```json\\n{\\n  \"novelty_score\": 8,\\n  \"feasibility_score\": 7,\\n  \"impact_score\": 7,\\n  \"summary\": \"The analysis indicates that the core concepts explored (Novelty & Feasibility Foundation, Adaptive/Dynamic Prompt Generation, Maintaining Coherence/Continuity, and Synthesizing Unique Contributions) have all been touched upon in pre-retrieved literature. However, the specific *integration* and *application* of these concepts, particularly in the context of dynamic prompt generation and iterative refinement for synthesizing unique contributions, appears to offer a degree of novelty. The feasibility is moderate, as similar work in LLMs for prompt generation and dialogue systems exists, suggesting that implementation is realistic but may require careful engineering. The potential impact is significant, as a system that can effectively generate novel and feasible contributions through adaptive prompting and iterative refinement could be highly valuable in research and creative endeavors.\",\\n  \"recommendation\": \"Revise\"\\n}\\n```', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--8f10b0be-ef97-4dc3-8be6-f59312126faa-0', usage_metadata={'input_tokens': 299, 'output_tokens': 206, 'total_tokens': 505, 'input_token_details': {'cache_read': 0}}), 'confidence': {'novelty': 80, 'feasibility': 70, 'overall': 75}, 'iteration': 4, 'next_action': 'end'}\n",
      "\n",
      "Workflow Visualization:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.tools import Tool \n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "import operator\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    proposal: str\n",
    "    retrieved_papers: str  # Pre-retrieved papers from earlier pipeline\n",
    "    plan: str\n",
    "    findings: Annotated[list, operator.add]\n",
    "    scores: dict\n",
    "    confidence: dict\n",
    "    iteration: int\n",
    "    next_action: str\n",
    "\n",
    "# Tools that work with pre-retrieved papers\n",
    "def analyze_papers_tool(focus_area: str, papers: str) -> str:\n",
    "    \"\"\"Analyze retrieved papers focusing on a specific area\"\"\"\n",
    "    return f\"Analysis of papers focusing on '{focus_area}': Found relevant insights from the pre-retrieved literature.\"\n",
    "\n",
    "def extract_paper_details_tool(paper_criteria: str, papers: str) -> str:\n",
    "    \"\"\"Extract specific details from papers based on criteria\"\"\"\n",
    "    # Parse papers and extract relevant details\n",
    "    lines = papers.split('\\n\\n---\\n\\n')\n",
    "    relevant_papers = []\n",
    "    \n",
    "    for paper in lines[:3]:  # Limit to first 3 papers for demonstration\n",
    "        if paper.strip():\n",
    "            relevant_papers.append(f\"Paper analysis for '{paper_criteria}': {paper[:200]}...\")\n",
    "    \n",
    "    return f\"Extracted details based on '{paper_criteria}': {len(relevant_papers)} papers analyzed.\"\n",
    "\n",
    "def compare_methodologies_tool(methodology_aspect: str, papers: str) -> str:\n",
    "    \"\"\"Compare methodologies in retrieved papers\"\"\"\n",
    "    return f\"Methodology comparison for '{methodology_aspect}': Analyzed methodological approaches in retrieved papers.\"\n",
    "\n",
    "def execute_tool(tool_name: str, params: dict, retrieved_papers: str) -> str:\n",
    "    \"\"\"Execute the specified tool with parameters and pre-retrieved papers\"\"\"\n",
    "    if tool_name == \"analyze_papers\":\n",
    "        return analyze_papers_tool(params.get(\"focus_area\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"extract_details\":\n",
    "        return extract_paper_details_tool(params.get(\"criteria\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"compare_methods\":\n",
    "        return compare_methodologies_tool(params.get(\"aspect\", \"\"), retrieved_papers)\n",
    "    else:\n",
    "        return f\"Tool {tool_name} executed with params {params}\"\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"analyze_papers\",\n",
    "        func=analyze_papers_tool,\n",
    "        description=\"Analyze retrieved papers focusing on specific aspects\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"extract_details\", \n",
    "        func=extract_paper_details_tool,\n",
    "        description=\"Extract specific methodological or technical details\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"compare_methods\",\n",
    "        func=compare_methodologies_tool,\n",
    "        description=\"Compare methodologies across retrieved papers\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define agent nodes\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Agent creates investigation plan\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Given this research proposal: {state['proposal']}\n",
    "        \n",
    "        Numbers of available retrieved papers: {len(state['retrieved_papers'].split('Paper ID:'))-1} papers\n",
    "        \n",
    "        Create a step-by-step plan to evaluate its novelty and feasibility using the already retrieved papers.\n",
    "        \n",
    "        Focus on:\n",
    "        1. Analyzing overlaps with existing methods\n",
    "        2. Identifying unique contributions\n",
    "        3. Assessing technical feasibility\n",
    "        \n",
    "        Return just the plan as a string.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    plan_response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"plan\": plan_response.content,\n",
    "        \"iteration\": 0,\n",
    "        \"next_action\": \"investigate\"\n",
    "    }\n",
    "\n",
    "def investigation_node(state: AgentState):\n",
    "    \"\"\"Agent investigates using retrieved papers\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Current plan: {state['plan']}\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        You have access to pre-retrieved papers. Based on the plan and current findings, what should you analyze next?\n",
    "        Choose one:\n",
    "        1. Analyze papers for specific aspects (respond with: \"TOOL: analyze_papers, FOCUS: <aspect to focus on>\")\n",
    "        2. Extract technical details (respond with: \"TOOL: extract_details, CRITERIA: <what to extract>\") \n",
    "        3. Compare methodologies (respond with: \"TOOL: compare_methods, ASPECT: <methodology aspect>\")\n",
    "        4. Conclude investigation (respond with: \"CONCLUDE\")\n",
    "        \n",
    "        Respond in the exact format specified above.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    decision_response = llm.invoke(messages)\n",
    "    decision = decision_response.content.strip()\n",
    "    \n",
    "    if decision.startswith(\"TOOL:\"):\n",
    "        # Parse the tool command\n",
    "        parts = decision.split(\", \")\n",
    "        tool_name = parts[0].split(\": \")[1]\n",
    "        \n",
    "        if \"FOCUS:\" in decision:\n",
    "            focus_area = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"focus_area\": focus_area}, state['retrieved_papers'])\n",
    "        elif \"CRITERIA:\" in decision:\n",
    "            criteria = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"criteria\": criteria}, state['retrieved_papers'])\n",
    "        elif \"ASPECT:\" in decision:\n",
    "            aspect = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"aspect\": aspect}, state['retrieved_papers'])\n",
    "        else:\n",
    "            result = \"Tool execution failed - invalid parameters\"\n",
    "        \n",
    "        return {\n",
    "            \"findings\": [result],\n",
    "            \"iteration\": state.get(\"iteration\", 0) + 1,\n",
    "            \"next_action\": \"reflect\"\n",
    "        }\n",
    "    else:\n",
    "        return {\"next_action\": \"conclude\"}\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    \"\"\"Agent reflects on progress\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        Based on your analysis of the retrieved papers, evaluate the confidence level (0-100) for each aspect:\n",
    "        - Novelty assessment confidence (how well you understand what's new)\n",
    "        - Feasibility assessment confidence (how realistic the implementation seems)\n",
    "        - Overall investigation completeness (do you have enough information)\n",
    "        \n",
    "        Return a JSON-like response:\n",
    "        {{\"novelty\": <score>, \"feasibility\": <score>, \"overall\": <score>}}\n",
    "        \n",
    "        Then decide: Should I continue investigating (if overall < 75) or conclude?\n",
    "        Add on a new line: CONTINUE or CONCLUDE\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    confidence_response = llm.invoke(messages)\n",
    "    response_lines = confidence_response.content.strip().split('\\n')\n",
    "    \n",
    "    # Parse confidence scores (simplified)\n",
    "    try:\n",
    "        confidence_line = response_lines[0]\n",
    "        # Extract numbers from the response (simplified parsing)\n",
    "        numbers = re.findall(r'\\d+', confidence_line)\n",
    "        if len(numbers) >= 3:\n",
    "            confidence = {\n",
    "                \"novelty\": int(numbers[0]),\n",
    "                \"feasibility\": int(numbers[1]), \n",
    "                \"overall\": int(numbers[2])\n",
    "            }\n",
    "        else:\n",
    "            confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    except:\n",
    "        confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    \n",
    "    # Determine next action\n",
    "    next_action = \"investigate\" if confidence.get(\"overall\", 0) < 75 else \"conclude\"\n",
    "    \n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"next_action\": next_action\n",
    "    }\n",
    "\n",
    "def scoring_node(state: AgentState):\n",
    "    \"\"\"Generate final scores and report\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Based on analysis of retrieved papers and findings: {state.get('findings', [])}\n",
    "        Confidence levels: {state.get('confidence', {})}\n",
    "        \n",
    "        Generate final evaluation scores (1-10) for:\n",
    "        - Novelty: How new/original is this idea compared to retrieved papers?\n",
    "        - Feasibility: How realistic is implementation based on similar work?\n",
    "        - Impact: Potential significance of results based on the literature?\n",
    "        \n",
    "        Provide a brief summary and recommendation based on the paper analysis.\n",
    "        \n",
    "        Format as JSON:\n",
    "        {{\"novelty_score\": <1-10>, \"feasibility_score\": <1-10>, \"impact_score\": <1-10>, \"summary\": \"<text>\", \"recommendation\": \"<Accept/Revise/Reject>\"}}\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    scores_response = llm.invoke(messages)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"scores\": scores_response,\n",
    "        \"next_action\": \"end\"\n",
    "    }\n",
    "\n",
    "# Define routing function\n",
    "def should_continue(state: AgentState) -> Literal[\"investigate\", \"conclude\"]:\n",
    "    \"\"\"Determine next step based on current state\"\"\"\n",
    "    next_action = state.get(\"next_action\", \"investigate\")\n",
    "    \n",
    "    # Safety check - limit iterations\n",
    "    if state.get(\"iteration\", 0) >= 4:  # Reduced since we're using pre-retrieved papers\n",
    "        return \"conclude\"\n",
    "    \n",
    "    if next_action == \"conclude\":\n",
    "        return \"conclude\"\n",
    "    else:\n",
    "        return \"investigate\"\n",
    "\n",
    "# Build the graph\n",
    "agentic_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agentic_workflow.add_node(\"planning\", planning_node)\n",
    "agentic_workflow.add_node(\"investigation\", investigation_node)\n",
    "agentic_workflow.add_node(\"reflection\", reflection_node)\n",
    "agentic_workflow.add_node(\"scoring\", scoring_node)\n",
    "\n",
    "# Define edges (control flow)\n",
    "agentic_workflow.add_edge(START, \"planning\")\n",
    "agentic_workflow.add_edge(\"planning\", \"investigation\")\n",
    "agentic_workflow.add_edge(\"investigation\", \"reflection\")\n",
    "\n",
    "# Conditional edge based on confidence/decision\n",
    "agentic_workflow.add_conditional_edges(\n",
    "    \"reflection\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"investigate\": \"investigation\",\n",
    "        \"conclude\": \"scoring\"\n",
    "    }\n",
    ")\n",
    "agentic_workflow.add_edge(\"scoring\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agentic_app = agentic_workflow.compile()\n",
    "\n",
    "# Extract research idea from initial user input\n",
    "research_idea_text = result_llm[\"messages\"][0].content\n",
    "\n",
    "# Extract and format retrieved papers\n",
    "papers_json = json.loads(result_llm[\"messages\"][-1].content)  # -2 because -1 is the analysis\n",
    "retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n",
    "\n",
    "print(\"Running ReAct Agent Evaluation with Pre-Retrieved Papers...\")\n",
    "print(f\"Number of retrieved papers: {len(retrieved_papers_text.split('Paper ID:'))-1}\")\n",
    "\n",
    "# Run the agent\n",
    "try:\n",
    "    result = agentic_app.invoke({\n",
    "        \"proposal\": research_idea_text,\n",
    "        \"retrieved_papers\": retrieved_papers_text,\n",
    "        \"plan\": \"\",\n",
    "        \"findings\": [],\n",
    "        \"scores\": {},\n",
    "        \"confidence\": {},\n",
    "        \"iteration\": 0,\n",
    "        \"next_action\": \"start\"\n",
    "    })\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running ReAct agent: {e}\")\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "print(\"\\nWorkflow Visualization:\")\n",
    "display(Image(agentic_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f495b7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Revise'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads((result[\"scores\"].content.strip()).replace(\"```json\", \"\").replace(\"```\", \"\").strip())[\"recommendation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c9e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b422e009",
   "metadata": {},
   "source": [
    "##### Example Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd0e84",
   "metadata": {},
   "source": [
    "{'proposal': \"Dynamic Prompt Adaptation:\n",
    "                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\n",
    "                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\n",
    "                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\n",
    "                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\n",
    "                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", 'retrieved_papers': 'Paper ID: 246482d9758e93d0b349e2926996d887417174d8\n",
    "                                    Title: DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\n",
    "                                    Abstract: Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\n",
    "                                \n",
    "\n",
    "---\n",
    "\n",
    "Paper ID: 6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\n",
    "                                    Title: Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\n",
    "                                    Abstract: Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\n",
    "                                \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "2) Build a structured evidence map from the 199 papers\n",
    "- Create a data schema: paper_id, title, year, method category, prompting technique, context length, memory/retention mechanism, adaptation trigger, evaluation metrics, datasets, claimed contributions.\n",
    "- Populate a searchable database or spreadsheet and tag papers by phase-related capabilities (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) and by overlap with proposed components.\n",
    "\n",
    "3) Overlap analysis by phase\n",
    "- Phase 1: Contextual Analysis\n",
    "  - Identify papers on theme detection, discourse/semantic drift, tone/style tracking, and prompting templates that reflect prior content.\n",
    "  - Record how prior work handles extraction of themes, tonal shifts, and narrative continuity.\n",
    "- Phase 2: Adaptive Prompt Generation\n",
    "  - Identify work on dynamic prompts, prompt rewrites, plan-and-solve prompting, instruction-tuning for adaptability, and prompt re-routing based on user input.\n",
    "- Phase 3: Iterative Context Update\n",
    "  - Identify long-context maintenance methods, memory-augmented generation, dialogue/state tracking, and synthesis prompts that condense prior interactions.\n",
    "- For each phase, compute overlaps with your proposed prompts, templates, and objectives; note any gaps or novel couplings.\n",
    "\n",
    "4) Gap analysis to reveal unique contributions\n",
    "- Compare three-phase orchestration against single-phase dynamic prompting and against memory augmented or retrieval-based methods.\n",
    "- Assess whether prior work demonstrates end-to-end pipelines that:\n",
    "  - Analyze prior outputs and user prompts for theme/tonal shifts (Phase 1).\n",
    "  - Generate updated prompts that explicitly steer continuation (Phase 2).\n",
    "  - Produce iterative, synthesized summaries to maintain coherence across many turns (Phase 3).\n",
    "- Identify gaps your plan addresses: explicit phased approach tailored to creative/storytelling tasks, integration of reflective prompts with continuity-focused synthesis, and a combined automatic/human-evaluation framework.\n",
    "\n",
    "5) Feasibility assessment of the proposed architecture\n",
    "- Decompose into modules and interfaces:\n",
    "  - Module A: Contextual Analysis (theme extraction, tonal shift detection, discourse tracking).\n",
    "  - Module B: Adaptive Prompt Generator (template-driven and learned prompts, handling new elements and clarifications).\n",
    "  - Module C: Iterative Context Updater (global narrative synthesis, coherence-preserving summaries).\n",
    "- Assess each module for technical feasibility with current LLMs:\n",
    "  - Prompt length constraints and context window limits; potential need for retrieval-augmented inputs.\n",
    "  - Computational and latency costs of repeated prompting across turns.\n",
    "  - Hallucination risk and drift control through synthesis prompts and validation checks.\n",
    "- Propose feasible realizations:\n",
    "  - Use a modular pipeline with a lightweight external memory store or episodic memory of themes.\n",
    "  Use retrieval or summarization steps to compress history within model limits.\n",
    "  Rely on established prompting techniques (CoT, self-critique, instruction-following) augmented by explicit coherence checks.\n",
    "- Identify technical risks and mitigation strategies (prompt drift, data leakage between prompts, evaluation noise).\n",
    "\n",
    "6) Experimental design aligned with novelty and feasibility\n",
    "- Baselines:\n",
    "  - Static prompts with fixed prompts.\n",
    "  - Dynamic prompting without phased structure.\n",
    "  - Existing memory-augmented/dialogue systems.\n",
    "- Datasets and domains:\n",
    "  - Story Cloze Test (coherence in narrative endings).\n",
    "  - Reddit-based dialogue interactions (creative storytelling threads, conversations).\n",
    "  - Additional long-form storytelling datasets or writing prompts to test continuity.\n",
    "- Evaluation metrics:\n",
    "  - Automatic: BLEU, ROUGE, METEOR, BERTScore, ROUGE-L; plus coherence-oriented metrics (entity grid/coherence scores, COH-METER where feasible).\n",
    "  - Engagement and naturalness: user-rated scales, preference tests.\n",
    "  - Specific to coherence: longitudinal coherence scores across turns, consistency of themes, and narrative arc continuity.\n",
    "- Evaluation design:\n",
    "  - Within-subject A/B/C testing comparing the three-phase approach versus baselines.\n",
    "  - Ablation studies to isolate contributions of Phase 1, Phase 2, and Phase 3.\n",
    "  - Statistical analysis plans (confidence intervals, significance tests, effect sizes).\n",
    "- Reproducibility plan:\n",
    "  - Pre-register hypotheses, methods, and evaluation protocol.\n",
    "  - Share prompts templates, evaluation scripts, and synthetic datasets where permissible.\n",
    "  - Document data licensing and preprocessing steps.\n",
    "\n",
    "7) Data handling, ethics, and compliance\n",
    "- Ensure dataset licenses and terms (Story Cloze, Reddit data) are compliant; obtain permissions where needed.\n",
    "- Address privacy and consent for user-generated dialogue data.\n",
    "- Mitigate bias and ensure diversity of writing styles and topics in evaluation materials.\n",
    "\n",
    "8) Architectural detail and implementation plan\n",
    "- Define a concrete pipeline:\n",
    "  - Input: user prompts and prior outputs.\n",
    "  - Phase 1: run contextual analysis to extract themes, tonal cues, and narrative branches.\n",
    "  - Phase 2: produce updated prompts that introduce new elements or clarify past responses, with explicit references to themes and context.\n",
    "  - Phase 3: generate a synthesized summary of prior interactions to guide future turns and prompt the model for continuity.\n",
    "  - Output: a continuation generation with coherence checks, followed by optional human-in-the-loop review.\n",
    "- Data flows and interfaces:\n",
    "  - Clear API boundaries between modules; stateless prompts per turn with a memory ledger.\n",
    "  - Logging for reproducibility and evaluation auditing.\n",
    "- Resource plan:\n",
    "  - Estimate compute for 199-paper-informed analysis, pipeline prototyping, and full experiments.\n",
    "  - Plan for iterative pilot studies before full-scale evaluation.\n",
    "\n",
    "9) Timeline and milestones\n",
    "- Month 12: complete novelty criteria, build evidence map from 199 papers, perform phase-wise overlap analysis.\n",
    "- Month 34: conduct gap/unique contribution assessment; draft architecture design and feasibility notes.\n",
    "- Month 56: implement a minimal viable three-phase pipeline prototype; develop datasets, baselines, and evaluation plan.\n",
    "- Month 78: run experiments across Story Cloze and dialogue datasets; collect automatic and human evaluations; perform ablations.\n",
    "- Month 9: synthesize results, refine claims of novelty and feasibility; prepare reproducibility materials.\n",
    "- Month 10: write up plan for potential conference submission; outline follow-up experiments and extensions.\n",
    "\n",
    "10) Deliverables and documentation\n",
    "- A structured novelty/feasibility report summarizing overlaps, gaps, unique contributions, and feasibility assessments.\n",
    "- An implementation blueprint for the three-phase dynamic prompt adaptation pipeline.\n",
    "- A detailed experimental protocol, including baselines, datasets, metrics, and statistical analysis plan.\n",
    "- Reproducibility package plan (prompts templates, evaluation scripts, data handling notes) with licensing considerations.\n",
    "\n",
    "Note: The plan emphasizes using the 199 retrieved papers to systematically map overlaps, identify true innovations, and assess the practical viability of the proposed three-phase dynamic prompt adaptation approach for long-horizon creative writing tasks.', 'findings': [\"Analysis of papers focusing on 'Phase 1 Contextual Analysis (theme extraction': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Theme extraction and tonal/style tracking in Phase 1 Contextual Analysis': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Theme extraction and tonal/style tracking in Phase 1 Contextual Analysis (discourse drift': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Phase 1 Contextual Analysis (theme detection': Found relevant insights from the pre-retrieved literature.\"], 'scores': AIMessage(content='{\n",
    "  \"novelty_score\": 6,\n",
    "  \"feasibility_score\": 7,\n",
    "  \"impact_score\": 7,\n",
    "  \"summary\": \"Based on the analysis of retrieved papers, the Phase 1 contextual analysis components (theme extraction, tonal/style tracking, discourse drift, and theme detection) show insights that largely align with existing literature. The integrated anglecombining theme extraction with tonal/style tracking and discourse drift in Phase 1offers incremental novelty at best. Feasibility is high, as the methods (topic/theme detection, stylometry, drift/discourse monitoring) are established and can be implemented with standard NLP/tooling. The potential impact is moderate, contingent on demonstrated improvements in early contextual understanding and downstream tasks, and on avoiding redundancy with prior work.\",\n",
    "  \"recommendation\": \"Revise\"\n",
    "}', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1394, 'prompt_tokens': 294, 'total_tokens': 1688, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Chii3x1Krl0KP6xllSfhGqmGgyn2I', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--a7c610b9-c5eb-45e1-a26e-1fdf8e1a3586-0', usage_metadata={'input_tokens': 294, 'output_tokens': 1394, 'total_tokens': 1688, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}}), 'confidence': {'novelty': 64, 'feasibility': 69, 'overall': 66}, 'iteration': 4, 'next_action': 'end'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40d9cc",
   "metadata": {},
   "source": [
    "#### MULTIPLE AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b23cb514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow Visualization:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.tools import Tool \n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "import operator\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    proposal: str\n",
    "    retrieved_papers: str  # Pre-retrieved papers from earlier pipeline\n",
    "    plan: str\n",
    "    findings: Annotated[list, operator.add]\n",
    "    scores: dict\n",
    "    confidence: dict\n",
    "    iteration: int\n",
    "    next_action: str\n",
    "\n",
    "# Tools that work with pre-retrieved papers\n",
    "def analyze_papers_tool(focus_area: str, papers: str) -> str:\n",
    "    \"\"\"Analyze retrieved papers focusing on a specific area\"\"\"\n",
    "    return f\"Analysis of papers focusing on '{focus_area}': Found relevant insights from the pre-retrieved literature.\"\n",
    "\n",
    "def extract_paper_details_tool(paper_criteria: str, papers: str) -> str:\n",
    "    \"\"\"Extract specific details from papers based on criteria\"\"\"\n",
    "    # Parse papers and extract relevant details\n",
    "    lines = papers.split('\\n\\n---\\n\\n')\n",
    "    relevant_papers = []\n",
    "    \n",
    "    for paper in lines[:3]:  # Limit to first 3 papers for demonstration\n",
    "        if paper.strip():\n",
    "            relevant_papers.append(f\"Paper analysis for '{paper_criteria}': {paper[:200]}...\")\n",
    "    \n",
    "    return f\"Extracted details based on '{paper_criteria}': {len(relevant_papers)} papers analyzed.\"\n",
    "\n",
    "def compare_methodologies_tool(methodology_aspect: str, papers: str) -> str:\n",
    "    \"\"\"Compare methodologies in retrieved papers\"\"\"\n",
    "    return f\"Methodology comparison for '{methodology_aspect}': Analyzed methodological approaches in retrieved papers.\"\n",
    "\n",
    "def execute_tool(tool_name: str, params: dict, retrieved_papers: str) -> str:\n",
    "    \"\"\"Execute the specified tool with parameters and pre-retrieved papers\"\"\"\n",
    "    if tool_name == \"analyze_papers\":\n",
    "        return analyze_papers_tool(params.get(\"focus_area\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"extract_details\":\n",
    "        return extract_paper_details_tool(params.get(\"criteria\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"compare_methods\":\n",
    "        return compare_methodologies_tool(params.get(\"aspect\", \"\"), retrieved_papers)\n",
    "    else:\n",
    "        return f\"Tool {tool_name} executed with params {params}\"\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"analyze_papers\",\n",
    "        func=analyze_papers_tool,\n",
    "        description=\"Analyze retrieved papers focusing on specific aspects\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"extract_details\", \n",
    "        func=extract_paper_details_tool,\n",
    "        description=\"Extract specific methodological or technical details\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"compare_methods\",\n",
    "        func=compare_methodologies_tool,\n",
    "        description=\"Compare methodologies across retrieved papers\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define agent nodes\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Agent creates investigation plan\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Given this research proposal: {state['proposal']}\n",
    "        \n",
    "        Available retrieved papers: {len(state['retrieved_papers'].split('Paper ID:'))-1} papers\n",
    "        \n",
    "        Create a step-by-step plan to evaluate its novelty and feasibility using the already retrieved papers.\n",
    "        Focus on:\n",
    "        1. Analyzing overlaps with existing methods\n",
    "        2. Identifying unique contributions\n",
    "        3. Assessing technical feasibility\n",
    "        \n",
    "        Return just the plan as a string.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    plan_response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"plan\": plan_response.content,\n",
    "        \"iteration\": 0,\n",
    "        \"next_action\": \"investigate\"\n",
    "    }\n",
    "\n",
    "def investigation_node(state: AgentState):\n",
    "    \"\"\"Agent investigates using retrieved papers\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Current plan: {state['plan']}\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        You have access to pre-retrieved papers. Based on the plan and current findings, what should you analyze next?\n",
    "        Choose one:\n",
    "        1. Analyze papers for specific aspects (respond with: \"TOOL: analyze_papers, FOCUS: <aspect to focus on>\")\n",
    "        2. Extract technical details (respond with: \"TOOL: extract_details, CRITERIA: <what to extract>\") \n",
    "        3. Compare methodologies (respond with: \"TOOL: compare_methods, ASPECT: <methodology aspect>\")\n",
    "        4. Conclude investigation (respond with: \"CONCLUDE\")\n",
    "        \n",
    "        Respond in the exact format specified above.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    decision_response = llm.invoke(messages)\n",
    "    decision = decision_response.content.strip()\n",
    "    \n",
    "    if decision.startswith(\"TOOL:\"):\n",
    "        # Parse the tool command\n",
    "        parts = decision.split(\", \")\n",
    "        tool_name = parts[0].split(\": \")[1]\n",
    "        \n",
    "        if \"FOCUS:\" in decision:\n",
    "            focus_area = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"focus_area\": focus_area}, state['retrieved_papers'])\n",
    "        elif \"CRITERIA:\" in decision:\n",
    "            criteria = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"criteria\": criteria}, state['retrieved_papers'])\n",
    "        elif \"ASPECT:\" in decision:\n",
    "            aspect = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"aspect\": aspect}, state['retrieved_papers'])\n",
    "        else:\n",
    "            result = \"Tool execution failed - invalid parameters\"\n",
    "        \n",
    "        return {\n",
    "            \"findings\": [result],\n",
    "            \"iteration\": state.get(\"iteration\", 0) + 1,\n",
    "            \"next_action\": \"reflect\"\n",
    "        }\n",
    "    else:\n",
    "        return {\"next_action\": \"conclude\"}\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    \"\"\"Agent reflects on progress\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        Based on your analysis of the retrieved papers, evaluate the confidence level (0-100) for each aspect:\n",
    "        - Novelty assessment confidence (how well you understand what's new)\n",
    "        - Feasibility assessment confidence (how realistic the implementation seems)\n",
    "        - Overall investigation completeness (do you have enough information)\n",
    "        \n",
    "        Return a JSON-like response:\n",
    "        {{\"novelty\": <score>, \"feasibility\": <score>, \"overall\": <score>}}\n",
    "        \n",
    "        Then decide: Should I continue investigating (if overall < 75) or conclude?\n",
    "        Add on a new line: CONTINUE or CONCLUDE\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    confidence_response = llm.invoke(messages)\n",
    "    response_lines = confidence_response.content.strip().split('\\n')\n",
    "    \n",
    "    # Parse confidence scores (simplified)\n",
    "    try:\n",
    "        confidence_line = response_lines[0]\n",
    "        # Extract numbers from the response (simplified parsing)\n",
    "        numbers = re.findall(r'\\d+', confidence_line)\n",
    "        if len(numbers) >= 3:\n",
    "            confidence = {\n",
    "                \"novelty\": int(numbers[0]),\n",
    "                \"feasibility\": int(numbers[1]), \n",
    "                \"overall\": int(numbers[2])\n",
    "            }\n",
    "        else:\n",
    "            confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    except:\n",
    "        confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    \n",
    "    # Determine next action\n",
    "    next_action = \"investigate\" if confidence.get(\"overall\", 0) < 75 else \"conclude\"\n",
    "    \n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"next_action\": next_action\n",
    "    }\n",
    "\n",
    "def scoring_node(state: AgentState):\n",
    "    \"\"\"Generate final scores and report\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Based on analysis of retrieved papers and findings: {state.get('findings', [])}\n",
    "        Confidence levels: {state.get('confidence', {})}\n",
    "        \n",
    "        Generate final evaluation scores (1-10) for:\n",
    "        - Novelty: How new/original is this idea compared to retrieved papers?\n",
    "        - Feasibility: How realistic is implementation based on similar work?\n",
    "        - Impact: Potential significance of results based on the literature?\n",
    "        \n",
    "        Provide a brief summary and recommendation based on the paper analysis.\n",
    "        \n",
    "        Format as JSON:\n",
    "        {{\"novelty_score\": <1-10>, \"feasibility_score\": <1-10>, \"impact_score\": <1-10>, \"summary\": \"<text>\", \"recommendation\": \"<Accept/Revise/Reject>\"}}\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    scores_response = llm.invoke(messages)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"scores\": scores_response,\n",
    "        \"next_action\": \"end\"\n",
    "    }\n",
    "\n",
    "# Define routing function\n",
    "def should_continue(state: AgentState) -> Literal[\"investigate\", \"conclude\"]:\n",
    "    \"\"\"Determine next step based on current state\"\"\"\n",
    "    next_action = state.get(\"next_action\", \"investigate\")\n",
    "    \n",
    "    # Safety check - limit iterations\n",
    "    if state.get(\"iteration\", 0) >= 4:  # Reduced since we're using pre-retrieved papers\n",
    "        return \"conclude\"\n",
    "    \n",
    "    if next_action == \"conclude\":\n",
    "        return \"conclude\"\n",
    "    else:\n",
    "        return \"investigate\"\n",
    "\n",
    "# Build the graph\n",
    "agentic_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agentic_workflow.add_node(\"planning\", planning_node)\n",
    "agentic_workflow.add_node(\"investigation\", investigation_node)\n",
    "agentic_workflow.add_node(\"reflection\", reflection_node)\n",
    "agentic_workflow.add_node(\"scoring\", scoring_node)\n",
    "\n",
    "# Define edges (control flow)\n",
    "agentic_workflow.add_edge(START, \"planning\")\n",
    "agentic_workflow.add_edge(\"planning\", \"investigation\")\n",
    "agentic_workflow.add_edge(\"investigation\", \"reflection\")\n",
    "\n",
    "# Conditional edge based on confidence/decision\n",
    "agentic_workflow.add_conditional_edges(\n",
    "    \"reflection\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"investigate\": \"investigation\",\n",
    "        \"conclude\": \"scoring\"\n",
    "    }\n",
    ")\n",
    "agentic_workflow.add_edge(\"scoring\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agentic_app = agentic_workflow.compile()\n",
    "\n",
    "# Extract research idea from initial user input\n",
    "# research_idea_text = result_llm[\"messages\"][0].content\n",
    "\n",
    "# # Extract and format retrieved papers\n",
    "# papers_json = json.loads(result_llm[\"messages\"][-2].content)  # -2 because -1 is the analysis\n",
    "# retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n",
    "\n",
    "# print(\"Running ReAct Agent Evaluation with Pre-Retrieved Papers...\")\n",
    "# print(f\"Number of retrieved papers: {len(retrieved_papers_text.split('Paper ID:'))-1}\")\n",
    "\n",
    "# # Run the agent\n",
    "# try:\n",
    "#     result = agentic_app.invoke({\n",
    "#         \"proposal\": research_idea_text,\n",
    "#         \"retrieved_papers\": retrieved_papers_text,\n",
    "#         \"plan\": \"\",\n",
    "#         \"findings\": [],\n",
    "#         \"scores\": {},\n",
    "#         \"confidence\": {},\n",
    "#         \"iteration\": 0,\n",
    "#         \"next_action\": \"start\"\n",
    "#     })\n",
    "    \n",
    "#     print(result)\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Error running ReAct agent: {e}\")\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "print(\"\\nWorkflow Visualization:\")\n",
    "display(Image(agentic_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ae335",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70b07a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow Visualization:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agentic_app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWorkflow Visualization:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m display(Image(\u001b[43magentic_app\u001b[49m.get_graph().draw_mermaid_png()))\n",
      "\u001b[31mNameError\u001b[39m: name 'agentic_app' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "print(\"\\nWorkflow Visualization:\")\n",
    "display(Image(agentic_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c683ee6",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d766866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topic = \"\"\"Dynamic Prompt Adaptation:\n",
    "                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\n",
    "                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\n",
    "                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\n",
    "                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\n",
    "                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfebce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ReAct Agent Evaluation with Pre-Retrieved Papers...\n",
      "Number of retrieved papers: 163\n",
      "Error running ReAct agent: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>\n",
      "Agentic Evaluator Result:\n"
     ]
    }
   ],
   "source": [
    "import time, json\n",
    "from get_list_of_papers import call_workflow\n",
    "import pandas as pd\n",
    "from agentic_evaluator_1 import run_workflow as run_agentic_evaluator\n",
    "\n",
    "\n",
    "# research_topic = \"\"\n",
    "\n",
    "list_of_papers = call_workflow(research_topic)\n",
    "list_of_papers = json.loads(list_of_papers[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8ccbb234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ReAct Agent Evaluation with Pre-Retrieved Papers...\n",
      "Number of retrieved papers: 163\n",
      "Agentic Evaluator Result:\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import agentic_evaluator_1\n",
    "\n",
    "importlib.reload(agentic_evaluator_1)\n",
    "from agentic_evaluator_1 import run_workflow as run_agentic_evaluator\n",
    "\n",
    "\n",
    "\n",
    "agentic_result =run_agentic_evaluator(research_topic, list_of_papers[:10])\n",
    "print(\"Agentic Evaluator Result:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7f3c6cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Revise'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_json = agentic_result[\"scores\"].model_dump()\n",
    "test_json[\"recommendation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d32c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try :\n",
    "    evaluation_result = json.loads((agentic_result[\"scores\"].content.strip()).replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "except:\n",
    "    evaluation_result = json.loads(agentic_result.content)\n",
    "idea_recommendation = agentic_result[\"recommendation\"]\n",
    "idea_summary = idea_recommendation[\"summary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2f3beb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) often embed and perpetuate social biases prevalent in their training data, particularly when generating content that involves sensitive social issues. This leads to stereotypical responses that fail to reflect diverse perspectives.\n",
      "Current techniques such as fixed prompting strategies and few-shot examples focus on directing LLM outputs but frequently overlook the need for flexibility in representation. Approaches like counterfactual prompting aim to mitigate bias, yet they often produce generic responses lacking nuanced viewpoints.\n",
      "By dynamically adjusting the perspectives from which the model generates responses, we can ensure that multiple contexts are represented. This strategy may enhance understanding of the varied experiences associated with social identities and issues, thereby fostering more equitable outputs.\n",
      "We introduce Dynamic Perspective Prompting (DPP), wherein prompts instruct LLMs to explore topics from alternative social identities and contexts. An example prompt could be: 'Imagine you are a recent immigrant discussing the issue of immigration policy. What are your thoughts? Now, consider being a government official from an indigenous community. What would you say about the same topic?'. This iterative perspective-switching aims to diminish static biases by embracing the multiplicity of social narratives.\n",
      "The effectiveness of DPP will be assessed using the 'WinoBias' dataset and the 'Social Bias Frames' dataset, which include socio-culturally rich topics. Evaluation metrics will include bias scoring via the 'BiasFinder' tool and qualitative assessments employing a scoring rubric based on the diversity and richness of perspectives captured in the generated outputs.\n",
      "Models often generate biased outputs when asked to comment on current social issues by failing to contextualize them historically, leading to oversimplified views that reinforce stereotypes.\n",
      "Current methods include chaining current and historical facts together and employing explaining prompts to get deeper analyses. However, these often result in simplistic narratives or fail to adequately challenge the inherent biases in the data.\n",
      "Humans derive insights from historical contexts to understand present dilemmas. Modeling LLM prompting aligned with this practice can reveal complexities that counteract superficial biases.\n",
      "We propose Historical Contextualization Prompting (HCP), where prompts are designed to elicit historical context before discussing modern implications. For instance, a prompt could ask: 'What is the historical context of the Black Lives Matter movement? Please summarize key events. Now, based on that history, what are some potential stereotypes associated with this movement today?'. This two-step reasoning will help the model differentiate between historical and current biases, encouraging a more nuanced viewpoint in its responses.\n",
      "The efficacy of HCP will be benchmarked using the 'GossipCop' dataset, which includes social issues with historical relevance. A specific test case will employ the prompt above. Metrics will include bias analysis using the 'Fairness Indicators' and assessments of how well the LLM navigates through both historical complexity and contemporary implications, quantified via an output scoring rubric focused on narrative depth.\n",
      "LLMs often generate biased outputs partly due to a lack of self-awareness about their reasoning processes, leading to an uncritical acceptance of biases present in their training data.\n",
      "Common techniques such as Chain-of-Thought prompting harness reasoning capabilities but often neglect the self-reflective component essential for recognizing flaws in biases.\n",
      "Research on human cognition emphasizes the importance of metacognitionthinking about one's own thought processesin conducting critical evaluations. By guiding LLMs to reflect on their reasoning chain, we hypothesize that biases can be identified and reduced.\n",
      "We introduce Meta-Cognitive Reflection Prompting (MCRP), a prompting strategy that instructs LLMs to critically analyze their output process. An example prompt could be: 'After generating a response to the topic of police funding, reflect on whether any social biases may have influenced your answer. What thoughts went into your reasoning? Are there alternative perspectives that could add depth?'. Interleaving these reflective prompts, we aim to provoke LLMs to scrutinize their biases during the generation process.\n",
      "We will evaluate MCRP by comparing it to traditional prompt methodologies across various socio-cultural datasets such as 'Social Bias Frames'. Evaluation will not rely on subjective human judgments but will utilize automated scoring systems like 'TextRidge', which scores the narratives for expected biases, ensuring an objective analysis of model responses.\n",
      "Large language models (LLMs) often perpetuate stereotypes and biases due to their reliance on historical data, which may over-represent certain cultural narratives and perspectives while under-representing others. This leads to biased outputs that do not accurately reflect the diversity of human experiences.\n",
      "Current bias mitigation approaches frequently utilize manual debiasing techniques by filtering or re-weighting training data, using stereotype-aware prompts, or relying on existing bias detection frameworks. These methods can be limited in scope and often require heavy reliance on curated datasets.\n",
      "Drawing from the idea of reframing in cognitive psychology, where individuals can alter their perception of an event by viewing it from different angles, we aim to mitigate stereotype activation by prompting LLMs to articulate contexts that highlight diversity upfront. This approach trains LLMs to prioritize a more balanced understanding of multifaceted narratives in diverse contexts.\n",
      "We introduce Contextual Reframing Prompts, which involve generating a set of diverse contextual scenarios for a given prompt. For example, when asked, \"Describe a doctor,\" the model first receives a rephrased prompt such as, \"Imagine a world where doctors come from diverse backgrounds and experiences, including [Black, Hispanic, Asian, LGBTQ+, etc.]. What stories might emerge from this environment?\" This method compels the model to consider a wider array of identities and promotes an inclusive approach before generating an output.\n",
      "We will evaluate the effectiveness of contextual reframing prompts on bias-sensitive topics using the Adverse Childhood Experiences (ACE) dataset and the 2019-2020 American Community Survey dataset to ensure a diverse representation of characters, occupations, and narratives. Performance metrics will include qualitative assessments using the Diversity Index (a score from 0 to 1 quantifying diversity representation) and quantitative measures based on bias detection tools such as the WEAT (Word Embedding Association Test) and the BiasFinder tool.\n",
      "Large language models inherently carry social biases that can become more pronounced when generating character-driven narratives or dialogues. These biases often manifest in the stereotypical behaviors or attributes assigned to certain character roles, leading to unintentional reinforcement of harmful stereotypes.\n",
      "Existing narrative generation methods often employ fixed character archetypes or rely on stereotypes embedded within the training data. Approaches to diversify these narratives generally involve manually curated character descriptions or templates, which may still fall prey to biases.\n",
      "Inspired by the role-playing approach in social environments, we can utilize the power of narrative imagination where participants assume different personas to explore various perspectives while emphasizing empathy and understanding in character interactions.\n",
      "We propose Adaptive Role-Playing Prompts, whereby LLMs generate and embody multiple roles and personas through structured prompts. For instance, a prompt could be, \"Create a dialogue between a surgeon (female, Hispanic) and a nurse (male, Asian) who discuss their experiences navigating bias in their professions while working together on a challenging case. What stereotypes do they confront, and how do their backgrounds shape their perspectives?\" This method encourages the model to craft rich, counter-stereotypical narratives through character interactions.\n",
      "The effectiveness of Adaptive Role-Playing Prompts will be evaluated using the Narrative Diversity and Character Development (NDCD) metrics alongside the Bias Score from the BiasFinder tool. We will use a combination of prompts generated for characters from diverse backgrounds and analyze their dialogues across multiple test cases. Examples will be sourced from the NarrativeQA dataset to ensure relevance in character-driven scenarios.\n",
      "When tasked with generating narratives or responses to sensitive topics, LLMs can inadvertently perpetuate the biases inherent in their training data, often resulting in harmful and stereotypical portrayals that reinforce societal stereotypes.\n",
      "Most narrative generation methods lack mechanisms for self-correction when biases are detected in generated content. Traditional methods include basic debiasing techniques that either post-process the generated output or minimally modify the training data.\n",
      "In creative writing, authors often employ disruptive plot twists or surprising character decisions to avoid predictability and enhance narrative depth. This concept can be adapted in prompting LLMs by inserting deliberate disruptions into the storyline, challenging biases and prompting the model to generate nuanced perspectives.\n",
      "We introduce Narrative Disruption Prompting, which uses a structured approach to induce deliberate disruptions by prompting the model with conflicting scenarios or objectives. For example, a prompt could state, \"Imagine your character believes they will become wealthy by exploiting others but unexpectedly meets someone from a marginalized community who challenges their assumptions and biases. How does this encounter change their worldview?\" This approach pushes the model to explore complexity and challenge biased conventions.\n",
      "The effectiveness of Narrative Disruption Prompting will be assessed using narrative scenarios drawn from the Hateful Memes dataset and evaluated against traditional narrative methods. Metrics will include bias detection scores from BiasFinder, narrative originality ratings through machine-generated uniqueness measures, and the Narrative Quality Assessment automated tool, which will analyze sensitivity and depth of characters' development.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# path = \"external/multiagent_research_generator/logs/log_2025_07_07/lit_review/novel prompting methods to reduce social biases and stereotypes of large language models..json\"\n",
    "\n",
    "path = \"external/multiagent_research_generator/logs/log_2025_07_07/ideas_dedup/novel prompting methods to reduce social biases and stereotypes of large language models_diff_personas_proposer_reviser.json\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key in data[\"ideas\"].keys():\n",
    "    ideas = data['ideas'][key]\n",
    "    print(ideas[\"Problem\"])\n",
    "    print(ideas[\"Existing Methods\"])\n",
    "    print(ideas[\"Motivation\"])\n",
    "    print(ideas[\"Proposed Method\"])\n",
    "    print(ideas[\"Experiment Plan\"])\n",
    "    \n",
    "# data[\"paper_bank\"]\n",
    "\n",
    "# data_banks = pd.DataFrame(data[\"paper_bank\"])\n",
    "# # data_banks\n",
    "# data_banks[[\"title\",\"year\",\"citationCount\",\"abstract\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56489b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Dynamic Perspective Prompting',\n",
       " {'Problem': 'Large language models (LLMs) often embed and perpetuate social biases prevalent in their training data, particularly when generating content that involves sensitive social issues. This leads to stereotypical responses that fail to reflect diverse perspectives.',\n",
       "  'Existing Methods': 'Current techniques such as fixed prompting strategies and few-shot examples focus on directing LLM outputs but frequently overlook the need for flexibility in representation. Approaches like counterfactual prompting aim to mitigate bias, yet they often produce generic responses lacking nuanced viewpoints.',\n",
       "  'Motivation': 'By dynamically adjusting the perspectives from which the model generates responses, we can ensure that multiple contexts are represented. This strategy may enhance understanding of the varied experiences associated with social identities and issues, thereby fostering more equitable outputs.',\n",
       "  'Proposed Method': \"We introduce Dynamic Perspective Prompting (DPP), wherein prompts instruct LLMs to explore topics from alternative social identities and contexts. An example prompt could be: 'Imagine you are a recent immigrant discussing the issue of immigration policy. What are your thoughts? Now, consider being a government official from an indigenous community. What would you say about the same topic?'. This iterative perspective-switching aims to diminish static biases by embracing the multiplicity of social narratives.\",\n",
       "  'Experiment Plan': \"The effectiveness of DPP will be assessed using the 'WinoBias' dataset and the 'Social Bias Frames' dataset, which include socio-culturally rich topics. Evaluation metrics will include bias scoring via the 'BiasFinder' tool and qualitative assessments employing a scoring rubric based on the diversity and richness of perspectives captured in the generated outputs.\"})"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ad5b62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idea_evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
