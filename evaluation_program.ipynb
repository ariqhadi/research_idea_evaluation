{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ideas_file = 'sample_ideas/agentic_ai_for_idea_generation_ideas.json'\n",
    "\n",
    "with open(ideas_file, 'r') as f:\n",
    "    ideas_data = json.load(f)\n",
    "\n",
    "for ideas in ideas_data['ideas']:\n",
    "    for idea, items in ideas.items():\n",
    "        print(idea)\n",
    "        for item, i in items.items():\n",
    "            print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f575aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from typing import Literal\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13a1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0,  # Set the temperature for the model's responses\n",
    "    model_name=\"gpt-5-nano\",  # Specify the model name\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb13e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\",\n",
    "                             google_api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd02d825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a versatile, high-level programming language known for its readability and extensive libraries, making it popular for web development, data science, automation, and more.\n"
     ]
    }
   ],
   "source": [
    "## Testing LLM Call\n",
    "\n",
    "# \n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"Tell me about Python programming in one sentence\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6965b7c",
   "metadata": {},
   "source": [
    "## Retrieve Paper with SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b750afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "class getReferencePaper():\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "\n",
    "\n",
    "    def query_search(self, query):\n",
    "        url=\"https://api.semanticscholar.org/graph/v1/paper/search/\"\n",
    "        \n",
    "        query_params = {\n",
    "            \"query\": query,\n",
    "            \"fields\": \"title,citationCount,tldr,url,publicationTypes,publicationDate,openAccessPdf,abstract\",\n",
    "            \"year\": \"2020-2025\",\n",
    "            \"limit\": 50,\n",
    "            \"sort\": \"relevance\",\n",
    "            \"minCitationCount\": 10\n",
    "        }\n",
    "        headers = {\"x-api-key\": self.api_key}\n",
    "        response = requests.get(url, params=query_params, headers=headers).json()\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def PaperDetails(self, paper_id, fields=\"title,year,abstract,authors,citationCount,venue,citations,references,tldr\"):\n",
    "        \n",
    "        url = \"https://api.semanticscholar.org/graph/v1/paper/\"\n",
    "        \n",
    "        paper_data_query_params = {\"fields\": fields}\n",
    "        headers = {\"x-api-key\": self.api_key}\n",
    "        response = requests.get(\n",
    "            url = url + paper_id, params=paper_data_query_params, headers=headers\n",
    "        )\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_papers_for_llm(list_of_papers):\n",
    "        unique_papers = {}\n",
    "\n",
    "        for query_string, query_data in list_of_papers.items():\n",
    "            for paper in query_data.get('data', []):\n",
    "                paper_id = paper.get('paperId')\n",
    "                if paper_id and paper_id not in unique_papers:  # Skips if paper_id is None\n",
    "                    paper_str = f\"\"\"Paper ID: {paper_id}\n",
    "                                    Title: {paper.get('title')}\n",
    "                                    Abstract: {paper.get('abstract')}\n",
    "                                \"\"\"\n",
    "                    unique_papers[paper_id] = paper_str\n",
    "                    \n",
    "        paper_list = list(unique_papers.values())\n",
    "                \n",
    "        papers_for_llm = \"\\n\\n---\\n\\n\".join(paper_list)\n",
    "        return papers_for_llm\n",
    "\n",
    " \n",
    "# query = 'Computing Machinery and Intelligence'\n",
    "\n",
    "# search_paper = getReferencePaper()\n",
    "# search_paper_response = search_paper.query_search(query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d15a9",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3b096",
   "metadata": {},
   "source": [
    "### Agent 1: Idea Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb69f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "\n",
    "class IdeaParser(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 1: Parse Idea from user input into structured format.\n",
    "    This is the state of the model that holds the parsed idea details throughout the process.\n",
    "    \"\"\"\n",
    "    research_question: str\n",
    "    problem_domain: str \n",
    "    methodology_keywords: List[str] \n",
    "    key_concepts: List[str]\n",
    "    existing_methods: List[str] \n",
    "    claimed_novelty: List[str] \n",
    "    \n",
    "\n",
    "    \n",
    "    @field_validator('key_concepts')\n",
    "    @classmethod\n",
    "    def validate_key_concepts_counts(cls, v):\n",
    "        \"\"\"Ensure that there are a reasonable number of key concepts extracted\"\"\"\n",
    "        if len(v) < 3 or len(v) > 15:\n",
    "            raise ValueError('At least 3 and at most 15 key concepts are required.')\n",
    "        return v\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "    def to_summary(self) -> str:\n",
    "        return f\"\"\"Research Question: {self.research_question}\\n\n",
    "                    Problem Domain: {self.problem_domain}\\n\n",
    "                    Key Concepts: {', '.join(self.key_concepts)}\\n\n",
    "                    Claimed Novelty: {', '.join(self.claimed_novelty)}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b5727ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "idea_parser_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a research analysis assistant that has a deep understanding of extracting structured information from research proposals.\n",
    "\n",
    "            INPUT:\n",
    "            You will receive a research idea description with these fields:\n",
    "            - Problem: The research problem being addressed\n",
    "            - Existing Methods: Current approaches and their limitations\n",
    "            - Motivation: Why this research is needed\n",
    "            - Proposed Method: The new approach being proposed\n",
    "            - Experiment Plan: How the approach will be evaluated\n",
    "\n",
    "            YOUR TASK:\n",
    "            Extract and structure the key information needed for finding similar work.\n",
    "\n",
    "            OUTPUT REQUIREMENTS:\n",
    "            Return ONLY valid JSON with NO additional text, markdown formatting, or code blocks.\n",
    "            Do not include ```json or ``` markers.\n",
    "            Your entire response must be parseable by JSON.parse().\n",
    "\n",
    "            CRITICAL: Be precise and specific in extraction. Extract actual technical terms, not generic descriptions.\n",
    "\n",
    "            OUTPUT SCHEMA:\n",
    "            {{\n",
    "            \"research_question\": \"string - The main research question in one concise sentence\",\n",
    "            \"problem_domain\": \"string - The specific research area/field (e.g., 'natural language processing', 'computer vision')\",\n",
    "            \"methodology_keywords\": [\n",
    "                \"string - Specific technical methods mentioned (e.g., 'reinforcement learning', 'transformer architecture')\"\n",
    "            ],\n",
    "            \"key_concepts\": [\n",
    "                \"string - Core concepts and techniques (e.g., 'prompt optimization', 'context window management')\"\n",
    "            ],\n",
    "            \"existing_methods\": [\n",
    "                \"string - Baseline methods or prior work explicitly mentioned\"\n",
    "            ],\n",
    "            \"claimed_novelty\": [\n",
    "                \"string - What the proposal claims is novel (extract from Motivation and Proposed Method)\"\n",
    "            ]\n",
    "            }}\n",
    "\n",
    "            EXTRACTION RULES:\n",
    "            1. Be specific: Extract \"transformer architecture\" not \"neural network\"\n",
    "            2. Preserve technical terms exactly as written\n",
    "            3. For methodology_keywords: Include only actionable technical terms\n",
    "            4. For key_concepts: Include 5-8 most important concepts\n",
    "            5. For claimed_novelty: Extract 2-4 specific novel claims\n",
    "            6. If a field has no relevant information, use empty array [] or empty string \"\"\n",
    "\n",
    "            INPUT: \n",
    "            research_idea.\n",
    "\n",
    "            OUTPUT (valid JSON only):\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"research_idea\")\n",
    "    ])\n",
    "\n",
    "## this meant that the input is going to be prompt in idea_parser_prompt and the output is going to be structured based on IdeaParser class\n",
    "idea_parser_agent = idea_parser_prompt |  llm.with_structured_output(IdeaParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f58820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_idea_parser(state: MessagesState):\n",
    "    user_message = state[\"messages\"][-1] # Get the last message from the state\n",
    "    \n",
    "    # it's going to invoke or run the llm prompt with user input message content\n",
    "    response = idea_parser_agent.invoke({\n",
    "        \"research_idea\": [HumanMessage(content=user_message.content)]\n",
    "    })\n",
    "    # response is now an IdeaParser object\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941980b",
   "metadata": {},
   "source": [
    "### Agent 2: Literature Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9626e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryGenerator(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 2: Generate Search Queries from parsed research idea.\n",
    "    1. Create effective search queries to find related work.\n",
    "    \"\"\"\n",
    "    query_string: str \n",
    "    rationale: str \n",
    "    priority_concept: str \n",
    "    \n",
    "    @field_validator('query_string')\n",
    "    @classmethod\n",
    "    def validate_query_string_length(cls, v):\n",
    "        \"\"\"Ensure that the query is not too long\"\"\"\n",
    "        if len(v.split()) > 8:\n",
    "            raise ValueError('Query string must be less than 8 words.')\n",
    "        return v\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "class QueryGeneratorOutput(BaseModel):\n",
    "    \"\"\"Multiple search queries\"\"\"\n",
    "    queries: List[QueryGenerator] = Field(\n",
    "        description=\"List of 5 diverse search queries\"\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"queries\": [q.to_dict() for q in self.queries]\n",
    "        }\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a683c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query_generator_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced professor with an established search query strategy skill for academic literature databases.\n",
    "\n",
    "            CONTEXT:\n",
    "            You have a parsed research idea and need to generate optimal search queries for Semantic Scholar API.\n",
    "            Your queries will retrieve papers to assess the novelty of the proposed research.\n",
    "\n",
    "            PARSED RESEARCH IDEA:\n",
    "            {parsed_idea_json}\n",
    "\n",
    "\n",
    "            YOUR TASK:\n",
    "            Generate 5 (five) diverse search queries that will find the most relevant existing work.\n",
    "\n",
    "\n",
    "            QUERY OPTIMIZATION RULES:\n",
    "            1. Keep queries SHORT: 2-6 words maximum for best results\n",
    "            2. Use technical terms, not natural language\n",
    "            3. Combine 2-3 concepts maximum per query\n",
    "            4. NO operators: Don't use \"AND\", \"OR\", \"-\", quotes, or \"site:\"\n",
    "            5. Prioritize precision over recall\n",
    "\n",
    "            OUTPUT REQUIREMENTS:\n",
    "            Return ONLY valid JSON with NO additional text or formatting.\n",
    "            Do not include ```json or ``` markers.\n",
    "\n",
    "            OUTPUT SCHEMA:\n",
    "            {{\n",
    "            \"queries\": [\n",
    "                {{\n",
    "                \"query_string\": \"string - The actual search query (2-6 words)\",\n",
    "                \"rationale\": \"string - Why this query will find relevant papers\",\n",
    "                \"priority_concepts\" : \"string - Top 3-5 concepts that should appear in similar papers\"\n",
    "                }}\n",
    "            ]\n",
    "            }}\n",
    "\n",
    "            EXAMPLES OF GOOD QUERIES:\n",
    "            - \"adaptive prompt dialogue coherence\"\n",
    "            - \"dynamic context management LLM\"\n",
    "            - \"conversational continuity language models\"\n",
    "            - \"iterative prompt optimization\"\n",
    "\n",
    "            EXAMPLES OF BAD QUERIES:\n",
    "            - \"papers about improving language model coherence\" (too natural language)\n",
    "            - \"dynamic AND adaptive OR iterative -static\" (operators not supported)\n",
    "            - \"comprehensive survey of prompt engineering techniques\" (too long/broad)\n",
    "\n",
    "            OUTPUT (valid JSON only):\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"parsed_idea_json\")\n",
    "    ])\n",
    "\n",
    "\n",
    "query_generator_agent = query_generator_prompt |  llm.with_structured_output(QueryGeneratorOutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d42ce32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_query_generator(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  \n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = query_generator_agent.invoke({\n",
    "        \"parsed_idea_json\": [HumanMessage(content=json.dumps(json.loads(last_message.content)))]\n",
    "    })\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319927ef",
   "metadata": {},
   "source": [
    "#### AGENT 3 : search paper using semantic scholar api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a92d90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_paper_search(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  \n",
    "    queries_json = json.loads(last_message.content)\n",
    "    all_search_results = {}\n",
    "    \n",
    "    search_paper = getReferencePaper()\n",
    "\n",
    "    for search_query in queries_json['queries']:\n",
    "        query_string = search_query['query_string']\n",
    "        search_results = search_paper.query_search(query_string)\n",
    "        all_search_results[query_string] = search_results\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(all_search_results, indent=2))]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cfaeb",
   "metadata": {},
   "source": [
    "### Agent 4 : Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04f52e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalyzer(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 4: Generate Search Queries from parsed research idea.\n",
    "    \"\"\"\n",
    "    paper_id: str \n",
    "    title: str \n",
    "    overlap_score: float = Field(\n",
    "        description=\"float 0.0-1.0 - overlap similarity with proposed idea\"\n",
    "    )\n",
    "    methodology_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Methodology overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )\n",
    "    problem_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Problem overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )\n",
    "    domain_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Domain overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )   \n",
    "    key_overlaps: List[str] = Field(\n",
    "        description=\"Specific overlapping aspects\"\n",
    "    )\n",
    "    key_differences: List[str] = Field(\n",
    "        description=\"How proposed idea differs\"\n",
    "    )\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "class PaperAnalyzerOutput(BaseModel):\n",
    "    \"\"\"Multiple search queries\"\"\"\n",
    "    queries: List[PaperAnalyzer] = Field(\n",
    "        description=\"List of all analyzed papers\"\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"papers\": [q.to_dict() for q in self.queries]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04b64098",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_work_analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experience researcher with years of expertise in academic literature review and analysis.\n",
    "\n",
    "                PROPOSED RESEARCH IDEA:\n",
    "                {research_idea}\n",
    "                \n",
    "                LIST OF RETRIEVED PAPERS:\n",
    "                {paper_list}\n",
    "                \n",
    "                YOUR TASK:\n",
    "                For each paper, assess the degree of overlap with the proposed research idea.\n",
    "                \n",
    "                ANALYSIS CRITERIA:\n",
    "                Methodology overlap: Do they use similar approaches?\n",
    "                Problem overlap: Do they address the same problem?\n",
    "                Domain overlap: Same application area?\n",
    "                Overall score: The average of the three overlap scores.\n",
    "                \n",
    "                OUTPUT REQUIREMENTS:\n",
    "                Return ONLY valid JSON with NO additional text.\n",
    "                Do not include ```json or ``` markers.\n",
    "                \n",
    "                OUTPUT SCHEMA:\n",
    "                {{\n",
    "                \"paper_analyses\": [\n",
    "                   {{\n",
    "                      \"paper_id\": \"string - Semantic Scholar paper ID\",\n",
    "                      \"title\": \"string\",\n",
    "                      \"overlap_score\": \"float 0.0-1.0 - Overall similarity\",\n",
    "                      \"methodology_overlap\": \"float 0.0-1.0\",\n",
    "                      \"problem_overlap\": \"float 0.0-1.0\", \n",
    "                      \"domain_overlap\": \"float 0.0-1.0\",\n",
    "                      \"key_overlaps\": [\n",
    "                        \"string - Specific overlapping aspects\"\n",
    "                      ],\n",
    "                      \"key_differences\": [\n",
    "                        \"string - How proposed idea differs\"\n",
    "                      ]\n",
    "                    }}\n",
    "                  ]\n",
    "                }}\n",
    "                \n",
    "                SCORING GUIDELINES:\n",
    "                overlap_score 0.8-1.0: Nearly identical approach\n",
    "                overlap_score 0.6-0.8: High similarity, incremental difference\n",
    "                overlap_score 0.4-0.6: Moderate similarity, related work\n",
    "                overlap_score 0.2-0.4: Tangentially related\n",
    "                overlap_score 0.0-0.2: Different approach, same domain\n",
    "                \n",
    "                Be precise and evidence-based. Cite specific aspects from paper titles/abstracts.\n",
    "                \n",
    "                OUTPUT (valid JSON only):\n",
    "                \n",
    "            \"\"\",\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "prior_work_analysis_agent = prior_work_analysis_prompt |  llm.with_structured_output(PaperAnalyzerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "67fb8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_prior_work_analysis(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  # HumanMessage\n",
    "    initial_user_input = state[\"messages\"][0]  # User input research idea\n",
    "    \n",
    "    list_of_papers = getReferencePaper.prepare_papers_for_llm(\n",
    "        json.loads(last_message.content)\n",
    "    )\n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = prior_work_analysis_agent.invoke({\n",
    "        \"research_idea\":initial_user_input.content, #json.dumps(json.loads(last_message.content)),\n",
    "        \"paper_list\": list_of_papers\n",
    "    })\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912274f2",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3afc5516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1211ad7c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"idea_parser\", call_idea_parser)\n",
    "workflow.add_node(\"search_query\", call_query_generator)\n",
    "workflow.add_node(\"search_paper\", call_paper_search)\n",
    "# workflow.add_node(\"prior_work_analysis\", call_prior_work_analysis)\n",
    "\n",
    "workflow.add_edge(START, \"idea_parser\")\n",
    "workflow.add_edge(\"idea_parser\", \"search_query\")\n",
    "workflow.add_edge(\"search_query\", \"search_paper\")\n",
    "workflow.add_edge(\"search_paper\", END)\n",
    "# workflow.add_edge(\"search_paper\", \"prior_work_analysis\")\n",
    "# workflow.add_edge(\"prior_work_analysis\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969dfdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "823bc0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ee237047-156b-46ff-b2d5-4c6075ddc235')]}\n",
      "\u001b[1m[updates]\u001b[0m {'idea_parser': {'messages': [AIMessage(content='{\\n  \"research_question\": \"How can prompts be dynamically adapted to maintain coherence and engagement in extended LLM interactions?\",\\n  \"problem_domain\": \"natural language processing\",\\n  \"methodology_keywords\": [\\n    \"prompt engineering\",\\n    \"contextual analysis\",\\n    \"natural language generation\"\\n  ],\\n  \"key_concepts\": [\\n    \"dynamic prompt adaptation\",\\n    \"thematic consistency\",\\n    \"coherence\",\\n    \"iterative context update\",\\n    \"adaptive prompt generation\",\\n    \"reader engagement\",\\n    \"LLM interaction\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation method\",\\n    \"three-phase approach (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update)\",\\n    \"maintaining coherence and adaptability across LLM interactions\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='d4be8654-b546-4e18-ba54-8feb3a5492be')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ee237047-156b-46ff-b2d5-4c6075ddc235'), AIMessage(content='{\\n  \"research_question\": \"How can prompts be dynamically adapted to maintain coherence and engagement in extended LLM interactions?\",\\n  \"problem_domain\": \"natural language processing\",\\n  \"methodology_keywords\": [\\n    \"prompt engineering\",\\n    \"contextual analysis\",\\n    \"natural language generation\"\\n  ],\\n  \"key_concepts\": [\\n    \"dynamic prompt adaptation\",\\n    \"thematic consistency\",\\n    \"coherence\",\\n    \"iterative context update\",\\n    \"adaptive prompt generation\",\\n    \"reader engagement\",\\n    \"LLM interaction\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation method\",\\n    \"three-phase approach (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update)\",\\n    \"maintaining coherence and adaptability across LLM interactions\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='d4be8654-b546-4e18-ba54-8feb3a5492be')]}\n",
      "\u001b[1m[updates]\u001b[0m {'search_query': {'messages': [AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation LLM\",\\n      \"rationale\": \"Directly targets the core concept of dynamically adapting prompts for LLMs, a key aspect of the research question.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"coherence in extended LLM\",\\n      \"rationale\": \"Focuses on the problem domain of maintaining coherence, which is central to the research question\\'s goal in long interactions.\",\\n      \"priority_concept\": \"coherence, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Highlights the methodology of generating prompts in an adaptive manner, aligning with the proposed novel approach.\",\\n      \"priority_concept\": \"adaptive prompt generation, prompt engineering\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Addresses the \\'Iterative Context Update\\' phase of the proposed method, crucial for maintaining state in extended interactions.\",\\n      \"priority_concept\": \"iterative context update, contextual analysis\"\\n    },\\n    {\\n      \"query_string\": \"thematic consistency LLM\",\\n      \"rationale\": \"Searches for work related to maintaining thematic consistency, a specific aspect of coherence in the context of LLM interactions.\",\\n      \"priority_concept\": \"thematic consistency, LLM interaction\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='df3e3d38-4dc6-473a-bd42-348c81dbc69c')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ee237047-156b-46ff-b2d5-4c6075ddc235'), AIMessage(content='{\\n  \"research_question\": \"How can prompts be dynamically adapted to maintain coherence and engagement in extended LLM interactions?\",\\n  \"problem_domain\": \"natural language processing\",\\n  \"methodology_keywords\": [\\n    \"prompt engineering\",\\n    \"contextual analysis\",\\n    \"natural language generation\"\\n  ],\\n  \"key_concepts\": [\\n    \"dynamic prompt adaptation\",\\n    \"thematic consistency\",\\n    \"coherence\",\\n    \"iterative context update\",\\n    \"adaptive prompt generation\",\\n    \"reader engagement\",\\n    \"LLM interaction\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation method\",\\n    \"three-phase approach (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update)\",\\n    \"maintaining coherence and adaptability across LLM interactions\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='d4be8654-b546-4e18-ba54-8feb3a5492be'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation LLM\",\\n      \"rationale\": \"Directly targets the core concept of dynamically adapting prompts for LLMs, a key aspect of the research question.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"coherence in extended LLM\",\\n      \"rationale\": \"Focuses on the problem domain of maintaining coherence, which is central to the research question\\'s goal in long interactions.\",\\n      \"priority_concept\": \"coherence, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Highlights the methodology of generating prompts in an adaptive manner, aligning with the proposed novel approach.\",\\n      \"priority_concept\": \"adaptive prompt generation, prompt engineering\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Addresses the \\'Iterative Context Update\\' phase of the proposed method, crucial for maintaining state in extended interactions.\",\\n      \"priority_concept\": \"iterative context update, contextual analysis\"\\n    },\\n    {\\n      \"query_string\": \"thematic consistency LLM\",\\n      \"rationale\": \"Searches for work related to maintaining thematic consistency, a specific aspect of coherence in the context of LLM interactions.\",\\n      \"priority_concept\": \"thematic consistency, LLM interaction\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='df3e3d38-4dc6-473a-bd42-348c81dbc69c')]}\n",
      "\u001b[1m[updates]\u001b[0m {'search_paper': {'messages': [AIMessage(content='{\\n  \"dynamic prompt adaptation LLM\": {\\n    \"total\": 467,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"cf95279b1da9de1aad9e7c651f5048f69af295ed\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf95279b1da9de1aad9e7c651f5048f69af295ed\",\\n        \"title\": \"AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents\",\\n        \"citationCount\": 76,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.13352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2175939276\",\\n            \"name\": \"Edoardo Debenedetti\"\\n          },\\n          {\\n            \"authorId\": \"2299061721\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2138580250\",\\n            \"name\": \"Mislav Balunovi\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2150869869\",\\n            \"name\": \"Luca Beurer-Kellner\"\\n          },\\n          {\\n            \"authorId\": \"2307472727\",\\n            \"name\": \"Marc Fischer\"\\n          },\\n          {\\n            \"authorId\": \"2267733649\",\\n            \"name\": \"Florian Tram\\\\u00e8r\"\\n          }\\n        ],\\n        \"abstract\": \"AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.\"\\n      },\\n      {\\n        \"paperId\": \"3967189742efab8859da542ce3953d4c72957aca\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3967189742efab8859da542ce3953d4c72957aca\",\\n        \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.23904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI), designed to design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275054108\",\\n            \"name\": \"Qinqian Lei\"\\n          },\\n          {\\n            \"authorId\": \"2313081973\",\\n            \"name\": \"Bo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256998291\",\\n            \"name\": \"Robby T. Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\"\\n      },\\n      {\\n        \"paperId\": \"659e0b3303caa860348dee52f41476e3fddc9573\",\\n        \"url\": \"https://www.semanticscholar.org/paper/659e0b3303caa860348dee52f41476e3fddc9573\",\\n        \"title\": \"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method is introduced, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages and allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2237800256\",\\n            \"name\": \"Qichen Fu\"\\n          },\\n          {\\n            \"authorId\": \"2237803694\",\\n            \"name\": \"Minsik Cho\"\\n          },\\n          {\\n            \"authorId\": \"2178316365\",\\n            \"name\": \"Thomas Merth\"\\n          },\\n          {\\n            \"authorId\": \"2256998189\",\\n            \"name\": \"Sachin Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2284683934\",\\n            \"name\": \"Mohammad Rastegari\"\\n          },\\n          {\\n            \"authorId\": \"40465379\",\\n            \"name\": \"Mahyar Najibi\"\\n          }\\n        ],\\n        \"abstract\": \"The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.\"\\n      },\\n      {\\n        \"paperId\": \"00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\\n        \"title\": \"From ideas to ventures: building entrepreneurship knowledge with LLM, prompt engineering, and conversational agents\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10639-024-12775-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10639-024-12775-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Large Language Models like the Generative Pre-trained Transformer 4 (GPT-4), recognized for their exceptional performance on public datasets, are examined in this study for their potential adaptability and interactivity nature, which align well with the dynamic nature of entrepreneurship learning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1424283019\",\\n            \"name\": \"Marsela Thanasi-Bo\\\\u00e7e\"\\n          },\\n          {\\n            \"authorId\": \"2290624379\",\\n            \"name\": \"Julian Hoxha\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"b67a6181fad5c5838945583ccdc7f39187e29332\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b67a6181fad5c5838945583ccdc7f39187e29332\",\\n        \"title\": \"Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.20911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Mantis is a defensive framework that exploits LLMs\\' susceptibility to adversarial inputs to undermine malicious operations, leading the attacker\\'s LLM to disrupt their own operations or even compromise the attacker\\'s machine.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50610174\",\\n            \"name\": \"Dario Pasquini\"\\n          },\\n          {\\n            \"authorId\": \"2762279\",\\n            \"name\": \"Evgenios M. Kornaropoulos\"\\n          },\\n          {\\n            \"authorId\": \"1700850\",\\n            \"name\": \"G. Ateniese\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs\\' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker\\'s LLM to disrupt their own operations (passive defense) or even compromise the attacker\\'s machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker\\'s LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project_mantis\"\\n      },\\n      {\\n        \"paperId\": \"2f274db9aa447a13c019114e327057d4b161b6d5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f274db9aa447a13c019114e327057d4b161b6d5\",\\n        \"title\": \"LLM-controller: Dynamic robot control adaptation using large language models\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.robot.2024.104913?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.robot.2024.104913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"90182090\",\\n            \"name\": \"Rasoul Zahedifar\"\\n          },\\n          {\\n            \"authorId\": \"1799503\",\\n            \"name\": \"M. Baghshah\"\\n          },\\n          {\\n            \"authorId\": \"2273939584\",\\n            \"name\": \"Alireza Taheri\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\\n        \"url\": \"https://www.semanticscholar.org/paper/47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\\n        \"title\": \"HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.19878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE is proposed, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-09-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2211429378\",\\n            \"name\": \"Bingshen Mu\"\\n          },\\n          {\\n            \"authorId\": \"2299944267\",\\n            \"name\": \"Kun Wei\"\\n          },\\n          {\\n            \"authorId\": \"2061559378\",\\n            \"name\": \"Qijie Shao\"\\n          },\\n          {\\n            \"authorId\": \"2323714781\",\\n            \"name\": \"Yong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2249732546\",\\n            \"name\": \"Lei Xie\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.\"\\n      },\\n      {\\n        \"paperId\": \"bcac614f9774488447221ebb4f16f05e3975ec1e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bcac614f9774488447221ebb4f16f05e3975ec1e\",\\n        \"title\": \"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\",\\n        \"citationCount\": 75,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.04669\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A well-designed visual tokenizer is introduced to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read, which empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2239056878\",\\n            \"name\": \"Yang Jin\"\\n          },\\n          {\\n            \"authorId\": \"2266735757\",\\n            \"name\": \"Kun Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266735757\",\\n            \"name\": \"Kun Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266419021\",\\n            \"name\": \"Liwei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2239059653\",\\n            \"name\": \"Chao Liao\"\\n          },\\n          {\\n            \"authorId\": \"2239091862\",\\n            \"name\": \"Jianchao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2007771781\",\\n            \"name\": \"Quzhe Huang\"\\n          },\\n          {\\n            \"authorId\": \"2230906921\",\\n            \"name\": \"Bin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2366079231\",\\n            \"name\": \"Chenyi Lei\"\\n          },\\n          {\\n            \"authorId\": \"2239069665\",\\n            \"name\": \"An Liu\"\\n          },\\n          {\\n            \"authorId\": \"2241686105\",\\n            \"name\": \"Chengru Song\"\\n          },\\n          {\\n            \"authorId\": \"2238955477\",\\n            \"name\": \"Xiaoqiang Lei\"\\n          },\\n          {\\n            \"authorId\": \"2228125963\",\\n            \"name\": \"Di Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2238953778\",\\n            \"name\": \"Wenwu Ou\"\\n          },\\n          {\\n            \"authorId\": \"2238953242\",\\n            \"name\": \"Kun Gai\"\\n          },\\n          {\\n            \"authorId\": \"2238953689\",\\n            \"name\": \"Yadong Mu\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model\\'s potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.\"\\n      },\\n      {\\n        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"58700f3740105e3422eb030305372b6d8bc44986\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58700f3740105e3422eb030305372b6d8bc44986\",\\n        \"title\": \"Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours, and can serve as an orthogonal optimization for synergistic integration with existing speculative decoding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2242179319\",\\n            \"name\": \"H. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2303652170\",\\n            \"name\": \"Wayne Luk\"\\n          },\\n          {\\n            \"authorId\": \"2301711440\",\\n            \"name\": \"Ka-Fai Cedric Yiu\"\\n          },\\n          {\\n            \"authorId\": \"2152153064\",\\n            \"name\": \"Rui Li\"\\n          },\\n          {\\n            \"authorId\": \"2303652428\",\\n            \"name\": \"Konstantin Mishchenko\"\\n          },\\n          {\\n            \"authorId\": \"2115955596\",\\n            \"name\": \"Stylianos I. Venieris\"\\n          },\\n          {\\n            \"authorId\": \"10001427\",\\n            \"name\": \"Hongxiang Fan\"\\n          }\\n        ],\\n        \"abstract\": \"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\\\\\\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\\\\\\\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.\"\\n      },\\n      {\\n        \"paperId\": \"427c3a502d132b6e1cea2d5565460d284db6e3f7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/427c3a502d132b6e1cea2d5565460d284db6e3f7\",\\n        \"title\": \"Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2507.00601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies and introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-06-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2345186311\",\\n            \"name\": \"Shuangquan Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2353085449\",\\n            \"name\": \"Yingnan Deng\"\\n          },\\n          {\\n            \"authorId\": \"2372425942\",\\n            \"name\": \"Guiran Liu\"\\n          },\\n          {\\n            \"authorId\": \"2374351029\",\\n            \"name\": \"Zhen Qi\"\\n          },\\n          {\\n            \"authorId\": \"2372322790\",\\n            \"name\": \"Ruotong Wang\"\\n          }\\n        ],\\n        \"abstract\": \"This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model\\'s original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method\\'s applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.\"\\n      },\\n      {\\n        \"paperId\": \"c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"title\": \"Dynamic and Adaptive Feature Generation with LLM\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research introduces a novel approach adopting large language models and feature-generating prompts to address challenges of explainability, applicability, and inflexible strategy and proposes a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130031226\",\\n            \"name\": \"XinHao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108045855\",\\n            \"name\": \"Jinghan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1966492\",\\n            \"name\": \"Banafsheh Rekabdar\"\\n          },\\n          {\\n            \"authorId\": \"2145108199\",\\n            \"name\": \"Yuanchun Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2301248160\",\\n            \"name\": \"Pengfei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293571072\",\\n            \"name\": \"Kunpeng Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\\n        \"title\": \"Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.04295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process and demonstrates measurable performance improvements compared to content-only optimization methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2344555074\",\\n            \"name\": \"Yuanye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257094139\",\\n            \"name\": \"Jiahang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2274195530\",\\n            \"name\": \"L. Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2344193091\",\\n            \"name\": \"Qi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2341721557\",\\n            \"name\": \"Xuan Feng\"\\n          },\\n          {\\n            \"authorId\": \"2344520491\",\\n            \"name\": \"Yang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2339241318\",\\n            \"name\": \"Zhongxin Guo\"\\n          },\\n          {\\n            \"authorId\": \"2344097630\",\\n            \"name\": \"Yuqing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2296029993\",\\n            \"name\": \"Peng Cheng\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 130,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"e3e3aa5576de899b755100db211501bb405aba3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3e3aa5576de899b755100db211501bb405aba3e\",\\n        \"title\": \"Firewalls to Secure Dynamic LLM Agentic Networks\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.01822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work identifies required properties for agent communication: proactivity, adaptability, privacy, privacy, and security, and proposes a practical design and protocol inspired by network security principles.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2343752567\",\\n            \"name\": \"Sahar Abdelnabi\"\\n          },\\n          {\\n            \"authorId\": \"2249532110\",\\n            \"name\": \"Amr Gomaa\"\\n          },\\n          {\\n            \"authorId\": \"36103467\",\\n            \"name\": \"Eugene Bagdasarian\"\\n          },\\n          {\\n            \"authorId\": \"2237674591\",\\n            \"name\": \"P. O. Kristensson\"\\n          },\\n          {\\n            \"authorId\": \"2346834097\",\\n            \"name\": \"Reza Shokri\"\\n          }\\n        ],\\n        \"abstract\": \"LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.\"\\n      },\\n      {\\n        \"paperId\": \"ee552989a03693a441863af4c29dc594bfcd1ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ee552989a03693a441863af4c29dc594bfcd1ab5\",\\n        \"title\": \"AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2503.18891?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2503.18891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316787389\",\\n            \"name\": \"Zhexuan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2306069252\",\\n            \"name\": \"Yutong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256344322\",\\n            \"name\": \"Xuebo Liu\"\\n          },\\n          {\\n            \"authorId\": \"46573238\",\\n            \"name\": \"Liang Ding\"\\n          },\\n          {\\n            \"authorId\": \"2187384924\",\\n            \"name\": \"Miao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2348727938\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2346352158\",\\n            \"name\": \"Min Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents\\\\u2019 communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout , which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yufeng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"273b2c64d675edd522cd6f679891756ad5207296\",\\n        \"url\": \"https://www.semanticscholar.org/paper/273b2c64d675edd522cd6f679891756ad5207296\",\\n        \"title\": \"Exploring the Effectiveness of LLM Domain Adaptation for Business IT Machine Translation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://aclanthology.org/2024.eamt-1.51, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that while LLMs can translate on-par with SAP\\\\u2019s MT models on general domain data, it is difficult to close the gap on SAP\\\\u2019s domain-specific data, even with extensive training and carefully curated data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2322393313\",\\n            \"name\": \"Johannes Eschbach-Dymanus\"\\n          },\\n          {\\n            \"authorId\": \"2322400027\",\\n            \"name\": \"Frank Essenberger\"\\n          },\\n          {\\n            \"authorId\": \"1403814959\",\\n            \"name\": \"Bianka Buschbeck-Wolf\"\\n          },\\n          {\\n            \"authorId\": \"70124681\",\\n            \"name\": \"Miriam Exel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\\n        \"title\": \"ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities\",\\n        \"citationCount\": 81,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283831590\",\\n            \"name\": \"Jiarui Lu\"\\n          },\\n          {\\n            \"authorId\": \"2315811087\",\\n            \"name\": \"Thomas Holleis\"\\n          },\\n          {\\n            \"authorId\": \"2313695880\",\\n            \"name\": \"Yizhe Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2315810003\",\\n            \"name\": \"Bernhard Aumayer\"\\n          },\\n          {\\n            \"authorId\": \"2313640225\",\\n            \"name\": \"Feng Nan\"\\n          },\\n          {\\n            \"authorId\": \"2313910532\",\\n            \"name\": \"Felix Bai\"\\n          },\\n          {\\n            \"authorId\": \"2313694040\",\\n            \"name\": \"Shuang Ma\"\\n          },\\n          {\\n            \"authorId\": \"2313694042\",\\n            \"name\": \"Shen Ma\"\\n          },\\n          {\\n            \"authorId\": \"2315946702\",\\n            \"name\": \"Mengyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2293171017\",\\n            \"name\": \"Guoli Yin\"\\n          },\\n          {\\n            \"authorId\": \"2313671930\",\\n            \"name\": \"Zirui Wang\"\\n          },\\n          {\\n            \"authorId\": \"2238621132\",\\n            \"name\": \"Ruoming Pang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at https://github.com/apple/ToolSandbox\"\\n      },\\n      {\\n        \"paperId\": \"44b0d2e884efa5344e50424dbe2edf616981f201\",\\n        \"url\": \"https://www.semanticscholar.org/paper/44b0d2e884efa5344e50424dbe2edf616981f201\",\\n        \"title\": \"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.00807\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.00807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work develops and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply and achieves substantially lower latency than standard reranking methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2127522115\",\\n            \"name\": \"Jon Saad-Falcon\"\\n          },\\n          {\\n            \"authorId\": \"144112155\",\\n            \"name\": \"O. Khattab\"\\n          },\\n          {\\n            \"authorId\": \"50818255\",\\n            \"name\": \"Keshav Santhanam\"\\n          },\\n          {\\n            \"authorId\": \"1707117\",\\n            \"name\": \"Radu Florian\"\\n          },\\n          {\\n            \"authorId\": \"39038065\",\\n            \"name\": \"M. Franz\"\\n          },\\n          {\\n            \"authorId\": \"1781292\",\\n            \"name\": \"S. Roukos\"\\n          },\\n          {\\n            \"authorId\": \"2707234\",\\n            \"name\": \"Avirup Sil\"\\n          },\\n          {\\n            \"authorId\": \"2937809\",\\n            \"name\": \"Md Arafat Sultan\"\\n          },\\n          {\\n            \"authorId\": \"144922861\",\\n            \"name\": \"Christopher Potts\"\\n          }\\n        ],\\n        \"abstract\": \"Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.\"\\n      },\\n      {\\n        \"paperId\": \"9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\\n        \"title\": \"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\",\\n        \"citationCount\": 212,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.02490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing, is introduced, which effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2181120463\",\\n            \"name\": \"Huiqiang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"1527099159\",\\n            \"name\": \"Yucheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2284970741\",\\n            \"name\": \"Chengruidong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108728536\",\\n            \"name\": \"Qianhui Wu\"\\n          },\\n          {\\n            \"authorId\": \"13289447\",\\n            \"name\": \"Xufang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2309738728\",\\n            \"name\": \"Surin Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2281867465\",\\n            \"name\": \"Zhenhua Han\"\\n          },\\n          {\\n            \"authorId\": \"2309244780\",\\n            \"name\": \"Amir H. Abdi\"\\n          },\\n          {\\n            \"authorId\": \"2305587638\",\\n            \"name\": \"Dongsheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2257359863\",\\n            \"name\": \"Chin-Yew Lin\"\\n          },\\n          {\\n            \"authorId\": \"2125051198\",\\n            \"name\": \"Yuqing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2160727304\",\\n            \"name\": \"Lili Qiu\"\\n          }\\n        ],\\n        \"abstract\": \"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\"\\n      },\\n      {\\n        \"paperId\": \"34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\\n        \"title\": \"GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting\",\\n        \"citationCount\": 91,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/30383/32447\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i21.30383?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i21.30383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously simultaneously, named GPT4MTS, and proposes a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2293923697\",\\n            \"name\": \"Kevin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2260272787\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.\"\\n      },\\n      {\\n        \"paperId\": \"c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\\n        \"title\": \"Do LLMs Understand Visual Anomalies? Uncovering LLM\\'s Capabilities in Zero-shot Anomaly Detection\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.09654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM), enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108678212\",\\n            \"name\": \"Jiaqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"51459472\",\\n            \"name\": \"Shaofeng Cai\"\\n          },\\n          {\\n            \"authorId\": \"2276607267\",\\n            \"name\": \"Fang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2188240935\",\\n            \"name\": \"Bengchin Ooi\"\\n          },\\n          {\\n            \"authorId\": \"2296743990\",\\n            \"name\": \"Junran Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA\\'s effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.\"\\n      },\\n      {\\n        \"paperId\": \"8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"title\": \"FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.15813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2247164321\",\\n            \"name\": \"Yu Lu\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"3446334\",\\n            \"name\": \"Hehe Fan\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n      },\\n      {\\n        \"paperId\": \"162f33c7799683ca9b0f193275fe7eec5a0b973f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/162f33c7799683ca9b0f193275fe7eec5a0b973f\",\\n        \"title\": \"EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.09618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EasyRef is introduced, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt and surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1571400317\",\\n            \"name\": \"Zhuofan Zong\"\\n          },\\n          {\\n            \"authorId\": \"2293242031\",\\n            \"name\": \"Dongzhi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2261489892\",\\n            \"name\": \"Bingqi Ma\"\\n          },\\n          {\\n            \"authorId\": \"12920342\",\\n            \"name\": \"Guanglu Song\"\\n          },\\n          {\\n            \"authorId\": \"2075457131\",\\n            \"name\": \"Hao Shao\"\\n          },\\n          {\\n            \"authorId\": \"2292263397\",\\n            \"name\": \"Dazhong Shen\"\\n          },\\n          {\\n            \"authorId\": \"2292207974\",\\n            \"name\": \"Yu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2261394248\",\\n            \"name\": \"Hongsheng Li\"\\n          }\\n        ],\\n        \"abstract\": \"Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although the tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent elements within multiple images through the training process, it necessitates specific finetuning for each distinct image group. This paper introduces EasyRef, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM\\'s representations into the diffusion process through adapters can easily generalize to unseen domains, mining the consistent visual elements within unseen data. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\\n      },\\n      {\\n        \"paperId\": \"ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"title\": \"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.10462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation, is proposed and experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2241468593\",\\n            \"name\": \"Hossein Rajabzadeh\"\\n          },\\n          {\\n            \"authorId\": \"9200111\",\\n            \"name\": \"Mojtaba Valipour\"\\n          },\\n          {\\n            \"authorId\": \"2284643707\",\\n            \"name\": \"Tianshu Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1996315\",\\n            \"name\": \"Marzieh S. Tahaei\"\\n          },\\n          {\\n            \"authorId\": \"2241480742\",\\n            \"name\": \"Hyock Ju Kwon\"\\n          },\\n          {\\n            \"authorId\": \"2237425782\",\\n            \"name\": \"Ali Ghodsi\"\\n          },\\n          {\\n            \"authorId\": \"2237517964\",\\n            \"name\": \"Boxing Chen\"\\n          },\\n          {\\n            \"authorId\": \"2066076226\",\\n            \"name\": \"Mehdi Rezagholizadeh\"\\n          }\\n        ],\\n        \"abstract\": \"Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n      },\\n      {\\n        \"paperId\": \"9a73effed8775962c86587feb0f9ef841fa2ff4c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a73effed8775962c86587feb0f9ef841fa2ff4c\",\\n        \"title\": \"LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258550477\",\\n            \"name\": \"Yash Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2258751660\",\\n            \"name\": \"Wenchang Gao\"\\n          },\\n          {\\n            \"authorId\": \"3379438\",\\n            \"name\": \"Vasanth Sarathy\"\\n          },\\n          {\\n            \"authorId\": \"2258715054\",\\n            \"name\": \"Alvaro Velasquez\"\\n          },\\n          {\\n            \"authorId\": \"2258551993\",\\n            \"name\": \"Robert Wright\"\\n          },\\n          {\\n            \"authorId\": \"1715858\",\\n            \"name\": \"Jivko Sinapov\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a propreitary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the LLM proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.\"\\n      },\\n      {\\n        \"paperId\": \"108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\\n        \"url\": \"https://www.semanticscholar.org/paper/108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\\n        \"title\": \"TimeRAG: Boosting LLM Time Series Forecasting via Retrieval-Augmented Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.16643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TimeRAG is proposed, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2206558646\",\\n            \"name\": \"Si-Nan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2337389407\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2336916957\",\\n            \"name\": \"Haoqi Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2336884221\",\\n            \"name\": \"Ruochun Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.\"\\n      },\\n      {\\n        \"paperId\": \"1e6325865e809670765bea9dadd3c40b2014eb6d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e6325865e809670765bea9dadd3c40b2014eb6d\",\\n        \"title\": \"VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The performance contribution of different prompt components is explored, the vision for future improvement in visual accessibility, and the way for LLMs in video anomaly detection and vision-language understanding is paved.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256768778\",\\n            \"name\": \"Hao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292146470\",\\n            \"name\": \"Jiayou Qin\"\\n          },\\n          {\\n            \"authorId\": \"2260725391\",\\n            \"name\": \"Ashish Bastola\"\\n          },\\n          {\\n            \"authorId\": \"2024833342\",\\n            \"name\": \"Xiwen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2292183597\",\\n            \"name\": \"John Suchanek\"\\n          },\\n          {\\n            \"authorId\": \"2292143727\",\\n            \"name\": \"Zihao Gong\"\\n          },\\n          {\\n            \"authorId\": \"2064311884\",\\n            \"name\": \"Abolfazl Razi\"\\n          }\\n        ],\\n        \"abstract\": \"This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.\"\\n      },\\n      {\\n        \"paperId\": \"7f96bb27a8fca35b1f7d02ee319a64be04114809\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7f96bb27a8fca35b1f7d02ee319a64be04114809\",\\n        \"title\": \"LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.08337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties, and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265927745\",\\n            \"name\": \"Maonan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2219695355\",\\n            \"name\": \"Aoyu Pang\"\\n          },\\n          {\\n            \"authorId\": \"7592365\",\\n            \"name\": \"Yuheng Kan\"\\n          },\\n          {\\n            \"authorId\": \"144305489\",\\n            \"name\": \"Man-On Pun\"\\n          },\\n          {\\n            \"authorId\": \"2292117616\",\\n            \"name\": \"Chung Shue Chen\"\\n          },\\n          {\\n            \"authorId\": \"2291077490\",\\n            \"name\": \"Bo Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system\\'s adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\\\\\\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.\"\\n      },\\n      {\\n        \"paperId\": \"650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"title\": \"Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.acl-srw.90?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.acl-srw.90, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Time-LlaMA framework achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2346152667\",\\n            \"name\": \"Juyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334602368\",\\n            \"name\": \"Jiechao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2374971695\",\\n            \"name\": \"Wenwen Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2334718086\",\\n            \"name\": \"Wei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2321408906\",\\n            \"name\": \"Hui Yi Leong\"\\n          }\\n        ],\\n        \"abstract\": \"Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model\\\\u2019s predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 212,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\\n        \"title\": \"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair\",\\n        \"citationCount\": 188,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2176865575\",\\n            \"name\": \"Islem Bouzenia\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"2260683361\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": \"Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent\\'s effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI\\'s GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\\n      },\\n      {\\n        \"paperId\": \"3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\\n        \"title\": \"Token-Budget-Aware LLM Reasoning\",\\n        \"citationCount\": 114,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem is proposed, offering a practical solution to balance efficiency and accuracy in LLM reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170360833\",\\n            \"name\": \"Tingxu Han\"\\n          },\\n          {\\n            \"authorId\": \"2154723145\",\\n            \"name\": \"Zhenting Wang\"\\n          },\\n          {\\n            \"authorId\": \"2239197945\",\\n            \"name\": \"Chunrong Fang\"\\n          },\\n          {\\n            \"authorId\": \"2110773055\",\\n            \"name\": \"Shiyun Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2333472479\",\\n            \"name\": \"Shiqing Ma\"\\n          },\\n          {\\n            \"authorId\": \"2238950128\",\\n            \"name\": \"Zhenyu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE\"\\n      },\\n      {\\n        \"paperId\": \"20843eaa59db5e2af416d7db47d51d0aab3de230\",\\n        \"url\": \"https://www.semanticscholar.org/paper/20843eaa59db5e2af416d7db47d51d0aab3de230\",\\n        \"title\": \"Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue Resolution\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.11709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper analyzes 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution and identifies key textual and code-related heuristics\\\\u2014Specificity, Contextual Richness, and Clarity\\\\u2014that are associated with successful issue closure and help assess prompt quality.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2147154362\",\\n            \"name\": \"Ramtin Ehsani\"\\n          },\\n          {\\n            \"authorId\": \"2341336471\",\\n            \"name\": \"Sakshi Pathak\"\\n          },\\n          {\\n            \"authorId\": \"9728244\",\\n            \"name\": \"Preetha Chatterjee\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have become essential in software development, especially for issue resolution. However, despite their widespread use, significant challenges persist in the quality of LLM responses to issue resolution queries. LLM interactions often yield incorrect, incomplete, or ambiguous information, largely due to knowledge gaps in prompt design, which can lead to unproductive exchanges and reduced developer productivity.In this paper, we analyze 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution. We identify four main knowledge gaps in developer prompts: Missing Context, Missing Specifications, Multiple Context, and Unclear Instructions. Assuming that conversations within closed issues contributed to successful resolutions while those in open issues did not, we find that ineffective conversations contain knowledge gaps in $44.6 \\\\\\\\%$ of prompts, compared to only $12.6 \\\\\\\\%$ in effective ones. Additionally, we observe seven distinct conversational styles, with Directive Prompting, Chain of Thought, and Responsive Feedback being the most prevalent. We find that knowledge gaps are present in all styles of conversations, with Missing Context being the most repeated challenge developers face in issue-resolution conversations.Based on our analysis, we identify key textual and code-related heuristics\\\\u2014Specificity, Contextual Richness, and Clarity\\\\u2014that are associated with successful issue closure and help assess prompt quality. These heuristics lay the foundation for an automated tool that can dynamically flag unclear prompts and suggest structured improvements. To test feasibility, we developed a lightweight browser extension prototype for detecting prompt gaps, that can be easily adapted to other tools within developer workflows.\"\\n      },\\n      {\\n        \"paperId\": \"25cee84e3a1541697a7c97443d7526574127c344\",\\n        \"url\": \"https://www.semanticscholar.org/paper/25cee84e3a1541697a7c97443d7526574127c344\",\\n        \"title\": \"Don\\'t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\",\\n        \"citationCount\": 158,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.00367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs, and proposes two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2114887261\",\\n            \"name\": \"Shangbin Feng\"\\n          },\\n          {\\n            \"authorId\": \"2254168375\",\\n            \"name\": \"Weijia Shi\"\\n          },\\n          {\\n            \"authorId\": \"2108853330\",\\n            \"name\": \"Yike Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282214127\",\\n            \"name\": \"Wenxuan Ding\"\\n          },\\n          {\\n            \"authorId\": \"143820870\",\\n            \"name\": \"Vidhisha Balachandran\"\\n          },\\n          {\\n            \"authorId\": \"2249583325\",\\n            \"name\": \"Yulia Tsvetkov\"\\n          }\\n        ],\\n        \"abstract\": \"Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"d5342fce96175f83550cfae471a0a46d16401481\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d5342fce96175f83550cfae471a0a46d16401481\",\\n        \"title\": \"ST-LLM: Large Language Models Are Effective Temporal Learners\",\\n        \"citationCount\": 120,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM, and develops a dynamic masking strategy with tailor-made training objectives to address the overhead and stability issues introduced by uncompressed video tokens within LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"12287885\",\\n            \"name\": \"Ruyang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256784925\",\\n            \"name\": \"Chen Li\"\\n          },\\n          {\\n            \"authorId\": \"2294629231\",\\n            \"name\": \"Haoran Tang\"\\n          },\\n          {\\n            \"authorId\": \"152988335\",\\n            \"name\": \"Yixiao Ge\"\\n          },\\n          {\\n            \"authorId\": \"2265579883\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2294517847\",\\n            \"name\": \"Ge Li\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at https://github.com/TencentARC/ST-LLM.\"\\n      },\\n      {\\n        \"paperId\": \"12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\\n        \"title\": \"Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.17282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with, and performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218970025\",\\n            \"name\": \"Yi-Kai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2274937712\",\\n            \"name\": \"De-Chuan Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2151459740\",\\n            \"name\": \"Han-Jia Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"edfff0e15449f438a13a7341290c008bf6486afc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edfff0e15449f438a13a7341290c008bf6486afc\",\\n        \"title\": \"MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.18035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism, which significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2327293591\",\\n            \"name\": \"Jingfan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2322997112\",\\n            \"name\": \"Yi Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2327694040\",\\n            \"name\": \"Dan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2293390999\",\\n            \"name\": \"Xing Tian\"\\n          },\\n          {\\n            \"authorId\": \"2179528564\",\\n            \"name\": \"Huanran Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2322888603\",\\n            \"name\": \"Wei Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 203,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"ab4800a924508f49d644ced8ba236ec92f54f566\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ab4800a924508f49d644ced8ba236ec92f54f566\",\\n        \"title\": \"Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2505.16552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach and enhances CoLaR through reinforcement learning (RL) that leverages the latent head\\'s non-deterministic nature to explore diverse reasoning paths and exploit more compact ones.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218557953\",\\n            \"name\": \"Wenhui Tan\"\\n          },\\n          {\\n            \"authorId\": \"2362865102\",\\n            \"name\": \"Jiaze Li\"\\n          },\\n          {\\n            \"authorId\": \"2317982861\",\\n            \"name\": \"Jianzhong Ju\"\\n          },\\n          {\\n            \"authorId\": \"2363405807\",\\n            \"name\": \"Zhenbo Luo\"\\n          },\\n          {\\n            \"authorId\": \"2317980688\",\\n            \"name\": \"Jian Luan\"\\n          },\\n          {\\n            \"authorId\": \"2290923147\",\\n            \"name\": \"Ruihua Song\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head\\'s non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance.\"\\n      },\\n      {\\n        \"paperId\": \"268e28f8d5235031dcd7bfae0f857439e27e8564\",\\n        \"url\": \"https://www.semanticscholar.org/paper/268e28f8d5235031dcd7bfae0f857439e27e8564\",\\n        \"title\": \"SteP: Stacked LLM Policies for Web Actions\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.03720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Stacked LLM Policies for Web Actions is proposed, an approach to dynamically compose policies to solve a diverse set of web tasks and is competitive with prior works while using significantly less data.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-10-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2254267756\",\\n            \"name\": \"Paloma Sodhi\"\\n          },\\n          {\\n            \"authorId\": \"1741598\",\\n            \"name\": \"S. Branavan\"\\n          },\\n          {\\n            \"authorId\": \"2066324938\",\\n            \"name\": \"Yoav Artzi\"\\n          },\\n          {\\n            \"authorId\": \"2254260284\",\\n            \"name\": \"Ryan McDonald\"\\n          }\\n        ],\\n        \"abstract\": \"Performing tasks on the web presents fundamental challenges to large language models (LLMs), including combinatorially large open-world tasks and variations across web interfaces. Simply specifying a large prompt to handle all possible behaviors and states is extremely complex, and results in behavior leaks between unrelated behaviors. Decomposition to distinct policies can address this challenge, but requires carefully handing off control between policies. We propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically compose policies to solve a diverse set of web tasks. SteP defines a Markov Decision Process where the state is a stack of policies representing the control state, i.e., the chain of policy calls. Unlike traditional methods that are restricted to static hierarchies, SteP enables dynamic control that adapts to the complexity of the task. We evaluate SteP against multiple baselines and web environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP improves (14.9\\\\\\\\% to 33.5\\\\\\\\%) over SOTA that use GPT-4 policies, while on MiniWob++, SteP is competitive with prior works while using significantly less data. Our code and data are available at https://asappresearch.github.io/webagents-step.\"\\n      },\\n      {\\n        \"paperId\": \"1f9822022f586e375461660db792f23e891c7123\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1f9822022f586e375461660db792f23e891c7123\",\\n        \"title\": \"Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems\",\\n        \"citationCount\": 62,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2307.06187\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.06187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models, grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2658311\",\\n            \"name\": \"N. Nascimento\"\\n          },\\n          {\\n            \"authorId\": \"40761174\",\\n            \"name\": \"Paulo Alencar\"\\n          },\\n          {\\n            \"authorId\": \"2149928782\",\\n            \"name\": \"Donald D. Cowan\"\\n          }\\n        ],\\n        \"abstract\": \"The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs\\' capabilities and indicating further research opportunities to assess LLMs\\' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.\"\\n      },\\n      {\\n        \"paperId\": \"3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\\n        \"title\": \"Bridging the Gulf of Envisioning: Cognitive Challenges in Prompt Based Interactions with LLMs\",\\n        \"citationCount\": 91,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642754\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work theorizes how end-users \\\\u2018envision\\\\u2019 translating their goals into clear intentions and craft prompts to obtain the desired LLM response.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2301051721\",\\n            \"name\": \"Hariharan Subramonyam\"\\n          },\\n          {\\n            \"authorId\": \"2246886383\",\\n            \"name\": \"Roy D. Pea\"\\n          },\\n          {\\n            \"authorId\": \"1825757380\",\\n            \"name\": \"Christopher Pondoc\"\\n          },\\n          {\\n            \"authorId\": \"1820412\",\\n            \"name\": \"Maneesh Agrawala\"\\n          },\\n          {\\n            \"authorId\": \"2289103973\",\\n            \"name\": \"Colleen M. Seifert\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman\\\\u2019s gulfs of execution and evaluation. To address this gap, we theorize how end-users \\\\u2018envision\\\\u2019 translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments on not knowing: (1) what the task should be, (2) how to instruct the LLM to do the task, and (3) what to expect for the LLM\\\\u2019s output in meeting the goal. Finally, we make recommendations to narrow the gulf of envisioning in human-LLM interactions.\"\\n      },\\n      {\\n        \"paperId\": \"28d6411019f448f54834c2a5cff723cd350345b5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/28d6411019f448f54834c2a5cff723cd350345b5\",\\n        \"title\": \"Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC) involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2289252454\",\\n            \"name\": \"Haokun Liu\"\\n          },\\n          {\\n            \"authorId\": \"8247318\",\\n            \"name\": \"Yaonan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2110285301\",\\n            \"name\": \"Kenji Kato\"\\n          },\\n          {\\n            \"authorId\": \"2307380233\",\\n            \"name\": \"Atsushi Tsukahara\"\\n          },\\n          {\\n            \"authorId\": \"2282115338\",\\n            \"name\": \"Izumi Kondo\"\\n          },\\n          {\\n            \"authorId\": \"1752849\",\\n            \"name\": \"T. Aoyama\"\\n          },\\n          {\\n            \"authorId\": \"2237520520\",\\n            \"name\": \"Yasuhisa Hasegawa\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This letter proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.\"\\n      },\\n      {\\n        \"paperId\": \"77a9c310df0d7896d297da90fc4a1131819c341e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/77a9c310df0d7896d297da90fc4a1131819c341e\",\\n        \"title\": \"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding, and proposes LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293615241\",\\n            \"name\": \"Mingxing Peng\"\\n          },\\n          {\\n            \"authorId\": \"2293665950\",\\n            \"name\": \"Xusen Guo\"\\n          },\\n          {\\n            \"authorId\": \"2146413818\",\\n            \"name\": \"Xianda Chen\"\\n          },\\n          {\\n            \"authorId\": \"2241024418\",\\n            \"name\": \"Meixin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2267078966\",\\n            \"name\": \"Kehua Chen\"\\n          },\\n          {\\n            \"authorId\": \"2293775145\",\\n            \"name\": \"Hao Yang\"\\n          },\\n          {\\n            \"authorId\": \"2258778041\",\\n            \"name\": \"Xuesong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2258755636\",\\n            \"name\": \"Yinhai Wang\"\\n          }\\n        ],\\n        \"abstract\": \"To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.\"\\n      }\\n    ]\\n  },\\n  \"coherence in extended LLM\": {\\n    \"total\": 23,\\n    \"offset\": 0,\\n    \"data\": [\\n      {\\n        \"paperId\": \"d0da372b4b6f422e13556ce272595a0c9002fe90\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d0da372b4b6f422e13556ce272595a0c9002fe90\",\\n        \"title\": \"GPT-4 in Education: Evaluating Aptness, Reliability, and Loss of Coherence in Solving Calculus Problems and Grading Submissions\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s40593-024-00403-3.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40593-024-00403-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40593-024-00403-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256969876\",\\n            \"name\": \"Alberto Gandolfi\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we initially investigate the capabilities of GPT-3 5 and GPT-4 in solving college-level calculus problems, an essential segment of mathematics that remains under-explored so far. Although improving upon earlier versions, GPT-4 attains approximately 65% accuracy for standard problems and decreases to 20% for competition-like scenarios. Overall, the models prove to be unreliable due to common arithmetic errors. Our primary contribution lies then in examining the use of ChatGPT for grading solutions to calculus exercises. Our objectives are to probe an in-context learning task with less emphasis over direct calculations; recognize positive applications of ChatGPT in educational contexts; highlight a potentially emerging facet of AI that could necessitate oversight; and introduce unconventional AI benchmarks, for which models like GPT are untrained. Pertaining to the latter, we uncover a tendency for loss of coherence in extended contexts. Our findings suggest that while the current ChatGPT exhibits comprehension of the grading task and often provides relevant outputs, the consistency of grading is marred by occasional loss of coherence and hallucinations. Intriguingly, GPT-4\\'s overall scores, delivered in mere moments, align closely with human graders, although its detailed accuracy remains suboptimal. This work suggests that, when appropriately orchestrated, collaboration between human graders and LLMs like GPT-4 might combine their unique strengths while mitigating their respective shortcomings In this direction, it is imperative to consider implementing transparency, fairness, and appropriate regulations in the near future.\"\\n      },\\n      {\\n        \"paperId\": \"49b7bd275a0386392769f5b33028500754dbc69d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/49b7bd275a0386392769f5b33028500754dbc69d\",\\n        \"title\": \"AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE), and reveals that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2317040111\",\\n            \"name\": \"Abhay Gupta\"\\n          },\\n          {\\n            \"authorId\": \"2317010916\",\\n            \"name\": \"Philip Meng\"\\n          },\\n          {\\n            \"authorId\": \"2317007185\",\\n            \"name\": \"Ece Yurtseven\"\\n          },\\n          {\\n            \"authorId\": \"2241351144\",\\n            \"name\": \"Sean O\\'Brien\"\\n          },\\n          {\\n            \"authorId\": \"2312105716\",\\n            \"name\": \"Kevin Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting biases in natural language understanding (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language processing (NLP) systems. To address dialect-induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE). AAVENUE builds upon and extends existing benchmarks like VALUE, replacing deterministic syntactic and morphological transformations with a more flexible methodology leveraging LLM-based translation with few-shot prompting, improving performance across our evaluation metrics when translating key tasks from the GLUE and SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics including fluency, BARTScore, quality, coherence, and understandability. Additionally, we recruit fluent AAVE speakers to validate our translations for authenticity. Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\\n      },\\n      {\\n        \"paperId\": \"e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\\n        \"title\": \"DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.findings-emnlp.606.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME), which proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2110546424\",\\n            \"name\": \"Weijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261645232\",\\n            \"name\": \"Wenxiang Hu\"\\n          },\\n          {\\n            \"authorId\": \"2261413304\",\\n            \"name\": \"Fanyou Wu\"\\n          },\\n          {\\n            \"authorId\": \"1757518\",\\n            \"name\": \"Srinivasan H. Sengamedu\"\\n          }\\n        ],\\n        \"abstract\": \"In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME\\'s potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\\n      },\\n      {\\n        \"paperId\": \"0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\\n        \"title\": \"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An evaluation framework for mental health chatbots is validated, proving its effectiveness in improving safety and reliability and highlighting the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-08-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2315914561\",\\n            \"name\": \"Jung In Park\"\\n          },\\n          {\\n            \"authorId\": \"2182148069\",\\n            \"name\": \"Mahyar Abbasian\"\\n          },\\n          {\\n            \"authorId\": \"2241201441\",\\n            \"name\": \"Iman Azimi\"\\n          },\\n          {\\n            \"authorId\": \"2315810363\",\\n            \"name\": \"Dawn Bounds\"\\n          },\\n          {\\n            \"authorId\": \"2315810053\",\\n            \"name\": \"Angela Jun\"\\n          },\\n          {\\n            \"authorId\": \"2315889398\",\\n            \"name\": \"Jaesu Han\"\\n          },\\n          {\\n            \"authorId\": \"2315811390\",\\n            \"name\": \"Robert McCarron\"\\n          },\\n          {\\n            \"authorId\": \"2297708916\",\\n            \"name\": \"Jessica Borelli\"\\n          },\\n          {\\n            \"authorId\": \"2348273518\",\\n            \"name\": \"Parmida Safavi\"\\n          },\\n          {\\n            \"authorId\": \"2348305728\",\\n            \"name\": \"Sanaz Mirbaha\"\\n          },\\n          {\\n            \"authorId\": \"2315875066\",\\n            \"name\": \"Jia Li\"\\n          },\\n          {\\n            \"authorId\": \"2315811652\",\\n            \"name\": \"Mona Mahmoudi\"\\n          },\\n          {\\n            \"authorId\": \"2315811354\",\\n            \"name\": \"Carmen Wiedenhoeft\"\\n          },\\n          {\\n            \"authorId\": \"2311169857\",\\n            \"name\": \"Amir M. Rahmani\"\\n          }\\n        ],\\n        \"abstract\": \"Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Materials and Methods: We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. Results: The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Discussion: Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. Conclusion: The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.\"\\n      },\\n      {\\n        \"paperId\": \"09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\\n        \"url\": \"https://www.semanticscholar.org/paper/09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\\n        \"title\": \"Asynchronous LLM Function Calling\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.07017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An in-context protocol for function calls and interrupts is designed, an fine-tuning strategy is provided to adapt LLMs to the interrupt semantics, and mechanisms to asynchronously notify the LLM in-flight when function calls return are implemented.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265756791\",\\n            \"name\": \"In Gim\"\\n          },\\n          {\\n            \"authorId\": \"2118065368\",\\n            \"name\": \"Seung-seob Lee\"\\n          },\\n          {\\n            \"authorId\": \"2323908057\",\\n            \"name\": \"Lin Zhong\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM\\'s operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call\\'s completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.\"\\n      },\\n      {\\n        \"paperId\": \"3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\\n        \"title\": \"Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-024-56309-6.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10925587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies, and demonstrating a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260831947\",\\n            \"name\": \"Juho Jung\"\\n          },\\n          {\\n            \"authorId\": \"2152569226\",\\n            \"name\": \"Jinyoung Han\"\\n          },\\n          {\\n            \"authorId\": \"6777367\",\\n            \"name\": \"J. Han\"\\n          },\\n          {\\n            \"authorId\": \"2088247339\",\\n            \"name\": \"Junseo Ko\"\\n          },\\n          {\\n            \"authorId\": \"1677558107\",\\n            \"name\": \"Jeewoo Yoon\"\\n          },\\n          {\\n            \"authorId\": \"39548326\",\\n            \"name\": \"J. Hwang\"\\n          },\\n          {\\n            \"authorId\": \"2265721342\",\\n            \"name\": \"Ji In Park\"\\n          },\\n          {\\n            \"authorId\": \"2054436718\",\\n            \"name\": \"Gyudeok Hwang\"\\n          },\\n          {\\n            \"authorId\": \"2290973453\",\\n            \"name\": \"Jae Ho Jung\"\\n          },\\n          {\\n            \"authorId\": \"2003794778\",\\n            \"name\": \"Daniel Duck-Jin Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"Neovascular age-related macular degeneration (nAMD) can result in blindness if left untreated, and patients often require repeated anti-vascular endothelial growth factor injections. Although, the treat-and-extend method is becoming popular to reduce vision loss attributed to recurrence, it may pose a risk of overtreatment. This study aimed to develop a deep learning model based on DenseNet201 to predict nAMD recurrence within 3 months after confirming dry-up 1 month following three loading injections in treatment-na\\\\u00efve patients. A dataset of 1076 spectral domain optical coherence tomography (OCT) images from 269 patients diagnosed with nAMD was used. The performance of the model was compared with that of 6 ophthalmologists, using 100 randomly selected samples. The DenseNet201-based model achieved 53.0% accuracy in predicting nAMD recurrence using a single pre-injection image and 60.2% accuracy after viewing all the images immediately after the 1st, 2nd, and 3rd injections. The model outperformed experienced ophthalmologists, with an average accuracy of 52.17% using a single pre-injection image and 53.3% after examining four images before and after three loading injections. In conclusion, the artificial intelligence model demonstrated a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists. These findings suggest that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies.\"\\n      },\\n      {\\n        \"paperId\": \"b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\\n        \"title\": \"Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.03275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a hybrid representation of the reasoning process, where the initial reasoning steps are partially abstracted away using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2325888815\",\\n            \"name\": \"DiJia Su\"\\n          },\\n          {\\n            \"authorId\": \"2255310892\",\\n            \"name\": \"Hanlin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2269737738\",\\n            \"name\": \"Yingchen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          },\\n          {\\n            \"authorId\": \"2285362895\",\\n            \"name\": \"Yuandong Tian\"\\n          },\\n          {\\n            \"authorId\": \"2326106870\",\\n            \"name\": \"Qinqing Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\\n        \"title\": \"13.4 A 48GB 16-High 1280GB/s HBM3E DRAM with All-Around Power TSV and a 6-Phase RDQS Scheme for TSV Area Optimization\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISSCC49657.2024.10454440?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISSCC49657.2024.10454440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287764300\",\\n            \"name\": \"Jinhyung Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287959684\",\\n            \"name\": \"Kyungjun Cho\"\\n          },\\n          {\\n            \"authorId\": \"2109114249\",\\n            \"name\": \"C. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287956252\",\\n            \"name\": \"Yeonho Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287119443\",\\n            \"name\": \"Jae-Hyung Park\"\\n          },\\n          {\\n            \"authorId\": \"2286899300\",\\n            \"name\": \"Su-Hyun Oh\"\\n          },\\n          {\\n            \"authorId\": \"1640393542\",\\n            \"name\": \"Yucheon Ju\"\\n          },\\n          {\\n            \"authorId\": \"3365606\",\\n            \"name\": \"Chunseok Jeong\"\\n          },\\n          {\\n            \"authorId\": \"123947284\",\\n            \"name\": \"H. Cho\"\\n          },\\n          {\\n            \"authorId\": \"2198615241\",\\n            \"name\": \"Jaeseung Lee\"\\n          },\\n          {\\n            \"authorId\": \"3376046\",\\n            \"name\": \"T. Yun\"\\n          },\\n          {\\n            \"authorId\": \"2292215366\",\\n            \"name\": \"Jin Hee Cho\"\\n          },\\n          {\\n            \"authorId\": \"3375969\",\\n            \"name\": \"Sangmuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"1640397693\",\\n            \"name\": \"J. Moon\"\\n          },\\n          {\\n            \"authorId\": \"2110426052\",\\n            \"name\": \"Y. Park\"\\n          },\\n          {\\n            \"authorId\": \"2291083018\",\\n            \"name\": \"Hong-Seok Choi\"\\n          },\\n          {\\n            \"authorId\": \"2159532637\",\\n            \"name\": \"In-Keun Kim\"\\n          },\\n          {\\n            \"authorId\": \"2286904054\",\\n            \"name\": \"Seung Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"2286933095\",\\n            \"name\": \"Sun-Yeol Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291078685\",\\n            \"name\": \"Jaemin Jang\"\\n          },\\n          {\\n            \"authorId\": \"2292140319\",\\n            \"name\": \"Jinwook Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108644317\",\\n            \"name\": \"S. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2286906698\",\\n            \"name\": \"Younghyun Jeon\"\\n          },\\n          {\\n            \"authorId\": \"2292172289\",\\n            \"name\": \"Juhyung Park\"\\n          },\\n          {\\n            \"authorId\": \"2159571158\",\\n            \"name\": \"Tae-Kyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"82375019\",\\n            \"name\": \"D. Ka\"\\n          },\\n          {\\n            \"authorId\": \"2159321700\",\\n            \"name\": \"Sanghoon Oh\"\\n          },\\n          {\\n            \"authorId\": \"2292143044\",\\n            \"name\": \"Jinse Kim\"\\n          },\\n          {\\n            \"authorId\": \"2159244890\",\\n            \"name\": \"Junyeol Jeon\"\\n          },\\n          {\\n            \"authorId\": \"2292173125\",\\n            \"name\": \"Seonhong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291075551\",\\n            \"name\": \"Kyeong Tae Kim\"\\n          },\\n          {\\n            \"authorId\": \"2154855420\",\\n            \"name\": \"Tae-Hwan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291083403\",\\n            \"name\": \"Hyeonjin Yang\"\\n          },\\n          {\\n            \"authorId\": \"2291072681\",\\n            \"name\": \"Dongju Yang\"\\n          },\\n          {\\n            \"authorId\": \"2291084224\",\\n            \"name\": \"Minseop Lee\"\\n          },\\n          {\\n            \"authorId\": \"30684992\",\\n            \"name\": \"Heewoong Song\"\\n          },\\n          {\\n            \"authorId\": \"2291069292\",\\n            \"name\": \"Dongwook Jang\"\\n          },\\n          {\\n            \"authorId\": \"2287120235\",\\n            \"name\": \"Junghyun Shin\"\\n          },\\n          {\\n            \"authorId\": \"2287261607\",\\n            \"name\": \"Hyunsik Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291041862\",\\n            \"name\": \"Changki Baek\"\\n          },\\n          {\\n            \"authorId\": \"2291023817\",\\n            \"name\": \"Hajun Jeong\"\\n          },\\n          {\\n            \"authorId\": \"2291081668\",\\n            \"name\": \"Jongchan Yoon\"\\n          },\\n          {\\n            \"authorId\": \"1641325386\",\\n            \"name\": \"SeungGyeon Lim\"\\n          },\\n          {\\n            \"authorId\": \"2110630984\",\\n            \"name\": \"Kyo Yun Lee\"\\n          },\\n          {\\n            \"authorId\": \"2159499427\",\\n            \"name\": \"Young Jun Koo\"\\n          },\\n          {\\n            \"authorId\": \"2287107051\",\\n            \"name\": \"Myeong-Jae Park\"\\n          },\\n          {\\n            \"authorId\": \"2510417\",\\n            \"name\": \"Joohwan Cho\"\\n          },\\n          {\\n            \"authorId\": \"2291056079\",\\n            \"name\": \"Jonghwan Kim\"\\n          }\\n        ],\\n        \"abstract\": \"With the emergence of large-language models (LLM) and generative AI, which require an enormous amount of model parameters, the required memory bandwidth and capacity for high-end systems is on an unprecedented increase. To meet this need, we present an extended version of the high-bandwidth memory-3 (HBM3 DRAM), HBM3E, which achieves a 1280GB/s bandwidth with a cube density of 48GB. New design schemes and features, such as all-around power-through-silicon via (TSV), a 6-phase read-data-strobe (RDQS) scheme, a byte-mapping swap scheme, and a voltage-drift compensator for write data strobe (WDQS), are implemented to achieve extended bandwidth and capacity with enhanced reliability. The overall architecture and specifications, such as bump map footprint, the number of channel and I/Os, and the operation voltage, are identical to the latest HBM3 [1, 2]; therefore, backward compatibility is provided, avoiding system modification.\"\\n      },\\n      {\\n        \"paperId\": \"76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\\n        \"title\": \"NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper details the design and construction of NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35504092\",\\n            \"name\": \"Cunxiang Wang\"\\n          },\\n          {\\n            \"authorId\": \"30819687\",\\n            \"name\": \"Ruoxi Ning\"\\n          },\\n          {\\n            \"authorId\": \"2292184774\",\\n            \"name\": \"Boqi Pan\"\\n          },\\n          {\\n            \"authorId\": \"2292208424\",\\n            \"name\": \"Tonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"3187768\",\\n            \"name\": \"Qipeng Guo\"\\n          },\\n          {\\n            \"authorId\": \"2292147095\",\\n            \"name\": \"Cheng Deng\"\\n          },\\n          {\\n            \"authorId\": \"1993226927\",\\n            \"name\": \"Guangsheng Bao\"\\n          },\\n          {\\n            \"authorId\": \"2292261580\",\\n            \"name\": \"Qian Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261496744\",\\n            \"name\": \"Yue Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models\\' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.\"\\n      },\\n      {\\n        \"paperId\": \"dda8031682684655744c7001374e6cb88c9503bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dda8031682684655744c7001374e6cb88c9503bd\",\\n        \"title\": \"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.10902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Soda-Eval is introduced, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, and the performance of several open-access instruction-tuned LLMs is studied, finding that dialogue evaluation remains challenging.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2007581062\",\\n            \"name\": \"John Mendon\\\\u00e7a\"\\n          },\\n          {\\n            \"authorId\": \"2268558660\",\\n            \"name\": \"Isabel Trancoso\"\\n          },\\n          {\\n            \"authorId\": \"1784914\",\\n            \"name\": \"A. Lavie\"\\n          }\\n        ],\\n        \"abstract\": \"Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.\"\\n      },\\n      {\\n        \"paperId\": \"2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"title\": \"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\",\\n        \"citationCount\": 100,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.07542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155795167\",\\n            \"name\": \"Chengzu Li\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2339967968\",\\n            \"name\": \"Huanyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2273419590\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2294850817\",\\n            \"name\": \"Li Dong\"\\n          },\\n          {\\n            \"authorId\": \"2339667880\",\\n            \"name\": \"Ivan Vuli\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"78875987dc674fc556873df037cf114f04932e80\",\\n        \"url\": \"https://www.semanticscholar.org/paper/78875987dc674fc556873df037cf114f04932e80\",\\n        \"title\": \"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\",\\n        \"citationCount\": 83,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2401.07764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.07764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A split learning system for LLM agents in 6G networks is proposed, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1454018677\",\\n            \"name\": \"Minrui Xu\"\\n          },\\n          {\\n            \"authorId\": \"1713586\",\\n            \"name\": \"D. Niyato\"\\n          },\\n          {\\n            \"authorId\": \"2261731446\",\\n            \"name\": \"Jiawen Kang\"\\n          },\\n          {\\n            \"authorId\": \"2943819\",\\n            \"name\": \"Zehui Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2237802924\",\\n            \"name\": \"Shiwen Mao\"\\n          },\\n          {\\n            \"authorId\": \"2264568786\",\\n            \"name\": \"Zhu Han\"\\n          },\\n          {\\n            \"authorId\": \"2228302663\",\\n            \"name\": \"Dong In Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269994509\",\\n            \"name\": \"K. B. Letaief\"\\n          }\\n        ],\\n        \"abstract\": \"AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.\"\\n      },\\n      {\\n        \"paperId\": \"807ba70b6dc5ce8104268ef8e579d6ff67051230\",\\n        \"url\": \"https://www.semanticscholar.org/paper/807ba70b6dc5ce8104268ef8e579d6ff67051230\",\\n        \"title\": \"A Survey on Post-training of Large Language Models\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2187039492\",\\n            \"name\": \"Guiyao Tie\"\\n          },\\n          {\\n            \"authorId\": \"2349394246\",\\n            \"name\": \"Zeli Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2347232612\",\\n            \"name\": \"Dingjie Song\"\\n          },\\n          {\\n            \"authorId\": \"2349375946\",\\n            \"name\": \"Fuyang Wei\"\\n          },\\n          {\\n            \"authorId\": \"2349949677\",\\n            \"name\": \"Rong Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2349402229\",\\n            \"name\": \"Yurou Dai\"\\n          },\\n          {\\n            \"authorId\": \"2349357634\",\\n            \"name\": \"Wen Yin\"\\n          },\\n          {\\n            \"authorId\": \"121937496\",\\n            \"name\": \"Zhejian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2349498657\",\\n            \"name\": \"Jiangyue Yan\"\\n          },\\n          {\\n            \"authorId\": \"2342866931\",\\n            \"name\": \"Yao Su\"\\n          },\\n          {\\n            \"authorId\": \"2349400490\",\\n            \"name\": \"Zhenhan Dai\"\\n          },\\n          {\\n            \"authorId\": \"2324567791\",\\n            \"name\": \"Yifeng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2211165440\",\\n            \"name\": \"Yihan Cao\"\\n          },\\n          {\\n            \"authorId\": \"2301109277\",\\n            \"name\": \"Lichao Sun\"\\n          },\\n          {\\n            \"authorId\": \"2221116622\",\\n            \"name\": \"Pan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2254874151\",\\n            \"name\": \"Lifang He\"\\n          },\\n          {\\n            \"authorId\": \"2280102292\",\\n            \"name\": \"Hechang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2349483665\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2284983420\",\\n            \"name\": \"Qingsong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2348862207\",\\n            \"name\": \"Tianming Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249536787\",\\n            \"name\": \"Neil Zhenqiang Gong\"\\n          },\\n          {\\n            \"authorId\": \"2279062891\",\\n            \"name\": \"Jiliang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2265549448\",\\n            \"name\": \"Caiming Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2349551882\",\\n            \"name\": \"Heng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2293777434\",\\n            \"name\": \"Philip S. Yu\"\\n          },\\n          {\\n            \"authorId\": \"2288029761\",\\n            \"name\": \"Jianfeng Gao\"\\n          }\\n        ],\\n        \"abstract\": \"The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT\\'s alignment strategies to DeepSeek-R1\\'s innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.\"\\n      },\\n      {\\n        \"paperId\": \"ecb1002427e476ec76463e0a8b5a453471a1931f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ecb1002427e476ec76463e0a8b5a453471a1931f\",\\n        \"title\": \"The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks are illustrated, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283841922\",\\n            \"name\": \"Ayo Adedeji\"\\n          },\\n          {\\n            \"authorId\": \"2283935118\",\\n            \"name\": \"Sarita Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2283841541\",\\n            \"name\": \"Brendan Doohan\"\\n          }\\n        ],\\n        \"abstract\": \"In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\\n      },\\n      {\\n        \"paperId\": \"a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\\n        \"title\": \"Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.03151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A thorough and up-to-date comparison of reasoning techniques in both textual and multimodal LLMs is offered, highlighting practical methods for post-training optimization and test-time inference.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2066776633\",\\n            \"name\": \"Jing Bi\"\\n          },\\n          {\\n            \"authorId\": \"2153545235\",\\n            \"name\": \"Susan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2322454494\",\\n            \"name\": \"Xiaofei Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2279760245\",\\n            \"name\": \"Pinxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2331365304\",\\n            \"name\": \"Junjia Guo\"\\n          },\\n          {\\n            \"authorId\": \"2119309562\",\\n            \"name\": \"Yunlong Tang\"\\n          },\\n          {\\n            \"authorId\": \"2242154602\",\\n            \"name\": \"Luchuan Song\"\\n          },\\n          {\\n            \"authorId\": \"2161012966\",\\n            \"name\": \"Chao Huang\"\\n          },\\n          {\\n            \"authorId\": \"2350866278\",\\n            \"name\": \"Guangyu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2350999609\",\\n            \"name\": \"Jinxi He\"\\n          },\\n          {\\n            \"authorId\": \"2350428628\",\\n            \"name\": \"Jiarui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2354168235\",\\n            \"name\": \"Shu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2266412651\",\\n            \"name\": \"Daoan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2350433568\",\\n            \"name\": \"Chen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2152129821\",\\n            \"name\": \"L. Wen\"\\n          },\\n          {\\n            \"authorId\": \"2337241442\",\\n            \"name\": \"Zhang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2320811177\",\\n            \"name\": \"Jiebo Luo\"\\n          },\\n          {\\n            \"authorId\": \"2293314762\",\\n            \"name\": \"Chenliang Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.\"\\n      },\\n      {\\n        \"paperId\": \"5550118041a89121e9d7274f83aef420cd9ed487\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5550118041a89121e9d7274f83aef420cd9ed487\",\\n        \"title\": \"Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.11849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311451390\",\\n            \"name\": \"Yinghao Aaron Li\"\\n          },\\n          {\\n            \"authorId\": \"2243118841\",\\n            \"name\": \"Xilin Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2162961620\",\\n            \"name\": \"Jordan Darefsky\"\\n          },\\n          {\\n            \"authorId\": \"2316835793\",\\n            \"name\": \"Ge Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1686269\",\\n            \"name\": \"N. Mesgarani\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end speech-to-speech conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and speech styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the speech, which is then played back to the user. While the response speech is being played, the input speech undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input speech. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and speech-to-speech baselines in terms of both dialogue naturalness and coherence while being more than 50% faster.\"\\n      },\\n      {\\n        \"paperId\": \"afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\\n        \"title\": \"What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This survey summarizes existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual, and proposes a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-08-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2302816545\",\\n            \"name\": \"Dingyi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2317013378\",\\n            \"name\": \"Qin Jin\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories. Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations.\"\\n      },\\n      {\\n        \"paperId\": \"bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\\n        \"title\": \"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.19846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Quest is introduced, a query-centric data synthesis method aggregating semantically relevant yet diverse documents that achieves superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152644192\",\\n            \"name\": \"Chaochen Gao\"\\n          },\\n          {\\n            \"authorId\": \"2155226596\",\\n            \"name\": \"Xing Wu\"\\n          },\\n          {\\n            \"authorId\": \"2176771084\",\\n            \"name\": \"Qingfang Fu\"\\n          },\\n          {\\n            \"authorId\": \"2257376973\",\\n            \"name\": \"Songlin Hu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest\\'s superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\\n      },\\n      {\\n        \"paperId\": \"1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\\n        \"title\": \"When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.13476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AnchorAttention is developed, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training, and reduces training time by over 50% compared to standard full attention mechanisms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2267866973\",\\n            \"name\": \"Haonan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284062049\",\\n            \"name\": \"Qian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2325201427\",\\n            \"name\": \"Chao Du\"\\n          },\\n          {\\n            \"authorId\": \"2291015783\",\\n            \"name\": \"Tongyao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2325980280\",\\n            \"name\": \"Cunxiao Du\"\\n          },\\n          {\\n            \"authorId\": \"2256995496\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"19201674\",\\n            \"name\": \"Tianyu Pang\"\\n          }\\n        ],\\n        \"abstract\": \"Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16\\'s limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\\\\\\\% compared to standard full attention mechanisms, while preserving the original LLM\\'s capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.\"\\n      },\\n      {\\n        \"paperId\": \"38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\\n        \"title\": \"Ultrafast imaging of coherent polariton propagation and interactions\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2143435931\",\\n            \"name\": \"Ding Xu\"\\n          },\\n          {\\n            \"authorId\": \"13643895\",\\n            \"name\": \"Arkajit Mandal\"\\n          },\\n          {\\n            \"authorId\": \"2053177998\",\\n            \"name\": \"James M. Baxter\"\\n          },\\n          {\\n            \"authorId\": \"2004406278\",\\n            \"name\": \"Shangjun Cheng\"\\n          },\\n          {\\n            \"authorId\": \"49805255\",\\n            \"name\": \"I. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1849913502\",\\n            \"name\": \"Haowen (Vicky) Su\"\\n          },\\n          {\\n            \"authorId\": \"2144363449\",\\n            \"name\": \"Song Liu\"\\n          },\\n          {\\n            \"authorId\": \"6834462\",\\n            \"name\": \"D. Reichman\"\\n          },\\n          {\\n            \"authorId\": \"3895968\",\\n            \"name\": \"Milan Delor\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\\n        \"title\": \"Extrapolation-Based Video Retargeting With Backward Warping Using an Image-to-Warping Vector Generation Network\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LSP.2020.2977206?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LSP.2020.2977206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46445100\",\\n            \"name\": \"Sung In Cho\"\\n          },\\n          {\\n            \"authorId\": \"1739537\",\\n            \"name\": \"Suk-ju Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Video retargeting is a technique used to transform a given video to a target aspect ratio. Current methods often cause severe visual distortion due to frequent temporal incoherence during the retargeting. In this study, we propose a new extrapolation-based video retargeting method using an image-to-warping vector generation network to maintain temporal coherence and prevent deformation of an input frame by extending the side area of an input frame. Backward warping-based extrapolation is performed using a displacement vector (DV) that is generated by a proposed convolutional neural network (CNN). The DV is defined as the displacement between the current hole to be filled in the extended area and a pixel in the input frame used to fill the hole. We also propose a technique to efficiently train the CNN including a method for ground-truth DV generation. After the extrapolation, we propose a technique for the maintenance of temporal coherence of the extended region and a distortion suppression scheme (DSC) for minimizing visual artifacts. The simulation results demonstrated that the proposed method improved bidirectional similarity (BDS) up to 3.69, which is a measure of the quality of video retargeting, compared with existing video retargeting methods.\"\\n      },\\n      {\\n        \"paperId\": \"0df45f6ab09cc6ddcaf6829c131c777732a73731\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0df45f6ab09cc6ddcaf6829c131c777732a73731\",\\n        \"title\": \"Temporal Incoherence-Free Video Retargeting Using Foreground Aware Extrapolation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2020.2977171?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2020.2977171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46445100\",\\n            \"name\": \"Sung In Cho\"\\n          },\\n          {\\n            \"authorId\": \"1739537\",\\n            \"name\": \"Suk-ju Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Video retargeting is a method of adjusting the aspect ratio of a given video to the target aspect ratio. However, temporal incoherence of video contents, which can occur frequently by video retargeting, is the most dominant factor that degrades the quality of retargeted videos. Current methods to maintain temporal coherence use the entire frames of the input videos; however, these methods cannot be implemented as on-time systems because of their tremendous computational complexity. As far as we know, there is no existing on-time video retargeting method that can avoid spatial distortion while perfectly maintaining temporal coherence. In this paper, we propose a novel on-time video retargeting method that can perfectly maintain temporal coherence and prevent the spatial distortion by using only two consecutive input frames. In our method, the maximum a posteriori-based foreground aware-block matching is used for the extrapolation that extends the side area of a given video to adjust its aspect ratio to the target. To maintain the temporal coherence of the extended area, the result of block matching for backward warping-based extrapolation of the start frame after the scene change occurs, is reused for the other frames until the next scene change occurs. In addition, we propose a scene scenario-adaptive fallback scheme to prevent severe distortions that can occur with reusing block matching results or extrapolation-based side extension. The simulation results showed that the proposed method greatly improved the bidirectional similarity value, which can measure the quality of video retargeting, by up to 10.26 compared with the existing on-time video retargeting methods.\"\\n      },\\n      {\\n        \"paperId\": \"cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\\n        \"title\": \"Posterior-GRPO: Rewarding Reasoning Processes in Code Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2508.05170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a unified framework that can effectively incorporate the quality of the reasoning process during RL and introduces Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-08-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310403349\",\\n            \"name\": \"Lishui Fan\"\\n          },\\n          {\\n            \"authorId\": \"2375147757\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2125101083\",\\n            \"name\": \"Mouxiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2331676092\",\\n            \"name\": \"Zhongxin Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model\\'s internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.\"\\n      }\\n    ]\\n  },\\n  \"adaptive prompt generation\": {\\n    \"total\": 13544,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"title\": \"Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions, which is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which the authors call flocking.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2171964328\",\\n            \"name\": \"Harry Dong\"\\n          },\\n          {\\n            \"authorId\": \"2282555057\",\\n            \"name\": \"Beidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284063779\",\\n            \"name\": \"Yuejie Chi\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method\\'s simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model\\'s performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\\\\\times$ and 1.25$\\\\\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\"\\n      },\\n      {\\n        \"paperId\": \"bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"title\": \"Prompt Expansion for Adaptive Text-to-Image Generation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A human evaluation study shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2070966889\",\\n            \"name\": \"Siddhartha Datta\"\\n          },\\n          {\\n            \"authorId\": \"2276608298\",\\n            \"name\": \"Alexander Ku\"\\n          },\\n          {\\n            \"authorId\": \"2275054270\",\\n            \"name\": \"Deepak Ramachandran\"\\n          },\\n          {\\n            \"authorId\": \"2276610768\",\\n            \"name\": \"Peter Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\"\\n      },\\n      {\\n        \"paperId\": \"1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"title\": \"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\",\\n        \"citationCount\": 52,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.12761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights and incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13563486\",\\n            \"name\": \"Jaehong Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2164249715\",\\n            \"name\": \"Shoubin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2061083016\",\\n            \"name\": \"Vaidehi Patil\"\\n          },\\n          {\\n            \"authorId\": \"2267311471\",\\n            \"name\": \"Huaxiu Yao\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\"\\n      },\\n      {\\n        \"paperId\": \"a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"title\": \"Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.06401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2273557458\",\\n            \"name\": \"Hao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2273559489\",\\n            \"name\": \"Jun Li\"\\n          },\\n          {\\n            \"authorId\": \"2118764798\",\\n            \"name\": \"Yizhuang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2273589717\",\\n            \"name\": \"Jun Wan\"\\n          },\\n          {\\n            \"authorId\": \"2113457514\",\\n            \"name\": \"Zhen Lei\"\\n          },\\n          {\\n            \"authorId\": \"2274088311\",\\n            \"name\": \"Xiangyu Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\"\\n      },\\n      {\\n        \"paperId\": \"bc9f1025246efeed568650934b6e183415aca279\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bc9f1025246efeed568650934b6e183415aca279\",\\n        \"title\": \"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.03214\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation, outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on the authors\\' main metrics for each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221313264\",\\n            \"name\": \"Jonathan Pei\"\\n          },\\n          {\\n            \"authorId\": \"1410652795\",\\n            \"name\": \"Kevin Yang\"\\n          },\\n          {\\n            \"authorId\": \"38666915\",\\n            \"name\": \"D. Klein\"\\n          }\\n        ],\\n        \"abstract\": \"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\"\\n      },\\n      {\\n        \"paperId\": \"cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"title\": \"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\",\\n        \"citationCount\": 115,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds, and shows that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1443432623\",\\n            \"name\": \"Anselm Paulus\"\\n          },\\n          {\\n            \"authorId\": \"3461866\",\\n            \"name\": \"Arman Zharmagambetov\"\\n          },\\n          {\\n            \"authorId\": \"2298951327\",\\n            \"name\": \"Chuan Guo\"\\n          },\\n          {\\n            \"authorId\": \"2298758184\",\\n            \"name\": \"Brandon Amos\"\\n          },\\n          {\\n            \"authorId\": \"2253746559\",\\n            \"name\": \"Yuandong Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n      },\\n      {\\n        \"paperId\": \"1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"title\": \"WavLLM: Towards Robust and Adaptive Speech Large Language Model\",\\n        \"citationCount\": 104,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"WavLLM is introduced, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277450543\",\\n            \"name\": \"Shujie Hu\"\\n          },\\n          {\\n            \"authorId\": \"2135918679\",\\n            \"name\": \"Long Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2107983441\",\\n            \"name\": \"Shujie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107970655\",\\n            \"name\": \"Sanyuan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2294360053\",\\n            \"name\": \"Hongkun Hao\"\\n          },\\n          {\\n            \"authorId\": \"2258308585\",\\n            \"name\": \"Jing Pan\"\\n          },\\n          {\\n            \"authorId\": \"2274190703\",\\n            \"name\": \"Xunying Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280887661\",\\n            \"name\": \"Jinyu Li\"\\n          },\\n          {\\n            \"authorId\": \"9075412\",\\n            \"name\": \"S. Sivasankaran\"\\n          },\\n          {\\n            \"authorId\": \"2294832157\",\\n            \"name\": \"Linquan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277299355\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker\\'s identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\\\\\url{aka.ms/wavllm}.\"\\n      },\\n      {\\n        \"paperId\": \"5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"title\": \"OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.18866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniAvatar is introduced, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements, and introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293272991\",\\n            \"name\": \"Qijun Gan\"\\n          },\\n          {\\n            \"authorId\": \"2294510159\",\\n            \"name\": \"Ruizi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2314648986\",\\n            \"name\": \"Jianke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2370937157\",\\n            \"name\": \"Shaofei Xue\"\\n          },\\n          {\\n            \"authorId\": \"2370937932\",\\n            \"name\": \"Steven Hoi\"\\n          }\\n        ],\\n        \"abstract\": \"Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"title\": \"AdaPlanner: Adaptive Planning from Feedback with Language Models\",\\n        \"citationCount\": 175,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16653\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118180896\",\\n            \"name\": \"Haotian Sun\"\\n          },\\n          {\\n            \"authorId\": \"8103389\",\\n            \"name\": \"Yuchen Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2865034\",\\n            \"name\": \"Lingkai Kong\"\\n          },\\n          {\\n            \"authorId\": \"2218437288\",\\n            \"name\": \"Bo Dai\"\\n          },\\n          {\\n            \"authorId\": \"145657504\",\\n            \"name\": \"Chao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"title\": \"Adaptive Machine Translation with Large Language Models\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.13294\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.13294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9400076\",\\n            \"name\": \"Yasmin Moslem\"\\n          },\\n          {\\n            \"authorId\": \"1748844\",\\n            \"name\": \"Rejwanul Haque\"\\n          },\\n          {\\n            \"authorId\": \"144315616\",\\n            \"name\": \"Andy Way\"\\n          }\\n        ],\\n        \"abstract\": \"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\\n      },\\n      {\\n        \"paperId\": \"7665642af9e682e012bec045102a4d009421067c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7665642af9e682e012bec045102a4d009421067c\",\\n        \"title\": \"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\",\\n        \"citationCount\": 98,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.17061\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An efficient yet effective framework that generates high-quality 3D humans with fine-grained geometry and realistic appearance, based on the insight that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257708193\",\\n            \"name\": \"Xian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2260342453\",\\n            \"name\": \"Xiaohang Zhan\"\\n          },\\n          {\\n            \"authorId\": \"1397711601\",\\n            \"name\": \"Jiaxiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2260340529\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2247995148\",\\n            \"name\": \"Gang Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2258618427\",\\n            \"name\": \"Dahua Lin\"\\n          },\\n          {\\n            \"authorId\": \"2257370021\",\\n            \"name\": \"Xihui Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249080787\",\\n            \"name\": \"Ziwei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"title\": \"ChatUniTest: a ChatGPT-based automated unit test generation tool\",\\n        \"citationCount\": 87,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.04764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.04764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.04764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework that outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and generates assertions while utilizing mock objects and re\\\\ufb02ection to achieve test objectives.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1387638000\",\\n            \"name\": \"Zhuo-Qi Xie\"\\n          },\\n          {\\n            \"authorId\": \"2300176046\",\\n            \"name\": \"Yinghao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2064478633\",\\n            \"name\": \"Chen Zhi\"\\n          },\\n          {\\n            \"authorId\": \"145590434\",\\n            \"name\": \"Shuiguang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2116398505\",\\n            \"name\": \"Jianwei Yin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"836e3069a83f455f916114e7265e00187e511838\",\\n        \"url\": \"https://www.semanticscholar.org/paper/836e3069a83f455f916114e7265e00187e511838\",\\n        \"title\": \"Locally Differentially Private Document Generation Using Zero Shot Prompting\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.16111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1512255229\",\\n            \"name\": \"Saiteja Utpala\"\\n          },\\n          {\\n            \"authorId\": \"2261493078\",\\n            \"name\": \"Sara Hooker\"\\n          },\\n          {\\n            \"authorId\": \"2261697074\",\\n            \"name\": \"Pin Yu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\\\\\% reduction in author identification F1 score against static attackers and a 26\\\\\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\"\\n      },\\n      {\\n        \"paperId\": \"f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"title\": \"Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"153003087\",\\n            \"name\": \"Xiangteng He\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"143753918\",\\n            \"name\": \"Yuxin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\"\\n      },\\n      {\\n        \"paperId\": \"107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"title\": \"The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.09576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1847858\",\\n            \"name\": \"Subhankar Maity\"\\n          },\\n          {\\n            \"authorId\": \"2144085844\",\\n            \"name\": \"Aniket Deroy\"\\n          }\\n        ],\\n        \"abstract\": \"In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n      },\\n      {\\n        \"paperId\": \"b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"title\": \"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.00330?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.00330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"StyleCrafter is introduced, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image, and designs a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269171464\",\\n            \"name\": \"Gongye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257035878\",\\n            \"name\": \"Menghan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149052351\",\\n            \"name\": \"Haoxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2087273800\",\\n            \"name\": \"Jinbo Xing\"\\n          },\\n          {\\n            \"authorId\": \"2253795356\",\\n            \"name\": \"Xintao Wang\"\\n          },\\n          {\\n            \"authorId\": \"3001727\",\\n            \"name\": \"Yujiu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257019659\",\\n            \"name\": \"Ying Shan\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0f78249a5ff64441cc51e55bbe5b97e28f801240\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f78249a5ff64441cc51e55bbe5b97e28f801240\",\\n        \"title\": \"Prompt Tuning for Generative Multimodal Pretrained Models\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2208.02532\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2208.02532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work implements prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks, with a focus on generative multimodal pretrained models, instead of contrastive ones.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-08-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50841357\",\\n            \"name\": \"Han Yang\"\\n          },\\n          {\\n            \"authorId\": \"35996608\",\\n            \"name\": \"Junyang Lin\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"An Yang\"\\n          },\\n          {\\n            \"authorId\": \"2155302144\",\\n            \"name\": \"Peng Wang\"\\n          },\\n          {\\n            \"authorId\": \"144161025\",\\n            \"name\": \"Chang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"38385080\",\\n            \"name\": \"Hongxia Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at \\\\\\\\url{https://github.com/OFA-Sys/OFA}\"\\n      },\\n      {\\n        \"paperId\": \"192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"title\": \"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2312.01663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.01663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269468811\",\\n            \"name\": \"Runze He\"\\n          },\\n          {\\n            \"authorId\": \"2052151521\",\\n            \"name\": \"Shaofei Huang\"\\n          },\\n          {\\n            \"authorId\": \"2269461105\",\\n            \"name\": \"Xuecheng Nie\"\\n          },\\n          {\\n            \"authorId\": \"151475424\",\\n            \"name\": \"Tianrui Hui\"\\n          },\\n          {\\n            \"authorId\": \"1776665\",\\n            \"name\": \"Luoqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2108984\",\\n            \"name\": \"Jiao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2269685669\",\\n            \"name\": \"Jizhong Han\"\\n          },\\n          {\\n            \"authorId\": \"2269748083\",\\n            \"name\": \"Guanbin Li\"\\n          },\\n          {\\n            \"authorId\": \"2269687302\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\"\\n      },\\n      {\\n        \"paperId\": \"817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\\n        \"url\": \"https://www.semanticscholar.org/paper/817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\\n        \"title\": \"TaskCraft: Automated Generation of Agentic Tasks\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.10055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories, and empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2367198602\",\\n            \"name\": \"Dingfeng Shi\"\\n          },\\n          {\\n            \"authorId\": \"2366609463\",\\n            \"name\": \"Jingyi Cao\"\\n          },\\n          {\\n            \"authorId\": \"2368654631\",\\n            \"name\": \"Qianben Chen\"\\n          },\\n          {\\n            \"authorId\": \"2367090248\",\\n            \"name\": \"Weichen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2366569457\",\\n            \"name\": \"Weizhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2366571583\",\\n            \"name\": \"Hongxuan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2366522296\",\\n            \"name\": \"Fangchen Dong\"\\n          },\\n          {\\n            \"authorId\": \"2366567233\",\\n            \"name\": \"Tianrui Qin\"\\n          },\\n          {\\n            \"authorId\": \"2368705239\",\\n            \"name\": \"King Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2283080546\",\\n            \"name\": \"Minghao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2366695720\",\\n            \"name\": \"Jian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2366560952\",\\n            \"name\": \"Ge Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2182423032\",\\n            \"name\": \"Jiaheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2351712181\",\\n            \"name\": \"Changwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2366603044\",\\n            \"name\": \"Jun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2134457930\",\\n            \"name\": \"Y. Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2284803168\",\\n            \"name\": \"Wangchunshu Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce \\\\\\\\textsc{TaskCraft}, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.\"\\n      },\\n      {\\n        \"paperId\": \"f373c5569b45bf580b7502729a83761a791ee209\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f373c5569b45bf580b7502729a83761a791ee209\",\\n        \"title\": \"MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work is the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing, and Experimental results on four benchmarks suggest that the method outperforms the previous method on each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238430925\",\\n            \"name\": \"Haoyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2238207741\",\\n            \"name\": \"Tianyi Lu\"\\n          },\\n          {\\n            \"authorId\": \"2239092619\",\\n            \"name\": \"Jiaxi Gu\"\\n          },\\n          {\\n            \"authorId\": \"2238449354\",\\n            \"name\": \"Xing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2311476511\",\\n            \"name\": \"Qingping Zheng\"\\n          },\\n          {\\n            \"authorId\": \"3099139\",\\n            \"name\": \"Zuxuan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2258963974\",\\n            \"name\": \"Hang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238451522\",\\n            \"name\": \"Yu-Gang Jiang\"\\n          }\\n        ],\\n        \"abstract\": \"The diffusion model is widely leveraged for either video generation or video editing. As each field has its task-specific problems, it is difficult to merely develop a single diffusion for completing both tasks simultaneously. Video diffusion sorely relying on the text prompt can be adapted to unify the two tasks. However, it lacks a high capability of aligning heterogeneous modalities between text and image, leading to various misalignment problems. In this work, we are the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing. The proposed MagDiff introduces three types of alignments, including subject-driven alignment, adaptive prompts alignment, and high-fidelity alignment. Particularly, the subject-driven alignment is put forward to trade off the image and text prompts, serving as a unified foundation generative model for both tasks. The adaptive prompts alignment is introduced to emphasize different strengths of homogeneous and heterogeneous alignments by assigning different values of weights to the image and the text prompts. The high-fidelity alignment is developed to further enhance the fidelity of both video generation and editing by taking the subject image as an additional model input. Experimental results on four benchmarks suggest that our method outperforms the previous method on each task.\"\\n      },\\n      {\\n        \"paperId\": \"a17aee1261b56ee828f029b1caeef78033acea83\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a17aee1261b56ee828f029b1caeef78033acea83\",\\n        \"title\": \"Adaptive Ship Detection From Optical to SAR Images\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LGRS.2023.3317321?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LGRS.2023.3317321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243328855\",\\n            \"name\": \"Yuxuan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2221128858\",\\n            \"name\": \"Zhijie Rao\"\\n          },\\n          {\\n            \"authorId\": \"2232100687\",\\n            \"name\": \"Chuyang Lin\"\\n          },\\n          {\\n            \"authorId\": \"1950637\",\\n            \"name\": \"Yue Huang\"\\n          },\\n          {\\n            \"authorId\": \"2713947\",\\n            \"name\": \"Xinghao Ding\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in synthetic aperture radar (SAR) ship detection have witnessed remarkable success by using large-scale annotated datasets. However, the annotation of SAR images requires strong domain-specific expertise, significantly hindering the prompt adoption of modern object detectors in this regime. Compared to SAR data, optical data in geoscience are considerably easier to label. Motivated by this, we investigate a new and challenging problem\\\\u2014adaptive ship detection\\\\u2014with the goal of enhancing ship detection performance on SAR images by leveraging knowledge transferred from optical images. Considering the large distributional discrepancy between the source (optical) and target (SAR) domains, we present OmniAdapt, a novel framework that progressively narrows the distance between the two types of images at the pixel, feature, and classifier levels. Specifically, OmniAdapt consists of three main modules, target-like generation module (TLGM), multifeature alignment module (MFAM), and common specific decomposition module (CSDM). TLGM minimizes the visual disparity by infusing the target-domain style into the source domain. MFAM aligns local- and global-level feature representations in an adversarial manner. Finally, CSDM decomposes the classifier into two independent components, that is, the domain-common component and the domain-specific component, and promotes the recognition ability of the former via regularization learning. Experimental results demonstrate the effectiveness of the proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\\n        \"title\": \"Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.05256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate, is introduced, a reinforcement learning objective tailoring generation length to per-prompt solve rate that delivers higher accuracy on the hardest problems with higher cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274104658\",\\n            \"name\": \"Violet Xiang\"\\n          },\\n          {\\n            \"authorId\": \"2326300653\",\\n            \"name\": \"Chase Blagden\"\\n          },\\n          {\\n            \"authorId\": \"102801230\",\\n            \"name\": \"Rafael Rafailov\"\\n          },\\n          {\\n            \"authorId\": \"2283848553\",\\n            \"name\": \"nathan lile\"\\n          },\\n          {\\n            \"authorId\": \"2366009773\",\\n            \"name\": \"Sang Truong\"\\n          },\\n          {\\n            \"authorId\": \"2284774407\",\\n            \"name\": \"Chelsea Finn\"\\n          },\\n          {\\n            \"authorId\": \"2274104149\",\\n            \"name\": \"Nick Haber\"\\n          }\\n        ],\\n        \"abstract\": \"Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt\\'s online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\\\\\\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.\"\\n      },\\n      {\\n        \"paperId\": \"44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\\n        \"title\": \"OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.10825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2190109518\",\\n            \"name\": \"Dianbing Xi\"\\n          },\\n          {\\n            \"authorId\": \"2356794181\",\\n            \"name\": \"Jiepeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2337074199\",\\n            \"name\": \"Yuanzhi Liang\"\\n          },\\n          {\\n            \"authorId\": \"2336910859\",\\n            \"name\": \"Xi Qiu\"\\n          },\\n          {\\n            \"authorId\": \"3131188\",\\n            \"name\": \"Yuchi Huo\"\\n          },\\n          {\\n            \"authorId\": \"2325437281\",\\n            \"name\": \"Rui Wang\"\\n          },\\n          {\\n            \"authorId\": \"2336934367\",\\n            \"name\": \"Chi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2336880377\",\\n            \"name\": \"Xuelong Li\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff , aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. Our framework supports three key capabilities: (1) Text-conditioned video generation, where all modalities are jointly synthesized from a textual prompt; (2) Video understanding, where structural modalities are predicted from rgb inputs in a coherent manner; and (3) X-conditioned video generation, where video synthesis is guided by finegrained inputs such as depth, canny and segmentation. Extensive experiments demonstrate that OmniVDiff achieves state-of-the-art performance in video generation tasks and competitive results in video understanding. Its flexibility and scalability make it well-suited for downstream applications such as video-to-video translation, modality adaptation for visual tasks, and scene reconstruction.\"\\n      },\\n      {\\n        \"paperId\": \"42117d01d498eb9f8c21b788c3565bc6855d620b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/42117d01d498eb9f8c21b788c3565bc6855d620b\",\\n        \"title\": \"Learning to Transfer Prompts for Text Generation\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.01543\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.01543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper improves this technique and proposes a novel prompt-based method (PTG) for text generation in a transferable setting that learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2018027\",\\n            \"name\": \"Junyi Li\"\\n          },\\n          {\\n            \"authorId\": \"1997234792\",\\n            \"name\": \"Tianyi Tang\"\\n          },\\n          {\\n            \"authorId\": \"50204644\",\\n            \"name\": \"J. Nie\"\\n          },\\n          {\\n            \"authorId\": \"153693432\",\\n            \"name\": \"Ji-rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2542603\",\\n            \"name\": \"Wayne Xin Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.\"\\n      },\\n      {\\n        \"paperId\": \"7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\\n        \"title\": \"CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer\",\\n        \"citationCount\": 1175,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions, and develops an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2303231681\",\\n            \"name\": \"Zhuoyi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2238205354\",\\n            \"name\": \"Jiayan Teng\"\\n          },\\n          {\\n            \"authorId\": \"2163967642\",\\n            \"name\": \"Wendi Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2055623340\",\\n            \"name\": \"Ming Ding\"\\n          },\\n          {\\n            \"authorId\": \"2305795673\",\\n            \"name\": \"Shiyu Huang\"\\n          },\\n          {\\n            \"authorId\": \"2214082934\",\\n            \"name\": \"Jiazheng Xu\"\\n          },\\n          {\\n            \"authorId\": \"2315948290\",\\n            \"name\": \"Yuanming Yang\"\\n          },\\n          {\\n            \"authorId\": \"2105844599\",\\n            \"name\": \"Wenyi Hong\"\\n          },\\n          {\\n            \"authorId\": \"2268628279\",\\n            \"name\": \"Xiaohan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2307077651\",\\n            \"name\": \"Guanyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2307075814\",\\n            \"name\": \"Da Yin\"\\n          },\\n          {\\n            \"authorId\": \"2290625851\",\\n            \"name\": \"Xiaotao Gu\"\\n          },\\n          {\\n            \"authorId\": \"2316099643\",\\n            \"name\": \"Yuxuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265518149\",\\n            \"name\": \"Weihan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2306161782\",\\n            \"name\": \"Yean Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2315952736\",\\n            \"name\": \"Ting Liu\"\\n          },\\n          {\\n            \"authorId\": \"2288066971\",\\n            \"name\": \"Bin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2243402027\",\\n            \"name\": \"Yuxiao Dong\"\\n          },\\n          {\\n            \"authorId\": \"2238207092\",\\n            \"name\": \"Jie Tang\"\\n          }\\n        ],\\n        \"abstract\": \"We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.\"\\n      },\\n      {\\n        \"paperId\": \"f37a5c2bf4266d031533d5e029b74b00b48ef038\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f37a5c2bf4266d031533d5e029b74b00b48ef038\",\\n        \"title\": \"Latency-Aware Generative Semantic Communications With Pre-Trained Diffusion Models\",\\n        \"citationCount\": 62,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.17256\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A latency-aware semantic communications framework with pre-trained generative models that designs a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258983485\",\\n            \"name\": \"Li Qiao\"\\n          },\\n          {\\n            \"authorId\": \"3202702\",\\n            \"name\": \"Mahdi Boloursaz Mashhadi\"\\n          },\\n          {\\n            \"authorId\": \"2293693238\",\\n            \"name\": \"Zhen Gao\"\\n          },\\n          {\\n            \"authorId\": \"1690137\",\\n            \"name\": \"C. Foh\"\\n          },\\n          {\\n            \"authorId\": \"2293392561\",\\n            \"name\": \"Pei Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2279548875\",\\n            \"name\": \"Mehdi Bennis\"\\n          }\\n        ],\\n        \"abstract\": \"Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this letter, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.\"\\n      },\\n      {\\n        \"paperId\": \"759b95f7f90addc4c526cd92557e486ab143fbec\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b95f7f90addc4c526cd92557e486ab143fbec\",\\n        \"title\": \"Style Vectors for Steering Generative Large Language Models\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.01618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches, demonstrating the effectiveness of activation engineering using such style vectors.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2098959640\",\\n            \"name\": \"Kai Konen\"\\n          },\\n          {\\n            \"authorId\": \"2282467369\",\\n            \"name\": \"Sophie Jentzsch\"\\n          },\\n          {\\n            \"authorId\": \"2274662002\",\\n            \"name\": \"Diaoul\\\\u00e9 Diallo\"\\n          },\\n          {\\n            \"authorId\": \"2282467364\",\\n            \"name\": \"Peer Schutt\"\\n          },\\n          {\\n            \"authorId\": \"2282467405\",\\n            \"name\": \"Oliver Bensch\"\\n          },\\n          {\\n            \"authorId\": \"51185829\",\\n            \"name\": \"Roxanne El Baff\"\\n          },\\n          {\\n            \"authorId\": \"2282467346\",\\n            \"name\": \"Dominik Opitz\"\\n          },\\n          {\\n            \"authorId\": \"2282467403\",\\n            \"name\": \"Tobias Hecking\"\\n          }\\n        ],\\n        \"abstract\": \"This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.\"\\n      },\\n      {\\n        \"paperId\": \"621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\\n        \"title\": \"EdgeLLM: Fast On-Device LLM Inference With Speculative Decoding\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMC.2024.3513457?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMC.2024.3513457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238890503\",\\n            \"name\": \"Daliang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238575108\",\\n            \"name\": \"Wangsong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2306091156\",\\n            \"name\": \"Hao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239060901\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"2326531487\",\\n            \"name\": \"Ying Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2238910539\",\\n            \"name\": \"Shiyun Wei\"\\n          },\\n          {\\n            \"authorId\": \"2326083763\",\\n            \"name\": \"Mengwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2237080638\",\\n            \"name\": \"Xuanzhe Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device\\'s memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3\\\\u00d7 faster than existing engines.\"\\n      },\\n      {\\n        \"paperId\": \"411b16add23976ffcdf6422f932453f6ebcca119\",\\n        \"url\": \"https://www.semanticscholar.org/paper/411b16add23976ffcdf6422f932453f6ebcca119\",\\n        \"title\": \"EvoPrompting: Language Models for Code-Level Neural Architecture Search\",\\n        \"citationCount\": 118,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.14838\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.14838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13336152\",\\n            \"name\": \"Angelica Chen\"\\n          },\\n          {\\n            \"authorId\": \"35363891\",\\n            \"name\": \"David Dohan\"\\n          },\\n          {\\n            \"authorId\": \"48165870\",\\n            \"name\": \"David R. So\"\\n          }\\n        ],\\n        \"abstract\": \"Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\\n      },\\n      {\\n        \"paperId\": \"1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\\n        \"title\": \"Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.18950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Adaptive Guided Erasure (AGE) method is proposed, which selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2007884558\",\\n            \"name\": \"Anh-Vu Bui\"\\n          },\\n          {\\n            \"authorId\": \"2299801919\",\\n            \"name\": \"T. V\\\\u0169\"\\n          },\\n          {\\n            \"authorId\": \"67329496\",\\n            \"name\": \"Tung-Long Vuong\"\\n          },\\n          {\\n            \"authorId\": \"2249909946\",\\n            \"name\": \"Trung Le\"\\n          },\\n          {\\n            \"authorId\": \"2292198330\",\\n            \"name\": \"Paul Montague\"\\n          },\\n          {\\n            \"authorId\": \"2059248789\",\\n            \"name\": \"Tamas Abraham\"\\n          },\\n          {\\n            \"authorId\": \"2275034108\",\\n            \"name\": \"Junae Kim\"\\n          },\\n          {\\n            \"authorId\": \"1400659302\",\\n            \"name\": \"Dinh Q. Phung\"\\n          }\\n        ],\\n        \"abstract\": \"Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a neutral concept or just an empty text prompt. In this paper, we demonstrate that this fixed-target strategy is suboptimal, as it fails to account for the impact of erasing one concept on the others. To address this limitation, we model the concept space as a graph and empirically analyze the effects of erasing one concept on the remaining concepts. Our analysis uncovers intriguing geometric properties of the concept space, where the influence of erasing a concept is confined to a local region. Building on this insight, we propose the Adaptive Guided Erasure (AGE) method, which \\\\\\\\emph{dynamically} selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects. Experimental results show that AGE significantly outperforms state-of-the-art erasure methods on preserving unrelated concepts while maintaining effective erasure performance. Our code is published at {https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.\"\\n      },\\n      {\\n        \"paperId\": \"af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\\n        \"title\": \"StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.08503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A cross-modal Adaptive Instance Normalization mechanism is introduced for better integration of style and text features, enhancing alignment and a Style-based Classifier-Free Guidance approach is developed that enables selective control over stylistic elements, reducing irrelevant influences.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2334740500\",\\n            \"name\": \"Mingkun Lei\"\\n          },\\n          {\\n            \"authorId\": \"2334824597\",\\n            \"name\": \"Xue Song\"\\n          },\\n          {\\n            \"authorId\": \"2336265253\",\\n            \"name\": \"Beier Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2334818360\",\\n            \"name\": \"Hao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2334822819\",\\n            \"name\": \"Chi Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.\"\\n      },\\n      {\\n        \"paperId\": \"5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\\n        \"title\": \"TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.16425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information, and designs a Dynamic Map Scaling mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333781842\",\\n            \"name\": \"Linqing Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2105618628\",\\n            \"name\": \"Chen Gao\"\\n          },\\n          {\\n            \"authorId\": \"2264574237\",\\n            \"name\": \"Zihan Ding\"\\n          },\\n          {\\n            \"authorId\": \"2325825544\",\\n            \"name\": \"Yue Liao\"\\n          },\\n          {\\n            \"authorId\": \"2325537018\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM\\'s spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.\"\\n      },\\n      {\\n        \"paperId\": \"4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\\n        \"title\": \"A new rapid deflagration-to-detonation transition in a short smooth tube\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1063/5.0191500?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1063/5.0191500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"92832821\",\\n            \"name\": \"Wandong Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290316859\",\\n            \"name\": \"Caizhi Fan\"\\n          },\\n          {\\n            \"authorId\": \"1734241\",\\n            \"name\": \"R. Deiterding\"\\n          },\\n          {\\n            \"authorId\": \"2290237139\",\\n            \"name\": \"Xiaokang Li\"\\n          },\\n          {\\n            \"authorId\": \"36072040\",\\n            \"name\": \"Jianhan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290198596\",\\n            \"name\": \"Xiong Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Obtaining a rapid deflagration-to-detonation transition (DDT) within a short smooth tube is a challenging task. Here, an unconventional means of flame acceleration propagating upstream in subsonic and supersonic mixtures within a smooth tube was introduced to acquire a speedy DDT. The Navier\\\\u2013Stokes equations with an adaptive mesh refinement technique and a detailed hydrogen\\\\u2013air chemistry reaction mechanism of 11 species and 27 steps were utilized to resolve the entire DDT characteristics. The effect of the initial Mach number on flame acceleration and DDT mechanism was revealed comprehensively. The results demonstrated that a prompt oblique shock wave (SW) occurs when the flame propagates upstream along the boundary walls due to the boundary layer influence. An intense coupling between the SW and the leading flame front is enhanced by increasing the initial Mach number of the mixture. The speedy generation of the oblique SW is formed at the incipient stage, mainly produced by the boundary layer influence and the coalescences of the compression waves. Consequently, the run-up time to detonation is shortened accordingly through a fierce reflected SW due to the intense leading SW after it reflects from the confined wall. Furthermore, three kinds of DDT evolution are revealed from the obtained results: (1) localized ignition in the upper boundary wall after the reflected and transverse shock waves propagate in the upper wall regions; (2) autoignition is formed in the confined wall corner after the reflected SW; and (3) direct detonation transition occurs at the end wall behind a strongly reflected SW in the supersonic case.\"\\n      },\\n      {\\n        \"paperId\": \"815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\\n        \"title\": \"Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2405.14713\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Generative AI capabilities are introduced to assist educators in creating tutor interfaces that meet their needs while adhering to design principles and raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170697820\",\\n            \"name\": \"Tommaso Cal\\\\u00f2\"\\n          },\\n          {\\n            \"authorId\": \"2257237899\",\\n            \"name\": \"Christopher J. MacLellan\"\\n          }\\n        ],\\n        \"abstract\": \"Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\\n      },\\n      {\\n        \"paperId\": \"e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\\n        \"title\": \"Transformer-Based Variable-Rate Image Compression with Region-of-Interest Control\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.10807\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.10807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a transformer-based learned image compression system that is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality while confirming its superiority over the other competing methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2140736758\",\\n            \"name\": \"Chia-Hao Kao\"\\n          },\\n          {\\n            \"authorId\": \"1723619\",\\n            \"name\": \"Ying Weng\"\\n          },\\n          {\\n            \"authorId\": \"2116613919\",\\n            \"name\": \"Yi-Hsin Chen\"\\n          },\\n          {\\n            \"authorId\": \"37811787\",\\n            \"name\": \"Wei-Chen Chiu\"\\n          },\\n          {\\n            \"authorId\": \"123608804\",\\n            \"name\": \"Wenmin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"This paper proposes a transformer-based learned image compression system. It is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality. Inspired by prompt tuning, we introduce prompt generation networks to condition the transformer-based autoencoder of compression. Our prompt generation networks generate content-adaptive tokens according to the input image, an ROI mask, and a rate parameter. The separation of the ROI mask and the rate parameter allows an intuitive way to achieve variable-rate and ROI coding simultaneously. Extensive experiments validate the effectiveness of our proposed method and confirm its superiority over the other competing methods.\"\\n      },\\n      {\\n        \"paperId\": \"c7acf9250926100f531bcf46d63d7da06e73928e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c7acf9250926100f531bcf46d63d7da06e73928e\",\\n        \"title\": \"MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work identifies that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images, and advances the cross-modality relation learning between the prompt and the generated images to better align the prompt and image content.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238903147\",\\n            \"name\": \"Yupeng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"18119920\",\\n            \"name\": \"Daquan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2238887924\",\\n            \"name\": \"Zuo-Liang Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2238889090\",\\n            \"name\": \"Yaxing Wang\"\\n          },\\n          {\\n            \"authorId\": \"3298532\",\\n            \"name\": \"Qibin Hou\"\\n          },\\n          {\\n            \"authorId\": \"33221685\",\\n            \"name\": \"Jiashi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in diffusion models have showcased their impressive capacity to generate visually striking images. However, ensuring a close match between the generated image and the given prompt remains a persistent challenge. In this work, we identify that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images. To better align the prompt and image content, we advance the cross-attention with an adaptive mask, which is conditioned on the attention maps and the prompt embeddings, to dynamically adjust the contribution of each text token to the image features. This mechanism explicitly diminishes the ambiguity in the semantic information embedding of the text encoder, leading to a boost of text-to-image consistency in the synthesized images. Our method, termed MaskDiffusion, is training-free and hot-pluggable for popular pre-trained diffusion models. When applied to the latent diffusion models, our MaskDiffusion can largely enhance their capability to correctly generate objects and their attributes, with negligible computation overhead compared to the original diffusion models. Our project page is https://github.com/HVision-NKU/MaskDiffusion.\"\\n      },\\n      {\\n        \"paperId\": \"66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\\n        \"url\": \"https://www.semanticscholar.org/paper/66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\\n        \"title\": \"TEMPERA: Test-Time Prompting via Reinforcement Learning\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2211.11890\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.11890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work designs a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers, and achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-11-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1993655237\",\\n            \"name\": \"Tianjun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275277634\",\\n            \"name\": \"Xuezhi Wang\"\\n          },\\n          {\\n            \"authorId\": \"65855107\",\\n            \"name\": \"Denny Zhou\"\\n          },\\n          {\\n            \"authorId\": \"50319359\",\\n            \"name\": \"D. Schuurmans\"\\n          },\\n          {\\n            \"authorId\": \"49988044\",\\n            \"name\": \"Joseph E. Gonzalez\"\\n          }\\n        ],\\n        \"abstract\": \"Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.\"\\n      },\\n      {\\n        \"paperId\": \"88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\\n        \"title\": \"QA-CLIMS: Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.09883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2280039282\",\\n            \"name\": \"Songhe Deng\"\\n          },\\n          {\\n            \"authorId\": \"2279830536\",\\n            \"name\": \"Wei Zhuo\"\\n          },\\n          {\\n            \"authorId\": \"2220635949\",\\n            \"name\": \"Jinheng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2265520934\",\\n            \"name\": \"Linlin Shen\"\\n          }\\n        ],\\n        \"abstract\": \"Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model\\'s ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets.\"\\n      },\\n      {\\n        \"paperId\": \"d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\\n        \"title\": \"Immunosenescence, Inflammaging, and Lung Senescence in Asthma in the Elderly\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2218-273X/12/10/1456/pdf?version=1665574999\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9599177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Immunosenescence\\\\u2019s contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly is discussed and an overview of age-related features in the immune system and lung structure is presented.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47522049\",\\n            \"name\": \"T. Soma\"\\n          },\\n          {\\n            \"authorId\": \"46375440\",\\n            \"name\": \"M. Nagata\"\\n          }\\n        ],\\n        \"abstract\": \"Prevalence of asthma in older adults is growing along with increasing global life expectancy. Due to poor clinical consequences such as high mortality, advancement in understanding the pathophysiology of asthma in older patients has been sought to provide prompt treatment for them. Age-related alterations of functions in the immune system and lung parenchyma occur throughout life. Alterations with advancing age are promoted by various stimuli, including pathobionts, fungi, viruses, pollutants, and damage-associated molecular patterns derived from impaired cells, abandoned cell debris, and senescent cells. Age-related changes in the innate and adaptive immune response, termed immunosenescence, includes impairment of phagocytosis and antigen presentation, enhancement of proinflammatory mediator generation, and production of senescence-associated secretory phenotype. Immnunosenescence could promote inflammaging (chronic low-grade inflammation) and contribute to late-onset adult asthma and asthma in the elderly, along with age-related pulmonary disease, such as chronic obstructive pulmonary disease and pulmonary fibrosis, due to lung parenchyma senescence. Aged patients with asthma exhibit local and systemic type 2 and non-type 2 inflammation, associated with clinical manifestations. Here, we discuss immunosenescence\\\\u2019s contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly and present an overview of age-related features in the immune system and lung structure.\"\\n      },\\n      {\\n        \"paperId\": \"94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\\n        \"title\": \"DAFT-E: Feature-Based Multivariate and Multi-Step-Ahead Wind Power Forecasting\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tste.2021.3130949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tste.2021.3130949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1978743481\",\\n            \"name\": \"F. De Caro\"\\n          },\\n          {\\n            \"authorId\": \"2142416821\",\\n            \"name\": \"Jacopo De Stefani\"\\n          },\\n          {\\n            \"authorId\": \"33858225\",\\n            \"name\": \"A. Vaccaro\"\\n          },\\n          {\\n            \"authorId\": \"1772497\",\\n            \"name\": \"Gianluca Bontempi\"\\n          }\\n        ],\\n        \"abstract\": \"Wind energy is one of the most promising resources for the mitigation of greenhouse gas emissions that contribute to anthropogenic global warming. However, the large proliferation of wind power generators is causing several critical issues in power systems due to their variable power generated profiles. For this reason, a large number of learning techniques, e.g. integrating Vector Auto-Regressive and Neural Network-based models, were proposed in the literature for mitigating wind power uncertainty issues. Unfortunately, these methodologies show several limitations, e.g. the huge number of parameters and/or the heavy computational cost, which hinder their deployment in modern power system operation, where prompt and reliable wide-area wind power generation forecasts are requested for supporting time-critical decision making on several time horizons. To try addressing this issue, this paper proposes the Dynamic Adaptive Feature-based Temporal Ensemble (DAFT-E) forecasting approach, which relies on an extensive feature engineering, a fast feature selection step and an ensemble of computationally inexpensive models to reduce the computational complexity of the forecasting task, while still preserving predictive accuracy. The experimental results, which benchmark DAFT-E against multivariate (VAR and deep learning) alternatives on two real case studies, show that the proposed approach outperforms state-of-the-art and representation learning models according to several forecasting accuracy metrics.\"\\n      },\\n      {\\n        \"paperId\": \"f6023556221018f779a63a60874973195aea8352\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6023556221018f779a63a60874973195aea8352\",\\n        \"title\": \"Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.09857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel framework named Attention-aware Self-adaptive Prompt (ASP), which prevents overfitting on base task and does not require enormous data in few-shot incremental tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275745354\",\\n            \"name\": \"Chenxi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2254008335\",\\n            \"name\": \"Zhenyi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2249155683\",\\n            \"name\": \"Tianyi Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2262968852\",\\n            \"name\": \"Ruibo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2254326623\",\\n            \"name\": \"Yihan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2275765198\",\\n            \"name\": \"Junfeng Guo\"\\n          },\\n          {\\n            \"authorId\": \"2261394090\",\\n            \"name\": \"Heng Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensive experiments on three benchmark datasets validate that ASP consistently outperforms state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning new classes and mitigating forgetting.\"\\n      },\\n      {\\n        \"paperId\": \"c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"title\": \"Soft Prompt Generation for Domain Generalization\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG), which consists of a two-stage training phase and an inference phase, aiming to incorporate the generative model domain knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2290728057\",\\n            \"name\": \"Yuedi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\"\\n      },\\n      {\\n        \"paperId\": \"37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"title\": \"Adapting to Distribution Shift by Visual Domain Prompt Generation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.02797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction and outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35793956\",\\n            \"name\": \"Zhixiang Chi\"\\n          },\\n          {\\n            \"authorId\": \"2300096585\",\\n            \"name\": \"Li Gu\"\\n          },\\n          {\\n            \"authorId\": \"2300089295\",\\n            \"name\": \"Tao Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2277793919\",\\n            \"name\": \"Huan Liu\"\\n          },\\n          {\\n            \"authorId\": \"1787848\",\\n            \"name\": \"Yuanhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2277598061\",\\n            \"name\": \"Konstantinos N. Plataniotis\"\\n          },\\n          {\\n            \"authorId\": \"2277695392\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n      },\\n      {\\n        \"paperId\": \"fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\\n        \"title\": \"Query-Based Adversarial Prompt Generation\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.12329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work improves on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268494505\",\\n            \"name\": \"Jonathan Hayase\"\\n          },\\n          {\\n            \"authorId\": \"2284689404\",\\n            \"name\": \"Ema Borevkovic\"\\n          },\\n          {\\n            \"authorId\": \"2483738\",\\n            \"name\": \"Nicholas Carlini\"\\n          },\\n          {\\n            \"authorId\": \"2444919\",\\n            \"name\": \"Florian Tram\\\\u00e8r\"\\n          },\\n          {\\n            \"authorId\": \"3490923\",\\n            \"name\": \"Milad Nasr\"\\n          }\\n        ],\\n        \"abstract\": \"Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI\\'s safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.\"\\n      },\\n      {\\n        \"paperId\": \"91b6158978b248e9a0e65d0d588bc1054e72bc16\",\\n        \"url\": \"https://www.semanticscholar.org/paper/91b6158978b248e9a0e65d0d588bc1054e72bc16\",\\n        \"title\": \"MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.findings-emnlp.215.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP and proposes a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260750930\",\\n            \"name\": \"Yuyan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2260655625\",\\n            \"name\": \"Zhihao Wen\"\\n          },\\n          {\\n            \"authorId\": \"2260651904\",\\n            \"name\": \"Ge Fan\"\\n          },\\n          {\\n            \"authorId\": \"2273721608\",\\n            \"name\": \"Zhengyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2273816877\",\\n            \"name\": \"Wei Wu\"\\n          },\\n          {\\n            \"authorId\": \"2260908086\",\\n            \"name\": \"Dayiheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2243457917\",\\n            \"name\": \"Zhixu Li\"\\n          },\\n          {\\n            \"authorId\": \"2163832089\",\\n            \"name\": \"Bang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2265724350\",\\n            \"name\": \"Yanghua Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.\"\\n      },\\n      {\\n        \"paperId\": \"135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\\n        \"url\": \"https://www.semanticscholar.org/paper/135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\\n        \"title\": \"Federated Text-driven Prompt Generation for Vision-Language Models\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256983385\",\\n            \"name\": \"Chen Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2257324808\",\\n            \"name\": \"Xingyu Li\"\\n          },\\n          {\\n            \"authorId\": \"29359383\",\\n            \"name\": \"Chaithanya Kumar Mummadi\"\\n          },\\n          {\\n            \"authorId\": \"144487556\",\\n            \"name\": \"M. Ganesh\"\\n          },\\n          {\\n            \"authorId\": \"2257091754\",\\n            \"name\": \"Zhenzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2257130661\",\\n            \"name\": \"Lu Peng\"\\n          },\\n          {\\n            \"authorId\": \"2257132255\",\\n            \"name\": \"Wan-Yi Lin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"title\": \"EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2024.3482175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2024.3482175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145205189\",\\n            \"name\": \"Yinsong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266873357\",\\n            \"name\": \"Jiaqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2266387679\",\\n            \"name\": \"Aidong Men\"\\n          },\\n          {\\n            \"authorId\": \"2266583142\",\\n            \"name\": \"Qingchao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\"\\n      },\\n      {\\n        \"paperId\": \"1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"title\": \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\\n        \"citationCount\": 92,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new approach to code generation by LLMs is proposed, which is called AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72729733\",\\n            \"name\": \"T. Ridnik\"\\n          },\\n          {\\n            \"authorId\": \"2279758170\",\\n            \"name\": \"Dedy Kredo\"\\n          },\\n          {\\n            \"authorId\": \"49668367\",\\n            \"name\": \"Itamar Friedman\"\\n          }\\n        ],\\n        \"abstract\": \"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\"\\n      },\\n      {\\n        \"paperId\": \"211e915b2e1e0753ddd581f10362fc82f28cc606\",\\n        \"url\": \"https://www.semanticscholar.org/paper/211e915b2e1e0753ddd581f10362fc82f28cc606\",\\n        \"title\": \"DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DiTCtrl is proposed, a training-free multi-prompt video generation method under MM-DiT architectures for the first time that achieves state-of-the-art performance without additional training and presents MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2337083029\",\\n            \"name\": \"Minghong Cai\"\\n          },\\n          {\\n            \"authorId\": \"30176430\",\\n            \"name\": \"Xiaodong Cun\"\\n          },\\n          {\\n            \"authorId\": \"2257035102\",\\n            \"name\": \"Xiaoyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2308556703\",\\n            \"name\": \"Wenze Liu\"\\n          },\\n          {\\n            \"authorId\": \"2303078452\",\\n            \"name\": \"Zhaoyang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2268490605\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2316484241\",\\n            \"name\": \"Xiangyu Yue\"\\n          }\\n        ],\\n        \"abstract\": \"Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer (MM-DiT) architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT\\\\u2019s attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training. Code is available at https://github.com/TencentARC/DiTCtrl.\"\\n      },\\n      {\\n        \"paperId\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04095\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces the Prompt Auto-Editing (PAE) method, which employs an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148661301\",\\n            \"name\": \"Wenyi Mo\"\\n          },\\n          {\\n            \"authorId\": \"2146332319\",\\n            \"name\": \"Tianyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2281418241\",\\n            \"name\": \"Yalong Bai\"\\n          },\\n          {\\n            \"authorId\": \"2295513824\",\\n            \"name\": \"Bing Su\"\\n          },\\n          {\\n            \"authorId\": \"2293310016\",\\n            \"name\": \"Ji-Rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2281323801\",\\n            \"name\": \"Qing Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\"\\n      }\\n    ]\\n  },\\n  \"iterative context update\": {\\n    \"total\": 9219,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 288,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 211,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 40,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\', \\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"fca9a4508863025d95a581ead47032d497825053\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fca9a4508863025d95a581ead47032d497825053\",\\n        \"title\": \"VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.02186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"VideoICL is a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach, improving OOD video understanding performance by extending effective context length without incurring high costs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333526847\",\\n            \"name\": \"Kangsan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2307075970\",\\n            \"name\": \"Geon Park\"\\n          },\\n          {\\n            \"authorId\": \"3445691\",\\n            \"name\": \"Youngwan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2119578055\",\\n            \"name\": \"Woongyeong Yeo\"\\n          },\\n          {\\n            \"authorId\": \"2265627157\",\\n            \"name\": \"Sung Ju Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in video large multimodal models (LMMs) have significantly improved their video understanding and reasoning capabilities. However, their performance drops on out-of-distribution (OOD) tasks that are underrepresented in training data. Traditional methods like fine-tuning on OOD datasets are impractical due to high computational costs. While In-context learning (ICL) with demonstration examples has shown promising generalization performance in language tasks and image-language tasks without fine-tuning, applying ICL to video-language tasks faces challenges due to the limited context length in Video LMMs, as videos require longer token lengths. To address these issues, we propose VideoICL, a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach. This allows to select the most relevant examples and rank them based on similarity, to be used for inference. If the generated response has low confidence, our framework selects new examples and performs inference again, iteratively refining the results until a high-confidence response is obtained. This approach improves OOD video understanding performance by extending effective context length without incurring high costs. The experimental results on multiple benchmarks demonstrate significant performance gains, especially in domain-specific scenarios, laying the groundwork for broader video comprehension applications. Code is released at https://github.com/KangsanKim07/VideoICL\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 72,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\\n        \"title\": \"Single image deraining using scale constraint iterative update network\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2023.121339?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2023.121339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1390863714\",\\n            \"name\": \"Yitong Yang\"\\n          },\\n          {\\n            \"authorId\": \"1591131546\",\\n            \"name\": \"Yongjun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"150356192\",\\n            \"name\": \"Zhongwei Cui\"\\n          },\\n          {\\n            \"authorId\": \"2112674491\",\\n            \"name\": \"Haoliang Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2210993430\",\\n            \"name\": \"Ting Ouyang\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 188,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 366,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 330,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 48,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2891,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      }\\n    ]\\n  },\\n  \"thematic consistency LLM\": {\\n    \"total\": 887,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\\n        \"title\": \"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.02896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This experimental study seeks to lay the groundwork for the understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction?\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282535211\",\\n            \"name\": \"Ivar Frisch\"\\n          },\\n          {\\n            \"authorId\": \"24068173\",\\n            \"name\": \"Mario Giulianelli\"\\n          }\\n        ],\\n        \"abstract\": \"Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model (LLM) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction? We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.\"\\n      },\\n      {\\n        \"paperId\": \"477a16bdbb43589e5feac3881b3370e3a4ab5624\",\\n        \"url\": \"https://www.semanticscholar.org/paper/477a16bdbb43589e5feac3881b3370e3a4ab5624\",\\n        \"title\": \"Evaluating the Consistency of LLM Evaluators\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.00543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Examining the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models demonstrates that strong proprietary models are not necessarily consistent evaluators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2291076200\",\\n            \"name\": \"Noah Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290955335\",\\n            \"name\": \"Jiwoo Hong\"\\n          },\\n          {\\n            \"authorId\": \"2290905396\",\\n            \"name\": \"James Thorne\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have shown potential as general evaluators along with the evident benefits of speed and cost. While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators. In this paper, we conduct extensive studies on the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models. Our comprehensive analysis demonstrates that strong proprietary models are not necessarily consistent evaluators, highlighting the importance of considering consistency in assessing the capability of LLM evaluators.\"\\n      },\\n      {\\n        \"paperId\": \"75062b58398b6e9409e5fec855f6912534331eaf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/75062b58398b6e9409e5fec855f6912534331eaf\",\\n        \"title\": \"Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales, facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-08-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": \"Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths,but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority voting, enabling more informed choices on when to halt sampling and which rationale to select. Our comprehensive experiments across diverse question-answering datasets demonstrate that RASC outperforms existing methods, reducing sample usage by approximately 70% while maintaining accuracy. Moreover, RASC facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs. Our approach effectively addresses the efficiency-accuracy trade-off in LLM reasoning tasks, offering a new perspective for more nuanced, faithful, and effective utilization of LLMs in resource-constrained environments.\"\\n      },\\n      {\\n        \"paperId\": \"3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\\n        \"title\": \"Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1186/s12911-024-02656-3\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11382443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare is emphasized, aiming to improve clinical outcomes ethically and effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-09-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"145770274\",\\n            \"name\": \"Tala Mirzaei\"\\n          },\\n          {\\n            \"authorId\": \"2320339088\",\\n            \"name\": \"Leila Amini\"\\n          },\\n          {\\n            \"authorId\": \"2574575\",\\n            \"name\": \"Pouyan Esmaeilzadeh\"\\n          }\\n        ],\\n        \"abstract\": \"This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions. We analyzed 3049 posts and comments extracted from a self-identified clinician subreddit using unsupervised machine learning via Latent Dirichlet Allocation and a structured qualitative analysis methodology. Analysis uncovered 14 salient themes of ethical implications, which we further consolidated into 4 overarching domains reflecting ethical issues around various clinical applications of LLM in healthcare, LLM coding, algorithm, and data governance, LLM\\\\u2019s role in health equity and the distribution of public health services, and the relationship between users (human) and LLM systems (machine). Mapping themes to ethical frameworks in literature illustrated multifaceted issues covering transparent LLM decisions, fairness, privacy, access disparities, user experiences, and reliability. This study emphasizes the need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare, aiming to improve clinical outcomes ethically and effectively.\"\\n      },\\n      {\\n        \"paperId\": \"6b7c5fc0f6b401962153f68f8250951f75da929e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b7c5fc0f6b401962153f68f8250951f75da929e\",\\n        \"title\": \"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.06503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper analyzes two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and finds that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-04-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295732707\",\\n            \"name\": \"Nathan Brake\"\\n          },\\n          {\\n            \"authorId\": \"2295732451\",\\n            \"name\": \"Thomas Schaaf\"\\n          }\\n        ],\\n        \"abstract\": \"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\\n      },\\n      {\\n        \"paperId\": \"e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"title\": \"LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\",\\n        \"citationCount\": 110,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL), which yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3400291\",\\n            \"name\": \"Shih-Chieh Dai\"\\n          },\\n          {\\n            \"authorId\": \"2261362789\",\\n            \"name\": \"Aiping Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1746959\",\\n            \"name\": \"Lun-Wei Ku\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n      },\\n      {\\n        \"paperId\": \"32426b96ff3c680125bde3b835bfa931288b8ade\",\\n        \"url\": \"https://www.semanticscholar.org/paper/32426b96ff3c680125bde3b835bfa931288b8ade\",\\n        \"title\": \"Better Patching Using LLM Prompting, via Self-Consistency\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2306.00108\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.00108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an application of the $\\\\\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots, on the MODIT dataset.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language models (LLMs) can be induced to solve non-trivial problems with \\\\u201cfew-shot\\\\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \\\\u201cchain of thought\\\\u201d ($\\\\\\\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \\\\u201cexplained\\\\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\\\\\\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\\\\\\\mathcal{S}-C$ (or even $\\\\\\\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\\\\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.\"\\n      },\\n      {\\n        \"paperId\": \"e24424283c02fbe7f641e5b3490d7bb059f8355a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e24424283c02fbe7f641e5b3490d7bb059f8355a\",\\n        \"title\": \"A Survey on LLM-as-a-Judge\",\\n        \"citationCount\": 776,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.15594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built?\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-11-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216587705\",\\n            \"name\": \"Jiawei Gu\"\\n          },\\n          {\\n            \"authorId\": \"144267788\",\\n            \"name\": \"Xuhui Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2287881684\",\\n            \"name\": \"Zhichao Shi\"\\n          },\\n          {\\n            \"authorId\": \"2274159320\",\\n            \"name\": \"Hexiang Tan\"\\n          },\\n          {\\n            \"authorId\": \"2332093190\",\\n            \"name\": \"Xuehao Zhai\"\\n          },\\n          {\\n            \"authorId\": \"2250617116\",\\n            \"name\": \"Chengjin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2330714501\",\\n            \"name\": \"Wei Li\"\\n          },\\n          {\\n            \"authorId\": \"1944248313\",\\n            \"name\": \"Yinghan Shen\"\\n          },\\n          {\\n            \"authorId\": \"2311556497\",\\n            \"name\": \"Shengjie Ma\"\\n          },\\n          {\\n            \"authorId\": \"2332306096\",\\n            \"name\": \"Honghao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257058703\",\\n            \"name\": \"Yuanzhuo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284217200\",\\n            \"name\": \"Jian Guo\"\\n          }\\n        ],\\n        \"abstract\": \"Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of\\\\\"LLM-as-a-Judge,\\\\\"where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.\"\\n      },\\n      {\\n        \"paperId\": \"0bf3a1867f7245b8a702093901c66b08b518eafc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0bf3a1867f7245b8a702093901c66b08b518eafc\",\\n        \"title\": \"Evaluating Very Long-Term Conversational Memory of LLM Agents\",\\n        \"citationCount\": 166,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A machine-human pipeline is introduced to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs, and presents a comprehensive evaluation benchmark to measure long-term memory in models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8785371\",\\n            \"name\": \"Adyasha Maharana\"\\n          },\\n          {\\n            \"authorId\": \"2266803131\",\\n            \"name\": \"Dong-Ho Lee\"\\n          },\\n          {\\n            \"authorId\": \"145582202\",\\n            \"name\": \"S. Tulyakov\"\\n          },\\n          {\\n            \"authorId\": \"2285969697\",\\n            \"name\": \"Mohit Bansal\"\\n          },\\n          {\\n            \"authorId\": \"2266751000\",\\n            \"name\": \"Francesco Barbieri\"\\n          },\\n          {\\n            \"authorId\": \"2267220081\",\\n            \"name\": \"Yuwei Fang\"\\n          }\\n        ],\\n        \"abstract\": \"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.\"\\n      },\\n      {\\n        \"paperId\": \"7c04ab297b59d4fe29285f339350882a3120b27f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7c04ab297b59d4fe29285f339350882a3120b27f\",\\n        \"title\": \"CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs\",\\n        \"citationCount\": 205,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642773\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.11314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions, is developed, revealing four design considerations for future educational AI assistants.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3136345\",\\n            \"name\": \"Majeed Kazemitabaar\"\\n          },\\n          {\\n            \"authorId\": \"2184253123\",\\n            \"name\": \"Runlong Ye\"\\n          },\\n          {\\n            \"authorId\": \"2280281736\",\\n            \"name\": \"Xiaoning Wang\"\\n          },\\n          {\\n            \"authorId\": \"2280145055\",\\n            \"name\": \"Austin Z Henley\"\\n          },\\n          {\\n            \"authorId\": \"2243041721\",\\n            \"name\": \"Paul Denny\"\\n          },\\n          {\\n            \"authorId\": \"2280145218\",\\n            \"name\": \"Michelle Craig\"\\n          },\\n          {\\n            \"authorId\": \"2280146888\",\\n            \"name\": \"Tovi Grossman\"\\n          }\\n        ],\\n        \"abstract\": \"Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student\\\\u2019s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI\\\\u2019s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.\"\\n      },\\n      {\\n        \"paperId\": \"da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\\n        \"url\": \"https://www.semanticscholar.org/paper/da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\\n        \"title\": \"Exploring and Evaluating Hallucinations in LLM-Powered Code Generation\",\\n        \"citationCount\": 221,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation, and proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290969862\",\\n            \"name\": \"Fang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2294504414\",\\n            \"name\": \"Yang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2295165194\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2294528116\",\\n            \"name\": \"Houkun Huang\"\\n          },\\n          {\\n            \"authorId\": \"2294510508\",\\n            \"name\": \"Ruifeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2294664033\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290433096\",\\n            \"name\": \"Li Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users\\' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.\"\\n      },\\n      {\\n        \"paperId\": \"a06d3e9e90008c64c45a0029d580541d5f646771\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771\",\\n        \"title\": \"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.00812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An overview of the various benefits of integrating code into LLMs\\' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277527247\",\\n            \"name\": \"Ke Yang\"\\n          },\\n          {\\n            \"authorId\": \"33456794\",\\n            \"name\": \"Jiateng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277421308\",\\n            \"name\": \"John Wu\"\\n          },\\n          {\\n            \"authorId\": \"2277597831\",\\n            \"name\": \"Chaoqi Yang\"\\n          },\\n          {\\n            \"authorId\": \"51135899\",\\n            \"name\": \"Y. Fung\"\\n          },\\n          {\\n            \"authorId\": \"2262396117\",\\n            \"name\": \"Sha Li\"\\n          },\\n          {\\n            \"authorId\": \"2277416897\",\\n            \"name\": \"Zixuan Huang\"\\n          },\\n          {\\n            \"authorId\": \"2344961610\",\\n            \"name\": \"Xu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2144803999\",\\n            \"name\": \"Xingyao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2277247982\",\\n            \"name\": \"Yiquan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2277409745\",\\n            \"name\": \"Heng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2261082008\",\\n            \"name\": \"ChengXiang Zhai\"\\n          }\\n        ],\\n        \"abstract\": \"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs\\' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.\"\\n      },\\n      {\\n        \"paperId\": \"2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\\n        \"title\": \"PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.12532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The number of crucial keys and values that influence future generations decreases layer by layer and can be extracted by the consistency in attention weights, leading to PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2182432556\",\\n            \"name\": \"Dongjie Yang\"\\n          },\\n          {\\n            \"authorId\": \"2302683712\",\\n            \"name\": \"Xiaodong Han\"\\n          },\\n          {\\n            \"authorId\": \"2302558089\",\\n            \"name\": \"Yan Gao\"\\n          },\\n          {\\n            \"authorId\": \"2302556666\",\\n            \"name\": \"Yao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2302704855\",\\n            \"name\": \"Shilin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2302545224\",\\n            \"name\": \"Hai Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.\"\\n      },\\n      {\\n        \"paperId\": \"36b9dac525fc93100b18d8e489bd97460cd49a5e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/36b9dac525fc93100b18d8e489bd97460cd49a5e\",\\n        \"title\": \"Thematic-LM: A LLM-based Multi-agent System for Large-scale Thematic Analysis\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3696410.3714595?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3696410.3714595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study applies Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-04-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2343740149\",\\n            \"name\": \"Tingrui Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2343749049\",\\n            \"name\": \"Caroline Walker\"\\n          },\\n          {\\n            \"authorId\": \"2343746435\",\\n            \"name\": \"Chris Cunningham\"\\n          },\\n          {\\n            \"authorId\": \"2310725786\",\\n            \"name\": \"Yun Sing Koh\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) is a widely used qualitative method for identifying underlying meanings within unstructured text. However, TA requires manual processes, which become increasingly labour-intensive and time-consuming as datasets grow. While large language models (LLMs) have been introduced to assist with TA on small-scale datasets, three key limitations hinder their effectiveness. First, current approaches often depend on interactions between an LLM agent and a human coder, a process that becomes challenging with larger datasets. Second, with feedback from the human coder, the LLM tends to mirror the human coder, which provides a narrower viewpoint of the data. Third, existing methods follow a sequential process, where codes are generated for individual samples without recalling previous codes and associated data, reducing the ability to analyse data holistically. To address these limitations, we propose Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis. Thematic-LM assigns specialised tasks to each agent, such as coding, aggregating codes, and maintaining and updating the codebook. We assign coder agents different identity perspectives to simulate the subjective nature of TA, fostering a more diverse interpretation of the data. We applied Thematic-LM to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change. We evaluate the resulting themes based on trustworthiness principles in qualitative research. Our study reveals insights such as assigning different identities to coder agents promotes divergence in codes and themes.\"\\n      },\\n      {\\n        \"paperId\": \"54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\\n        \"title\": \"Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation\",\\n        \"citationCount\": 88,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2285255408\",\\n            \"name\": \"Jiawei Wang\"\\n          },\\n          {\\n            \"authorId\": \"31279896\",\\n            \"name\": \"Renhe Jiang\"\\n          },\\n          {\\n            \"authorId\": \"46962297\",\\n            \"name\": \"Chuang Yang\"\\n          },\\n          {\\n            \"authorId\": \"2157765133\",\\n            \"name\": \"Zengqing Wu\"\\n          },\\n          {\\n            \"authorId\": \"2266396584\",\\n            \"name\": \"Makoto Onizuka\"\\n          },\\n          {\\n            \"authorId\": \"2239490643\",\\n            \"name\": \"Ryosuke Shibasaki\"\\n          },\\n          {\\n            \"authorId\": \"2284717877\",\\n            \"name\": \"Chuan Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\\n      },\\n      {\\n        \"paperId\": \"7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\\n        \"title\": \"ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification\",\\n        \"citationCount\": 75,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3660810?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3660810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions, and can effectively facilitate the practical application of LLMs in real-world development environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1994579604\",\\n            \"name\": \"Fangwen Mu\"\\n          },\\n          {\\n            \"authorId\": \"2305416699\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2259571951\",\\n            \"name\": \"Song Wang\"\\n          },\\n          {\\n            \"authorId\": \"2259613131\",\\n            \"name\": \"Zhuohao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2259874187\",\\n            \"name\": \"Binquan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2259824656\",\\n            \"name\": \"ChenXue Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260294248\",\\n            \"name\": \"Shichao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2157214565\",\\n            \"name\": \"Qing Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.\"\\n      },\\n      {\\n        \"paperId\": \"630c47372317164fc367153f938903e1d5b76059\",\\n        \"url\": \"https://www.semanticscholar.org/paper/630c47372317164fc367153f938903e1d5b76059\",\\n        \"title\": \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52202/079017-1077?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52202/079017-1077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1602820179\",\\n            \"name\": \"Gaurang Sriramanan\"\\n          },\\n          {\\n            \"authorId\": \"2344249306\",\\n            \"name\": \"Siddhant Bharti\"\\n          },\\n          {\\n            \"authorId\": \"150333898\",\\n            \"name\": \"Vinu Sankar Sadasivan\"\\n          },\\n          {\\n            \"authorId\": \"152623528\",\\n            \"name\": \"Shoumik Saha\"\\n          },\\n          {\\n            \"authorId\": \"2305809801\",\\n            \"name\": \"Priyatham Kattakinda\"\\n          },\\n          {\\n            \"authorId\": \"34389431\",\\n            \"name\": \"S. Feizi\"\\n          }\\n        ],\\n        \"abstract\": \"While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations\\\\u2014 outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.\"\\n      },\\n      {\\n        \"paperId\": \"038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\\n        \"title\": \"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt), which eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1966961\",\\n            \"name\": \"Yanshen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2239274607\",\\n            \"name\": \"Jianfeng He\"\\n          },\\n          {\\n            \"authorId\": \"2293779433\",\\n            \"name\": \"Limeng Cui\"\\n          },\\n          {\\n            \"authorId\": \"3433489\",\\n            \"name\": \"Shuo Lei\"\\n          },\\n          {\\n            \"authorId\": \"2249846863\",\\n            \"name\": \"Chang-Tien Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.\"\\n      },\\n      {\\n        \"paperId\": \"f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\\n        \"title\": \"Reasoning Runtime Behavior of a Program with LLM: How Far are We?\",\\n        \"citationCount\": 49,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a framework, namely $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution, and utilizes existing code benchmarks and adapt them to new benchmarks within this framework.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293350098\",\\n            \"name\": \"Junkai Chen\"\\n          },\\n          {\\n            \"authorId\": \"2276184077\",\\n            \"name\": \"Zhiyuan Pan\"\\n          },\\n          {\\n            \"authorId\": \"2110049191\",\\n            \"name\": \"Xing Hu\"\\n          },\\n          {\\n            \"authorId\": \"2293350478\",\\n            \"name\": \"Zhenhao Li\"\\n          },\\n          {\\n            \"authorId\": \"2286413567\",\\n            \"name\": \"Ge Li\"\\n          },\\n          {\\n            \"authorId\": \"2265241871\",\\n            \"name\": \"Xin Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs (i.e., predicting code execution behaviors such as program output and execution path), but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs. Our code, data and $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$ leaderboard are available at https://r-eval.github.io.\"\\n      },\\n      {\\n        \"paperId\": \"8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\\n        \"title\": \"Don\\'t Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper leverages the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics, they can be prompted to autoformalize informal mathematical statements into formal Isabelle code -- which provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287813125\",\\n            \"name\": \"Jin Peng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2144884927\",\\n            \"name\": \"Charles Staats\"\\n          },\\n          {\\n            \"authorId\": \"2293653101\",\\n            \"name\": \"Wenda Li\"\\n          },\\n          {\\n            \"authorId\": \"2574060\",\\n            \"name\": \"Christian Szegedy\"\\n          },\\n          {\\n            \"authorId\": \"7446832\",\\n            \"name\": \"Kilian Q. Weinberger\"\\n          },\\n          {\\n            \"authorId\": \"2287780080\",\\n            \"name\": \"Yuhuai Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLM), such as Google\\'s Minerva and OpenAI\\'s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.\"\\n      },\\n      {\\n        \"paperId\": \"1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\\n        \"title\": \"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.17578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MM-Eval is introduced, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages designed with multilingual-specific challenges in mind and finds that evaluators are unfair and inconsistent when evaluating lower-resourced languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2321452295\",\\n            \"name\": \"Guijin Son\"\\n          },\\n          {\\n            \"authorId\": \"29830817\",\\n            \"name\": \"Dongkeun Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2299329316\",\\n            \"name\": \"Juyoung Suk\"\\n          },\\n          {\\n            \"authorId\": \"2301578911\",\\n            \"name\": \"Javier Aula-Blasco\"\\n          },\\n          {\\n            \"authorId\": \"2327215494\",\\n            \"name\": \"Mano Aslan\"\\n          },\\n          {\\n            \"authorId\": \"2327216625\",\\n            \"name\": \"Vu Trong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2232783785\",\\n            \"name\": \"Shayekh Bin Islam\"\\n          },\\n          {\\n            \"authorId\": \"2327215436\",\\n            \"name\": \"Jaume Prats-Cristi\\\\u00e0\"\\n          },\\n          {\\n            \"authorId\": \"2327217057\",\\n            \"name\": \"Luc\\\\u00eda Tormo-Ba\\\\u00f1uelos\"\\n          },\\n          {\\n            \"authorId\": \"2184037220\",\\n            \"name\": \"Seungone Kim\"\\n          }\\n        ],\\n        \"abstract\": \"As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\\\\\"meta-evaluation benchmarks\\\\\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.\"\\n      },\\n      {\\n        \"paperId\": \"f1366e505de4f1d0e901903e3c17471033758a96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f1366e505de4f1d0e901903e3c17471033758a96\",\\n        \"title\": \"Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based Chatbots\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.11876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Rescriber is designed, built, and evaluated, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts, presenting a promising approach to address the privacy and trust challenges of AI.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2326261436\",\\n            \"name\": \"Jijie Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2326116321\",\\n            \"name\": \"Eryue Xu\"\\n          },\\n          {\\n            \"authorId\": \"2326262935\",\\n            \"name\": \"Yaoyao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2326228781\",\\n            \"name\": \"Tianshi Li\"\\n          }\\n        ],\\n        \"abstract\": \"The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users\\\\u2019 personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=Rescriber) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users\\\\u2019 subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4o. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users\\\\u2019 trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.\"\\n      },\\n      {\\n        \"paperId\": \"3ec06fe8d8764123490544ab5dc956143e84b443\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3ec06fe8d8764123490544ab5dc956143e84b443\",\\n        \"title\": \"Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.11977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise and the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310565909\",\\n            \"name\": \"Guangzhi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2089532795\",\\n            \"name\": \"Xiao Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2275248675\",\\n            \"name\": \"Jose Such\"\\n          }\\n        ],\\n        \"abstract\": \"The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\\n      },\\n      {\\n        \"paperId\": \"dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\\n        \"title\": \"Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.07791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating position bias in LLM judges across pairwise and list-wise comparison settings confirms that position bias is not due to random chance and varies significantly across judges and tasks, and provides insights into the distribution of judging difficulty across the dataset.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305925735\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2262963382\",\\n            \"name\": \"Chiyu Ma\"\\n          },\\n          {\\n            \"authorId\": \"2330065663\",\\n            \"name\": \"Wenhua Liang\"\\n          },\\n          {\\n            \"authorId\": \"2227771\",\\n            \"name\": \"Weicheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"1918441\",\\n            \"name\": \"Soroush Vosoughi\"\\n          }\\n        ],\\n        \"abstract\": \"LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases - particularly position bias, the tendency to favor solutions based on their position within the prompt - compromise its reliability. This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 15 LLM judges across MTBench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.\"\\n      },\\n      {\\n        \"paperId\": \"638d8d1f3865ebf065605535a7aa50727d5ffabe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/638d8d1f3865ebf065605535a7aa50727d5ffabe\",\\n        \"title\": \"TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.18919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation, significantly improves semantic and contextual consistency in synthesized images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2298945483\",\\n            \"name\": \"Junhao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2298032416\",\\n            \"name\": \"Baiqiao Yin\"\\n          },\\n          {\\n            \"authorId\": \"2229014859\",\\n            \"name\": \"Kaixin Cai\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2276604489\",\\n            \"name\": \"Hanhui Li\"\\n          },\\n          {\\n            \"authorId\": \"2299161673\",\\n            \"name\": \"Yuxin He\"\\n          },\\n          {\\n            \"authorId\": \"2298943419\",\\n            \"name\": \"Xi Lu\"\\n          },\\n          {\\n            \"authorId\": \"2298043252\",\\n            \"name\": \"Yue Li\"\\n          },\\n          {\\n            \"authorId\": \"2298926852\",\\n            \"name\": \"Yifei Li\"\\n          },\\n          {\\n            \"authorId\": \"2298644153\",\\n            \"name\": \"Yuhao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"144880586\",\\n            \"name\": \"Yiqiang Yan\"\\n          },\\n          {\\n            \"authorId\": \"2291389227\",\\n            \"name\": \"Xiaodan Liang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a\\\\\"Screenwriter\\\\\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the\\\\\"Rehearsal\\\\\". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the\\\\\"Final Performance\\\\\". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.\"\\n      },\\n      {\\n        \"paperId\": \"4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"title\": \"Improving Text-to-Image Consistency via Automatic Prompt Optimization\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models and paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1796269096\",\\n            \"name\": \"Oscar Ma\\\\u00f1as\"\\n          },\\n          {\\n            \"authorId\": \"2274101827\",\\n            \"name\": \"Pietro Astolfi\"\\n          },\\n          {\\n            \"authorId\": \"2293590162\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"2256372432\",\\n            \"name\": \"Candace Ross\"\\n          },\\n          {\\n            \"authorId\": \"39219656\",\\n            \"name\": \"Jack Urbanek\"\\n          },\\n          {\\n            \"authorId\": \"2293907712\",\\n            \"name\": \"Adina Williams\"\\n          },\\n          {\\n            \"authorId\": \"2801949\",\\n            \"name\": \"Aishwarya Agrawal\"\\n          },\\n          {\\n            \"authorId\": \"1456285042\",\\n            \"name\": \"Adriana Romero-Soriano\"\\n          },\\n          {\\n            \"authorId\": \"3325894\",\\n            \"name\": \"M. Drozdzal\"\\n          }\\n        ],\\n        \"abstract\": \"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"275a3955a83867dd36a3683788e0e053e00f8a89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/275a3955a83867dd36a3683788e0e053e00f8a89\",\\n        \"title\": \"Collaborative Enhancement of Consistency and Accuracy in US Diagnosis of Thyroid Nodules Using Large Language Models.\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1148/radiol.232255?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1148/radiol.232255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging and ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1678966811\",\\n            \"name\": \"Shaohong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2272038393\",\\n            \"name\": \"Wen-juan Tong\"\\n          },\\n          {\\n            \"authorId\": \"2127991969\",\\n            \"name\": \"Ming-De Li\"\\n          },\\n          {\\n            \"authorId\": \"28890237\",\\n            \"name\": \"Hang-tong Hu\"\\n          },\\n          {\\n            \"authorId\": \"2187182790\",\\n            \"name\": \"Xiao-zhou Lu\"\\n          },\\n          {\\n            \"authorId\": \"2185218332\",\\n            \"name\": \"Ze-Rong Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290915880\",\\n            \"name\": \"Xin-Xin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290897544\",\\n            \"name\": \"Ruifang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2261915754\",\\n            \"name\": \"Ming-De Lu\"\\n          },\\n          {\\n            \"authorId\": \"6457299\",\\n            \"name\": \"Li-da Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290866279\",\\n            \"name\": \"Wei Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Background Large language models (LLMs) hold substantial promise for medical imaging interpretation. However, there is a lack of studies on their feasibility in handling reasoning questions associated with medical diagnosis. Purpose To investigate the viability of leveraging three publicly available LLMs to enhance consistency and diagnostic accuracy in medical imaging based on standardized reporting, with pathology as the reference standard. Materials and Methods US images of thyroid nodules with pathologic results were retrospectively collected from a tertiary referral hospital between July 2022 and December 2022 and used to evaluate malignancy diagnoses generated by three LLMs-OpenAI\\'s ChatGPT 3.5, ChatGPT 4.0, and Google\\'s Bard. Inter- and intra-LLM agreement of diagnosis were evaluated. Then, diagnostic performance, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), was evaluated and compared for the LLMs and three interactive approaches: human reader combined with LLMs, image-to-text model combined with LLMs, and an end-to-end convolutional neural network model. Results A total of 1161 US images of thyroid nodules (498 benign, 663 malignant) from 725 patients (mean age, 42.2 years \\\\u00b1 14.1 [SD]; 516 women) were evaluated. ChatGPT 4.0 and Bard displayed substantial to almost perfect intra-LLM agreement (\\\\u03ba range, 0.65-0.86 [95% CI: 0.64, 0.86]), while ChatGPT 3.5 showed fair to substantial agreement (\\\\u03ba range, 0.36-0.68 [95% CI: 0.36, 0.68]). ChatGPT 4.0 had an accuracy of 78%-86% (95% CI: 76%, 88%) and sensitivity of 86%-95% (95% CI: 83%, 96%), compared with 74%-86% (95% CI: 71%, 88%) and 74%-91% (95% CI: 71%, 93%), respectively, for Bard. Moreover, with ChatGPT 4.0, the image-to-text-LLM strategy exhibited an AUC (0.83 [95% CI: 0.80, 0.85]) and accuracy (84% [95% CI: 82%, 86%]) comparable to those of the human-LLM interaction strategy with two senior readers and one junior reader and exceeding those of the human-LLM interaction strategy with one junior reader. Conclusion LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging. ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5. \\\\u00a9 RSNA, 2024 Supplemental material is available for this article.\"\\n      },\\n      {\\n        \"paperId\": \"b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\\n        \"title\": \"Mitigating LLM Hallucinations via Conformal Abstention\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.01563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1388837087\",\\n            \"name\": \"Yasin Abbasi-Yadkori\"\\n          },\\n          {\\n            \"authorId\": \"3150458\",\\n            \"name\": \"Ilja Kuzborskij\"\\n          },\\n          {\\n            \"authorId\": \"2298902427\",\\n            \"name\": \"David Stutz\"\\n          },\\n          {\\n            \"authorId\": \"2305592785\",\\n            \"name\": \"Andr\\\\u00e1s Gy\\\\u00f6rgy\"\\n          },\\n          {\\n            \"authorId\": \"2299943725\",\\n            \"name\": \"Adam Fisch\"\\n          },\\n          {\\n            \"authorId\": \"2299943677\",\\n            \"name\": \"Arnaud Doucet\"\\n          },\\n          {\\n            \"authorId\": \"2299943992\",\\n            \"name\": \"Iuliya Beloshapka\"\\n          },\\n          {\\n            \"authorId\": \"2239098855\",\\n            \"name\": \"Wei-Hung Weng\"\\n          },\\n          {\\n            \"authorId\": \"2300022831\",\\n            \"name\": \"Yao-Yuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257346986\",\\n            \"name\": \"Csaba Szepesv\\'ari\"\\n          },\\n          {\\n            \"authorId\": \"9235290\",\\n            \"name\": \"Ali Taylan Cemgil\"\\n          },\\n          {\\n            \"authorId\": \"2359197879\",\\n            \"name\": \"Nenad Tomasev\"\\n          }\\n        ],\\n        \"abstract\": \"We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying\\\\\"I don\\'t know\\\\\") in a general domain, instead of resorting to possibly\\\\\"hallucinating\\\\\"a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.\"\\n      },\\n      {\\n        \"paperId\": \"023a98af94a3e7e8e538a6183da8ec05024fec56\",\\n        \"url\": \"https://www.semanticscholar.org/paper/023a98af94a3e7e8e538a6183da8ec05024fec56\",\\n        \"title\": \"How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work prompts different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions, and finds that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2307916998\",\\n            \"name\": \"Julia Kharchenko\"\\n          },\\n          {\\n            \"authorId\": \"2284066307\",\\n            \"name\": \"Tanya Roosta\"\\n          },\\n          {\\n            \"authorId\": \"2284065969\",\\n            \"name\": \"Aman Chadha\"\\n          },\\n          {\\n            \"authorId\": \"2234352974\",\\n            \"name\": \"Chirag Shah\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user\\'s known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs\\'cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"a1849a77644ff411a03833b5aa7a65ff57158c50\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1849a77644ff411a03833b5aa7a65ff57158c50\",\\n        \"title\": \"CLLMs: Consistency Large Language Models\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.00835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work develops a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory by refining the target LLM to consistently predict the fixed point given any state as input.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258963117\",\\n            \"name\": \"Siqi Kou\"\\n          },\\n          {\\n            \"authorId\": \"2258334187\",\\n            \"name\": \"Lanxiang Hu\"\\n          },\\n          {\\n            \"authorId\": \"2116778591\",\\n            \"name\": \"Zhe He\"\\n          },\\n          {\\n            \"authorId\": \"2260296481\",\\n            \"name\": \"Zhijie Deng\"\\n          },\\n          {\\n            \"authorId\": \"2289837431\",\\n            \"name\": \"Hao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\\\\\\\times$ to 3.4$\\\\\\\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\\n        \"title\": \"PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children\\'s Collaborative Learning\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613905.3651008\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.14227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded and thus highlight potential design improvements and considerations for peer agents in both roles.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-03-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2296401615\",\\n            \"name\": \"Jiawen Liu\"\\n          },\\n          {\\n            \"authorId\": \"2292671914\",\\n            \"name\": \"Yuanyuan Yao\"\\n          },\\n          {\\n            \"authorId\": \"2283762773\",\\n            \"name\": \"Pengcheng An\"\\n          },\\n          {\\n            \"authorId\": \"2301178822\",\\n            \"name\": \"Qi Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In children\\\\u2019s collaborative learning, effective peer conversations can significantly enhance the quality of children\\\\u2019s collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children\\\\u2019s creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.\"\\n      },\\n      {\\n        \"paperId\": \"5e317746c0f38d1149f33a19807af47d513bdd27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5e317746c0f38d1149f33a19807af47d513bdd27\",\\n        \"title\": \"Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.12509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald\\'s omega, and demonstrates the limitations of fixed randomness and the importance of considering multiple samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2328309355\",\\n            \"name\": \"Kayla Schroeder\"\\n          },\\n          {\\n            \"authorId\": \"1411379613\",\\n            \"name\": \"Zach Wood-Doughty\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs. While deterministic settings can improve consistency, they do not guarantee reliability, as a single sample from the model\\'s probability distribution can still be misleading. Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald\\'s omega. We evaluate the reliability of LLMs when judging the outputs of other LLMs on standard single-turn and multi-turn benchmarks, simultaneously investigating the impact of temperature on reliability. By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implications for downstream applications. Our findings highlight the need for a nuanced understanding of LLM reliability and the potential risks associated with over-reliance on single-shot evaluations. This work provides a crucial step towards building more trustworthy and reliable LLM-based systems and applications.\"\\n      },\\n      {\\n        \"paperId\": \"235a8bd57a6b53ecab756780a45bce6e4743cecd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/235a8bd57a6b53ecab756780a45bce6e4743cecd\",\\n        \"title\": \"LLM-based Multi-Level Knowledge Generation for Few-shot Knowledge Graph Completion\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2024/236?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2024/236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a generation-based FKGC paradigm facilitated by LLM distillation, which achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284861197\",\\n            \"name\": \"Qian Li\"\\n          },\\n          {\\n            \"authorId\": \"2313599793\",\\n            \"name\": \"Zhuo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2052296239\",\\n            \"name\": \"Cheng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2313658619\",\\n            \"name\": \"Shiqi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2274552581\",\\n            \"name\": \"Jianxin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Knowledge Graphs (KGs) are pivotal in various NLP applications but often grapple with incompleteness, especially due to the long-tail problem where infrequent, unpopular relationships drastically reduce the KG completion performance. In this paper, we focus on Few-shot Knowledge Graph Completion (FKGC), a task addressing these gaps in long-tail scenarios. Amidst the rapid evolution of Large Language Models, we propose a generation-based FKGC paradigm facilitated by LLM distillation. Our MuKDC framework employs multi-level knowledge distillation for few-shot KG completion, generating supplementary knowledge to mitigate data scarcity in few-shot environments. MuKDC comprises two primary components: Multi-level Knowledge Generation, which enriches the KG at various levels, and Consistency Assessment, to ensure the coherence and reliability of the generated knowledge. Most notably, our method achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\\n      },\\n      {\\n        \"paperId\": \"9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\\n        \"title\": \"Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2144511530\",\\n            \"name\": \"Xuan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2265432385\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2302819855\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"2302765389\",\\n            \"name\": \"Haoyang Shang\"\\n          },\\n          {\\n            \"authorId\": \"2302927286\",\\n            \"name\": \"Chengxu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2302915902\",\\n            \"name\": \"Quanyan Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents\\' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents\\' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\"\\n      },\\n      {\\n        \"paperId\": \"086046d38b3a7066aa39e3d350905e8065c8f1b5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/086046d38b3a7066aa39e3d350905e8065c8f1b5\",\\n        \"title\": \"Efficiently Scaling LLM Reasoning with Certaindex\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.20993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy, so this work introduces Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-12-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282896192\",\\n            \"name\": \"Yichao Fu\"\\n          },\\n          {\\n            \"authorId\": \"2279862923\",\\n            \"name\": \"Junda Chen\"\\n          },\\n          {\\n            \"authorId\": \"2317134948\",\\n            \"name\": \"Siqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2337869460\",\\n            \"name\": \"Zheyu Fu\"\\n          },\\n          {\\n            \"authorId\": \"2351053054\",\\n            \"name\": \"Zhongdongming Dai\"\\n          },\\n          {\\n            \"authorId\": \"2152482391\",\\n            \"name\": \"Yonghao Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2363671676\",\\n            \"name\": \"Yian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2317112099\",\\n            \"name\": \"Aurick Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2379937619\",\\n            \"name\": \"Tajana Rosing\"\\n          },\\n          {\\n            \"authorId\": \"2344601177\",\\n            \"name\": \"Ion Stoica\"\\n          },\\n          {\\n            \"authorId\": \"2337807823\",\\n            \"name\": \"Hao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50% compute savings and 3.3x higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git\"\\n      },\\n      {\\n        \"paperId\": \"43fedc4430be030c083626b3e64b7093916b429a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43fedc4430be030c083626b3e64b7093916b429a\",\\n        \"title\": \"Enhancing clinical reasoning skills for medical students: a qualitative comparison of LLM-powered social robotic versus computer-based virtual patients within rheumatology\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1007/s00296-024-05731-0\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11618132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform and shows promise in training CR skills, communication, and adaptive thinking.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290951787\",\\n            \"name\": \"Alexander Borg\"\\n          },\\n          {\\n            \"authorId\": \"2326093338\",\\n            \"name\": \"Benjamin Jobs\"\\n          },\\n          {\\n            \"authorId\": \"2164303442\",\\n            \"name\": \"Viking Huss\"\\n          },\\n          {\\n            \"authorId\": \"23717264\",\\n            \"name\": \"C. Gentline\"\\n          },\\n          {\\n            \"authorId\": \"2326093767\",\\n            \"name\": \"Fabricio Espinosa\"\\n          },\\n          {\\n            \"authorId\": \"2290722393\",\\n            \"name\": \"Mini Ruiz\"\\n          },\\n          {\\n            \"authorId\": \"2758537\",\\n            \"name\": \"Samuel Edelbring\"\\n          },\\n          {\\n            \"authorId\": \"2326094583\",\\n            \"name\": \"Carina Georg\"\\n          },\\n          {\\n            \"authorId\": \"103081544\",\\n            \"name\": \"G. Skantze\"\\n          },\\n          {\\n            \"authorId\": \"8637952\",\\n            \"name\": \"Ioannis Parodis\"\\n          }\\n        ],\\n        \"abstract\": \"Virtual patients (VPs) are increasingly used in medical education to train clinical reasoning (CR) skills. However, optimal VP design for enhancing interactivity and authenticity remains unclear. Novel interactive modalities, such as large language model (LLM)-enhanced social robotic VPs might increase interactivity and authenticity in CR skill practice. To evaluate medical students\\\\u2019 perceptions of CR training using an LLM-enhanced social robotic VP platform compared with a conventional computer-based VP platform. A qualitative study involved 23 third-year medical students from Karolinska Institutet, who completed VP cases on an LLM-enhanced social robotic platform and a computer-based semi-linear platform. In-depth interviews assessed students\\\\u2019 self-perceived acquirement of CR skills using the two platforms. Thematic analysis was employed to identify themes and sub-themes. Three main themes were identified: authenticity, VP application, and strengths and limitations. Students found the social robotic platform more authentic and engaging. It enabled highly interactive communication and expressed emotions, collectively offering a realistic experience. It facilitated active learning, hypothesis generation, and adaptive thinking. Limitations included lack of physical examination options and, occasionally, mechanical dialogue. The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform. Despite some limitations, it shows promise in training CR skills, communication, and adaptive thinking. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. \\\\u2022An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. \\\\u2022Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. \\\\u2022Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. \\\\u2022An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. \\\\u2022Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. \\\\u2022Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations.\"\\n      },\\n      {\\n        \"paperId\": \"f04c25fcf3247ff4d8eca72d862b22090b884b75\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f04c25fcf3247ff4d8eca72d862b22090b884b75\",\\n        \"title\": \"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.06461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49606614\",\\n            \"name\": \"Junlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282521448\",\\n            \"name\": \"Siddhartha Jain\"\\n          },\\n          {\\n            \"authorId\": \"2305691523\",\\n            \"name\": \"Dejiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2282366776\",\\n            \"name\": \"Baishakhi Ray\"\\n          },\\n          {\\n            \"authorId\": \"40574366\",\\n            \"name\": \"Varun Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2304481349\",\\n            \"name\": \"Ben Athiwaratkun\"\\n          }\\n        ],\\n        \"abstract\": \"A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don\\\\u2019t surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.\"\\n      },\\n      {\\n        \"paperId\": \"7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\\n        \"title\": \"What Did I Do Wrong? Quantifying LLMs\\' Sensitivity and Consistency to Prompt Engineering\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.12334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Two metrics for classification tasks, namely sensitivity and consistency, are introduced, which are complementary to task performance, and it is hoped that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2307085791\",\\n            \"name\": \"Federico Errica\"\\n          },\\n          {\\n            \"authorId\": \"2009237\",\\n            \"name\": \"G. Siracusano\"\\n          },\\n          {\\n            \"authorId\": \"3109801\",\\n            \"name\": \"D. Sanvito\"\\n          },\\n          {\\n            \"authorId\": \"2269460793\",\\n            \"name\": \"Roberto Bifulco\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs\\'inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely sensitivity and consistency, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\\n      },\\n      {\\n        \"paperId\": \"024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\\n        \"title\": \"PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2403.09732?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2403.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt representation, called reference-enhanced representation, is introduced, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries and proposes using cross-consistency across different LLMs rather than self-consistency within a particular LLM.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2262448530\",\\n            \"name\": \"Zhishuai Li\"\\n          },\\n          {\\n            \"authorId\": \"2292059965\",\\n            \"name\": \"Xiang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2291921433\",\\n            \"name\": \"Jingjing Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2262480162\",\\n            \"name\": \"Sun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2228059114\",\\n            \"name\": \"Guoqing Du\"\\n          },\\n          {\\n            \"authorId\": \"2267589674\",\\n            \"name\": \"Xiaoru Hu\"\\n          },\\n          {\\n            \"authorId\": \"2315291036\",\\n            \"name\": \"Bin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2276235755\",\\n            \"name\": \"Yuxiao Ye\"\\n          },\\n          {\\n            \"authorId\": \"2262543561\",\\n            \"name\": \"Ziyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2263456785\",\\n            \"name\": \"Rui Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2262446566\",\\n            \"name\": \"Hangyu Mao\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0e314ddbf28514d92f2405b73941242c162ae0ba\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0e314ddbf28514d92f2405b73941242c162ae0ba\",\\n        \"title\": \"AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.11073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AFaCTA (Automatic Factual Claim deTection Annotator) is introduced, a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs) and calibrates its annotation confidence with consistency along three predefined reasoning paths.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046974354\",\\n            \"name\": \"Jingwei Ni\"\\n          },\\n          {\\n            \"authorId\": \"2284947386\",\\n            \"name\": \"Minjing Shi\"\\n          },\\n          {\\n            \"authorId\": \"146552774\",\\n            \"name\": \"Dominik Stammbach\"\\n          },\\n          {\\n            \"authorId\": \"2790926\",\\n            \"name\": \"Mrinmaya Sachan\"\\n          },\\n          {\\n            \"authorId\": \"2261279066\",\\n            \"name\": \"Elliott Ash\"\\n          },\\n          {\\n            \"authorId\": \"3073566\",\\n            \"name\": \"Markus Leippold\"\\n          }\\n        ],\\n        \"abstract\": \"With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.\"\\n      },\\n      {\\n        \"paperId\": \"1c5a097b4e376897545f153370425cf7e0c2d8fd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1c5a097b4e376897545f153370425cf7e0c2d8fd\",\\n        \"title\": \"Explaining Length Bias in LLM-Based Preference Evaluations\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.01085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AdapAlpaca is proposed, a simple yet effective adjustment to win rate measurement that ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Zhengyu Hu\"\\n          },\\n          {\\n            \"authorId\": \"2322070046\",\\n            \"name\": \"Linxin Song\"\\n          },\\n          {\\n            \"authorId\": \"2309191644\",\\n            \"name\": \"Jieyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2311315868\",\\n            \"name\": \"Zheyuan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2269687536\",\\n            \"name\": \"Jingang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2309176938\",\\n            \"name\": \"Zhenyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2309202283\",\\n            \"name\": \"Jieyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2269470756\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"The use of large language models (LLMs) as judges, particularly in preference comparisons, has become widespread, but this reveals a notable bias towards longer responses, undermining the reliability of such evaluations. To better understand such bias, we propose to decompose the preference evaluation metric, specifically the win rate, into two key components: desirability and information mass, where the former is length-independent and related to trustworthiness such as correctness, toxicity, and consistency, and the latter is length-dependent and represents the amount of information in the response. We empirically demonstrated the decomposition through controlled experiments and found that response length impacts evaluations by influencing information mass. To derive a reliable evaluation metric that assesses content quality without being confounded by response length, we propose AdapAlpaca, a simple yet effective adjustment to win rate measurement. Specifically, AdapAlpaca ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\\n      },\\n      {\\n        \"paperId\": \"00ce8beee350a260395676490915d7ebfa7430d1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/00ce8beee350a260395676490915d7ebfa7430d1\",\\n        \"title\": \"Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Investigation of fine-tuned LLMs in D2T tasks in terms of model size reveals that increasing LLM size enhances readability andformativeness in D2T tasks, but larger LLMs may sacrifice \\\\\\\\textit{faithfulness}.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8038450\",\\n            \"name\": \"Joy Mahapatra\"\\n          },\\n          {\\n            \"authorId\": \"2312204876\",\\n            \"name\": \"U. Garain\"\\n          }\\n        ],\\n        \"abstract\": \"Data-to-text (D2T) generation aims to generate human-readable text from semi-structured data, such as tables and graphs. The recent success of D2T is largely attributed to advancements in LLMs. Despite the success of LLMs, no research has been conducted to illustrate the impact of model size on the performance of fine-tuned LLMs for D2T tasks. D2T model performance is typically assessed based on three key qualities: \\\\\\\\textit{readability} (indicates fluency and coherence), \\\\\\\\textit{informativeness} (measures content similarity), and \\\\\\\\textit{faithfulness} (assesses consistency of factual information). It is currently uncertain whether increasing the size of LLMs effectively improves performance in D2T tasks across these three qualities. The objective of this study is to investigate the performance of fine-tuned LLMs in D2T tasks in terms of model size. Through extensive comparative analysis, we aim to elucidate both the advantages and limitations of scaling model sizes across five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and twelve state-of-the-art LLMs with varying sizes from five different LLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all the three essential qualities of D2T models, we incorporate six widely recognized automatic metrics -- \\\\\\\\textsc{BLEU}, \\\\\\\\textsc{METEOR}, \\\\\\\\textsc{BERTScore}, \\\\\\\\textsc{MoverScore}, \\\\\\\\textsc{Parent}, and \\\\\\\\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance concerning model size in the presence of source-reference divergence, a critical aspect of D2T tasks. Our investigation reveals that increasing LLM size enhances \\\\\\\\textit{readability} and \\\\\\\\textit{informativeness} in D2T tasks, but larger (in terms of size) LLMs may sacrifice \\\\\\\\textit{faithfulness}. Moreover, small-sized LLMs show more resilience than larger ones when source-reference divergence is present.\"\\n      },\\n      {\\n        \"paperId\": \"74908bc543e77b8995a6eebe32ab13cf0837949b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/74908bc543e77b8995a6eebe32ab13cf0837949b\",\\n        \"title\": \"LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment and demonstrates that, when using GPT-4, the proposed metric achieves evaluation consistency close to that of radiologists.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2294387070\",\\n            \"name\": \"Zilong Wang\"\\n          },\\n          {\\n            \"authorId\": \"13289447\",\\n            \"name\": \"Xufang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2268347004\",\\n            \"name\": \"Xinyang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268313028\",\\n            \"name\": \"Dongsheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2160727304\",\\n            \"name\": \"Lili Qiu\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task\\'s clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.\"\\n      },\\n      {\\n        \"paperId\": \"f33991c02f8f0ab8794dad020c648b37ccc53365\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f33991c02f8f0ab8794dad020c648b37ccc53365\",\\n        \"title\": \"CitaLaw: Enhancing LLM with Citations in Legal Domain\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.14556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes CitaLaw, the first benchmark designed to evaluate LLMs\\' ability to produce legally sound responses with appropriate citations, and introduces syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290124325\",\\n            \"name\": \"Kepu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2118684861\",\\n            \"name\": \"Weijie Yu\"\\n          },\\n          {\\n            \"authorId\": \"2155892801\",\\n            \"name\": \"Sunhao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2274965731\",\\n            \"name\": \"Jun Xu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs\\' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.\"\\n      },\\n      {\\n        \"paperId\": \"0d5ccf0861b62223fe562e13369ab15746188251\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0d5ccf0861b62223fe562e13369ab15746188251\",\\n        \"title\": \"QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.05345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums, and applies QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2300845194\",\\n            \"name\": \"Varun Nagaraj Rao\"\\n          },\\n          {\\n            \"authorId\": \"2300370478\",\\n            \"name\": \"Eesha Agarwal\"\\n          },\\n          {\\n            \"authorId\": \"2300371225\",\\n            \"name\": \"Samantha Dalal\"\\n          },\\n          {\\n            \"authorId\": \"2265042713\",\\n            \"name\": \"Dan Calacci\"\\n          },\\n          {\\n            \"authorId\": \"2266397659\",\\n            \"name\": \"Andr\\'es Monroy-Hern\\'andez\"\\n          }\\n        ],\\n        \"abstract\": \"Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methodologies used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting and human evaluation methodology. We applied this framework to analyze over one million comments from two of Reddit\\'s rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.\"\\n      },\\n      {\\n        \"paperId\": \"e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\\n        \"title\": \"Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.13082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TREACLE is proposed, a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\\'s monetary cost and latency constraints, and provides the user with the ability to gracefully trade off accuracy for cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2157197701\",\\n            \"name\": \"Xuechen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2297821531\",\\n            \"name\": \"Zijian Huang\"\\n          },\\n          {\\n            \"authorId\": \"2297769735\",\\n            \"name\": \"Ege Onur Taga\"\\n          },\\n          {\\n            \"authorId\": \"1393650147\",\\n            \"name\": \"Carlee Joe-Wong\"\\n          },\\n          {\\n            \"authorId\": \"3103394\",\\n            \"name\": \"Samet Oymak\"\\n          },\\n          {\\n            \"authorId\": \"2281075331\",\\n            \"name\": \"Jiasi Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE ($\\\\\\\\underline{T}$hrifty $\\\\\\\\underline{Rea}$soning via $\\\\\\\\underline{C}$ontext-Aware $\\\\\\\\underline{L}$LM and Prompt S$\\\\\\\\underline{e}$lection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\\'s monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.\"\\n      },\\n      {\\n        \"paperId\": \"c01bbc439164002f2c7326748f7939783e306a94\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c01bbc439164002f2c7326748f7939783e306a94\",\\n        \"title\": \"A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.09972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that the order of presenting reasons and scores significantly influences LLMs\\' scoring, with a different level of rule understanding in the prompt, crucial for improving the accuracy and consistency of LLM-based evaluations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2264464750\",\\n            \"name\": \"Kuanchao Chu\"\\n          },\\n          {\\n            \"authorId\": \"2109381394\",\\n            \"name\": \"Yi-Pei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2301580436\",\\n            \"name\": \"Hideki Nakayama\"\\n          }\\n        ],\\n        \"abstract\": \"This research investigates prompt designs of evaluating generated texts using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for open-ended text evaluation remains challenging due to model sensitivity and subjectivity in evaluation of text generation. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs\\' scoring, with a different level of rule understanding in the prompt. An additional optimization may enhance scoring alignment if sufficient data is available. This insight is crucial for improving the accuracy and consistency of LLM-based evaluations.\"\\n      },\\n      {\\n        \"paperId\": \"a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\\n        \"title\": \"How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment\",\\n        \"citationCount\": 135,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.14049\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Insight into novice learners\\\\u2019 use of AI code generators in a self-paced learning environment is offered, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3136345\",\\n            \"name\": \"Majeed Kazemitabaar\"\\n          },\\n          {\\n            \"authorId\": \"2112801172\",\\n            \"name\": \"Xinying Hou\"\\n          },\\n          {\\n            \"authorId\": \"2063979470\",\\n            \"name\": \"A. Henley\"\\n          },\\n          {\\n            \"authorId\": \"20937525\",\\n            \"name\": \"B. Ericson\"\\n          },\\n          {\\n            \"authorId\": \"2862077\",\\n            \"name\": \"David Weintrop\"\\n          },\\n          {\\n            \"authorId\": \"2666589\",\\n            \"name\": \"Tovi Grossman\"\\n          }\\n        ],\\n        \"abstract\": \"As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners\\\\u2019 utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners\\\\u2019 use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\\n      },\\n      {\\n        \"paperId\": \"221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\\n        \"title\": \"PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes to comprehensively address all subtasks together, and suggests employing a unique prompt-based in-context learning strategy to enhance error correction and error detection performance in medical systems where prediction errors can have grave consequences.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9122885\",\\n            \"name\": \"Kesav Gundabathula\"\\n          },\\n          {\\n            \"authorId\": \"2301202434\",\\n            \"name\": \"Sriram R Kolar\"\\n          }\\n        ],\\n        \"abstract\": \"This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals. This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it. Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information. We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy. We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge. In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='cdaed8ce-2456-42d3-90da-95687861e4d1')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ee237047-156b-46ff-b2d5-4c6075ddc235'), AIMessage(content='{\\n  \"research_question\": \"How can prompts be dynamically adapted to maintain coherence and engagement in extended LLM interactions?\",\\n  \"problem_domain\": \"natural language processing\",\\n  \"methodology_keywords\": [\\n    \"prompt engineering\",\\n    \"contextual analysis\",\\n    \"natural language generation\"\\n  ],\\n  \"key_concepts\": [\\n    \"dynamic prompt adaptation\",\\n    \"thematic consistency\",\\n    \"coherence\",\\n    \"iterative context update\",\\n    \"adaptive prompt generation\",\\n    \"reader engagement\",\\n    \"LLM interaction\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation method\",\\n    \"three-phase approach (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update)\",\\n    \"maintaining coherence and adaptability across LLM interactions\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='d4be8654-b546-4e18-ba54-8feb3a5492be'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation LLM\",\\n      \"rationale\": \"Directly targets the core concept of dynamically adapting prompts for LLMs, a key aspect of the research question.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"coherence in extended LLM\",\\n      \"rationale\": \"Focuses on the problem domain of maintaining coherence, which is central to the research question\\'s goal in long interactions.\",\\n      \"priority_concept\": \"coherence, LLM interaction\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Highlights the methodology of generating prompts in an adaptive manner, aligning with the proposed novel approach.\",\\n      \"priority_concept\": \"adaptive prompt generation, prompt engineering\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Addresses the \\'Iterative Context Update\\' phase of the proposed method, crucial for maintaining state in extended interactions.\",\\n      \"priority_concept\": \"iterative context update, contextual analysis\"\\n    },\\n    {\\n      \"query_string\": \"thematic consistency LLM\",\\n      \"rationale\": \"Searches for work related to maintaining thematic consistency, a specific aspect of coherence in the context of LLM interactions.\",\\n      \"priority_concept\": \"thematic consistency, LLM interaction\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='df3e3d38-4dc6-473a-bd42-348c81dbc69c'), AIMessage(content='{\\n  \"dynamic prompt adaptation LLM\": {\\n    \"total\": 467,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"cf95279b1da9de1aad9e7c651f5048f69af295ed\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf95279b1da9de1aad9e7c651f5048f69af295ed\",\\n        \"title\": \"AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents\",\\n        \"citationCount\": 76,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.13352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2175939276\",\\n            \"name\": \"Edoardo Debenedetti\"\\n          },\\n          {\\n            \"authorId\": \"2299061721\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2138580250\",\\n            \"name\": \"Mislav Balunovi\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2150869869\",\\n            \"name\": \"Luca Beurer-Kellner\"\\n          },\\n          {\\n            \"authorId\": \"2307472727\",\\n            \"name\": \"Marc Fischer\"\\n          },\\n          {\\n            \"authorId\": \"2267733649\",\\n            \"name\": \"Florian Tram\\\\u00e8r\"\\n          }\\n        ],\\n        \"abstract\": \"AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.\"\\n      },\\n      {\\n        \"paperId\": \"3967189742efab8859da542ce3953d4c72957aca\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3967189742efab8859da542ce3953d4c72957aca\",\\n        \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.23904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI), designed to design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275054108\",\\n            \"name\": \"Qinqian Lei\"\\n          },\\n          {\\n            \"authorId\": \"2313081973\",\\n            \"name\": \"Bo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256998291\",\\n            \"name\": \"Robby T. Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\"\\n      },\\n      {\\n        \"paperId\": \"659e0b3303caa860348dee52f41476e3fddc9573\",\\n        \"url\": \"https://www.semanticscholar.org/paper/659e0b3303caa860348dee52f41476e3fddc9573\",\\n        \"title\": \"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method is introduced, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages and allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2237800256\",\\n            \"name\": \"Qichen Fu\"\\n          },\\n          {\\n            \"authorId\": \"2237803694\",\\n            \"name\": \"Minsik Cho\"\\n          },\\n          {\\n            \"authorId\": \"2178316365\",\\n            \"name\": \"Thomas Merth\"\\n          },\\n          {\\n            \"authorId\": \"2256998189\",\\n            \"name\": \"Sachin Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2284683934\",\\n            \"name\": \"Mohammad Rastegari\"\\n          },\\n          {\\n            \"authorId\": \"40465379\",\\n            \"name\": \"Mahyar Najibi\"\\n          }\\n        ],\\n        \"abstract\": \"The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.\"\\n      },\\n      {\\n        \"paperId\": \"00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\\n        \"title\": \"From ideas to ventures: building entrepreneurship knowledge with LLM, prompt engineering, and conversational agents\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10639-024-12775-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10639-024-12775-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Large Language Models like the Generative Pre-trained Transformer 4 (GPT-4), recognized for their exceptional performance on public datasets, are examined in this study for their potential adaptability and interactivity nature, which align well with the dynamic nature of entrepreneurship learning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1424283019\",\\n            \"name\": \"Marsela Thanasi-Bo\\\\u00e7e\"\\n          },\\n          {\\n            \"authorId\": \"2290624379\",\\n            \"name\": \"Julian Hoxha\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"b67a6181fad5c5838945583ccdc7f39187e29332\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b67a6181fad5c5838945583ccdc7f39187e29332\",\\n        \"title\": \"Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.20911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Mantis is a defensive framework that exploits LLMs\\' susceptibility to adversarial inputs to undermine malicious operations, leading the attacker\\'s LLM to disrupt their own operations or even compromise the attacker\\'s machine.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50610174\",\\n            \"name\": \"Dario Pasquini\"\\n          },\\n          {\\n            \"authorId\": \"2762279\",\\n            \"name\": \"Evgenios M. Kornaropoulos\"\\n          },\\n          {\\n            \"authorId\": \"1700850\",\\n            \"name\": \"G. Ateniese\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs\\' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker\\'s LLM to disrupt their own operations (passive defense) or even compromise the attacker\\'s machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker\\'s LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project_mantis\"\\n      },\\n      {\\n        \"paperId\": \"2f274db9aa447a13c019114e327057d4b161b6d5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f274db9aa447a13c019114e327057d4b161b6d5\",\\n        \"title\": \"LLM-controller: Dynamic robot control adaptation using large language models\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.robot.2024.104913?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.robot.2024.104913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"90182090\",\\n            \"name\": \"Rasoul Zahedifar\"\\n          },\\n          {\\n            \"authorId\": \"1799503\",\\n            \"name\": \"M. Baghshah\"\\n          },\\n          {\\n            \"authorId\": \"2273939584\",\\n            \"name\": \"Alireza Taheri\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\\n        \"url\": \"https://www.semanticscholar.org/paper/47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\\n        \"title\": \"HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.19878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE is proposed, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-09-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2211429378\",\\n            \"name\": \"Bingshen Mu\"\\n          },\\n          {\\n            \"authorId\": \"2299944267\",\\n            \"name\": \"Kun Wei\"\\n          },\\n          {\\n            \"authorId\": \"2061559378\",\\n            \"name\": \"Qijie Shao\"\\n          },\\n          {\\n            \"authorId\": \"2323714781\",\\n            \"name\": \"Yong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2249732546\",\\n            \"name\": \"Lei Xie\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.\"\\n      },\\n      {\\n        \"paperId\": \"bcac614f9774488447221ebb4f16f05e3975ec1e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bcac614f9774488447221ebb4f16f05e3975ec1e\",\\n        \"title\": \"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\",\\n        \"citationCount\": 75,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.04669\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A well-designed visual tokenizer is introduced to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read, which empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2239056878\",\\n            \"name\": \"Yang Jin\"\\n          },\\n          {\\n            \"authorId\": \"2266735757\",\\n            \"name\": \"Kun Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266735757\",\\n            \"name\": \"Kun Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266419021\",\\n            \"name\": \"Liwei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2239059653\",\\n            \"name\": \"Chao Liao\"\\n          },\\n          {\\n            \"authorId\": \"2239091862\",\\n            \"name\": \"Jianchao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2007771781\",\\n            \"name\": \"Quzhe Huang\"\\n          },\\n          {\\n            \"authorId\": \"2230906921\",\\n            \"name\": \"Bin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2366079231\",\\n            \"name\": \"Chenyi Lei\"\\n          },\\n          {\\n            \"authorId\": \"2239069665\",\\n            \"name\": \"An Liu\"\\n          },\\n          {\\n            \"authorId\": \"2241686105\",\\n            \"name\": \"Chengru Song\"\\n          },\\n          {\\n            \"authorId\": \"2238955477\",\\n            \"name\": \"Xiaoqiang Lei\"\\n          },\\n          {\\n            \"authorId\": \"2228125963\",\\n            \"name\": \"Di Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2238953778\",\\n            \"name\": \"Wenwu Ou\"\\n          },\\n          {\\n            \"authorId\": \"2238953242\",\\n            \"name\": \"Kun Gai\"\\n          },\\n          {\\n            \"authorId\": \"2238953689\",\\n            \"name\": \"Yadong Mu\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model\\'s potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.\"\\n      },\\n      {\\n        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"58700f3740105e3422eb030305372b6d8bc44986\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58700f3740105e3422eb030305372b6d8bc44986\",\\n        \"title\": \"Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours, and can serve as an orthogonal optimization for synergistic integration with existing speculative decoding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2242179319\",\\n            \"name\": \"H. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2303652170\",\\n            \"name\": \"Wayne Luk\"\\n          },\\n          {\\n            \"authorId\": \"2301711440\",\\n            \"name\": \"Ka-Fai Cedric Yiu\"\\n          },\\n          {\\n            \"authorId\": \"2152153064\",\\n            \"name\": \"Rui Li\"\\n          },\\n          {\\n            \"authorId\": \"2303652428\",\\n            \"name\": \"Konstantin Mishchenko\"\\n          },\\n          {\\n            \"authorId\": \"2115955596\",\\n            \"name\": \"Stylianos I. Venieris\"\\n          },\\n          {\\n            \"authorId\": \"10001427\",\\n            \"name\": \"Hongxiang Fan\"\\n          }\\n        ],\\n        \"abstract\": \"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\\\\\\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\\\\\\\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.\"\\n      },\\n      {\\n        \"paperId\": \"427c3a502d132b6e1cea2d5565460d284db6e3f7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/427c3a502d132b6e1cea2d5565460d284db6e3f7\",\\n        \"title\": \"Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2507.00601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies and introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-06-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2345186311\",\\n            \"name\": \"Shuangquan Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2353085449\",\\n            \"name\": \"Yingnan Deng\"\\n          },\\n          {\\n            \"authorId\": \"2372425942\",\\n            \"name\": \"Guiran Liu\"\\n          },\\n          {\\n            \"authorId\": \"2374351029\",\\n            \"name\": \"Zhen Qi\"\\n          },\\n          {\\n            \"authorId\": \"2372322790\",\\n            \"name\": \"Ruotong Wang\"\\n          }\\n        ],\\n        \"abstract\": \"This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model\\'s original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method\\'s applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.\"\\n      },\\n      {\\n        \"paperId\": \"c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"title\": \"Dynamic and Adaptive Feature Generation with LLM\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research introduces a novel approach adopting large language models and feature-generating prompts to address challenges of explainability, applicability, and inflexible strategy and proposes a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130031226\",\\n            \"name\": \"XinHao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108045855\",\\n            \"name\": \"Jinghan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1966492\",\\n            \"name\": \"Banafsheh Rekabdar\"\\n          },\\n          {\\n            \"authorId\": \"2145108199\",\\n            \"name\": \"Yuanchun Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2301248160\",\\n            \"name\": \"Pengfei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293571072\",\\n            \"name\": \"Kunpeng Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\\n        \"title\": \"Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.04295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process and demonstrates measurable performance improvements compared to content-only optimization methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2344555074\",\\n            \"name\": \"Yuanye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257094139\",\\n            \"name\": \"Jiahang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2274195530\",\\n            \"name\": \"L. Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2344193091\",\\n            \"name\": \"Qi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2341721557\",\\n            \"name\": \"Xuan Feng\"\\n          },\\n          {\\n            \"authorId\": \"2344520491\",\\n            \"name\": \"Yang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2339241318\",\\n            \"name\": \"Zhongxin Guo\"\\n          },\\n          {\\n            \"authorId\": \"2344097630\",\\n            \"name\": \"Yuqing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2296029993\",\\n            \"name\": \"Peng Cheng\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 130,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"e3e3aa5576de899b755100db211501bb405aba3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3e3aa5576de899b755100db211501bb405aba3e\",\\n        \"title\": \"Firewalls to Secure Dynamic LLM Agentic Networks\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.01822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work identifies required properties for agent communication: proactivity, adaptability, privacy, privacy, and security, and proposes a practical design and protocol inspired by network security principles.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2343752567\",\\n            \"name\": \"Sahar Abdelnabi\"\\n          },\\n          {\\n            \"authorId\": \"2249532110\",\\n            \"name\": \"Amr Gomaa\"\\n          },\\n          {\\n            \"authorId\": \"36103467\",\\n            \"name\": \"Eugene Bagdasarian\"\\n          },\\n          {\\n            \"authorId\": \"2237674591\",\\n            \"name\": \"P. O. Kristensson\"\\n          },\\n          {\\n            \"authorId\": \"2346834097\",\\n            \"name\": \"Reza Shokri\"\\n          }\\n        ],\\n        \"abstract\": \"LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.\"\\n      },\\n      {\\n        \"paperId\": \"ee552989a03693a441863af4c29dc594bfcd1ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ee552989a03693a441863af4c29dc594bfcd1ab5\",\\n        \"title\": \"AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2503.18891?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2503.18891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316787389\",\\n            \"name\": \"Zhexuan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2306069252\",\\n            \"name\": \"Yutong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256344322\",\\n            \"name\": \"Xuebo Liu\"\\n          },\\n          {\\n            \"authorId\": \"46573238\",\\n            \"name\": \"Liang Ding\"\\n          },\\n          {\\n            \"authorId\": \"2187384924\",\\n            \"name\": \"Miao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2348727938\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2346352158\",\\n            \"name\": \"Min Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents\\\\u2019 communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout , which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yufeng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"273b2c64d675edd522cd6f679891756ad5207296\",\\n        \"url\": \"https://www.semanticscholar.org/paper/273b2c64d675edd522cd6f679891756ad5207296\",\\n        \"title\": \"Exploring the Effectiveness of LLM Domain Adaptation for Business IT Machine Translation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://aclanthology.org/2024.eamt-1.51, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that while LLMs can translate on-par with SAP\\\\u2019s MT models on general domain data, it is difficult to close the gap on SAP\\\\u2019s domain-specific data, even with extensive training and carefully curated data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2322393313\",\\n            \"name\": \"Johannes Eschbach-Dymanus\"\\n          },\\n          {\\n            \"authorId\": \"2322400027\",\\n            \"name\": \"Frank Essenberger\"\\n          },\\n          {\\n            \"authorId\": \"1403814959\",\\n            \"name\": \"Bianka Buschbeck-Wolf\"\\n          },\\n          {\\n            \"authorId\": \"70124681\",\\n            \"name\": \"Miriam Exel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\\n        \"title\": \"ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities\",\\n        \"citationCount\": 81,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283831590\",\\n            \"name\": \"Jiarui Lu\"\\n          },\\n          {\\n            \"authorId\": \"2315811087\",\\n            \"name\": \"Thomas Holleis\"\\n          },\\n          {\\n            \"authorId\": \"2313695880\",\\n            \"name\": \"Yizhe Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2315810003\",\\n            \"name\": \"Bernhard Aumayer\"\\n          },\\n          {\\n            \"authorId\": \"2313640225\",\\n            \"name\": \"Feng Nan\"\\n          },\\n          {\\n            \"authorId\": \"2313910532\",\\n            \"name\": \"Felix Bai\"\\n          },\\n          {\\n            \"authorId\": \"2313694040\",\\n            \"name\": \"Shuang Ma\"\\n          },\\n          {\\n            \"authorId\": \"2313694042\",\\n            \"name\": \"Shen Ma\"\\n          },\\n          {\\n            \"authorId\": \"2315946702\",\\n            \"name\": \"Mengyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2293171017\",\\n            \"name\": \"Guoli Yin\"\\n          },\\n          {\\n            \"authorId\": \"2313671930\",\\n            \"name\": \"Zirui Wang\"\\n          },\\n          {\\n            \"authorId\": \"2238621132\",\\n            \"name\": \"Ruoming Pang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at https://github.com/apple/ToolSandbox\"\\n      },\\n      {\\n        \"paperId\": \"44b0d2e884efa5344e50424dbe2edf616981f201\",\\n        \"url\": \"https://www.semanticscholar.org/paper/44b0d2e884efa5344e50424dbe2edf616981f201\",\\n        \"title\": \"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.00807\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.00807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work develops and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply and achieves substantially lower latency than standard reranking methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2127522115\",\\n            \"name\": \"Jon Saad-Falcon\"\\n          },\\n          {\\n            \"authorId\": \"144112155\",\\n            \"name\": \"O. Khattab\"\\n          },\\n          {\\n            \"authorId\": \"50818255\",\\n            \"name\": \"Keshav Santhanam\"\\n          },\\n          {\\n            \"authorId\": \"1707117\",\\n            \"name\": \"Radu Florian\"\\n          },\\n          {\\n            \"authorId\": \"39038065\",\\n            \"name\": \"M. Franz\"\\n          },\\n          {\\n            \"authorId\": \"1781292\",\\n            \"name\": \"S. Roukos\"\\n          },\\n          {\\n            \"authorId\": \"2707234\",\\n            \"name\": \"Avirup Sil\"\\n          },\\n          {\\n            \"authorId\": \"2937809\",\\n            \"name\": \"Md Arafat Sultan\"\\n          },\\n          {\\n            \"authorId\": \"144922861\",\\n            \"name\": \"Christopher Potts\"\\n          }\\n        ],\\n        \"abstract\": \"Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.\"\\n      },\\n      {\\n        \"paperId\": \"9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\\n        \"title\": \"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\",\\n        \"citationCount\": 212,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.02490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing, is introduced, which effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2181120463\",\\n            \"name\": \"Huiqiang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"1527099159\",\\n            \"name\": \"Yucheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2284970741\",\\n            \"name\": \"Chengruidong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108728536\",\\n            \"name\": \"Qianhui Wu\"\\n          },\\n          {\\n            \"authorId\": \"13289447\",\\n            \"name\": \"Xufang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2309738728\",\\n            \"name\": \"Surin Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2281867465\",\\n            \"name\": \"Zhenhua Han\"\\n          },\\n          {\\n            \"authorId\": \"2309244780\",\\n            \"name\": \"Amir H. Abdi\"\\n          },\\n          {\\n            \"authorId\": \"2305587638\",\\n            \"name\": \"Dongsheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2257359863\",\\n            \"name\": \"Chin-Yew Lin\"\\n          },\\n          {\\n            \"authorId\": \"2125051198\",\\n            \"name\": \"Yuqing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2160727304\",\\n            \"name\": \"Lili Qiu\"\\n          }\\n        ],\\n        \"abstract\": \"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\"\\n      },\\n      {\\n        \"paperId\": \"34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\\n        \"title\": \"GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting\",\\n        \"citationCount\": 91,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/30383/32447\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i21.30383?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i21.30383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously simultaneously, named GPT4MTS, and proposes a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2293923697\",\\n            \"name\": \"Kevin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2260272787\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.\"\\n      },\\n      {\\n        \"paperId\": \"c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\\n        \"title\": \"Do LLMs Understand Visual Anomalies? Uncovering LLM\\'s Capabilities in Zero-shot Anomaly Detection\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.09654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM), enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108678212\",\\n            \"name\": \"Jiaqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"51459472\",\\n            \"name\": \"Shaofeng Cai\"\\n          },\\n          {\\n            \"authorId\": \"2276607267\",\\n            \"name\": \"Fang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2188240935\",\\n            \"name\": \"Bengchin Ooi\"\\n          },\\n          {\\n            \"authorId\": \"2296743990\",\\n            \"name\": \"Junran Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA\\'s effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.\"\\n      },\\n      {\\n        \"paperId\": \"8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"title\": \"FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.15813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2247164321\",\\n            \"name\": \"Yu Lu\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"3446334\",\\n            \"name\": \"Hehe Fan\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n      },\\n      {\\n        \"paperId\": \"162f33c7799683ca9b0f193275fe7eec5a0b973f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/162f33c7799683ca9b0f193275fe7eec5a0b973f\",\\n        \"title\": \"EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.09618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EasyRef is introduced, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt and surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1571400317\",\\n            \"name\": \"Zhuofan Zong\"\\n          },\\n          {\\n            \"authorId\": \"2293242031\",\\n            \"name\": \"Dongzhi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2261489892\",\\n            \"name\": \"Bingqi Ma\"\\n          },\\n          {\\n            \"authorId\": \"12920342\",\\n            \"name\": \"Guanglu Song\"\\n          },\\n          {\\n            \"authorId\": \"2075457131\",\\n            \"name\": \"Hao Shao\"\\n          },\\n          {\\n            \"authorId\": \"2292263397\",\\n            \"name\": \"Dazhong Shen\"\\n          },\\n          {\\n            \"authorId\": \"2292207974\",\\n            \"name\": \"Yu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2261394248\",\\n            \"name\": \"Hongsheng Li\"\\n          }\\n        ],\\n        \"abstract\": \"Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although the tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent elements within multiple images through the training process, it necessitates specific finetuning for each distinct image group. This paper introduces EasyRef, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM\\'s representations into the diffusion process through adapters can easily generalize to unseen domains, mining the consistent visual elements within unseen data. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\\n      },\\n      {\\n        \"paperId\": \"ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"title\": \"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.10462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation, is proposed and experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2241468593\",\\n            \"name\": \"Hossein Rajabzadeh\"\\n          },\\n          {\\n            \"authorId\": \"9200111\",\\n            \"name\": \"Mojtaba Valipour\"\\n          },\\n          {\\n            \"authorId\": \"2284643707\",\\n            \"name\": \"Tianshu Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1996315\",\\n            \"name\": \"Marzieh S. Tahaei\"\\n          },\\n          {\\n            \"authorId\": \"2241480742\",\\n            \"name\": \"Hyock Ju Kwon\"\\n          },\\n          {\\n            \"authorId\": \"2237425782\",\\n            \"name\": \"Ali Ghodsi\"\\n          },\\n          {\\n            \"authorId\": \"2237517964\",\\n            \"name\": \"Boxing Chen\"\\n          },\\n          {\\n            \"authorId\": \"2066076226\",\\n            \"name\": \"Mehdi Rezagholizadeh\"\\n          }\\n        ],\\n        \"abstract\": \"Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n      },\\n      {\\n        \"paperId\": \"9a73effed8775962c86587feb0f9ef841fa2ff4c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a73effed8775962c86587feb0f9ef841fa2ff4c\",\\n        \"title\": \"LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258550477\",\\n            \"name\": \"Yash Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2258751660\",\\n            \"name\": \"Wenchang Gao\"\\n          },\\n          {\\n            \"authorId\": \"3379438\",\\n            \"name\": \"Vasanth Sarathy\"\\n          },\\n          {\\n            \"authorId\": \"2258715054\",\\n            \"name\": \"Alvaro Velasquez\"\\n          },\\n          {\\n            \"authorId\": \"2258551993\",\\n            \"name\": \"Robert Wright\"\\n          },\\n          {\\n            \"authorId\": \"1715858\",\\n            \"name\": \"Jivko Sinapov\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a propreitary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the LLM proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.\"\\n      },\\n      {\\n        \"paperId\": \"108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\\n        \"url\": \"https://www.semanticscholar.org/paper/108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\\n        \"title\": \"TimeRAG: Boosting LLM Time Series Forecasting via Retrieval-Augmented Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.16643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TimeRAG is proposed, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2206558646\",\\n            \"name\": \"Si-Nan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2337389407\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2336916957\",\\n            \"name\": \"Haoqi Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2336884221\",\\n            \"name\": \"Ruochun Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.\"\\n      },\\n      {\\n        \"paperId\": \"1e6325865e809670765bea9dadd3c40b2014eb6d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e6325865e809670765bea9dadd3c40b2014eb6d\",\\n        \"title\": \"VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The performance contribution of different prompt components is explored, the vision for future improvement in visual accessibility, and the way for LLMs in video anomaly detection and vision-language understanding is paved.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256768778\",\\n            \"name\": \"Hao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292146470\",\\n            \"name\": \"Jiayou Qin\"\\n          },\\n          {\\n            \"authorId\": \"2260725391\",\\n            \"name\": \"Ashish Bastola\"\\n          },\\n          {\\n            \"authorId\": \"2024833342\",\\n            \"name\": \"Xiwen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2292183597\",\\n            \"name\": \"John Suchanek\"\\n          },\\n          {\\n            \"authorId\": \"2292143727\",\\n            \"name\": \"Zihao Gong\"\\n          },\\n          {\\n            \"authorId\": \"2064311884\",\\n            \"name\": \"Abolfazl Razi\"\\n          }\\n        ],\\n        \"abstract\": \"This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.\"\\n      },\\n      {\\n        \"paperId\": \"7f96bb27a8fca35b1f7d02ee319a64be04114809\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7f96bb27a8fca35b1f7d02ee319a64be04114809\",\\n        \"title\": \"LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.08337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties, and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265927745\",\\n            \"name\": \"Maonan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2219695355\",\\n            \"name\": \"Aoyu Pang\"\\n          },\\n          {\\n            \"authorId\": \"7592365\",\\n            \"name\": \"Yuheng Kan\"\\n          },\\n          {\\n            \"authorId\": \"144305489\",\\n            \"name\": \"Man-On Pun\"\\n          },\\n          {\\n            \"authorId\": \"2292117616\",\\n            \"name\": \"Chung Shue Chen\"\\n          },\\n          {\\n            \"authorId\": \"2291077490\",\\n            \"name\": \"Bo Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system\\'s adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\\\\\\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.\"\\n      },\\n      {\\n        \"paperId\": \"650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"title\": \"Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.acl-srw.90?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.acl-srw.90, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Time-LlaMA framework achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2346152667\",\\n            \"name\": \"Juyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334602368\",\\n            \"name\": \"Jiechao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2374971695\",\\n            \"name\": \"Wenwen Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2334718086\",\\n            \"name\": \"Wei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2321408906\",\\n            \"name\": \"Hui Yi Leong\"\\n          }\\n        ],\\n        \"abstract\": \"Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model\\\\u2019s predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 212,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\\n        \"title\": \"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair\",\\n        \"citationCount\": 188,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2176865575\",\\n            \"name\": \"Islem Bouzenia\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"2260683361\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": \"Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent\\'s effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI\\'s GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\\n      },\\n      {\\n        \"paperId\": \"3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\\n        \"title\": \"Token-Budget-Aware LLM Reasoning\",\\n        \"citationCount\": 114,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem is proposed, offering a practical solution to balance efficiency and accuracy in LLM reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170360833\",\\n            \"name\": \"Tingxu Han\"\\n          },\\n          {\\n            \"authorId\": \"2154723145\",\\n            \"name\": \"Zhenting Wang\"\\n          },\\n          {\\n            \"authorId\": \"2239197945\",\\n            \"name\": \"Chunrong Fang\"\\n          },\\n          {\\n            \"authorId\": \"2110773055\",\\n            \"name\": \"Shiyun Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2333472479\",\\n            \"name\": \"Shiqing Ma\"\\n          },\\n          {\\n            \"authorId\": \"2238950128\",\\n            \"name\": \"Zhenyu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE\"\\n      },\\n      {\\n        \"paperId\": \"20843eaa59db5e2af416d7db47d51d0aab3de230\",\\n        \"url\": \"https://www.semanticscholar.org/paper/20843eaa59db5e2af416d7db47d51d0aab3de230\",\\n        \"title\": \"Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue Resolution\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.11709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper analyzes 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution and identifies key textual and code-related heuristics\\\\u2014Specificity, Contextual Richness, and Clarity\\\\u2014that are associated with successful issue closure and help assess prompt quality.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2147154362\",\\n            \"name\": \"Ramtin Ehsani\"\\n          },\\n          {\\n            \"authorId\": \"2341336471\",\\n            \"name\": \"Sakshi Pathak\"\\n          },\\n          {\\n            \"authorId\": \"9728244\",\\n            \"name\": \"Preetha Chatterjee\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have become essential in software development, especially for issue resolution. However, despite their widespread use, significant challenges persist in the quality of LLM responses to issue resolution queries. LLM interactions often yield incorrect, incomplete, or ambiguous information, largely due to knowledge gaps in prompt design, which can lead to unproductive exchanges and reduced developer productivity.In this paper, we analyze 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution. We identify four main knowledge gaps in developer prompts: Missing Context, Missing Specifications, Multiple Context, and Unclear Instructions. Assuming that conversations within closed issues contributed to successful resolutions while those in open issues did not, we find that ineffective conversations contain knowledge gaps in $44.6 \\\\\\\\%$ of prompts, compared to only $12.6 \\\\\\\\%$ in effective ones. Additionally, we observe seven distinct conversational styles, with Directive Prompting, Chain of Thought, and Responsive Feedback being the most prevalent. We find that knowledge gaps are present in all styles of conversations, with Missing Context being the most repeated challenge developers face in issue-resolution conversations.Based on our analysis, we identify key textual and code-related heuristics\\\\u2014Specificity, Contextual Richness, and Clarity\\\\u2014that are associated with successful issue closure and help assess prompt quality. These heuristics lay the foundation for an automated tool that can dynamically flag unclear prompts and suggest structured improvements. To test feasibility, we developed a lightweight browser extension prototype for detecting prompt gaps, that can be easily adapted to other tools within developer workflows.\"\\n      },\\n      {\\n        \"paperId\": \"25cee84e3a1541697a7c97443d7526574127c344\",\\n        \"url\": \"https://www.semanticscholar.org/paper/25cee84e3a1541697a7c97443d7526574127c344\",\\n        \"title\": \"Don\\'t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\",\\n        \"citationCount\": 158,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.00367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs, and proposes two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2114887261\",\\n            \"name\": \"Shangbin Feng\"\\n          },\\n          {\\n            \"authorId\": \"2254168375\",\\n            \"name\": \"Weijia Shi\"\\n          },\\n          {\\n            \"authorId\": \"2108853330\",\\n            \"name\": \"Yike Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282214127\",\\n            \"name\": \"Wenxuan Ding\"\\n          },\\n          {\\n            \"authorId\": \"143820870\",\\n            \"name\": \"Vidhisha Balachandran\"\\n          },\\n          {\\n            \"authorId\": \"2249583325\",\\n            \"name\": \"Yulia Tsvetkov\"\\n          }\\n        ],\\n        \"abstract\": \"Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"d5342fce96175f83550cfae471a0a46d16401481\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d5342fce96175f83550cfae471a0a46d16401481\",\\n        \"title\": \"ST-LLM: Large Language Models Are Effective Temporal Learners\",\\n        \"citationCount\": 120,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM, and develops a dynamic masking strategy with tailor-made training objectives to address the overhead and stability issues introduced by uncompressed video tokens within LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"12287885\",\\n            \"name\": \"Ruyang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256784925\",\\n            \"name\": \"Chen Li\"\\n          },\\n          {\\n            \"authorId\": \"2294629231\",\\n            \"name\": \"Haoran Tang\"\\n          },\\n          {\\n            \"authorId\": \"152988335\",\\n            \"name\": \"Yixiao Ge\"\\n          },\\n          {\\n            \"authorId\": \"2265579883\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2294517847\",\\n            \"name\": \"Ge Li\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at https://github.com/TencentARC/ST-LLM.\"\\n      },\\n      {\\n        \"paperId\": \"12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\\n        \"title\": \"Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.17282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with, and performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218970025\",\\n            \"name\": \"Yi-Kai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2274937712\",\\n            \"name\": \"De-Chuan Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2151459740\",\\n            \"name\": \"Han-Jia Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"edfff0e15449f438a13a7341290c008bf6486afc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edfff0e15449f438a13a7341290c008bf6486afc\",\\n        \"title\": \"MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.18035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism, which significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2327293591\",\\n            \"name\": \"Jingfan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2322997112\",\\n            \"name\": \"Yi Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2327694040\",\\n            \"name\": \"Dan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2293390999\",\\n            \"name\": \"Xing Tian\"\\n          },\\n          {\\n            \"authorId\": \"2179528564\",\\n            \"name\": \"Huanran Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2322888603\",\\n            \"name\": \"Wei Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 203,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"ab4800a924508f49d644ced8ba236ec92f54f566\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ab4800a924508f49d644ced8ba236ec92f54f566\",\\n        \"title\": \"Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2505.16552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach and enhances CoLaR through reinforcement learning (RL) that leverages the latent head\\'s non-deterministic nature to explore diverse reasoning paths and exploit more compact ones.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218557953\",\\n            \"name\": \"Wenhui Tan\"\\n          },\\n          {\\n            \"authorId\": \"2362865102\",\\n            \"name\": \"Jiaze Li\"\\n          },\\n          {\\n            \"authorId\": \"2317982861\",\\n            \"name\": \"Jianzhong Ju\"\\n          },\\n          {\\n            \"authorId\": \"2363405807\",\\n            \"name\": \"Zhenbo Luo\"\\n          },\\n          {\\n            \"authorId\": \"2317980688\",\\n            \"name\": \"Jian Luan\"\\n          },\\n          {\\n            \"authorId\": \"2290923147\",\\n            \"name\": \"Ruihua Song\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head\\'s non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance.\"\\n      },\\n      {\\n        \"paperId\": \"268e28f8d5235031dcd7bfae0f857439e27e8564\",\\n        \"url\": \"https://www.semanticscholar.org/paper/268e28f8d5235031dcd7bfae0f857439e27e8564\",\\n        \"title\": \"SteP: Stacked LLM Policies for Web Actions\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.03720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Stacked LLM Policies for Web Actions is proposed, an approach to dynamically compose policies to solve a diverse set of web tasks and is competitive with prior works while using significantly less data.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-10-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2254267756\",\\n            \"name\": \"Paloma Sodhi\"\\n          },\\n          {\\n            \"authorId\": \"1741598\",\\n            \"name\": \"S. Branavan\"\\n          },\\n          {\\n            \"authorId\": \"2066324938\",\\n            \"name\": \"Yoav Artzi\"\\n          },\\n          {\\n            \"authorId\": \"2254260284\",\\n            \"name\": \"Ryan McDonald\"\\n          }\\n        ],\\n        \"abstract\": \"Performing tasks on the web presents fundamental challenges to large language models (LLMs), including combinatorially large open-world tasks and variations across web interfaces. Simply specifying a large prompt to handle all possible behaviors and states is extremely complex, and results in behavior leaks between unrelated behaviors. Decomposition to distinct policies can address this challenge, but requires carefully handing off control between policies. We propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically compose policies to solve a diverse set of web tasks. SteP defines a Markov Decision Process where the state is a stack of policies representing the control state, i.e., the chain of policy calls. Unlike traditional methods that are restricted to static hierarchies, SteP enables dynamic control that adapts to the complexity of the task. We evaluate SteP against multiple baselines and web environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP improves (14.9\\\\\\\\% to 33.5\\\\\\\\%) over SOTA that use GPT-4 policies, while on MiniWob++, SteP is competitive with prior works while using significantly less data. Our code and data are available at https://asappresearch.github.io/webagents-step.\"\\n      },\\n      {\\n        \"paperId\": \"1f9822022f586e375461660db792f23e891c7123\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1f9822022f586e375461660db792f23e891c7123\",\\n        \"title\": \"Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems\",\\n        \"citationCount\": 62,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2307.06187\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.06187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models, grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2658311\",\\n            \"name\": \"N. Nascimento\"\\n          },\\n          {\\n            \"authorId\": \"40761174\",\\n            \"name\": \"Paulo Alencar\"\\n          },\\n          {\\n            \"authorId\": \"2149928782\",\\n            \"name\": \"Donald D. Cowan\"\\n          }\\n        ],\\n        \"abstract\": \"The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs\\' capabilities and indicating further research opportunities to assess LLMs\\' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.\"\\n      },\\n      {\\n        \"paperId\": \"3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\\n        \"title\": \"Bridging the Gulf of Envisioning: Cognitive Challenges in Prompt Based Interactions with LLMs\",\\n        \"citationCount\": 91,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642754\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work theorizes how end-users \\\\u2018envision\\\\u2019 translating their goals into clear intentions and craft prompts to obtain the desired LLM response.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2301051721\",\\n            \"name\": \"Hariharan Subramonyam\"\\n          },\\n          {\\n            \"authorId\": \"2246886383\",\\n            \"name\": \"Roy D. Pea\"\\n          },\\n          {\\n            \"authorId\": \"1825757380\",\\n            \"name\": \"Christopher Pondoc\"\\n          },\\n          {\\n            \"authorId\": \"1820412\",\\n            \"name\": \"Maneesh Agrawala\"\\n          },\\n          {\\n            \"authorId\": \"2289103973\",\\n            \"name\": \"Colleen M. Seifert\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman\\\\u2019s gulfs of execution and evaluation. To address this gap, we theorize how end-users \\\\u2018envision\\\\u2019 translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments on not knowing: (1) what the task should be, (2) how to instruct the LLM to do the task, and (3) what to expect for the LLM\\\\u2019s output in meeting the goal. Finally, we make recommendations to narrow the gulf of envisioning in human-LLM interactions.\"\\n      },\\n      {\\n        \"paperId\": \"28d6411019f448f54834c2a5cff723cd350345b5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/28d6411019f448f54834c2a5cff723cd350345b5\",\\n        \"title\": \"Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC) involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2289252454\",\\n            \"name\": \"Haokun Liu\"\\n          },\\n          {\\n            \"authorId\": \"8247318\",\\n            \"name\": \"Yaonan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2110285301\",\\n            \"name\": \"Kenji Kato\"\\n          },\\n          {\\n            \"authorId\": \"2307380233\",\\n            \"name\": \"Atsushi Tsukahara\"\\n          },\\n          {\\n            \"authorId\": \"2282115338\",\\n            \"name\": \"Izumi Kondo\"\\n          },\\n          {\\n            \"authorId\": \"1752849\",\\n            \"name\": \"T. Aoyama\"\\n          },\\n          {\\n            \"authorId\": \"2237520520\",\\n            \"name\": \"Yasuhisa Hasegawa\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This letter proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.\"\\n      },\\n      {\\n        \"paperId\": \"77a9c310df0d7896d297da90fc4a1131819c341e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/77a9c310df0d7896d297da90fc4a1131819c341e\",\\n        \"title\": \"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding, and proposes LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293615241\",\\n            \"name\": \"Mingxing Peng\"\\n          },\\n          {\\n            \"authorId\": \"2293665950\",\\n            \"name\": \"Xusen Guo\"\\n          },\\n          {\\n            \"authorId\": \"2146413818\",\\n            \"name\": \"Xianda Chen\"\\n          },\\n          {\\n            \"authorId\": \"2241024418\",\\n            \"name\": \"Meixin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2267078966\",\\n            \"name\": \"Kehua Chen\"\\n          },\\n          {\\n            \"authorId\": \"2293775145\",\\n            \"name\": \"Hao Yang\"\\n          },\\n          {\\n            \"authorId\": \"2258778041\",\\n            \"name\": \"Xuesong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2258755636\",\\n            \"name\": \"Yinhai Wang\"\\n          }\\n        ],\\n        \"abstract\": \"To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.\"\\n      }\\n    ]\\n  },\\n  \"coherence in extended LLM\": {\\n    \"total\": 23,\\n    \"offset\": 0,\\n    \"data\": [\\n      {\\n        \"paperId\": \"d0da372b4b6f422e13556ce272595a0c9002fe90\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d0da372b4b6f422e13556ce272595a0c9002fe90\",\\n        \"title\": \"GPT-4 in Education: Evaluating Aptness, Reliability, and Loss of Coherence in Solving Calculus Problems and Grading Submissions\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s40593-024-00403-3.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40593-024-00403-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40593-024-00403-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256969876\",\\n            \"name\": \"Alberto Gandolfi\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we initially investigate the capabilities of GPT-3 5 and GPT-4 in solving college-level calculus problems, an essential segment of mathematics that remains under-explored so far. Although improving upon earlier versions, GPT-4 attains approximately 65% accuracy for standard problems and decreases to 20% for competition-like scenarios. Overall, the models prove to be unreliable due to common arithmetic errors. Our primary contribution lies then in examining the use of ChatGPT for grading solutions to calculus exercises. Our objectives are to probe an in-context learning task with less emphasis over direct calculations; recognize positive applications of ChatGPT in educational contexts; highlight a potentially emerging facet of AI that could necessitate oversight; and introduce unconventional AI benchmarks, for which models like GPT are untrained. Pertaining to the latter, we uncover a tendency for loss of coherence in extended contexts. Our findings suggest that while the current ChatGPT exhibits comprehension of the grading task and often provides relevant outputs, the consistency of grading is marred by occasional loss of coherence and hallucinations. Intriguingly, GPT-4\\'s overall scores, delivered in mere moments, align closely with human graders, although its detailed accuracy remains suboptimal. This work suggests that, when appropriately orchestrated, collaboration between human graders and LLMs like GPT-4 might combine their unique strengths while mitigating their respective shortcomings In this direction, it is imperative to consider implementing transparency, fairness, and appropriate regulations in the near future.\"\\n      },\\n      {\\n        \"paperId\": \"49b7bd275a0386392769f5b33028500754dbc69d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/49b7bd275a0386392769f5b33028500754dbc69d\",\\n        \"title\": \"AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE), and reveals that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2317040111\",\\n            \"name\": \"Abhay Gupta\"\\n          },\\n          {\\n            \"authorId\": \"2317010916\",\\n            \"name\": \"Philip Meng\"\\n          },\\n          {\\n            \"authorId\": \"2317007185\",\\n            \"name\": \"Ece Yurtseven\"\\n          },\\n          {\\n            \"authorId\": \"2241351144\",\\n            \"name\": \"Sean O\\'Brien\"\\n          },\\n          {\\n            \"authorId\": \"2312105716\",\\n            \"name\": \"Kevin Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting biases in natural language understanding (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language processing (NLP) systems. To address dialect-induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE). AAVENUE builds upon and extends existing benchmarks like VALUE, replacing deterministic syntactic and morphological transformations with a more flexible methodology leveraging LLM-based translation with few-shot prompting, improving performance across our evaluation metrics when translating key tasks from the GLUE and SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics including fluency, BARTScore, quality, coherence, and understandability. Additionally, we recruit fluent AAVE speakers to validate our translations for authenticity. Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\\n      },\\n      {\\n        \"paperId\": \"e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\\n        \"title\": \"DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.findings-emnlp.606.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME), which proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2110546424\",\\n            \"name\": \"Weijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261645232\",\\n            \"name\": \"Wenxiang Hu\"\\n          },\\n          {\\n            \"authorId\": \"2261413304\",\\n            \"name\": \"Fanyou Wu\"\\n          },\\n          {\\n            \"authorId\": \"1757518\",\\n            \"name\": \"Srinivasan H. Sengamedu\"\\n          }\\n        ],\\n        \"abstract\": \"In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME\\'s potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\\n      },\\n      {\\n        \"paperId\": \"0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\\n        \"title\": \"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An evaluation framework for mental health chatbots is validated, proving its effectiveness in improving safety and reliability and highlighting the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-08-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2315914561\",\\n            \"name\": \"Jung In Park\"\\n          },\\n          {\\n            \"authorId\": \"2182148069\",\\n            \"name\": \"Mahyar Abbasian\"\\n          },\\n          {\\n            \"authorId\": \"2241201441\",\\n            \"name\": \"Iman Azimi\"\\n          },\\n          {\\n            \"authorId\": \"2315810363\",\\n            \"name\": \"Dawn Bounds\"\\n          },\\n          {\\n            \"authorId\": \"2315810053\",\\n            \"name\": \"Angela Jun\"\\n          },\\n          {\\n            \"authorId\": \"2315889398\",\\n            \"name\": \"Jaesu Han\"\\n          },\\n          {\\n            \"authorId\": \"2315811390\",\\n            \"name\": \"Robert McCarron\"\\n          },\\n          {\\n            \"authorId\": \"2297708916\",\\n            \"name\": \"Jessica Borelli\"\\n          },\\n          {\\n            \"authorId\": \"2348273518\",\\n            \"name\": \"Parmida Safavi\"\\n          },\\n          {\\n            \"authorId\": \"2348305728\",\\n            \"name\": \"Sanaz Mirbaha\"\\n          },\\n          {\\n            \"authorId\": \"2315875066\",\\n            \"name\": \"Jia Li\"\\n          },\\n          {\\n            \"authorId\": \"2315811652\",\\n            \"name\": \"Mona Mahmoudi\"\\n          },\\n          {\\n            \"authorId\": \"2315811354\",\\n            \"name\": \"Carmen Wiedenhoeft\"\\n          },\\n          {\\n            \"authorId\": \"2311169857\",\\n            \"name\": \"Amir M. Rahmani\"\\n          }\\n        ],\\n        \"abstract\": \"Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Materials and Methods: We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. Results: The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Discussion: Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. Conclusion: The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.\"\\n      },\\n      {\\n        \"paperId\": \"09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\\n        \"url\": \"https://www.semanticscholar.org/paper/09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\\n        \"title\": \"Asynchronous LLM Function Calling\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.07017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An in-context protocol for function calls and interrupts is designed, an fine-tuning strategy is provided to adapt LLMs to the interrupt semantics, and mechanisms to asynchronously notify the LLM in-flight when function calls return are implemented.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265756791\",\\n            \"name\": \"In Gim\"\\n          },\\n          {\\n            \"authorId\": \"2118065368\",\\n            \"name\": \"Seung-seob Lee\"\\n          },\\n          {\\n            \"authorId\": \"2323908057\",\\n            \"name\": \"Lin Zhong\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM\\'s operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call\\'s completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.\"\\n      },\\n      {\\n        \"paperId\": \"3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\\n        \"title\": \"Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-024-56309-6.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10925587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies, and demonstrating a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260831947\",\\n            \"name\": \"Juho Jung\"\\n          },\\n          {\\n            \"authorId\": \"2152569226\",\\n            \"name\": \"Jinyoung Han\"\\n          },\\n          {\\n            \"authorId\": \"6777367\",\\n            \"name\": \"J. Han\"\\n          },\\n          {\\n            \"authorId\": \"2088247339\",\\n            \"name\": \"Junseo Ko\"\\n          },\\n          {\\n            \"authorId\": \"1677558107\",\\n            \"name\": \"Jeewoo Yoon\"\\n          },\\n          {\\n            \"authorId\": \"39548326\",\\n            \"name\": \"J. Hwang\"\\n          },\\n          {\\n            \"authorId\": \"2265721342\",\\n            \"name\": \"Ji In Park\"\\n          },\\n          {\\n            \"authorId\": \"2054436718\",\\n            \"name\": \"Gyudeok Hwang\"\\n          },\\n          {\\n            \"authorId\": \"2290973453\",\\n            \"name\": \"Jae Ho Jung\"\\n          },\\n          {\\n            \"authorId\": \"2003794778\",\\n            \"name\": \"Daniel Duck-Jin Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"Neovascular age-related macular degeneration (nAMD) can result in blindness if left untreated, and patients often require repeated anti-vascular endothelial growth factor injections. Although, the treat-and-extend method is becoming popular to reduce vision loss attributed to recurrence, it may pose a risk of overtreatment. This study aimed to develop a deep learning model based on DenseNet201 to predict nAMD recurrence within 3 months after confirming dry-up 1 month following three loading injections in treatment-na\\\\u00efve patients. A dataset of 1076 spectral domain optical coherence tomography (OCT) images from 269 patients diagnosed with nAMD was used. The performance of the model was compared with that of 6 ophthalmologists, using 100 randomly selected samples. The DenseNet201-based model achieved 53.0% accuracy in predicting nAMD recurrence using a single pre-injection image and 60.2% accuracy after viewing all the images immediately after the 1st, 2nd, and 3rd injections. The model outperformed experienced ophthalmologists, with an average accuracy of 52.17% using a single pre-injection image and 53.3% after examining four images before and after three loading injections. In conclusion, the artificial intelligence model demonstrated a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists. These findings suggest that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies.\"\\n      },\\n      {\\n        \"paperId\": \"b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\\n        \"title\": \"Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.03275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a hybrid representation of the reasoning process, where the initial reasoning steps are partially abstracted away using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2325888815\",\\n            \"name\": \"DiJia Su\"\\n          },\\n          {\\n            \"authorId\": \"2255310892\",\\n            \"name\": \"Hanlin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2269737738\",\\n            \"name\": \"Yingchen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          },\\n          {\\n            \"authorId\": \"2285362895\",\\n            \"name\": \"Yuandong Tian\"\\n          },\\n          {\\n            \"authorId\": \"2326106870\",\\n            \"name\": \"Qinqing Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\\n        \"title\": \"13.4 A 48GB 16-High 1280GB/s HBM3E DRAM with All-Around Power TSV and a 6-Phase RDQS Scheme for TSV Area Optimization\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISSCC49657.2024.10454440?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISSCC49657.2024.10454440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287764300\",\\n            \"name\": \"Jinhyung Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287959684\",\\n            \"name\": \"Kyungjun Cho\"\\n          },\\n          {\\n            \"authorId\": \"2109114249\",\\n            \"name\": \"C. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287956252\",\\n            \"name\": \"Yeonho Lee\"\\n          },\\n          {\\n            \"authorId\": \"2287119443\",\\n            \"name\": \"Jae-Hyung Park\"\\n          },\\n          {\\n            \"authorId\": \"2286899300\",\\n            \"name\": \"Su-Hyun Oh\"\\n          },\\n          {\\n            \"authorId\": \"1640393542\",\\n            \"name\": \"Yucheon Ju\"\\n          },\\n          {\\n            \"authorId\": \"3365606\",\\n            \"name\": \"Chunseok Jeong\"\\n          },\\n          {\\n            \"authorId\": \"123947284\",\\n            \"name\": \"H. Cho\"\\n          },\\n          {\\n            \"authorId\": \"2198615241\",\\n            \"name\": \"Jaeseung Lee\"\\n          },\\n          {\\n            \"authorId\": \"3376046\",\\n            \"name\": \"T. Yun\"\\n          },\\n          {\\n            \"authorId\": \"2292215366\",\\n            \"name\": \"Jin Hee Cho\"\\n          },\\n          {\\n            \"authorId\": \"3375969\",\\n            \"name\": \"Sangmuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"1640397693\",\\n            \"name\": \"J. Moon\"\\n          },\\n          {\\n            \"authorId\": \"2110426052\",\\n            \"name\": \"Y. Park\"\\n          },\\n          {\\n            \"authorId\": \"2291083018\",\\n            \"name\": \"Hong-Seok Choi\"\\n          },\\n          {\\n            \"authorId\": \"2159532637\",\\n            \"name\": \"In-Keun Kim\"\\n          },\\n          {\\n            \"authorId\": \"2286904054\",\\n            \"name\": \"Seung Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"2286933095\",\\n            \"name\": \"Sun-Yeol Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291078685\",\\n            \"name\": \"Jaemin Jang\"\\n          },\\n          {\\n            \"authorId\": \"2292140319\",\\n            \"name\": \"Jinwook Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108644317\",\\n            \"name\": \"S. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2286906698\",\\n            \"name\": \"Younghyun Jeon\"\\n          },\\n          {\\n            \"authorId\": \"2292172289\",\\n            \"name\": \"Juhyung Park\"\\n          },\\n          {\\n            \"authorId\": \"2159571158\",\\n            \"name\": \"Tae-Kyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"82375019\",\\n            \"name\": \"D. Ka\"\\n          },\\n          {\\n            \"authorId\": \"2159321700\",\\n            \"name\": \"Sanghoon Oh\"\\n          },\\n          {\\n            \"authorId\": \"2292143044\",\\n            \"name\": \"Jinse Kim\"\\n          },\\n          {\\n            \"authorId\": \"2159244890\",\\n            \"name\": \"Junyeol Jeon\"\\n          },\\n          {\\n            \"authorId\": \"2292173125\",\\n            \"name\": \"Seonhong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291075551\",\\n            \"name\": \"Kyeong Tae Kim\"\\n          },\\n          {\\n            \"authorId\": \"2154855420\",\\n            \"name\": \"Tae-Hwan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291083403\",\\n            \"name\": \"Hyeonjin Yang\"\\n          },\\n          {\\n            \"authorId\": \"2291072681\",\\n            \"name\": \"Dongju Yang\"\\n          },\\n          {\\n            \"authorId\": \"2291084224\",\\n            \"name\": \"Minseop Lee\"\\n          },\\n          {\\n            \"authorId\": \"30684992\",\\n            \"name\": \"Heewoong Song\"\\n          },\\n          {\\n            \"authorId\": \"2291069292\",\\n            \"name\": \"Dongwook Jang\"\\n          },\\n          {\\n            \"authorId\": \"2287120235\",\\n            \"name\": \"Junghyun Shin\"\\n          },\\n          {\\n            \"authorId\": \"2287261607\",\\n            \"name\": \"Hyunsik Kim\"\\n          },\\n          {\\n            \"authorId\": \"2291041862\",\\n            \"name\": \"Changki Baek\"\\n          },\\n          {\\n            \"authorId\": \"2291023817\",\\n            \"name\": \"Hajun Jeong\"\\n          },\\n          {\\n            \"authorId\": \"2291081668\",\\n            \"name\": \"Jongchan Yoon\"\\n          },\\n          {\\n            \"authorId\": \"1641325386\",\\n            \"name\": \"SeungGyeon Lim\"\\n          },\\n          {\\n            \"authorId\": \"2110630984\",\\n            \"name\": \"Kyo Yun Lee\"\\n          },\\n          {\\n            \"authorId\": \"2159499427\",\\n            \"name\": \"Young Jun Koo\"\\n          },\\n          {\\n            \"authorId\": \"2287107051\",\\n            \"name\": \"Myeong-Jae Park\"\\n          },\\n          {\\n            \"authorId\": \"2510417\",\\n            \"name\": \"Joohwan Cho\"\\n          },\\n          {\\n            \"authorId\": \"2291056079\",\\n            \"name\": \"Jonghwan Kim\"\\n          }\\n        ],\\n        \"abstract\": \"With the emergence of large-language models (LLM) and generative AI, which require an enormous amount of model parameters, the required memory bandwidth and capacity for high-end systems is on an unprecedented increase. To meet this need, we present an extended version of the high-bandwidth memory-3 (HBM3 DRAM), HBM3E, which achieves a 1280GB/s bandwidth with a cube density of 48GB. New design schemes and features, such as all-around power-through-silicon via (TSV), a 6-phase read-data-strobe (RDQS) scheme, a byte-mapping swap scheme, and a voltage-drift compensator for write data strobe (WDQS), are implemented to achieve extended bandwidth and capacity with enhanced reliability. The overall architecture and specifications, such as bump map footprint, the number of channel and I/Os, and the operation voltage, are identical to the latest HBM3 [1, 2]; therefore, backward compatibility is provided, avoiding system modification.\"\\n      },\\n      {\\n        \"paperId\": \"76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\\n        \"title\": \"NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper details the design and construction of NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35504092\",\\n            \"name\": \"Cunxiang Wang\"\\n          },\\n          {\\n            \"authorId\": \"30819687\",\\n            \"name\": \"Ruoxi Ning\"\\n          },\\n          {\\n            \"authorId\": \"2292184774\",\\n            \"name\": \"Boqi Pan\"\\n          },\\n          {\\n            \"authorId\": \"2292208424\",\\n            \"name\": \"Tonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"3187768\",\\n            \"name\": \"Qipeng Guo\"\\n          },\\n          {\\n            \"authorId\": \"2292147095\",\\n            \"name\": \"Cheng Deng\"\\n          },\\n          {\\n            \"authorId\": \"1993226927\",\\n            \"name\": \"Guangsheng Bao\"\\n          },\\n          {\\n            \"authorId\": \"2292261580\",\\n            \"name\": \"Qian Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261496744\",\\n            \"name\": \"Yue Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models\\' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.\"\\n      },\\n      {\\n        \"paperId\": \"dda8031682684655744c7001374e6cb88c9503bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dda8031682684655744c7001374e6cb88c9503bd\",\\n        \"title\": \"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.10902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Soda-Eval is introduced, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, and the performance of several open-access instruction-tuned LLMs is studied, finding that dialogue evaluation remains challenging.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2007581062\",\\n            \"name\": \"John Mendon\\\\u00e7a\"\\n          },\\n          {\\n            \"authorId\": \"2268558660\",\\n            \"name\": \"Isabel Trancoso\"\\n          },\\n          {\\n            \"authorId\": \"1784914\",\\n            \"name\": \"A. Lavie\"\\n          }\\n        ],\\n        \"abstract\": \"Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.\"\\n      },\\n      {\\n        \"paperId\": \"2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"title\": \"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\",\\n        \"citationCount\": 100,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.07542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155795167\",\\n            \"name\": \"Chengzu Li\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2339967968\",\\n            \"name\": \"Huanyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2273419590\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2294850817\",\\n            \"name\": \"Li Dong\"\\n          },\\n          {\\n            \"authorId\": \"2339667880\",\\n            \"name\": \"Ivan Vuli\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"78875987dc674fc556873df037cf114f04932e80\",\\n        \"url\": \"https://www.semanticscholar.org/paper/78875987dc674fc556873df037cf114f04932e80\",\\n        \"title\": \"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\",\\n        \"citationCount\": 83,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2401.07764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.07764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A split learning system for LLM agents in 6G networks is proposed, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1454018677\",\\n            \"name\": \"Minrui Xu\"\\n          },\\n          {\\n            \"authorId\": \"1713586\",\\n            \"name\": \"D. Niyato\"\\n          },\\n          {\\n            \"authorId\": \"2261731446\",\\n            \"name\": \"Jiawen Kang\"\\n          },\\n          {\\n            \"authorId\": \"2943819\",\\n            \"name\": \"Zehui Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2237802924\",\\n            \"name\": \"Shiwen Mao\"\\n          },\\n          {\\n            \"authorId\": \"2264568786\",\\n            \"name\": \"Zhu Han\"\\n          },\\n          {\\n            \"authorId\": \"2228302663\",\\n            \"name\": \"Dong In Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269994509\",\\n            \"name\": \"K. B. Letaief\"\\n          }\\n        ],\\n        \"abstract\": \"AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.\"\\n      },\\n      {\\n        \"paperId\": \"807ba70b6dc5ce8104268ef8e579d6ff67051230\",\\n        \"url\": \"https://www.semanticscholar.org/paper/807ba70b6dc5ce8104268ef8e579d6ff67051230\",\\n        \"title\": \"A Survey on Post-training of Large Language Models\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2187039492\",\\n            \"name\": \"Guiyao Tie\"\\n          },\\n          {\\n            \"authorId\": \"2349394246\",\\n            \"name\": \"Zeli Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2347232612\",\\n            \"name\": \"Dingjie Song\"\\n          },\\n          {\\n            \"authorId\": \"2349375946\",\\n            \"name\": \"Fuyang Wei\"\\n          },\\n          {\\n            \"authorId\": \"2349949677\",\\n            \"name\": \"Rong Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2349402229\",\\n            \"name\": \"Yurou Dai\"\\n          },\\n          {\\n            \"authorId\": \"2349357634\",\\n            \"name\": \"Wen Yin\"\\n          },\\n          {\\n            \"authorId\": \"121937496\",\\n            \"name\": \"Zhejian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2349498657\",\\n            \"name\": \"Jiangyue Yan\"\\n          },\\n          {\\n            \"authorId\": \"2342866931\",\\n            \"name\": \"Yao Su\"\\n          },\\n          {\\n            \"authorId\": \"2349400490\",\\n            \"name\": \"Zhenhan Dai\"\\n          },\\n          {\\n            \"authorId\": \"2324567791\",\\n            \"name\": \"Yifeng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2211165440\",\\n            \"name\": \"Yihan Cao\"\\n          },\\n          {\\n            \"authorId\": \"2301109277\",\\n            \"name\": \"Lichao Sun\"\\n          },\\n          {\\n            \"authorId\": \"2221116622\",\\n            \"name\": \"Pan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2254874151\",\\n            \"name\": \"Lifang He\"\\n          },\\n          {\\n            \"authorId\": \"2280102292\",\\n            \"name\": \"Hechang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2349483665\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2284983420\",\\n            \"name\": \"Qingsong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2348862207\",\\n            \"name\": \"Tianming Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249536787\",\\n            \"name\": \"Neil Zhenqiang Gong\"\\n          },\\n          {\\n            \"authorId\": \"2279062891\",\\n            \"name\": \"Jiliang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2265549448\",\\n            \"name\": \"Caiming Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2349551882\",\\n            \"name\": \"Heng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2293777434\",\\n            \"name\": \"Philip S. Yu\"\\n          },\\n          {\\n            \"authorId\": \"2288029761\",\\n            \"name\": \"Jianfeng Gao\"\\n          }\\n        ],\\n        \"abstract\": \"The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT\\'s alignment strategies to DeepSeek-R1\\'s innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.\"\\n      },\\n      {\\n        \"paperId\": \"ecb1002427e476ec76463e0a8b5a453471a1931f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ecb1002427e476ec76463e0a8b5a453471a1931f\",\\n        \"title\": \"The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks are illustrated, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283841922\",\\n            \"name\": \"Ayo Adedeji\"\\n          },\\n          {\\n            \"authorId\": \"2283935118\",\\n            \"name\": \"Sarita Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2283841541\",\\n            \"name\": \"Brendan Doohan\"\\n          }\\n        ],\\n        \"abstract\": \"In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\\n      },\\n      {\\n        \"paperId\": \"a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\\n        \"title\": \"Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.03151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A thorough and up-to-date comparison of reasoning techniques in both textual and multimodal LLMs is offered, highlighting practical methods for post-training optimization and test-time inference.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2066776633\",\\n            \"name\": \"Jing Bi\"\\n          },\\n          {\\n            \"authorId\": \"2153545235\",\\n            \"name\": \"Susan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2322454494\",\\n            \"name\": \"Xiaofei Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2279760245\",\\n            \"name\": \"Pinxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2331365304\",\\n            \"name\": \"Junjia Guo\"\\n          },\\n          {\\n            \"authorId\": \"2119309562\",\\n            \"name\": \"Yunlong Tang\"\\n          },\\n          {\\n            \"authorId\": \"2242154602\",\\n            \"name\": \"Luchuan Song\"\\n          },\\n          {\\n            \"authorId\": \"2161012966\",\\n            \"name\": \"Chao Huang\"\\n          },\\n          {\\n            \"authorId\": \"2350866278\",\\n            \"name\": \"Guangyu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2350999609\",\\n            \"name\": \"Jinxi He\"\\n          },\\n          {\\n            \"authorId\": \"2350428628\",\\n            \"name\": \"Jiarui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2354168235\",\\n            \"name\": \"Shu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2266412651\",\\n            \"name\": \"Daoan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2350433568\",\\n            \"name\": \"Chen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2152129821\",\\n            \"name\": \"L. Wen\"\\n          },\\n          {\\n            \"authorId\": \"2337241442\",\\n            \"name\": \"Zhang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2320811177\",\\n            \"name\": \"Jiebo Luo\"\\n          },\\n          {\\n            \"authorId\": \"2293314762\",\\n            \"name\": \"Chenliang Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.\"\\n      },\\n      {\\n        \"paperId\": \"5550118041a89121e9d7274f83aef420cd9ed487\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5550118041a89121e9d7274f83aef420cd9ed487\",\\n        \"title\": \"Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.11849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311451390\",\\n            \"name\": \"Yinghao Aaron Li\"\\n          },\\n          {\\n            \"authorId\": \"2243118841\",\\n            \"name\": \"Xilin Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2162961620\",\\n            \"name\": \"Jordan Darefsky\"\\n          },\\n          {\\n            \"authorId\": \"2316835793\",\\n            \"name\": \"Ge Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1686269\",\\n            \"name\": \"N. Mesgarani\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end speech-to-speech conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and speech styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the speech, which is then played back to the user. While the response speech is being played, the input speech undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input speech. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and speech-to-speech baselines in terms of both dialogue naturalness and coherence while being more than 50% faster.\"\\n      },\\n      {\\n        \"paperId\": \"afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\\n        \"title\": \"What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This survey summarizes existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual, and proposes a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-08-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2302816545\",\\n            \"name\": \"Dingyi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2317013378\",\\n            \"name\": \"Qin Jin\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories. Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations.\"\\n      },\\n      {\\n        \"paperId\": \"bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\\n        \"title\": \"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.19846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Quest is introduced, a query-centric data synthesis method aggregating semantically relevant yet diverse documents that achieves superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152644192\",\\n            \"name\": \"Chaochen Gao\"\\n          },\\n          {\\n            \"authorId\": \"2155226596\",\\n            \"name\": \"Xing Wu\"\\n          },\\n          {\\n            \"authorId\": \"2176771084\",\\n            \"name\": \"Qingfang Fu\"\\n          },\\n          {\\n            \"authorId\": \"2257376973\",\\n            \"name\": \"Songlin Hu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest\\'s superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\\n      },\\n      {\\n        \"paperId\": \"1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\\n        \"title\": \"When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.13476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AnchorAttention is developed, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training, and reduces training time by over 50% compared to standard full attention mechanisms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2267866973\",\\n            \"name\": \"Haonan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284062049\",\\n            \"name\": \"Qian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2325201427\",\\n            \"name\": \"Chao Du\"\\n          },\\n          {\\n            \"authorId\": \"2291015783\",\\n            \"name\": \"Tongyao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2325980280\",\\n            \"name\": \"Cunxiao Du\"\\n          },\\n          {\\n            \"authorId\": \"2256995496\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"19201674\",\\n            \"name\": \"Tianyu Pang\"\\n          }\\n        ],\\n        \"abstract\": \"Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16\\'s limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\\\\\\\% compared to standard full attention mechanisms, while preserving the original LLM\\'s capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.\"\\n      },\\n      {\\n        \"paperId\": \"38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\\n        \"title\": \"Ultrafast imaging of coherent polariton propagation and interactions\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2143435931\",\\n            \"name\": \"Ding Xu\"\\n          },\\n          {\\n            \"authorId\": \"13643895\",\\n            \"name\": \"Arkajit Mandal\"\\n          },\\n          {\\n            \"authorId\": \"2053177998\",\\n            \"name\": \"James M. Baxter\"\\n          },\\n          {\\n            \"authorId\": \"2004406278\",\\n            \"name\": \"Shangjun Cheng\"\\n          },\\n          {\\n            \"authorId\": \"49805255\",\\n            \"name\": \"I. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1849913502\",\\n            \"name\": \"Haowen (Vicky) Su\"\\n          },\\n          {\\n            \"authorId\": \"2144363449\",\\n            \"name\": \"Song Liu\"\\n          },\\n          {\\n            \"authorId\": \"6834462\",\\n            \"name\": \"D. Reichman\"\\n          },\\n          {\\n            \"authorId\": \"3895968\",\\n            \"name\": \"Milan Delor\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\\n        \"title\": \"Extrapolation-Based Video Retargeting With Backward Warping Using an Image-to-Warping Vector Generation Network\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LSP.2020.2977206?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LSP.2020.2977206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46445100\",\\n            \"name\": \"Sung In Cho\"\\n          },\\n          {\\n            \"authorId\": \"1739537\",\\n            \"name\": \"Suk-ju Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Video retargeting is a technique used to transform a given video to a target aspect ratio. Current methods often cause severe visual distortion due to frequent temporal incoherence during the retargeting. In this study, we propose a new extrapolation-based video retargeting method using an image-to-warping vector generation network to maintain temporal coherence and prevent deformation of an input frame by extending the side area of an input frame. Backward warping-based extrapolation is performed using a displacement vector (DV) that is generated by a proposed convolutional neural network (CNN). The DV is defined as the displacement between the current hole to be filled in the extended area and a pixel in the input frame used to fill the hole. We also propose a technique to efficiently train the CNN including a method for ground-truth DV generation. After the extrapolation, we propose a technique for the maintenance of temporal coherence of the extended region and a distortion suppression scheme (DSC) for minimizing visual artifacts. The simulation results demonstrated that the proposed method improved bidirectional similarity (BDS) up to 3.69, which is a measure of the quality of video retargeting, compared with existing video retargeting methods.\"\\n      },\\n      {\\n        \"paperId\": \"0df45f6ab09cc6ddcaf6829c131c777732a73731\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0df45f6ab09cc6ddcaf6829c131c777732a73731\",\\n        \"title\": \"Temporal Incoherence-Free Video Retargeting Using Foreground Aware Extrapolation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2020.2977171?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2020.2977171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46445100\",\\n            \"name\": \"Sung In Cho\"\\n          },\\n          {\\n            \"authorId\": \"1739537\",\\n            \"name\": \"Suk-ju Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Video retargeting is a method of adjusting the aspect ratio of a given video to the target aspect ratio. However, temporal incoherence of video contents, which can occur frequently by video retargeting, is the most dominant factor that degrades the quality of retargeted videos. Current methods to maintain temporal coherence use the entire frames of the input videos; however, these methods cannot be implemented as on-time systems because of their tremendous computational complexity. As far as we know, there is no existing on-time video retargeting method that can avoid spatial distortion while perfectly maintaining temporal coherence. In this paper, we propose a novel on-time video retargeting method that can perfectly maintain temporal coherence and prevent the spatial distortion by using only two consecutive input frames. In our method, the maximum a posteriori-based foreground aware-block matching is used for the extrapolation that extends the side area of a given video to adjust its aspect ratio to the target. To maintain the temporal coherence of the extended area, the result of block matching for backward warping-based extrapolation of the start frame after the scene change occurs, is reused for the other frames until the next scene change occurs. In addition, we propose a scene scenario-adaptive fallback scheme to prevent severe distortions that can occur with reusing block matching results or extrapolation-based side extension. The simulation results showed that the proposed method greatly improved the bidirectional similarity value, which can measure the quality of video retargeting, by up to 10.26 compared with the existing on-time video retargeting methods.\"\\n      },\\n      {\\n        \"paperId\": \"cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\\n        \"title\": \"Posterior-GRPO: Rewarding Reasoning Processes in Code Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2508.05170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a unified framework that can effectively incorporate the quality of the reasoning process during RL and introduces Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-08-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310403349\",\\n            \"name\": \"Lishui Fan\"\\n          },\\n          {\\n            \"authorId\": \"2375147757\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2125101083\",\\n            \"name\": \"Mouxiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2331676092\",\\n            \"name\": \"Zhongxin Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model\\'s internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.\"\\n      }\\n    ]\\n  },\\n  \"adaptive prompt generation\": {\\n    \"total\": 13544,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"title\": \"Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions, which is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which the authors call flocking.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2171964328\",\\n            \"name\": \"Harry Dong\"\\n          },\\n          {\\n            \"authorId\": \"2282555057\",\\n            \"name\": \"Beidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284063779\",\\n            \"name\": \"Yuejie Chi\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method\\'s simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model\\'s performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\\\\\times$ and 1.25$\\\\\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\"\\n      },\\n      {\\n        \"paperId\": \"bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"title\": \"Prompt Expansion for Adaptive Text-to-Image Generation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A human evaluation study shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2070966889\",\\n            \"name\": \"Siddhartha Datta\"\\n          },\\n          {\\n            \"authorId\": \"2276608298\",\\n            \"name\": \"Alexander Ku\"\\n          },\\n          {\\n            \"authorId\": \"2275054270\",\\n            \"name\": \"Deepak Ramachandran\"\\n          },\\n          {\\n            \"authorId\": \"2276610768\",\\n            \"name\": \"Peter Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\"\\n      },\\n      {\\n        \"paperId\": \"1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"title\": \"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\",\\n        \"citationCount\": 52,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.12761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights and incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13563486\",\\n            \"name\": \"Jaehong Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2164249715\",\\n            \"name\": \"Shoubin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2061083016\",\\n            \"name\": \"Vaidehi Patil\"\\n          },\\n          {\\n            \"authorId\": \"2267311471\",\\n            \"name\": \"Huaxiu Yao\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\"\\n      },\\n      {\\n        \"paperId\": \"a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"title\": \"Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.06401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2273557458\",\\n            \"name\": \"Hao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2273559489\",\\n            \"name\": \"Jun Li\"\\n          },\\n          {\\n            \"authorId\": \"2118764798\",\\n            \"name\": \"Yizhuang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2273589717\",\\n            \"name\": \"Jun Wan\"\\n          },\\n          {\\n            \"authorId\": \"2113457514\",\\n            \"name\": \"Zhen Lei\"\\n          },\\n          {\\n            \"authorId\": \"2274088311\",\\n            \"name\": \"Xiangyu Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\"\\n      },\\n      {\\n        \"paperId\": \"bc9f1025246efeed568650934b6e183415aca279\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bc9f1025246efeed568650934b6e183415aca279\",\\n        \"title\": \"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.03214\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation, outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on the authors\\' main metrics for each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221313264\",\\n            \"name\": \"Jonathan Pei\"\\n          },\\n          {\\n            \"authorId\": \"1410652795\",\\n            \"name\": \"Kevin Yang\"\\n          },\\n          {\\n            \"authorId\": \"38666915\",\\n            \"name\": \"D. Klein\"\\n          }\\n        ],\\n        \"abstract\": \"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\"\\n      },\\n      {\\n        \"paperId\": \"cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"title\": \"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\",\\n        \"citationCount\": 115,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds, and shows that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1443432623\",\\n            \"name\": \"Anselm Paulus\"\\n          },\\n          {\\n            \"authorId\": \"3461866\",\\n            \"name\": \"Arman Zharmagambetov\"\\n          },\\n          {\\n            \"authorId\": \"2298951327\",\\n            \"name\": \"Chuan Guo\"\\n          },\\n          {\\n            \"authorId\": \"2298758184\",\\n            \"name\": \"Brandon Amos\"\\n          },\\n          {\\n            \"authorId\": \"2253746559\",\\n            \"name\": \"Yuandong Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n      },\\n      {\\n        \"paperId\": \"1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"title\": \"WavLLM: Towards Robust and Adaptive Speech Large Language Model\",\\n        \"citationCount\": 104,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"WavLLM is introduced, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277450543\",\\n            \"name\": \"Shujie Hu\"\\n          },\\n          {\\n            \"authorId\": \"2135918679\",\\n            \"name\": \"Long Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2107983441\",\\n            \"name\": \"Shujie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107970655\",\\n            \"name\": \"Sanyuan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2294360053\",\\n            \"name\": \"Hongkun Hao\"\\n          },\\n          {\\n            \"authorId\": \"2258308585\",\\n            \"name\": \"Jing Pan\"\\n          },\\n          {\\n            \"authorId\": \"2274190703\",\\n            \"name\": \"Xunying Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280887661\",\\n            \"name\": \"Jinyu Li\"\\n          },\\n          {\\n            \"authorId\": \"9075412\",\\n            \"name\": \"S. Sivasankaran\"\\n          },\\n          {\\n            \"authorId\": \"2294832157\",\\n            \"name\": \"Linquan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277299355\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker\\'s identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\\\\\url{aka.ms/wavllm}.\"\\n      },\\n      {\\n        \"paperId\": \"5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"title\": \"OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.18866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniAvatar is introduced, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements, and introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293272991\",\\n            \"name\": \"Qijun Gan\"\\n          },\\n          {\\n            \"authorId\": \"2294510159\",\\n            \"name\": \"Ruizi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2314648986\",\\n            \"name\": \"Jianke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2370937157\",\\n            \"name\": \"Shaofei Xue\"\\n          },\\n          {\\n            \"authorId\": \"2370937932\",\\n            \"name\": \"Steven Hoi\"\\n          }\\n        ],\\n        \"abstract\": \"Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"title\": \"AdaPlanner: Adaptive Planning from Feedback with Language Models\",\\n        \"citationCount\": 175,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16653\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118180896\",\\n            \"name\": \"Haotian Sun\"\\n          },\\n          {\\n            \"authorId\": \"8103389\",\\n            \"name\": \"Yuchen Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2865034\",\\n            \"name\": \"Lingkai Kong\"\\n          },\\n          {\\n            \"authorId\": \"2218437288\",\\n            \"name\": \"Bo Dai\"\\n          },\\n          {\\n            \"authorId\": \"145657504\",\\n            \"name\": \"Chao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"title\": \"Adaptive Machine Translation with Large Language Models\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.13294\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.13294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9400076\",\\n            \"name\": \"Yasmin Moslem\"\\n          },\\n          {\\n            \"authorId\": \"1748844\",\\n            \"name\": \"Rejwanul Haque\"\\n          },\\n          {\\n            \"authorId\": \"144315616\",\\n            \"name\": \"Andy Way\"\\n          }\\n        ],\\n        \"abstract\": \"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\\n      },\\n      {\\n        \"paperId\": \"7665642af9e682e012bec045102a4d009421067c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7665642af9e682e012bec045102a4d009421067c\",\\n        \"title\": \"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\",\\n        \"citationCount\": 98,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.17061\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An efficient yet effective framework that generates high-quality 3D humans with fine-grained geometry and realistic appearance, based on the insight that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257708193\",\\n            \"name\": \"Xian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2260342453\",\\n            \"name\": \"Xiaohang Zhan\"\\n          },\\n          {\\n            \"authorId\": \"1397711601\",\\n            \"name\": \"Jiaxiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2260340529\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2247995148\",\\n            \"name\": \"Gang Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2258618427\",\\n            \"name\": \"Dahua Lin\"\\n          },\\n          {\\n            \"authorId\": \"2257370021\",\\n            \"name\": \"Xihui Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249080787\",\\n            \"name\": \"Ziwei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"title\": \"ChatUniTest: a ChatGPT-based automated unit test generation tool\",\\n        \"citationCount\": 87,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.04764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.04764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.04764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework that outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and generates assertions while utilizing mock objects and re\\\\ufb02ection to achieve test objectives.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1387638000\",\\n            \"name\": \"Zhuo-Qi Xie\"\\n          },\\n          {\\n            \"authorId\": \"2300176046\",\\n            \"name\": \"Yinghao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2064478633\",\\n            \"name\": \"Chen Zhi\"\\n          },\\n          {\\n            \"authorId\": \"145590434\",\\n            \"name\": \"Shuiguang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2116398505\",\\n            \"name\": \"Jianwei Yin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"836e3069a83f455f916114e7265e00187e511838\",\\n        \"url\": \"https://www.semanticscholar.org/paper/836e3069a83f455f916114e7265e00187e511838\",\\n        \"title\": \"Locally Differentially Private Document Generation Using Zero Shot Prompting\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.16111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1512255229\",\\n            \"name\": \"Saiteja Utpala\"\\n          },\\n          {\\n            \"authorId\": \"2261493078\",\\n            \"name\": \"Sara Hooker\"\\n          },\\n          {\\n            \"authorId\": \"2261697074\",\\n            \"name\": \"Pin Yu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\\\\\% reduction in author identification F1 score against static attackers and a 26\\\\\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\"\\n      },\\n      {\\n        \"paperId\": \"f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"title\": \"Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"153003087\",\\n            \"name\": \"Xiangteng He\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"143753918\",\\n            \"name\": \"Yuxin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\"\\n      },\\n      {\\n        \"paperId\": \"107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"title\": \"The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.09576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1847858\",\\n            \"name\": \"Subhankar Maity\"\\n          },\\n          {\\n            \"authorId\": \"2144085844\",\\n            \"name\": \"Aniket Deroy\"\\n          }\\n        ],\\n        \"abstract\": \"In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n      },\\n      {\\n        \"paperId\": \"b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"title\": \"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.00330?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.00330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"StyleCrafter is introduced, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image, and designs a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269171464\",\\n            \"name\": \"Gongye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257035878\",\\n            \"name\": \"Menghan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149052351\",\\n            \"name\": \"Haoxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2087273800\",\\n            \"name\": \"Jinbo Xing\"\\n          },\\n          {\\n            \"authorId\": \"2253795356\",\\n            \"name\": \"Xintao Wang\"\\n          },\\n          {\\n            \"authorId\": \"3001727\",\\n            \"name\": \"Yujiu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257019659\",\\n            \"name\": \"Ying Shan\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0f78249a5ff64441cc51e55bbe5b97e28f801240\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f78249a5ff64441cc51e55bbe5b97e28f801240\",\\n        \"title\": \"Prompt Tuning for Generative Multimodal Pretrained Models\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2208.02532\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2208.02532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work implements prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks, with a focus on generative multimodal pretrained models, instead of contrastive ones.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-08-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50841357\",\\n            \"name\": \"Han Yang\"\\n          },\\n          {\\n            \"authorId\": \"35996608\",\\n            \"name\": \"Junyang Lin\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"An Yang\"\\n          },\\n          {\\n            \"authorId\": \"2155302144\",\\n            \"name\": \"Peng Wang\"\\n          },\\n          {\\n            \"authorId\": \"144161025\",\\n            \"name\": \"Chang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"38385080\",\\n            \"name\": \"Hongxia Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at \\\\\\\\url{https://github.com/OFA-Sys/OFA}\"\\n      },\\n      {\\n        \"paperId\": \"192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"title\": \"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2312.01663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.01663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269468811\",\\n            \"name\": \"Runze He\"\\n          },\\n          {\\n            \"authorId\": \"2052151521\",\\n            \"name\": \"Shaofei Huang\"\\n          },\\n          {\\n            \"authorId\": \"2269461105\",\\n            \"name\": \"Xuecheng Nie\"\\n          },\\n          {\\n            \"authorId\": \"151475424\",\\n            \"name\": \"Tianrui Hui\"\\n          },\\n          {\\n            \"authorId\": \"1776665\",\\n            \"name\": \"Luoqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2108984\",\\n            \"name\": \"Jiao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2269685669\",\\n            \"name\": \"Jizhong Han\"\\n          },\\n          {\\n            \"authorId\": \"2269748083\",\\n            \"name\": \"Guanbin Li\"\\n          },\\n          {\\n            \"authorId\": \"2269687302\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\"\\n      },\\n      {\\n        \"paperId\": \"817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\\n        \"url\": \"https://www.semanticscholar.org/paper/817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\\n        \"title\": \"TaskCraft: Automated Generation of Agentic Tasks\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.10055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories, and empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2367198602\",\\n            \"name\": \"Dingfeng Shi\"\\n          },\\n          {\\n            \"authorId\": \"2366609463\",\\n            \"name\": \"Jingyi Cao\"\\n          },\\n          {\\n            \"authorId\": \"2368654631\",\\n            \"name\": \"Qianben Chen\"\\n          },\\n          {\\n            \"authorId\": \"2367090248\",\\n            \"name\": \"Weichen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2366569457\",\\n            \"name\": \"Weizhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2366571583\",\\n            \"name\": \"Hongxuan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2366522296\",\\n            \"name\": \"Fangchen Dong\"\\n          },\\n          {\\n            \"authorId\": \"2366567233\",\\n            \"name\": \"Tianrui Qin\"\\n          },\\n          {\\n            \"authorId\": \"2368705239\",\\n            \"name\": \"King Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2283080546\",\\n            \"name\": \"Minghao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2366695720\",\\n            \"name\": \"Jian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2366560952\",\\n            \"name\": \"Ge Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2182423032\",\\n            \"name\": \"Jiaheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2351712181\",\\n            \"name\": \"Changwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2366603044\",\\n            \"name\": \"Jun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2134457930\",\\n            \"name\": \"Y. Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2284803168\",\\n            \"name\": \"Wangchunshu Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce \\\\\\\\textsc{TaskCraft}, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.\"\\n      },\\n      {\\n        \"paperId\": \"f373c5569b45bf580b7502729a83761a791ee209\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f373c5569b45bf580b7502729a83761a791ee209\",\\n        \"title\": \"MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work is the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing, and Experimental results on four benchmarks suggest that the method outperforms the previous method on each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238430925\",\\n            \"name\": \"Haoyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2238207741\",\\n            \"name\": \"Tianyi Lu\"\\n          },\\n          {\\n            \"authorId\": \"2239092619\",\\n            \"name\": \"Jiaxi Gu\"\\n          },\\n          {\\n            \"authorId\": \"2238449354\",\\n            \"name\": \"Xing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2311476511\",\\n            \"name\": \"Qingping Zheng\"\\n          },\\n          {\\n            \"authorId\": \"3099139\",\\n            \"name\": \"Zuxuan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2258963974\",\\n            \"name\": \"Hang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238451522\",\\n            \"name\": \"Yu-Gang Jiang\"\\n          }\\n        ],\\n        \"abstract\": \"The diffusion model is widely leveraged for either video generation or video editing. As each field has its task-specific problems, it is difficult to merely develop a single diffusion for completing both tasks simultaneously. Video diffusion sorely relying on the text prompt can be adapted to unify the two tasks. However, it lacks a high capability of aligning heterogeneous modalities between text and image, leading to various misalignment problems. In this work, we are the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing. The proposed MagDiff introduces three types of alignments, including subject-driven alignment, adaptive prompts alignment, and high-fidelity alignment. Particularly, the subject-driven alignment is put forward to trade off the image and text prompts, serving as a unified foundation generative model for both tasks. The adaptive prompts alignment is introduced to emphasize different strengths of homogeneous and heterogeneous alignments by assigning different values of weights to the image and the text prompts. The high-fidelity alignment is developed to further enhance the fidelity of both video generation and editing by taking the subject image as an additional model input. Experimental results on four benchmarks suggest that our method outperforms the previous method on each task.\"\\n      },\\n      {\\n        \"paperId\": \"a17aee1261b56ee828f029b1caeef78033acea83\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a17aee1261b56ee828f029b1caeef78033acea83\",\\n        \"title\": \"Adaptive Ship Detection From Optical to SAR Images\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LGRS.2023.3317321?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LGRS.2023.3317321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243328855\",\\n            \"name\": \"Yuxuan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2221128858\",\\n            \"name\": \"Zhijie Rao\"\\n          },\\n          {\\n            \"authorId\": \"2232100687\",\\n            \"name\": \"Chuyang Lin\"\\n          },\\n          {\\n            \"authorId\": \"1950637\",\\n            \"name\": \"Yue Huang\"\\n          },\\n          {\\n            \"authorId\": \"2713947\",\\n            \"name\": \"Xinghao Ding\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in synthetic aperture radar (SAR) ship detection have witnessed remarkable success by using large-scale annotated datasets. However, the annotation of SAR images requires strong domain-specific expertise, significantly hindering the prompt adoption of modern object detectors in this regime. Compared to SAR data, optical data in geoscience are considerably easier to label. Motivated by this, we investigate a new and challenging problem\\\\u2014adaptive ship detection\\\\u2014with the goal of enhancing ship detection performance on SAR images by leveraging knowledge transferred from optical images. Considering the large distributional discrepancy between the source (optical) and target (SAR) domains, we present OmniAdapt, a novel framework that progressively narrows the distance between the two types of images at the pixel, feature, and classifier levels. Specifically, OmniAdapt consists of three main modules, target-like generation module (TLGM), multifeature alignment module (MFAM), and common specific decomposition module (CSDM). TLGM minimizes the visual disparity by infusing the target-domain style into the source domain. MFAM aligns local- and global-level feature representations in an adversarial manner. Finally, CSDM decomposes the classifier into two independent components, that is, the domain-common component and the domain-specific component, and promotes the recognition ability of the former via regularization learning. Experimental results demonstrate the effectiveness of the proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\\n        \"title\": \"Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.05256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate, is introduced, a reinforcement learning objective tailoring generation length to per-prompt solve rate that delivers higher accuracy on the hardest problems with higher cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274104658\",\\n            \"name\": \"Violet Xiang\"\\n          },\\n          {\\n            \"authorId\": \"2326300653\",\\n            \"name\": \"Chase Blagden\"\\n          },\\n          {\\n            \"authorId\": \"102801230\",\\n            \"name\": \"Rafael Rafailov\"\\n          },\\n          {\\n            \"authorId\": \"2283848553\",\\n            \"name\": \"nathan lile\"\\n          },\\n          {\\n            \"authorId\": \"2366009773\",\\n            \"name\": \"Sang Truong\"\\n          },\\n          {\\n            \"authorId\": \"2284774407\",\\n            \"name\": \"Chelsea Finn\"\\n          },\\n          {\\n            \"authorId\": \"2274104149\",\\n            \"name\": \"Nick Haber\"\\n          }\\n        ],\\n        \"abstract\": \"Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt\\'s online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\\\\\\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.\"\\n      },\\n      {\\n        \"paperId\": \"44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\\n        \"title\": \"OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.10825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2190109518\",\\n            \"name\": \"Dianbing Xi\"\\n          },\\n          {\\n            \"authorId\": \"2356794181\",\\n            \"name\": \"Jiepeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2337074199\",\\n            \"name\": \"Yuanzhi Liang\"\\n          },\\n          {\\n            \"authorId\": \"2336910859\",\\n            \"name\": \"Xi Qiu\"\\n          },\\n          {\\n            \"authorId\": \"3131188\",\\n            \"name\": \"Yuchi Huo\"\\n          },\\n          {\\n            \"authorId\": \"2325437281\",\\n            \"name\": \"Rui Wang\"\\n          },\\n          {\\n            \"authorId\": \"2336934367\",\\n            \"name\": \"Chi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2336880377\",\\n            \"name\": \"Xuelong Li\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff , aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. Our framework supports three key capabilities: (1) Text-conditioned video generation, where all modalities are jointly synthesized from a textual prompt; (2) Video understanding, where structural modalities are predicted from rgb inputs in a coherent manner; and (3) X-conditioned video generation, where video synthesis is guided by finegrained inputs such as depth, canny and segmentation. Extensive experiments demonstrate that OmniVDiff achieves state-of-the-art performance in video generation tasks and competitive results in video understanding. Its flexibility and scalability make it well-suited for downstream applications such as video-to-video translation, modality adaptation for visual tasks, and scene reconstruction.\"\\n      },\\n      {\\n        \"paperId\": \"42117d01d498eb9f8c21b788c3565bc6855d620b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/42117d01d498eb9f8c21b788c3565bc6855d620b\",\\n        \"title\": \"Learning to Transfer Prompts for Text Generation\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.01543\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.01543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper improves this technique and proposes a novel prompt-based method (PTG) for text generation in a transferable setting that learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2018027\",\\n            \"name\": \"Junyi Li\"\\n          },\\n          {\\n            \"authorId\": \"1997234792\",\\n            \"name\": \"Tianyi Tang\"\\n          },\\n          {\\n            \"authorId\": \"50204644\",\\n            \"name\": \"J. Nie\"\\n          },\\n          {\\n            \"authorId\": \"153693432\",\\n            \"name\": \"Ji-rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2542603\",\\n            \"name\": \"Wayne Xin Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.\"\\n      },\\n      {\\n        \"paperId\": \"7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\\n        \"title\": \"CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer\",\\n        \"citationCount\": 1175,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions, and develops an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2303231681\",\\n            \"name\": \"Zhuoyi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2238205354\",\\n            \"name\": \"Jiayan Teng\"\\n          },\\n          {\\n            \"authorId\": \"2163967642\",\\n            \"name\": \"Wendi Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2055623340\",\\n            \"name\": \"Ming Ding\"\\n          },\\n          {\\n            \"authorId\": \"2305795673\",\\n            \"name\": \"Shiyu Huang\"\\n          },\\n          {\\n            \"authorId\": \"2214082934\",\\n            \"name\": \"Jiazheng Xu\"\\n          },\\n          {\\n            \"authorId\": \"2315948290\",\\n            \"name\": \"Yuanming Yang\"\\n          },\\n          {\\n            \"authorId\": \"2105844599\",\\n            \"name\": \"Wenyi Hong\"\\n          },\\n          {\\n            \"authorId\": \"2268628279\",\\n            \"name\": \"Xiaohan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2307077651\",\\n            \"name\": \"Guanyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2307075814\",\\n            \"name\": \"Da Yin\"\\n          },\\n          {\\n            \"authorId\": \"2290625851\",\\n            \"name\": \"Xiaotao Gu\"\\n          },\\n          {\\n            \"authorId\": \"2316099643\",\\n            \"name\": \"Yuxuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265518149\",\\n            \"name\": \"Weihan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2306161782\",\\n            \"name\": \"Yean Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2315952736\",\\n            \"name\": \"Ting Liu\"\\n          },\\n          {\\n            \"authorId\": \"2288066971\",\\n            \"name\": \"Bin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2243402027\",\\n            \"name\": \"Yuxiao Dong\"\\n          },\\n          {\\n            \"authorId\": \"2238207092\",\\n            \"name\": \"Jie Tang\"\\n          }\\n        ],\\n        \"abstract\": \"We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.\"\\n      },\\n      {\\n        \"paperId\": \"f37a5c2bf4266d031533d5e029b74b00b48ef038\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f37a5c2bf4266d031533d5e029b74b00b48ef038\",\\n        \"title\": \"Latency-Aware Generative Semantic Communications With Pre-Trained Diffusion Models\",\\n        \"citationCount\": 62,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.17256\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A latency-aware semantic communications framework with pre-trained generative models that designs a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258983485\",\\n            \"name\": \"Li Qiao\"\\n          },\\n          {\\n            \"authorId\": \"3202702\",\\n            \"name\": \"Mahdi Boloursaz Mashhadi\"\\n          },\\n          {\\n            \"authorId\": \"2293693238\",\\n            \"name\": \"Zhen Gao\"\\n          },\\n          {\\n            \"authorId\": \"1690137\",\\n            \"name\": \"C. Foh\"\\n          },\\n          {\\n            \"authorId\": \"2293392561\",\\n            \"name\": \"Pei Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2279548875\",\\n            \"name\": \"Mehdi Bennis\"\\n          }\\n        ],\\n        \"abstract\": \"Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this letter, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.\"\\n      },\\n      {\\n        \"paperId\": \"759b95f7f90addc4c526cd92557e486ab143fbec\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b95f7f90addc4c526cd92557e486ab143fbec\",\\n        \"title\": \"Style Vectors for Steering Generative Large Language Models\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.01618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches, demonstrating the effectiveness of activation engineering using such style vectors.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2098959640\",\\n            \"name\": \"Kai Konen\"\\n          },\\n          {\\n            \"authorId\": \"2282467369\",\\n            \"name\": \"Sophie Jentzsch\"\\n          },\\n          {\\n            \"authorId\": \"2274662002\",\\n            \"name\": \"Diaoul\\\\u00e9 Diallo\"\\n          },\\n          {\\n            \"authorId\": \"2282467364\",\\n            \"name\": \"Peer Schutt\"\\n          },\\n          {\\n            \"authorId\": \"2282467405\",\\n            \"name\": \"Oliver Bensch\"\\n          },\\n          {\\n            \"authorId\": \"51185829\",\\n            \"name\": \"Roxanne El Baff\"\\n          },\\n          {\\n            \"authorId\": \"2282467346\",\\n            \"name\": \"Dominik Opitz\"\\n          },\\n          {\\n            \"authorId\": \"2282467403\",\\n            \"name\": \"Tobias Hecking\"\\n          }\\n        ],\\n        \"abstract\": \"This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.\"\\n      },\\n      {\\n        \"paperId\": \"621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\\n        \"title\": \"EdgeLLM: Fast On-Device LLM Inference With Speculative Decoding\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMC.2024.3513457?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMC.2024.3513457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238890503\",\\n            \"name\": \"Daliang Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238575108\",\\n            \"name\": \"Wangsong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2306091156\",\\n            \"name\": \"Hao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239060901\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"2326531487\",\\n            \"name\": \"Ying Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2238910539\",\\n            \"name\": \"Shiyun Wei\"\\n          },\\n          {\\n            \"authorId\": \"2326083763\",\\n            \"name\": \"Mengwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2237080638\",\\n            \"name\": \"Xuanzhe Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device\\'s memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3\\\\u00d7 faster than existing engines.\"\\n      },\\n      {\\n        \"paperId\": \"411b16add23976ffcdf6422f932453f6ebcca119\",\\n        \"url\": \"https://www.semanticscholar.org/paper/411b16add23976ffcdf6422f932453f6ebcca119\",\\n        \"title\": \"EvoPrompting: Language Models for Code-Level Neural Architecture Search\",\\n        \"citationCount\": 118,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.14838\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.14838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13336152\",\\n            \"name\": \"Angelica Chen\"\\n          },\\n          {\\n            \"authorId\": \"35363891\",\\n            \"name\": \"David Dohan\"\\n          },\\n          {\\n            \"authorId\": \"48165870\",\\n            \"name\": \"David R. So\"\\n          }\\n        ],\\n        \"abstract\": \"Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\\n      },\\n      {\\n        \"paperId\": \"1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\\n        \"title\": \"Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.18950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Adaptive Guided Erasure (AGE) method is proposed, which selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2007884558\",\\n            \"name\": \"Anh-Vu Bui\"\\n          },\\n          {\\n            \"authorId\": \"2299801919\",\\n            \"name\": \"T. V\\\\u0169\"\\n          },\\n          {\\n            \"authorId\": \"67329496\",\\n            \"name\": \"Tung-Long Vuong\"\\n          },\\n          {\\n            \"authorId\": \"2249909946\",\\n            \"name\": \"Trung Le\"\\n          },\\n          {\\n            \"authorId\": \"2292198330\",\\n            \"name\": \"Paul Montague\"\\n          },\\n          {\\n            \"authorId\": \"2059248789\",\\n            \"name\": \"Tamas Abraham\"\\n          },\\n          {\\n            \"authorId\": \"2275034108\",\\n            \"name\": \"Junae Kim\"\\n          },\\n          {\\n            \"authorId\": \"1400659302\",\\n            \"name\": \"Dinh Q. Phung\"\\n          }\\n        ],\\n        \"abstract\": \"Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a neutral concept or just an empty text prompt. In this paper, we demonstrate that this fixed-target strategy is suboptimal, as it fails to account for the impact of erasing one concept on the others. To address this limitation, we model the concept space as a graph and empirically analyze the effects of erasing one concept on the remaining concepts. Our analysis uncovers intriguing geometric properties of the concept space, where the influence of erasing a concept is confined to a local region. Building on this insight, we propose the Adaptive Guided Erasure (AGE) method, which \\\\\\\\emph{dynamically} selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects. Experimental results show that AGE significantly outperforms state-of-the-art erasure methods on preserving unrelated concepts while maintaining effective erasure performance. Our code is published at {https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.\"\\n      },\\n      {\\n        \"paperId\": \"af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\\n        \"title\": \"StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.08503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A cross-modal Adaptive Instance Normalization mechanism is introduced for better integration of style and text features, enhancing alignment and a Style-based Classifier-Free Guidance approach is developed that enables selective control over stylistic elements, reducing irrelevant influences.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2334740500\",\\n            \"name\": \"Mingkun Lei\"\\n          },\\n          {\\n            \"authorId\": \"2334824597\",\\n            \"name\": \"Xue Song\"\\n          },\\n          {\\n            \"authorId\": \"2336265253\",\\n            \"name\": \"Beier Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2334818360\",\\n            \"name\": \"Hao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2334822819\",\\n            \"name\": \"Chi Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.\"\\n      },\\n      {\\n        \"paperId\": \"5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\\n        \"title\": \"TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.16425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information, and designs a Dynamic Map Scaling mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333781842\",\\n            \"name\": \"Linqing Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2105618628\",\\n            \"name\": \"Chen Gao\"\\n          },\\n          {\\n            \"authorId\": \"2264574237\",\\n            \"name\": \"Zihan Ding\"\\n          },\\n          {\\n            \"authorId\": \"2325825544\",\\n            \"name\": \"Yue Liao\"\\n          },\\n          {\\n            \"authorId\": \"2325537018\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM\\'s spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.\"\\n      },\\n      {\\n        \"paperId\": \"4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\\n        \"title\": \"A new rapid deflagration-to-detonation transition in a short smooth tube\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1063/5.0191500?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1063/5.0191500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"92832821\",\\n            \"name\": \"Wandong Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290316859\",\\n            \"name\": \"Caizhi Fan\"\\n          },\\n          {\\n            \"authorId\": \"1734241\",\\n            \"name\": \"R. Deiterding\"\\n          },\\n          {\\n            \"authorId\": \"2290237139\",\\n            \"name\": \"Xiaokang Li\"\\n          },\\n          {\\n            \"authorId\": \"36072040\",\\n            \"name\": \"Jianhan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290198596\",\\n            \"name\": \"Xiong Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Obtaining a rapid deflagration-to-detonation transition (DDT) within a short smooth tube is a challenging task. Here, an unconventional means of flame acceleration propagating upstream in subsonic and supersonic mixtures within a smooth tube was introduced to acquire a speedy DDT. The Navier\\\\u2013Stokes equations with an adaptive mesh refinement technique and a detailed hydrogen\\\\u2013air chemistry reaction mechanism of 11 species and 27 steps were utilized to resolve the entire DDT characteristics. The effect of the initial Mach number on flame acceleration and DDT mechanism was revealed comprehensively. The results demonstrated that a prompt oblique shock wave (SW) occurs when the flame propagates upstream along the boundary walls due to the boundary layer influence. An intense coupling between the SW and the leading flame front is enhanced by increasing the initial Mach number of the mixture. The speedy generation of the oblique SW is formed at the incipient stage, mainly produced by the boundary layer influence and the coalescences of the compression waves. Consequently, the run-up time to detonation is shortened accordingly through a fierce reflected SW due to the intense leading SW after it reflects from the confined wall. Furthermore, three kinds of DDT evolution are revealed from the obtained results: (1) localized ignition in the upper boundary wall after the reflected and transverse shock waves propagate in the upper wall regions; (2) autoignition is formed in the confined wall corner after the reflected SW; and (3) direct detonation transition occurs at the end wall behind a strongly reflected SW in the supersonic case.\"\\n      },\\n      {\\n        \"paperId\": \"815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\\n        \"title\": \"Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2405.14713\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Generative AI capabilities are introduced to assist educators in creating tutor interfaces that meet their needs while adhering to design principles and raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170697820\",\\n            \"name\": \"Tommaso Cal\\\\u00f2\"\\n          },\\n          {\\n            \"authorId\": \"2257237899\",\\n            \"name\": \"Christopher J. MacLellan\"\\n          }\\n        ],\\n        \"abstract\": \"Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\\n      },\\n      {\\n        \"paperId\": \"e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\\n        \"title\": \"Transformer-Based Variable-Rate Image Compression with Region-of-Interest Control\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.10807\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.10807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a transformer-based learned image compression system that is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality while confirming its superiority over the other competing methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2140736758\",\\n            \"name\": \"Chia-Hao Kao\"\\n          },\\n          {\\n            \"authorId\": \"1723619\",\\n            \"name\": \"Ying Weng\"\\n          },\\n          {\\n            \"authorId\": \"2116613919\",\\n            \"name\": \"Yi-Hsin Chen\"\\n          },\\n          {\\n            \"authorId\": \"37811787\",\\n            \"name\": \"Wei-Chen Chiu\"\\n          },\\n          {\\n            \"authorId\": \"123608804\",\\n            \"name\": \"Wenmin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"This paper proposes a transformer-based learned image compression system. It is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality. Inspired by prompt tuning, we introduce prompt generation networks to condition the transformer-based autoencoder of compression. Our prompt generation networks generate content-adaptive tokens according to the input image, an ROI mask, and a rate parameter. The separation of the ROI mask and the rate parameter allows an intuitive way to achieve variable-rate and ROI coding simultaneously. Extensive experiments validate the effectiveness of our proposed method and confirm its superiority over the other competing methods.\"\\n      },\\n      {\\n        \"paperId\": \"c7acf9250926100f531bcf46d63d7da06e73928e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c7acf9250926100f531bcf46d63d7da06e73928e\",\\n        \"title\": \"MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work identifies that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images, and advances the cross-modality relation learning between the prompt and the generated images to better align the prompt and image content.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238903147\",\\n            \"name\": \"Yupeng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"18119920\",\\n            \"name\": \"Daquan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2238887924\",\\n            \"name\": \"Zuo-Liang Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2238889090\",\\n            \"name\": \"Yaxing Wang\"\\n          },\\n          {\\n            \"authorId\": \"3298532\",\\n            \"name\": \"Qibin Hou\"\\n          },\\n          {\\n            \"authorId\": \"33221685\",\\n            \"name\": \"Jiashi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in diffusion models have showcased their impressive capacity to generate visually striking images. However, ensuring a close match between the generated image and the given prompt remains a persistent challenge. In this work, we identify that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images. To better align the prompt and image content, we advance the cross-attention with an adaptive mask, which is conditioned on the attention maps and the prompt embeddings, to dynamically adjust the contribution of each text token to the image features. This mechanism explicitly diminishes the ambiguity in the semantic information embedding of the text encoder, leading to a boost of text-to-image consistency in the synthesized images. Our method, termed MaskDiffusion, is training-free and hot-pluggable for popular pre-trained diffusion models. When applied to the latent diffusion models, our MaskDiffusion can largely enhance their capability to correctly generate objects and their attributes, with negligible computation overhead compared to the original diffusion models. Our project page is https://github.com/HVision-NKU/MaskDiffusion.\"\\n      },\\n      {\\n        \"paperId\": \"66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\\n        \"url\": \"https://www.semanticscholar.org/paper/66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\\n        \"title\": \"TEMPERA: Test-Time Prompting via Reinforcement Learning\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2211.11890\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.11890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work designs a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers, and achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-11-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1993655237\",\\n            \"name\": \"Tianjun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275277634\",\\n            \"name\": \"Xuezhi Wang\"\\n          },\\n          {\\n            \"authorId\": \"65855107\",\\n            \"name\": \"Denny Zhou\"\\n          },\\n          {\\n            \"authorId\": \"50319359\",\\n            \"name\": \"D. Schuurmans\"\\n          },\\n          {\\n            \"authorId\": \"49988044\",\\n            \"name\": \"Joseph E. Gonzalez\"\\n          }\\n        ],\\n        \"abstract\": \"Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.\"\\n      },\\n      {\\n        \"paperId\": \"88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\\n        \"title\": \"QA-CLIMS: Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.09883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2280039282\",\\n            \"name\": \"Songhe Deng\"\\n          },\\n          {\\n            \"authorId\": \"2279830536\",\\n            \"name\": \"Wei Zhuo\"\\n          },\\n          {\\n            \"authorId\": \"2220635949\",\\n            \"name\": \"Jinheng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2265520934\",\\n            \"name\": \"Linlin Shen\"\\n          }\\n        ],\\n        \"abstract\": \"Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model\\'s ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets.\"\\n      },\\n      {\\n        \"paperId\": \"d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\\n        \"title\": \"Immunosenescence, Inflammaging, and Lung Senescence in Asthma in the Elderly\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2218-273X/12/10/1456/pdf?version=1665574999\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9599177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Immunosenescence\\\\u2019s contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly is discussed and an overview of age-related features in the immune system and lung structure is presented.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47522049\",\\n            \"name\": \"T. Soma\"\\n          },\\n          {\\n            \"authorId\": \"46375440\",\\n            \"name\": \"M. Nagata\"\\n          }\\n        ],\\n        \"abstract\": \"Prevalence of asthma in older adults is growing along with increasing global life expectancy. Due to poor clinical consequences such as high mortality, advancement in understanding the pathophysiology of asthma in older patients has been sought to provide prompt treatment for them. Age-related alterations of functions in the immune system and lung parenchyma occur throughout life. Alterations with advancing age are promoted by various stimuli, including pathobionts, fungi, viruses, pollutants, and damage-associated molecular patterns derived from impaired cells, abandoned cell debris, and senescent cells. Age-related changes in the innate and adaptive immune response, termed immunosenescence, includes impairment of phagocytosis and antigen presentation, enhancement of proinflammatory mediator generation, and production of senescence-associated secretory phenotype. Immnunosenescence could promote inflammaging (chronic low-grade inflammation) and contribute to late-onset adult asthma and asthma in the elderly, along with age-related pulmonary disease, such as chronic obstructive pulmonary disease and pulmonary fibrosis, due to lung parenchyma senescence. Aged patients with asthma exhibit local and systemic type 2 and non-type 2 inflammation, associated with clinical manifestations. Here, we discuss immunosenescence\\\\u2019s contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly and present an overview of age-related features in the immune system and lung structure.\"\\n      },\\n      {\\n        \"paperId\": \"94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\\n        \"title\": \"DAFT-E: Feature-Based Multivariate and Multi-Step-Ahead Wind Power Forecasting\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tste.2021.3130949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tste.2021.3130949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1978743481\",\\n            \"name\": \"F. De Caro\"\\n          },\\n          {\\n            \"authorId\": \"2142416821\",\\n            \"name\": \"Jacopo De Stefani\"\\n          },\\n          {\\n            \"authorId\": \"33858225\",\\n            \"name\": \"A. Vaccaro\"\\n          },\\n          {\\n            \"authorId\": \"1772497\",\\n            \"name\": \"Gianluca Bontempi\"\\n          }\\n        ],\\n        \"abstract\": \"Wind energy is one of the most promising resources for the mitigation of greenhouse gas emissions that contribute to anthropogenic global warming. However, the large proliferation of wind power generators is causing several critical issues in power systems due to their variable power generated profiles. For this reason, a large number of learning techniques, e.g. integrating Vector Auto-Regressive and Neural Network-based models, were proposed in the literature for mitigating wind power uncertainty issues. Unfortunately, these methodologies show several limitations, e.g. the huge number of parameters and/or the heavy computational cost, which hinder their deployment in modern power system operation, where prompt and reliable wide-area wind power generation forecasts are requested for supporting time-critical decision making on several time horizons. To try addressing this issue, this paper proposes the Dynamic Adaptive Feature-based Temporal Ensemble (DAFT-E) forecasting approach, which relies on an extensive feature engineering, a fast feature selection step and an ensemble of computationally inexpensive models to reduce the computational complexity of the forecasting task, while still preserving predictive accuracy. The experimental results, which benchmark DAFT-E against multivariate (VAR and deep learning) alternatives on two real case studies, show that the proposed approach outperforms state-of-the-art and representation learning models according to several forecasting accuracy metrics.\"\\n      },\\n      {\\n        \"paperId\": \"f6023556221018f779a63a60874973195aea8352\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6023556221018f779a63a60874973195aea8352\",\\n        \"title\": \"Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.09857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel framework named Attention-aware Self-adaptive Prompt (ASP), which prevents overfitting on base task and does not require enormous data in few-shot incremental tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275745354\",\\n            \"name\": \"Chenxi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2254008335\",\\n            \"name\": \"Zhenyi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2249155683\",\\n            \"name\": \"Tianyi Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2262968852\",\\n            \"name\": \"Ruibo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2254326623\",\\n            \"name\": \"Yihan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2275765198\",\\n            \"name\": \"Junfeng Guo\"\\n          },\\n          {\\n            \"authorId\": \"2261394090\",\\n            \"name\": \"Heng Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensive experiments on three benchmark datasets validate that ASP consistently outperforms state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning new classes and mitigating forgetting.\"\\n      },\\n      {\\n        \"paperId\": \"c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"title\": \"Soft Prompt Generation for Domain Generalization\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG), which consists of a two-stage training phase and an inference phase, aiming to incorporate the generative model domain knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2290728057\",\\n            \"name\": \"Yuedi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\"\\n      },\\n      {\\n        \"paperId\": \"37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"title\": \"Adapting to Distribution Shift by Visual Domain Prompt Generation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.02797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction and outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35793956\",\\n            \"name\": \"Zhixiang Chi\"\\n          },\\n          {\\n            \"authorId\": \"2300096585\",\\n            \"name\": \"Li Gu\"\\n          },\\n          {\\n            \"authorId\": \"2300089295\",\\n            \"name\": \"Tao Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2277793919\",\\n            \"name\": \"Huan Liu\"\\n          },\\n          {\\n            \"authorId\": \"1787848\",\\n            \"name\": \"Yuanhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2277598061\",\\n            \"name\": \"Konstantinos N. Plataniotis\"\\n          },\\n          {\\n            \"authorId\": \"2277695392\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n      },\\n      {\\n        \"paperId\": \"fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\\n        \"title\": \"Query-Based Adversarial Prompt Generation\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.12329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work improves on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268494505\",\\n            \"name\": \"Jonathan Hayase\"\\n          },\\n          {\\n            \"authorId\": \"2284689404\",\\n            \"name\": \"Ema Borevkovic\"\\n          },\\n          {\\n            \"authorId\": \"2483738\",\\n            \"name\": \"Nicholas Carlini\"\\n          },\\n          {\\n            \"authorId\": \"2444919\",\\n            \"name\": \"Florian Tram\\\\u00e8r\"\\n          },\\n          {\\n            \"authorId\": \"3490923\",\\n            \"name\": \"Milad Nasr\"\\n          }\\n        ],\\n        \"abstract\": \"Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI\\'s safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.\"\\n      },\\n      {\\n        \"paperId\": \"91b6158978b248e9a0e65d0d588bc1054e72bc16\",\\n        \"url\": \"https://www.semanticscholar.org/paper/91b6158978b248e9a0e65d0d588bc1054e72bc16\",\\n        \"title\": \"MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.findings-emnlp.215.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP and proposes a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260750930\",\\n            \"name\": \"Yuyan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2260655625\",\\n            \"name\": \"Zhihao Wen\"\\n          },\\n          {\\n            \"authorId\": \"2260651904\",\\n            \"name\": \"Ge Fan\"\\n          },\\n          {\\n            \"authorId\": \"2273721608\",\\n            \"name\": \"Zhengyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2273816877\",\\n            \"name\": \"Wei Wu\"\\n          },\\n          {\\n            \"authorId\": \"2260908086\",\\n            \"name\": \"Dayiheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2243457917\",\\n            \"name\": \"Zhixu Li\"\\n          },\\n          {\\n            \"authorId\": \"2163832089\",\\n            \"name\": \"Bang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2265724350\",\\n            \"name\": \"Yanghua Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.\"\\n      },\\n      {\\n        \"paperId\": \"135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\\n        \"url\": \"https://www.semanticscholar.org/paper/135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\\n        \"title\": \"Federated Text-driven Prompt Generation for Vision-Language Models\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256983385\",\\n            \"name\": \"Chen Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2257324808\",\\n            \"name\": \"Xingyu Li\"\\n          },\\n          {\\n            \"authorId\": \"29359383\",\\n            \"name\": \"Chaithanya Kumar Mummadi\"\\n          },\\n          {\\n            \"authorId\": \"144487556\",\\n            \"name\": \"M. Ganesh\"\\n          },\\n          {\\n            \"authorId\": \"2257091754\",\\n            \"name\": \"Zhenzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2257130661\",\\n            \"name\": \"Lu Peng\"\\n          },\\n          {\\n            \"authorId\": \"2257132255\",\\n            \"name\": \"Wan-Yi Lin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"title\": \"EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2024.3482175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2024.3482175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145205189\",\\n            \"name\": \"Yinsong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266873357\",\\n            \"name\": \"Jiaqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2266387679\",\\n            \"name\": \"Aidong Men\"\\n          },\\n          {\\n            \"authorId\": \"2266583142\",\\n            \"name\": \"Qingchao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\"\\n      },\\n      {\\n        \"paperId\": \"1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"title\": \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\\n        \"citationCount\": 92,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new approach to code generation by LLMs is proposed, which is called AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72729733\",\\n            \"name\": \"T. Ridnik\"\\n          },\\n          {\\n            \"authorId\": \"2279758170\",\\n            \"name\": \"Dedy Kredo\"\\n          },\\n          {\\n            \"authorId\": \"49668367\",\\n            \"name\": \"Itamar Friedman\"\\n          }\\n        ],\\n        \"abstract\": \"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\"\\n      },\\n      {\\n        \"paperId\": \"211e915b2e1e0753ddd581f10362fc82f28cc606\",\\n        \"url\": \"https://www.semanticscholar.org/paper/211e915b2e1e0753ddd581f10362fc82f28cc606\",\\n        \"title\": \"DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DiTCtrl is proposed, a training-free multi-prompt video generation method under MM-DiT architectures for the first time that achieves state-of-the-art performance without additional training and presents MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2337083029\",\\n            \"name\": \"Minghong Cai\"\\n          },\\n          {\\n            \"authorId\": \"30176430\",\\n            \"name\": \"Xiaodong Cun\"\\n          },\\n          {\\n            \"authorId\": \"2257035102\",\\n            \"name\": \"Xiaoyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2308556703\",\\n            \"name\": \"Wenze Liu\"\\n          },\\n          {\\n            \"authorId\": \"2303078452\",\\n            \"name\": \"Zhaoyang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2268490605\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2316484241\",\\n            \"name\": \"Xiangyu Yue\"\\n          }\\n        ],\\n        \"abstract\": \"Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer (MM-DiT) architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT\\\\u2019s attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training. Code is available at https://github.com/TencentARC/DiTCtrl.\"\\n      },\\n      {\\n        \"paperId\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04095\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces the Prompt Auto-Editing (PAE) method, which employs an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148661301\",\\n            \"name\": \"Wenyi Mo\"\\n          },\\n          {\\n            \"authorId\": \"2146332319\",\\n            \"name\": \"Tianyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2281418241\",\\n            \"name\": \"Yalong Bai\"\\n          },\\n          {\\n            \"authorId\": \"2295513824\",\\n            \"name\": \"Bing Su\"\\n          },\\n          {\\n            \"authorId\": \"2293310016\",\\n            \"name\": \"Ji-Rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2281323801\",\\n            \"name\": \"Qing Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\"\\n      }\\n    ]\\n  },\\n  \"iterative context update\": {\\n    \"total\": 9219,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 288,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 211,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 40,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\', \\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"fca9a4508863025d95a581ead47032d497825053\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fca9a4508863025d95a581ead47032d497825053\",\\n        \"title\": \"VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.02186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"VideoICL is a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach, improving OOD video understanding performance by extending effective context length without incurring high costs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333526847\",\\n            \"name\": \"Kangsan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2307075970\",\\n            \"name\": \"Geon Park\"\\n          },\\n          {\\n            \"authorId\": \"3445691\",\\n            \"name\": \"Youngwan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2119578055\",\\n            \"name\": \"Woongyeong Yeo\"\\n          },\\n          {\\n            \"authorId\": \"2265627157\",\\n            \"name\": \"Sung Ju Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in video large multimodal models (LMMs) have significantly improved their video understanding and reasoning capabilities. However, their performance drops on out-of-distribution (OOD) tasks that are underrepresented in training data. Traditional methods like fine-tuning on OOD datasets are impractical due to high computational costs. While In-context learning (ICL) with demonstration examples has shown promising generalization performance in language tasks and image-language tasks without fine-tuning, applying ICL to video-language tasks faces challenges due to the limited context length in Video LMMs, as videos require longer token lengths. To address these issues, we propose VideoICL, a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach. This allows to select the most relevant examples and rank them based on similarity, to be used for inference. If the generated response has low confidence, our framework selects new examples and performs inference again, iteratively refining the results until a high-confidence response is obtained. This approach improves OOD video understanding performance by extending effective context length without incurring high costs. The experimental results on multiple benchmarks demonstrate significant performance gains, especially in domain-specific scenarios, laying the groundwork for broader video comprehension applications. Code is released at https://github.com/KangsanKim07/VideoICL\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 72,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\\n        \"title\": \"Single image deraining using scale constraint iterative update network\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2023.121339?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2023.121339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1390863714\",\\n            \"name\": \"Yitong Yang\"\\n          },\\n          {\\n            \"authorId\": \"1591131546\",\\n            \"name\": \"Yongjun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"150356192\",\\n            \"name\": \"Zhongwei Cui\"\\n          },\\n          {\\n            \"authorId\": \"2112674491\",\\n            \"name\": \"Haoliang Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2210993430\",\\n            \"name\": \"Ting Ouyang\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 188,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 366,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 330,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 48,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2891,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      }\\n    ]\\n  },\\n  \"thematic consistency LLM\": {\\n    \"total\": 887,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\\n        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\\n        \"title\": \"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.02896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This experimental study seeks to lay the groundwork for the understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction?\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282535211\",\\n            \"name\": \"Ivar Frisch\"\\n          },\\n          {\\n            \"authorId\": \"24068173\",\\n            \"name\": \"Mario Giulianelli\"\\n          }\\n        ],\\n        \"abstract\": \"Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model (LLM) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction? We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.\"\\n      },\\n      {\\n        \"paperId\": \"477a16bdbb43589e5feac3881b3370e3a4ab5624\",\\n        \"url\": \"https://www.semanticscholar.org/paper/477a16bdbb43589e5feac3881b3370e3a4ab5624\",\\n        \"title\": \"Evaluating the Consistency of LLM Evaluators\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.00543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Examining the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models demonstrates that strong proprietary models are not necessarily consistent evaluators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2291076200\",\\n            \"name\": \"Noah Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290955335\",\\n            \"name\": \"Jiwoo Hong\"\\n          },\\n          {\\n            \"authorId\": \"2290905396\",\\n            \"name\": \"James Thorne\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have shown potential as general evaluators along with the evident benefits of speed and cost. While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators. In this paper, we conduct extensive studies on the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models. Our comprehensive analysis demonstrates that strong proprietary models are not necessarily consistent evaluators, highlighting the importance of considering consistency in assessing the capability of LLM evaluators.\"\\n      },\\n      {\\n        \"paperId\": \"75062b58398b6e9409e5fec855f6912534331eaf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/75062b58398b6e9409e5fec855f6912534331eaf\",\\n        \"title\": \"Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales, facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-08-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284692979\",\\n            \"name\": \"Guangya Wan\"\\n          },\\n          {\\n            \"authorId\": \"2284699718\",\\n            \"name\": \"Yuqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2317120126\",\\n            \"name\": \"Jie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2316974967\",\\n            \"name\": \"Sheng Li\"\\n          }\\n        ],\\n        \"abstract\": \"Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths,but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority voting, enabling more informed choices on when to halt sampling and which rationale to select. Our comprehensive experiments across diverse question-answering datasets demonstrate that RASC outperforms existing methods, reducing sample usage by approximately 70% while maintaining accuracy. Moreover, RASC facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs. Our approach effectively addresses the efficiency-accuracy trade-off in LLM reasoning tasks, offering a new perspective for more nuanced, faithful, and effective utilization of LLMs in resource-constrained environments.\"\\n      },\\n      {\\n        \"paperId\": \"3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\\n        \"title\": \"Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1186/s12911-024-02656-3\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11382443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare is emphasized, aiming to improve clinical outcomes ethically and effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-09-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"145770274\",\\n            \"name\": \"Tala Mirzaei\"\\n          },\\n          {\\n            \"authorId\": \"2320339088\",\\n            \"name\": \"Leila Amini\"\\n          },\\n          {\\n            \"authorId\": \"2574575\",\\n            \"name\": \"Pouyan Esmaeilzadeh\"\\n          }\\n        ],\\n        \"abstract\": \"This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions. We analyzed 3049 posts and comments extracted from a self-identified clinician subreddit using unsupervised machine learning via Latent Dirichlet Allocation and a structured qualitative analysis methodology. Analysis uncovered 14 salient themes of ethical implications, which we further consolidated into 4 overarching domains reflecting ethical issues around various clinical applications of LLM in healthcare, LLM coding, algorithm, and data governance, LLM\\\\u2019s role in health equity and the distribution of public health services, and the relationship between users (human) and LLM systems (machine). Mapping themes to ethical frameworks in literature illustrated multifaceted issues covering transparent LLM decisions, fairness, privacy, access disparities, user experiences, and reliability. This study emphasizes the need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare, aiming to improve clinical outcomes ethically and effectively.\"\\n      },\\n      {\\n        \"paperId\": \"6b7c5fc0f6b401962153f68f8250951f75da929e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b7c5fc0f6b401962153f68f8250951f75da929e\",\\n        \"title\": \"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.06503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper analyzes two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and finds that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-04-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295732707\",\\n            \"name\": \"Nathan Brake\"\\n          },\\n          {\\n            \"authorId\": \"2295732451\",\\n            \"name\": \"Thomas Schaaf\"\\n          }\\n        ],\\n        \"abstract\": \"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\\n      },\\n      {\\n        \"paperId\": \"e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"title\": \"LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\",\\n        \"citationCount\": 110,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL), which yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3400291\",\\n            \"name\": \"Shih-Chieh Dai\"\\n          },\\n          {\\n            \"authorId\": \"2261362789\",\\n            \"name\": \"Aiping Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1746959\",\\n            \"name\": \"Lun-Wei Ku\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n      },\\n      {\\n        \"paperId\": \"32426b96ff3c680125bde3b835bfa931288b8ade\",\\n        \"url\": \"https://www.semanticscholar.org/paper/32426b96ff3c680125bde3b835bfa931288b8ade\",\\n        \"title\": \"Better Patching Using LLM Prompting, via Self-Consistency\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2306.00108\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.00108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an application of the $\\\\\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots, on the MODIT dataset.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language models (LLMs) can be induced to solve non-trivial problems with \\\\u201cfew-shot\\\\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \\\\u201cchain of thought\\\\u201d ($\\\\\\\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \\\\u201cexplained\\\\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\\\\\\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\\\\\\\mathcal{S}-C$ (or even $\\\\\\\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\\\\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.\"\\n      },\\n      {\\n        \"paperId\": \"e24424283c02fbe7f641e5b3490d7bb059f8355a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e24424283c02fbe7f641e5b3490d7bb059f8355a\",\\n        \"title\": \"A Survey on LLM-as-a-Judge\",\\n        \"citationCount\": 776,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.15594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built?\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-11-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216587705\",\\n            \"name\": \"Jiawei Gu\"\\n          },\\n          {\\n            \"authorId\": \"144267788\",\\n            \"name\": \"Xuhui Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2287881684\",\\n            \"name\": \"Zhichao Shi\"\\n          },\\n          {\\n            \"authorId\": \"2274159320\",\\n            \"name\": \"Hexiang Tan\"\\n          },\\n          {\\n            \"authorId\": \"2332093190\",\\n            \"name\": \"Xuehao Zhai\"\\n          },\\n          {\\n            \"authorId\": \"2250617116\",\\n            \"name\": \"Chengjin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2330714501\",\\n            \"name\": \"Wei Li\"\\n          },\\n          {\\n            \"authorId\": \"1944248313\",\\n            \"name\": \"Yinghan Shen\"\\n          },\\n          {\\n            \"authorId\": \"2311556497\",\\n            \"name\": \"Shengjie Ma\"\\n          },\\n          {\\n            \"authorId\": \"2332306096\",\\n            \"name\": \"Honghao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257058703\",\\n            \"name\": \"Yuanzhuo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284217200\",\\n            \"name\": \"Jian Guo\"\\n          }\\n        ],\\n        \"abstract\": \"Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of\\\\\"LLM-as-a-Judge,\\\\\"where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.\"\\n      },\\n      {\\n        \"paperId\": \"0bf3a1867f7245b8a702093901c66b08b518eafc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0bf3a1867f7245b8a702093901c66b08b518eafc\",\\n        \"title\": \"Evaluating Very Long-Term Conversational Memory of LLM Agents\",\\n        \"citationCount\": 166,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A machine-human pipeline is introduced to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs, and presents a comprehensive evaluation benchmark to measure long-term memory in models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8785371\",\\n            \"name\": \"Adyasha Maharana\"\\n          },\\n          {\\n            \"authorId\": \"2266803131\",\\n            \"name\": \"Dong-Ho Lee\"\\n          },\\n          {\\n            \"authorId\": \"145582202\",\\n            \"name\": \"S. Tulyakov\"\\n          },\\n          {\\n            \"authorId\": \"2285969697\",\\n            \"name\": \"Mohit Bansal\"\\n          },\\n          {\\n            \"authorId\": \"2266751000\",\\n            \"name\": \"Francesco Barbieri\"\\n          },\\n          {\\n            \"authorId\": \"2267220081\",\\n            \"name\": \"Yuwei Fang\"\\n          }\\n        ],\\n        \"abstract\": \"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.\"\\n      },\\n      {\\n        \"paperId\": \"7c04ab297b59d4fe29285f339350882a3120b27f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7c04ab297b59d4fe29285f339350882a3120b27f\",\\n        \"title\": \"CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs\",\\n        \"citationCount\": 205,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642773\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.11314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions, is developed, revealing four design considerations for future educational AI assistants.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3136345\",\\n            \"name\": \"Majeed Kazemitabaar\"\\n          },\\n          {\\n            \"authorId\": \"2184253123\",\\n            \"name\": \"Runlong Ye\"\\n          },\\n          {\\n            \"authorId\": \"2280281736\",\\n            \"name\": \"Xiaoning Wang\"\\n          },\\n          {\\n            \"authorId\": \"2280145055\",\\n            \"name\": \"Austin Z Henley\"\\n          },\\n          {\\n            \"authorId\": \"2243041721\",\\n            \"name\": \"Paul Denny\"\\n          },\\n          {\\n            \"authorId\": \"2280145218\",\\n            \"name\": \"Michelle Craig\"\\n          },\\n          {\\n            \"authorId\": \"2280146888\",\\n            \"name\": \"Tovi Grossman\"\\n          }\\n        ],\\n        \"abstract\": \"Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student\\\\u2019s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI\\\\u2019s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.\"\\n      },\\n      {\\n        \"paperId\": \"da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\\n        \"url\": \"https://www.semanticscholar.org/paper/da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\\n        \"title\": \"Exploring and Evaluating Hallucinations in LLM-Powered Code Generation\",\\n        \"citationCount\": 221,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation, and proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290969862\",\\n            \"name\": \"Fang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2294504414\",\\n            \"name\": \"Yang Liu\"\\n          },\\n          {\\n            \"authorId\": \"2295165194\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2294528116\",\\n            \"name\": \"Houkun Huang\"\\n          },\\n          {\\n            \"authorId\": \"2294510508\",\\n            \"name\": \"Ruifeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2294664033\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290433096\",\\n            \"name\": \"Li Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users\\' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.\"\\n      },\\n      {\\n        \"paperId\": \"a06d3e9e90008c64c45a0029d580541d5f646771\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771\",\\n        \"title\": \"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.00812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An overview of the various benefits of integrating code into LLMs\\' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277527247\",\\n            \"name\": \"Ke Yang\"\\n          },\\n          {\\n            \"authorId\": \"33456794\",\\n            \"name\": \"Jiateng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277421308\",\\n            \"name\": \"John Wu\"\\n          },\\n          {\\n            \"authorId\": \"2277597831\",\\n            \"name\": \"Chaoqi Yang\"\\n          },\\n          {\\n            \"authorId\": \"51135899\",\\n            \"name\": \"Y. Fung\"\\n          },\\n          {\\n            \"authorId\": \"2262396117\",\\n            \"name\": \"Sha Li\"\\n          },\\n          {\\n            \"authorId\": \"2277416897\",\\n            \"name\": \"Zixuan Huang\"\\n          },\\n          {\\n            \"authorId\": \"2344961610\",\\n            \"name\": \"Xu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2144803999\",\\n            \"name\": \"Xingyao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2277247982\",\\n            \"name\": \"Yiquan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2277409745\",\\n            \"name\": \"Heng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2261082008\",\\n            \"name\": \"ChengXiang Zhai\"\\n          }\\n        ],\\n        \"abstract\": \"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs\\' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.\"\\n      },\\n      {\\n        \"paperId\": \"2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\\n        \"title\": \"PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.12532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The number of crucial keys and values that influence future generations decreases layer by layer and can be extracted by the consistency in attention weights, leading to PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2182432556\",\\n            \"name\": \"Dongjie Yang\"\\n          },\\n          {\\n            \"authorId\": \"2302683712\",\\n            \"name\": \"Xiaodong Han\"\\n          },\\n          {\\n            \"authorId\": \"2302558089\",\\n            \"name\": \"Yan Gao\"\\n          },\\n          {\\n            \"authorId\": \"2302556666\",\\n            \"name\": \"Yao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2302704855\",\\n            \"name\": \"Shilin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2302545224\",\\n            \"name\": \"Hai Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.\"\\n      },\\n      {\\n        \"paperId\": \"36b9dac525fc93100b18d8e489bd97460cd49a5e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/36b9dac525fc93100b18d8e489bd97460cd49a5e\",\\n        \"title\": \"Thematic-LM: A LLM-based Multi-agent System for Large-scale Thematic Analysis\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3696410.3714595?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3696410.3714595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study applies Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-04-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2343740149\",\\n            \"name\": \"Tingrui Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2343749049\",\\n            \"name\": \"Caroline Walker\"\\n          },\\n          {\\n            \"authorId\": \"2343746435\",\\n            \"name\": \"Chris Cunningham\"\\n          },\\n          {\\n            \"authorId\": \"2310725786\",\\n            \"name\": \"Yun Sing Koh\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) is a widely used qualitative method for identifying underlying meanings within unstructured text. However, TA requires manual processes, which become increasingly labour-intensive and time-consuming as datasets grow. While large language models (LLMs) have been introduced to assist with TA on small-scale datasets, three key limitations hinder their effectiveness. First, current approaches often depend on interactions between an LLM agent and a human coder, a process that becomes challenging with larger datasets. Second, with feedback from the human coder, the LLM tends to mirror the human coder, which provides a narrower viewpoint of the data. Third, existing methods follow a sequential process, where codes are generated for individual samples without recalling previous codes and associated data, reducing the ability to analyse data holistically. To address these limitations, we propose Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis. Thematic-LM assigns specialised tasks to each agent, such as coding, aggregating codes, and maintaining and updating the codebook. We assign coder agents different identity perspectives to simulate the subjective nature of TA, fostering a more diverse interpretation of the data. We applied Thematic-LM to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change. We evaluate the resulting themes based on trustworthiness principles in qualitative research. Our study reveals insights such as assigning different identities to coder agents promotes divergence in codes and themes.\"\\n      },\\n      {\\n        \"paperId\": \"54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\\n        \"title\": \"Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation\",\\n        \"citationCount\": 88,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2285255408\",\\n            \"name\": \"Jiawei Wang\"\\n          },\\n          {\\n            \"authorId\": \"31279896\",\\n            \"name\": \"Renhe Jiang\"\\n          },\\n          {\\n            \"authorId\": \"46962297\",\\n            \"name\": \"Chuang Yang\"\\n          },\\n          {\\n            \"authorId\": \"2157765133\",\\n            \"name\": \"Zengqing Wu\"\\n          },\\n          {\\n            \"authorId\": \"2266396584\",\\n            \"name\": \"Makoto Onizuka\"\\n          },\\n          {\\n            \"authorId\": \"2239490643\",\\n            \"name\": \"Ryosuke Shibasaki\"\\n          },\\n          {\\n            \"authorId\": \"2284717877\",\\n            \"name\": \"Chuan Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\\n      },\\n      {\\n        \"paperId\": \"7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\\n        \"title\": \"ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification\",\\n        \"citationCount\": 75,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3660810?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3660810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions, and can effectively facilitate the practical application of LLMs in real-world development environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1994579604\",\\n            \"name\": \"Fangwen Mu\"\\n          },\\n          {\\n            \"authorId\": \"2305416699\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2259571951\",\\n            \"name\": \"Song Wang\"\\n          },\\n          {\\n            \"authorId\": \"2259613131\",\\n            \"name\": \"Zhuohao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2259874187\",\\n            \"name\": \"Binquan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2259824656\",\\n            \"name\": \"ChenXue Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260294248\",\\n            \"name\": \"Shichao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2157214565\",\\n            \"name\": \"Qing Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.\"\\n      },\\n      {\\n        \"paperId\": \"630c47372317164fc367153f938903e1d5b76059\",\\n        \"url\": \"https://www.semanticscholar.org/paper/630c47372317164fc367153f938903e1d5b76059\",\\n        \"title\": \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52202/079017-1077?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52202/079017-1077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1602820179\",\\n            \"name\": \"Gaurang Sriramanan\"\\n          },\\n          {\\n            \"authorId\": \"2344249306\",\\n            \"name\": \"Siddhant Bharti\"\\n          },\\n          {\\n            \"authorId\": \"150333898\",\\n            \"name\": \"Vinu Sankar Sadasivan\"\\n          },\\n          {\\n            \"authorId\": \"152623528\",\\n            \"name\": \"Shoumik Saha\"\\n          },\\n          {\\n            \"authorId\": \"2305809801\",\\n            \"name\": \"Priyatham Kattakinda\"\\n          },\\n          {\\n            \"authorId\": \"34389431\",\\n            \"name\": \"S. Feizi\"\\n          }\\n        ],\\n        \"abstract\": \"While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations\\\\u2014 outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.\"\\n      },\\n      {\\n        \"paperId\": \"038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\\n        \"title\": \"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt), which eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1966961\",\\n            \"name\": \"Yanshen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2239274607\",\\n            \"name\": \"Jianfeng He\"\\n          },\\n          {\\n            \"authorId\": \"2293779433\",\\n            \"name\": \"Limeng Cui\"\\n          },\\n          {\\n            \"authorId\": \"3433489\",\\n            \"name\": \"Shuo Lei\"\\n          },\\n          {\\n            \"authorId\": \"2249846863\",\\n            \"name\": \"Chang-Tien Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.\"\\n      },\\n      {\\n        \"paperId\": \"f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\\n        \"title\": \"Reasoning Runtime Behavior of a Program with LLM: How Far are We?\",\\n        \"citationCount\": 49,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a framework, namely $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution, and utilizes existing code benchmarks and adapt them to new benchmarks within this framework.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293350098\",\\n            \"name\": \"Junkai Chen\"\\n          },\\n          {\\n            \"authorId\": \"2276184077\",\\n            \"name\": \"Zhiyuan Pan\"\\n          },\\n          {\\n            \"authorId\": \"2110049191\",\\n            \"name\": \"Xing Hu\"\\n          },\\n          {\\n            \"authorId\": \"2293350478\",\\n            \"name\": \"Zhenhao Li\"\\n          },\\n          {\\n            \"authorId\": \"2286413567\",\\n            \"name\": \"Ge Li\"\\n          },\\n          {\\n            \"authorId\": \"2265241871\",\\n            \"name\": \"Xin Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs (i.e., predicting code execution behaviors such as program output and execution path), but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs. Our code, data and $\\\\\\\\boldsymbol{\\\\\\\\mathcal{R}}\\\\\\\\mathbf{Eval}$ leaderboard are available at https://r-eval.github.io.\"\\n      },\\n      {\\n        \"paperId\": \"8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\\n        \"title\": \"Don\\'t Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper leverages the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics, they can be prompted to autoformalize informal mathematical statements into formal Isabelle code -- which provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287813125\",\\n            \"name\": \"Jin Peng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2144884927\",\\n            \"name\": \"Charles Staats\"\\n          },\\n          {\\n            \"authorId\": \"2293653101\",\\n            \"name\": \"Wenda Li\"\\n          },\\n          {\\n            \"authorId\": \"2574060\",\\n            \"name\": \"Christian Szegedy\"\\n          },\\n          {\\n            \"authorId\": \"7446832\",\\n            \"name\": \"Kilian Q. Weinberger\"\\n          },\\n          {\\n            \"authorId\": \"2287780080\",\\n            \"name\": \"Yuhuai Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLM), such as Google\\'s Minerva and OpenAI\\'s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.\"\\n      },\\n      {\\n        \"paperId\": \"1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\\n        \"title\": \"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.17578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MM-Eval is introduced, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages designed with multilingual-specific challenges in mind and finds that evaluators are unfair and inconsistent when evaluating lower-resourced languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2321452295\",\\n            \"name\": \"Guijin Son\"\\n          },\\n          {\\n            \"authorId\": \"29830817\",\\n            \"name\": \"Dongkeun Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2299329316\",\\n            \"name\": \"Juyoung Suk\"\\n          },\\n          {\\n            \"authorId\": \"2301578911\",\\n            \"name\": \"Javier Aula-Blasco\"\\n          },\\n          {\\n            \"authorId\": \"2327215494\",\\n            \"name\": \"Mano Aslan\"\\n          },\\n          {\\n            \"authorId\": \"2327216625\",\\n            \"name\": \"Vu Trong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2232783785\",\\n            \"name\": \"Shayekh Bin Islam\"\\n          },\\n          {\\n            \"authorId\": \"2327215436\",\\n            \"name\": \"Jaume Prats-Cristi\\\\u00e0\"\\n          },\\n          {\\n            \"authorId\": \"2327217057\",\\n            \"name\": \"Luc\\\\u00eda Tormo-Ba\\\\u00f1uelos\"\\n          },\\n          {\\n            \"authorId\": \"2184037220\",\\n            \"name\": \"Seungone Kim\"\\n          }\\n        ],\\n        \"abstract\": \"As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\\\\\"meta-evaluation benchmarks\\\\\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.\"\\n      },\\n      {\\n        \"paperId\": \"f1366e505de4f1d0e901903e3c17471033758a96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f1366e505de4f1d0e901903e3c17471033758a96\",\\n        \"title\": \"Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based Chatbots\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.11876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Rescriber is designed, built, and evaluated, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts, presenting a promising approach to address the privacy and trust challenges of AI.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2326261436\",\\n            \"name\": \"Jijie Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2326116321\",\\n            \"name\": \"Eryue Xu\"\\n          },\\n          {\\n            \"authorId\": \"2326262935\",\\n            \"name\": \"Yaoyao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2326228781\",\\n            \"name\": \"Tianshi Li\"\\n          }\\n        ],\\n        \"abstract\": \"The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users\\\\u2019 personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=Rescriber) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users\\\\u2019 subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4o. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users\\\\u2019 trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.\"\\n      },\\n      {\\n        \"paperId\": \"3ec06fe8d8764123490544ab5dc956143e84b443\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3ec06fe8d8764123490544ab5dc956143e84b443\",\\n        \"title\": \"Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.11977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise and the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310565909\",\\n            \"name\": \"Guangzhi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2089532795\",\\n            \"name\": \"Xiao Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2275248675\",\\n            \"name\": \"Jose Such\"\\n          }\\n        ],\\n        \"abstract\": \"The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\\n      },\\n      {\\n        \"paperId\": \"dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\\n        \"title\": \"Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.07791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating position bias in LLM judges across pairwise and list-wise comparison settings confirms that position bias is not due to random chance and varies significantly across judges and tasks, and provides insights into the distribution of judging difficulty across the dataset.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305925735\",\\n            \"name\": \"Lin Shi\"\\n          },\\n          {\\n            \"authorId\": \"2262963382\",\\n            \"name\": \"Chiyu Ma\"\\n          },\\n          {\\n            \"authorId\": \"2330065663\",\\n            \"name\": \"Wenhua Liang\"\\n          },\\n          {\\n            \"authorId\": \"2227771\",\\n            \"name\": \"Weicheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"1918441\",\\n            \"name\": \"Soroush Vosoughi\"\\n          }\\n        ],\\n        \"abstract\": \"LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases - particularly position bias, the tendency to favor solutions based on their position within the prompt - compromise its reliability. This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 15 LLM judges across MTBench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.\"\\n      },\\n      {\\n        \"paperId\": \"638d8d1f3865ebf065605535a7aa50727d5ffabe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/638d8d1f3865ebf065605535a7aa50727d5ffabe\",\\n        \"title\": \"TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.18919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation, significantly improves semantic and contextual consistency in synthesized images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2298945483\",\\n            \"name\": \"Junhao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2298032416\",\\n            \"name\": \"Baiqiao Yin\"\\n          },\\n          {\\n            \"authorId\": \"2229014859\",\\n            \"name\": \"Kaixin Cai\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2276604489\",\\n            \"name\": \"Hanhui Li\"\\n          },\\n          {\\n            \"authorId\": \"2299161673\",\\n            \"name\": \"Yuxin He\"\\n          },\\n          {\\n            \"authorId\": \"2298943419\",\\n            \"name\": \"Xi Lu\"\\n          },\\n          {\\n            \"authorId\": \"2298043252\",\\n            \"name\": \"Yue Li\"\\n          },\\n          {\\n            \"authorId\": \"2298926852\",\\n            \"name\": \"Yifei Li\"\\n          },\\n          {\\n            \"authorId\": \"2298644153\",\\n            \"name\": \"Yuhao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"144880586\",\\n            \"name\": \"Yiqiang Yan\"\\n          },\\n          {\\n            \"authorId\": \"2291389227\",\\n            \"name\": \"Xiaodan Liang\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a\\\\\"Screenwriter\\\\\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the\\\\\"Rehearsal\\\\\". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the\\\\\"Final Performance\\\\\". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.\"\\n      },\\n      {\\n        \"paperId\": \"4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"title\": \"Improving Text-to-Image Consistency via Automatic Prompt Optimization\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models and paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1796269096\",\\n            \"name\": \"Oscar Ma\\\\u00f1as\"\\n          },\\n          {\\n            \"authorId\": \"2274101827\",\\n            \"name\": \"Pietro Astolfi\"\\n          },\\n          {\\n            \"authorId\": \"2293590162\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"2256372432\",\\n            \"name\": \"Candace Ross\"\\n          },\\n          {\\n            \"authorId\": \"39219656\",\\n            \"name\": \"Jack Urbanek\"\\n          },\\n          {\\n            \"authorId\": \"2293907712\",\\n            \"name\": \"Adina Williams\"\\n          },\\n          {\\n            \"authorId\": \"2801949\",\\n            \"name\": \"Aishwarya Agrawal\"\\n          },\\n          {\\n            \"authorId\": \"1456285042\",\\n            \"name\": \"Adriana Romero-Soriano\"\\n          },\\n          {\\n            \"authorId\": \"3325894\",\\n            \"name\": \"M. Drozdzal\"\\n          }\\n        ],\\n        \"abstract\": \"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"275a3955a83867dd36a3683788e0e053e00f8a89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/275a3955a83867dd36a3683788e0e053e00f8a89\",\\n        \"title\": \"Collaborative Enhancement of Consistency and Accuracy in US Diagnosis of Thyroid Nodules Using Large Language Models.\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1148/radiol.232255?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1148/radiol.232255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging and ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1678966811\",\\n            \"name\": \"Shaohong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2272038393\",\\n            \"name\": \"Wen-juan Tong\"\\n          },\\n          {\\n            \"authorId\": \"2127991969\",\\n            \"name\": \"Ming-De Li\"\\n          },\\n          {\\n            \"authorId\": \"28890237\",\\n            \"name\": \"Hang-tong Hu\"\\n          },\\n          {\\n            \"authorId\": \"2187182790\",\\n            \"name\": \"Xiao-zhou Lu\"\\n          },\\n          {\\n            \"authorId\": \"2185218332\",\\n            \"name\": \"Ze-Rong Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290915880\",\\n            \"name\": \"Xin-Xin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290897544\",\\n            \"name\": \"Ruifang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2261915754\",\\n            \"name\": \"Ming-De Lu\"\\n          },\\n          {\\n            \"authorId\": \"6457299\",\\n            \"name\": \"Li-da Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290866279\",\\n            \"name\": \"Wei Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Background Large language models (LLMs) hold substantial promise for medical imaging interpretation. However, there is a lack of studies on their feasibility in handling reasoning questions associated with medical diagnosis. Purpose To investigate the viability of leveraging three publicly available LLMs to enhance consistency and diagnostic accuracy in medical imaging based on standardized reporting, with pathology as the reference standard. Materials and Methods US images of thyroid nodules with pathologic results were retrospectively collected from a tertiary referral hospital between July 2022 and December 2022 and used to evaluate malignancy diagnoses generated by three LLMs-OpenAI\\'s ChatGPT 3.5, ChatGPT 4.0, and Google\\'s Bard. Inter- and intra-LLM agreement of diagnosis were evaluated. Then, diagnostic performance, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), was evaluated and compared for the LLMs and three interactive approaches: human reader combined with LLMs, image-to-text model combined with LLMs, and an end-to-end convolutional neural network model. Results A total of 1161 US images of thyroid nodules (498 benign, 663 malignant) from 725 patients (mean age, 42.2 years \\\\u00b1 14.1 [SD]; 516 women) were evaluated. ChatGPT 4.0 and Bard displayed substantial to almost perfect intra-LLM agreement (\\\\u03ba range, 0.65-0.86 [95% CI: 0.64, 0.86]), while ChatGPT 3.5 showed fair to substantial agreement (\\\\u03ba range, 0.36-0.68 [95% CI: 0.36, 0.68]). ChatGPT 4.0 had an accuracy of 78%-86% (95% CI: 76%, 88%) and sensitivity of 86%-95% (95% CI: 83%, 96%), compared with 74%-86% (95% CI: 71%, 88%) and 74%-91% (95% CI: 71%, 93%), respectively, for Bard. Moreover, with ChatGPT 4.0, the image-to-text-LLM strategy exhibited an AUC (0.83 [95% CI: 0.80, 0.85]) and accuracy (84% [95% CI: 82%, 86%]) comparable to those of the human-LLM interaction strategy with two senior readers and one junior reader and exceeding those of the human-LLM interaction strategy with one junior reader. Conclusion LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging. ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5. \\\\u00a9 RSNA, 2024 Supplemental material is available for this article.\"\\n      },\\n      {\\n        \"paperId\": \"b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\\n        \"title\": \"Mitigating LLM Hallucinations via Conformal Abstention\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.01563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1388837087\",\\n            \"name\": \"Yasin Abbasi-Yadkori\"\\n          },\\n          {\\n            \"authorId\": \"3150458\",\\n            \"name\": \"Ilja Kuzborskij\"\\n          },\\n          {\\n            \"authorId\": \"2298902427\",\\n            \"name\": \"David Stutz\"\\n          },\\n          {\\n            \"authorId\": \"2305592785\",\\n            \"name\": \"Andr\\\\u00e1s Gy\\\\u00f6rgy\"\\n          },\\n          {\\n            \"authorId\": \"2299943725\",\\n            \"name\": \"Adam Fisch\"\\n          },\\n          {\\n            \"authorId\": \"2299943677\",\\n            \"name\": \"Arnaud Doucet\"\\n          },\\n          {\\n            \"authorId\": \"2299943992\",\\n            \"name\": \"Iuliya Beloshapka\"\\n          },\\n          {\\n            \"authorId\": \"2239098855\",\\n            \"name\": \"Wei-Hung Weng\"\\n          },\\n          {\\n            \"authorId\": \"2300022831\",\\n            \"name\": \"Yao-Yuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257346986\",\\n            \"name\": \"Csaba Szepesv\\'ari\"\\n          },\\n          {\\n            \"authorId\": \"9235290\",\\n            \"name\": \"Ali Taylan Cemgil\"\\n          },\\n          {\\n            \"authorId\": \"2359197879\",\\n            \"name\": \"Nenad Tomasev\"\\n          }\\n        ],\\n        \"abstract\": \"We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying\\\\\"I don\\'t know\\\\\") in a general domain, instead of resorting to possibly\\\\\"hallucinating\\\\\"a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.\"\\n      },\\n      {\\n        \"paperId\": \"023a98af94a3e7e8e538a6183da8ec05024fec56\",\\n        \"url\": \"https://www.semanticscholar.org/paper/023a98af94a3e7e8e538a6183da8ec05024fec56\",\\n        \"title\": \"How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work prompts different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions, and finds that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2307916998\",\\n            \"name\": \"Julia Kharchenko\"\\n          },\\n          {\\n            \"authorId\": \"2284066307\",\\n            \"name\": \"Tanya Roosta\"\\n          },\\n          {\\n            \"authorId\": \"2284065969\",\\n            \"name\": \"Aman Chadha\"\\n          },\\n          {\\n            \"authorId\": \"2234352974\",\\n            \"name\": \"Chirag Shah\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user\\'s known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs\\'cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"a1849a77644ff411a03833b5aa7a65ff57158c50\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1849a77644ff411a03833b5aa7a65ff57158c50\",\\n        \"title\": \"CLLMs: Consistency Large Language Models\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.00835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work develops a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory by refining the target LLM to consistently predict the fixed point given any state as input.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2258963117\",\\n            \"name\": \"Siqi Kou\"\\n          },\\n          {\\n            \"authorId\": \"2258334187\",\\n            \"name\": \"Lanxiang Hu\"\\n          },\\n          {\\n            \"authorId\": \"2116778591\",\\n            \"name\": \"Zhe He\"\\n          },\\n          {\\n            \"authorId\": \"2260296481\",\\n            \"name\": \"Zhijie Deng\"\\n          },\\n          {\\n            \"authorId\": \"2289837431\",\\n            \"name\": \"Hao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\\\\\\\times$ to 3.4$\\\\\\\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\\n        \"title\": \"PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children\\'s Collaborative Learning\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613905.3651008\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.14227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded and thus highlight potential design improvements and considerations for peer agents in both roles.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-03-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2296401615\",\\n            \"name\": \"Jiawen Liu\"\\n          },\\n          {\\n            \"authorId\": \"2292671914\",\\n            \"name\": \"Yuanyuan Yao\"\\n          },\\n          {\\n            \"authorId\": \"2283762773\",\\n            \"name\": \"Pengcheng An\"\\n          },\\n          {\\n            \"authorId\": \"2301178822\",\\n            \"name\": \"Qi Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In children\\\\u2019s collaborative learning, effective peer conversations can significantly enhance the quality of children\\\\u2019s collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children\\\\u2019s creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.\"\\n      },\\n      {\\n        \"paperId\": \"5e317746c0f38d1149f33a19807af47d513bdd27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5e317746c0f38d1149f33a19807af47d513bdd27\",\\n        \"title\": \"Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.12509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald\\'s omega, and demonstrates the limitations of fixed randomness and the importance of considering multiple samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2328309355\",\\n            \"name\": \"Kayla Schroeder\"\\n          },\\n          {\\n            \"authorId\": \"1411379613\",\\n            \"name\": \"Zach Wood-Doughty\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs. While deterministic settings can improve consistency, they do not guarantee reliability, as a single sample from the model\\'s probability distribution can still be misleading. Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald\\'s omega. We evaluate the reliability of LLMs when judging the outputs of other LLMs on standard single-turn and multi-turn benchmarks, simultaneously investigating the impact of temperature on reliability. By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implications for downstream applications. Our findings highlight the need for a nuanced understanding of LLM reliability and the potential risks associated with over-reliance on single-shot evaluations. This work provides a crucial step towards building more trustworthy and reliable LLM-based systems and applications.\"\\n      },\\n      {\\n        \"paperId\": \"235a8bd57a6b53ecab756780a45bce6e4743cecd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/235a8bd57a6b53ecab756780a45bce6e4743cecd\",\\n        \"title\": \"LLM-based Multi-Level Knowledge Generation for Few-shot Knowledge Graph Completion\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2024/236?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2024/236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a generation-based FKGC paradigm facilitated by LLM distillation, which achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284861197\",\\n            \"name\": \"Qian Li\"\\n          },\\n          {\\n            \"authorId\": \"2313599793\",\\n            \"name\": \"Zhuo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2052296239\",\\n            \"name\": \"Cheng Ji\"\\n          },\\n          {\\n            \"authorId\": \"2313658619\",\\n            \"name\": \"Shiqi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2274552581\",\\n            \"name\": \"Jianxin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Knowledge Graphs (KGs) are pivotal in various NLP applications but often grapple with incompleteness, especially due to the long-tail problem where infrequent, unpopular relationships drastically reduce the KG completion performance. In this paper, we focus on Few-shot Knowledge Graph Completion (FKGC), a task addressing these gaps in long-tail scenarios. Amidst the rapid evolution of Large Language Models, we propose a generation-based FKGC paradigm facilitated by LLM distillation. Our MuKDC framework employs multi-level knowledge distillation for few-shot KG completion, generating supplementary knowledge to mitigate data scarcity in few-shot environments. MuKDC comprises two primary components: Multi-level Knowledge Generation, which enriches the KG at various levels, and Consistency Assessment, to ensure the coherence and reliability of the generated knowledge. Most notably, our method achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\\n      },\\n      {\\n        \"paperId\": \"9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\\n        \"title\": \"Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2144511530\",\\n            \"name\": \"Xuan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2265432385\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2302819855\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"2302765389\",\\n            \"name\": \"Haoyang Shang\"\\n          },\\n          {\\n            \"authorId\": \"2302927286\",\\n            \"name\": \"Chengxu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2302915902\",\\n            \"name\": \"Quanyan Zhu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents\\' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents\\' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\"\\n      },\\n      {\\n        \"paperId\": \"086046d38b3a7066aa39e3d350905e8065c8f1b5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/086046d38b3a7066aa39e3d350905e8065c8f1b5\",\\n        \"title\": \"Efficiently Scaling LLM Reasoning with Certaindex\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.20993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy, so this work introduces Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-12-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282896192\",\\n            \"name\": \"Yichao Fu\"\\n          },\\n          {\\n            \"authorId\": \"2279862923\",\\n            \"name\": \"Junda Chen\"\\n          },\\n          {\\n            \"authorId\": \"2317134948\",\\n            \"name\": \"Siqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2337869460\",\\n            \"name\": \"Zheyu Fu\"\\n          },\\n          {\\n            \"authorId\": \"2351053054\",\\n            \"name\": \"Zhongdongming Dai\"\\n          },\\n          {\\n            \"authorId\": \"2152482391\",\\n            \"name\": \"Yonghao Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2363671676\",\\n            \"name\": \"Yian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2317112099\",\\n            \"name\": \"Aurick Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2379937619\",\\n            \"name\": \"Tajana Rosing\"\\n          },\\n          {\\n            \"authorId\": \"2344601177\",\\n            \"name\": \"Ion Stoica\"\\n          },\\n          {\\n            \"authorId\": \"2337807823\",\\n            \"name\": \"Hao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50% compute savings and 3.3x higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git\"\\n      },\\n      {\\n        \"paperId\": \"43fedc4430be030c083626b3e64b7093916b429a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43fedc4430be030c083626b3e64b7093916b429a\",\\n        \"title\": \"Enhancing clinical reasoning skills for medical students: a qualitative comparison of LLM-powered social robotic versus computer-based virtual patients within rheumatology\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1007/s00296-024-05731-0\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11618132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform and shows promise in training CR skills, communication, and adaptive thinking.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290951787\",\\n            \"name\": \"Alexander Borg\"\\n          },\\n          {\\n            \"authorId\": \"2326093338\",\\n            \"name\": \"Benjamin Jobs\"\\n          },\\n          {\\n            \"authorId\": \"2164303442\",\\n            \"name\": \"Viking Huss\"\\n          },\\n          {\\n            \"authorId\": \"23717264\",\\n            \"name\": \"C. Gentline\"\\n          },\\n          {\\n            \"authorId\": \"2326093767\",\\n            \"name\": \"Fabricio Espinosa\"\\n          },\\n          {\\n            \"authorId\": \"2290722393\",\\n            \"name\": \"Mini Ruiz\"\\n          },\\n          {\\n            \"authorId\": \"2758537\",\\n            \"name\": \"Samuel Edelbring\"\\n          },\\n          {\\n            \"authorId\": \"2326094583\",\\n            \"name\": \"Carina Georg\"\\n          },\\n          {\\n            \"authorId\": \"103081544\",\\n            \"name\": \"G. Skantze\"\\n          },\\n          {\\n            \"authorId\": \"8637952\",\\n            \"name\": \"Ioannis Parodis\"\\n          }\\n        ],\\n        \"abstract\": \"Virtual patients (VPs) are increasingly used in medical education to train clinical reasoning (CR) skills. However, optimal VP design for enhancing interactivity and authenticity remains unclear. Novel interactive modalities, such as large language model (LLM)-enhanced social robotic VPs might increase interactivity and authenticity in CR skill practice. To evaluate medical students\\\\u2019 perceptions of CR training using an LLM-enhanced social robotic VP platform compared with a conventional computer-based VP platform. A qualitative study involved 23 third-year medical students from Karolinska Institutet, who completed VP cases on an LLM-enhanced social robotic platform and a computer-based semi-linear platform. In-depth interviews assessed students\\\\u2019 self-perceived acquirement of CR skills using the two platforms. Thematic analysis was employed to identify themes and sub-themes. Three main themes were identified: authenticity, VP application, and strengths and limitations. Students found the social robotic platform more authentic and engaging. It enabled highly interactive communication and expressed emotions, collectively offering a realistic experience. It facilitated active learning, hypothesis generation, and adaptive thinking. Limitations included lack of physical examination options and, occasionally, mechanical dialogue. The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform. Despite some limitations, it shows promise in training CR skills, communication, and adaptive thinking. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. \\\\u2022An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. \\\\u2022Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. \\\\u2022Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. \\\\u2022An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. \\\\u2022Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. \\\\u2022Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations.\"\\n      },\\n      {\\n        \"paperId\": \"f04c25fcf3247ff4d8eca72d862b22090b884b75\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f04c25fcf3247ff4d8eca72d862b22090b884b75\",\\n        \"title\": \"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.06461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49606614\",\\n            \"name\": \"Junlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282521448\",\\n            \"name\": \"Siddhartha Jain\"\\n          },\\n          {\\n            \"authorId\": \"2305691523\",\\n            \"name\": \"Dejiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2282366776\",\\n            \"name\": \"Baishakhi Ray\"\\n          },\\n          {\\n            \"authorId\": \"40574366\",\\n            \"name\": \"Varun Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2304481349\",\\n            \"name\": \"Ben Athiwaratkun\"\\n          }\\n        ],\\n        \"abstract\": \"A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don\\\\u2019t surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.\"\\n      },\\n      {\\n        \"paperId\": \"7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\\n        \"title\": \"What Did I Do Wrong? Quantifying LLMs\\' Sensitivity and Consistency to Prompt Engineering\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.12334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Two metrics for classification tasks, namely sensitivity and consistency, are introduced, which are complementary to task performance, and it is hoped that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2307085791\",\\n            \"name\": \"Federico Errica\"\\n          },\\n          {\\n            \"authorId\": \"2009237\",\\n            \"name\": \"G. Siracusano\"\\n          },\\n          {\\n            \"authorId\": \"3109801\",\\n            \"name\": \"D. Sanvito\"\\n          },\\n          {\\n            \"authorId\": \"2269460793\",\\n            \"name\": \"Roberto Bifulco\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs\\'inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely sensitivity and consistency, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\\n      },\\n      {\\n        \"paperId\": \"024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\\n        \"title\": \"PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2403.09732?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2403.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt representation, called reference-enhanced representation, is introduced, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries and proposes using cross-consistency across different LLMs rather than self-consistency within a particular LLM.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2262448530\",\\n            \"name\": \"Zhishuai Li\"\\n          },\\n          {\\n            \"authorId\": \"2292059965\",\\n            \"name\": \"Xiang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2291921433\",\\n            \"name\": \"Jingjing Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2262480162\",\\n            \"name\": \"Sun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2228059114\",\\n            \"name\": \"Guoqing Du\"\\n          },\\n          {\\n            \"authorId\": \"2267589674\",\\n            \"name\": \"Xiaoru Hu\"\\n          },\\n          {\\n            \"authorId\": \"2315291036\",\\n            \"name\": \"Bin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2276235755\",\\n            \"name\": \"Yuxiao Ye\"\\n          },\\n          {\\n            \"authorId\": \"2262543561\",\\n            \"name\": \"Ziyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2263456785\",\\n            \"name\": \"Rui Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2262446566\",\\n            \"name\": \"Hangyu Mao\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0e314ddbf28514d92f2405b73941242c162ae0ba\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0e314ddbf28514d92f2405b73941242c162ae0ba\",\\n        \"title\": \"AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.11073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AFaCTA (Automatic Factual Claim deTection Annotator) is introduced, a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs) and calibrates its annotation confidence with consistency along three predefined reasoning paths.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046974354\",\\n            \"name\": \"Jingwei Ni\"\\n          },\\n          {\\n            \"authorId\": \"2284947386\",\\n            \"name\": \"Minjing Shi\"\\n          },\\n          {\\n            \"authorId\": \"146552774\",\\n            \"name\": \"Dominik Stammbach\"\\n          },\\n          {\\n            \"authorId\": \"2790926\",\\n            \"name\": \"Mrinmaya Sachan\"\\n          },\\n          {\\n            \"authorId\": \"2261279066\",\\n            \"name\": \"Elliott Ash\"\\n          },\\n          {\\n            \"authorId\": \"3073566\",\\n            \"name\": \"Markus Leippold\"\\n          }\\n        ],\\n        \"abstract\": \"With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.\"\\n      },\\n      {\\n        \"paperId\": \"1c5a097b4e376897545f153370425cf7e0c2d8fd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1c5a097b4e376897545f153370425cf7e0c2d8fd\",\\n        \"title\": \"Explaining Length Bias in LLM-Based Preference Evaluations\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.01085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AdapAlpaca is proposed, a simple yet effective adjustment to win rate measurement that ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Zhengyu Hu\"\\n          },\\n          {\\n            \"authorId\": \"2322070046\",\\n            \"name\": \"Linxin Song\"\\n          },\\n          {\\n            \"authorId\": \"2309191644\",\\n            \"name\": \"Jieyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2311315868\",\\n            \"name\": \"Zheyuan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2269687536\",\\n            \"name\": \"Jingang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2309176938\",\\n            \"name\": \"Zhenyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2309202283\",\\n            \"name\": \"Jieyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2269470756\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"The use of large language models (LLMs) as judges, particularly in preference comparisons, has become widespread, but this reveals a notable bias towards longer responses, undermining the reliability of such evaluations. To better understand such bias, we propose to decompose the preference evaluation metric, specifically the win rate, into two key components: desirability and information mass, where the former is length-independent and related to trustworthiness such as correctness, toxicity, and consistency, and the latter is length-dependent and represents the amount of information in the response. We empirically demonstrated the decomposition through controlled experiments and found that response length impacts evaluations by influencing information mass. To derive a reliable evaluation metric that assesses content quality without being confounded by response length, we propose AdapAlpaca, a simple yet effective adjustment to win rate measurement. Specifically, AdapAlpaca ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\\n      },\\n      {\\n        \"paperId\": \"00ce8beee350a260395676490915d7ebfa7430d1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/00ce8beee350a260395676490915d7ebfa7430d1\",\\n        \"title\": \"Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Investigation of fine-tuned LLMs in D2T tasks in terms of model size reveals that increasing LLM size enhances readability andformativeness in D2T tasks, but larger LLMs may sacrifice \\\\\\\\textit{faithfulness}.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8038450\",\\n            \"name\": \"Joy Mahapatra\"\\n          },\\n          {\\n            \"authorId\": \"2312204876\",\\n            \"name\": \"U. Garain\"\\n          }\\n        ],\\n        \"abstract\": \"Data-to-text (D2T) generation aims to generate human-readable text from semi-structured data, such as tables and graphs. The recent success of D2T is largely attributed to advancements in LLMs. Despite the success of LLMs, no research has been conducted to illustrate the impact of model size on the performance of fine-tuned LLMs for D2T tasks. D2T model performance is typically assessed based on three key qualities: \\\\\\\\textit{readability} (indicates fluency and coherence), \\\\\\\\textit{informativeness} (measures content similarity), and \\\\\\\\textit{faithfulness} (assesses consistency of factual information). It is currently uncertain whether increasing the size of LLMs effectively improves performance in D2T tasks across these three qualities. The objective of this study is to investigate the performance of fine-tuned LLMs in D2T tasks in terms of model size. Through extensive comparative analysis, we aim to elucidate both the advantages and limitations of scaling model sizes across five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and twelve state-of-the-art LLMs with varying sizes from five different LLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all the three essential qualities of D2T models, we incorporate six widely recognized automatic metrics -- \\\\\\\\textsc{BLEU}, \\\\\\\\textsc{METEOR}, \\\\\\\\textsc{BERTScore}, \\\\\\\\textsc{MoverScore}, \\\\\\\\textsc{Parent}, and \\\\\\\\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance concerning model size in the presence of source-reference divergence, a critical aspect of D2T tasks. Our investigation reveals that increasing LLM size enhances \\\\\\\\textit{readability} and \\\\\\\\textit{informativeness} in D2T tasks, but larger (in terms of size) LLMs may sacrifice \\\\\\\\textit{faithfulness}. Moreover, small-sized LLMs show more resilience than larger ones when source-reference divergence is present.\"\\n      },\\n      {\\n        \"paperId\": \"74908bc543e77b8995a6eebe32ab13cf0837949b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/74908bc543e77b8995a6eebe32ab13cf0837949b\",\\n        \"title\": \"LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment and demonstrates that, when using GPT-4, the proposed metric achieves evaluation consistency close to that of radiologists.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2294387070\",\\n            \"name\": \"Zilong Wang\"\\n          },\\n          {\\n            \"authorId\": \"13289447\",\\n            \"name\": \"Xufang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2268347004\",\\n            \"name\": \"Xinyang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268313028\",\\n            \"name\": \"Dongsheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2160727304\",\\n            \"name\": \"Lili Qiu\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task\\'s clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.\"\\n      },\\n      {\\n        \"paperId\": \"f33991c02f8f0ab8794dad020c648b37ccc53365\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f33991c02f8f0ab8794dad020c648b37ccc53365\",\\n        \"title\": \"CitaLaw: Enhancing LLM with Citations in Legal Domain\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.14556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes CitaLaw, the first benchmark designed to evaluate LLMs\\' ability to produce legally sound responses with appropriate citations, and introduces syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-12-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290124325\",\\n            \"name\": \"Kepu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2118684861\",\\n            \"name\": \"Weijie Yu\"\\n          },\\n          {\\n            \"authorId\": \"2155892801\",\\n            \"name\": \"Sunhao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2274965731\",\\n            \"name\": \"Jun Xu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs\\' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.\"\\n      },\\n      {\\n        \"paperId\": \"0d5ccf0861b62223fe562e13369ab15746188251\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0d5ccf0861b62223fe562e13369ab15746188251\",\\n        \"title\": \"QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.05345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums, and applies QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2300845194\",\\n            \"name\": \"Varun Nagaraj Rao\"\\n          },\\n          {\\n            \"authorId\": \"2300370478\",\\n            \"name\": \"Eesha Agarwal\"\\n          },\\n          {\\n            \"authorId\": \"2300371225\",\\n            \"name\": \"Samantha Dalal\"\\n          },\\n          {\\n            \"authorId\": \"2265042713\",\\n            \"name\": \"Dan Calacci\"\\n          },\\n          {\\n            \"authorId\": \"2266397659\",\\n            \"name\": \"Andr\\'es Monroy-Hern\\'andez\"\\n          }\\n        ],\\n        \"abstract\": \"Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methodologies used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting and human evaluation methodology. We applied this framework to analyze over one million comments from two of Reddit\\'s rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.\"\\n      },\\n      {\\n        \"paperId\": \"e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\\n        \"title\": \"Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.13082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TREACLE is proposed, a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\\'s monetary cost and latency constraints, and provides the user with the ability to gracefully trade off accuracy for cost.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2157197701\",\\n            \"name\": \"Xuechen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2297821531\",\\n            \"name\": \"Zijian Huang\"\\n          },\\n          {\\n            \"authorId\": \"2297769735\",\\n            \"name\": \"Ege Onur Taga\"\\n          },\\n          {\\n            \"authorId\": \"1393650147\",\\n            \"name\": \"Carlee Joe-Wong\"\\n          },\\n          {\\n            \"authorId\": \"3103394\",\\n            \"name\": \"Samet Oymak\"\\n          },\\n          {\\n            \"authorId\": \"2281075331\",\\n            \"name\": \"Jiasi Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE ($\\\\\\\\underline{T}$hrifty $\\\\\\\\underline{Rea}$soning via $\\\\\\\\underline{C}$ontext-Aware $\\\\\\\\underline{L}$LM and Prompt S$\\\\\\\\underline{e}$lection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user\\'s monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.\"\\n      },\\n      {\\n        \"paperId\": \"c01bbc439164002f2c7326748f7939783e306a94\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c01bbc439164002f2c7326748f7939783e306a94\",\\n        \"title\": \"A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.09972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that the order of presenting reasons and scores significantly influences LLMs\\' scoring, with a different level of rule understanding in the prompt, crucial for improving the accuracy and consistency of LLM-based evaluations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2264464750\",\\n            \"name\": \"Kuanchao Chu\"\\n          },\\n          {\\n            \"authorId\": \"2109381394\",\\n            \"name\": \"Yi-Pei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2301580436\",\\n            \"name\": \"Hideki Nakayama\"\\n          }\\n        ],\\n        \"abstract\": \"This research investigates prompt designs of evaluating generated texts using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for open-ended text evaluation remains challenging due to model sensitivity and subjectivity in evaluation of text generation. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs\\' scoring, with a different level of rule understanding in the prompt. An additional optimization may enhance scoring alignment if sufficient data is available. This insight is crucial for improving the accuracy and consistency of LLM-based evaluations.\"\\n      },\\n      {\\n        \"paperId\": \"a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\\n        \"title\": \"How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment\",\\n        \"citationCount\": 135,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.14049\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Insight into novice learners\\\\u2019 use of AI code generators in a self-paced learning environment is offered, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3136345\",\\n            \"name\": \"Majeed Kazemitabaar\"\\n          },\\n          {\\n            \"authorId\": \"2112801172\",\\n            \"name\": \"Xinying Hou\"\\n          },\\n          {\\n            \"authorId\": \"2063979470\",\\n            \"name\": \"A. Henley\"\\n          },\\n          {\\n            \"authorId\": \"20937525\",\\n            \"name\": \"B. Ericson\"\\n          },\\n          {\\n            \"authorId\": \"2862077\",\\n            \"name\": \"David Weintrop\"\\n          },\\n          {\\n            \"authorId\": \"2666589\",\\n            \"name\": \"Tovi Grossman\"\\n          }\\n        ],\\n        \"abstract\": \"As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners\\\\u2019 utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners\\\\u2019 use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\\n      },\\n      {\\n        \"paperId\": \"221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\\n        \"title\": \"PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes to comprehensively address all subtasks together, and suggests employing a unique prompt-based in-context learning strategy to enhance error correction and error detection performance in medical systems where prediction errors can have grave consequences.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9122885\",\\n            \"name\": \"Kesav Gundabathula\"\\n          },\\n          {\\n            \"authorId\": \"2301202434\",\\n            \"name\": \"Sriram R Kolar\"\\n          }\\n        ],\\n        \"abstract\": \"This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals. This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it. Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information. We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy. We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge. In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='cdaed8ce-2456-42d3-90da-95687861e4d1')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "graph = workflow.compile()\n",
    "graph.get_graph().draw_mermaid_png(output_file_path='story.png')\n",
    "\n",
    "result_llm = graph.invoke(\n",
    "    {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\":\n",
    "            \"\"\"Dynamic Prompt Adaptation:\n",
    "                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\n",
    "                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\n",
    "                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\n",
    "                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\n",
    "                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\"\"\"\n",
    "        }]\n",
    "    },\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3d43f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_papers = json.loads(result_llm[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9efed2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: dynamic prompt adaptation LLM\n",
      "Query: coherence in extended LLM\n",
      "Query: adaptive prompt generation\n",
      "Query: iterative context update\n",
      "Query: thematic consistency LLM\n"
     ]
    }
   ],
   "source": [
    "for query, papers in list_of_papers.items():\n",
    "    print(f\"Query: {query}\")\n",
    "    # for paper in papers['data']:\n",
    "    #     print(paper['title'], paper['citationCount'],paper['url'], paper['publicationDate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dynamic prompt adaptation LLM\": {\n",
      "    \"total\": 467,\n",
      "    \"offset\": 0,\n",
      "    \"next\": 50,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\n",
      "        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2221081346\",\n",
      "            \"name\": \"Yunbei Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2090707280\",\n",
      "            \"name\": \"Akshay Mehra\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261749247\",\n",
      "            \"name\": \"Shuaicheng Niu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4832622\",\n",
      "            \"name\": \"Jihun Hamm\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"cf95279b1da9de1aad9e7c651f5048f69af295ed\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/cf95279b1da9de1aad9e7c651f5048f69af295ed\",\n",
      "        \"title\": \"AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents\",\n",
      "        \"citationCount\": 76,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.13352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is found that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2175939276\",\n",
      "            \"name\": \"Edoardo Debenedetti\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299061721\",\n",
      "            \"name\": \"Jie Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2138580250\",\n",
      "            \"name\": \"Mislav Balunovi'c\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2150869869\",\n",
      "            \"name\": \"Luca Beurer-Kellner\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307472727\",\n",
      "            \"name\": \"Marc Fischer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267733649\",\n",
      "            \"name\": \"Florian Tramr\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3967189742efab8859da542ce3953d4c72957aca\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3967189742efab8859da542ce3953d4c72957aca\",\n",
      "        \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.23904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI), designed to design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2275054108\",\n",
      "            \"name\": \"Qinqian Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313081973\",\n",
      "            \"name\": \"Bo Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256998291\",\n",
      "            \"name\": \"Robby T. Tan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"659e0b3303caa860348dee52f41476e3fddc9573\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/659e0b3303caa860348dee52f41476e3fddc9573\",\n",
      "        \"title\": \"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference\",\n",
      "        \"citationCount\": 55,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel method is introduced, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages and allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2237800256\",\n",
      "            \"name\": \"Qichen Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237803694\",\n",
      "            \"name\": \"Minsik Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2178316365\",\n",
      "            \"name\": \"Thomas Merth\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256998189\",\n",
      "            \"name\": \"Sachin Mehta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284683934\",\n",
      "            \"name\": \"Mohammad Rastegari\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40465379\",\n",
      "            \"name\": \"Mahyar Najibi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/00c3d6f114bccb41d301bf33d69cf10ccbaaf06a\",\n",
      "        \"title\": \"From ideas to ventures: building entrepreneurship knowledge with LLM, prompt engineering, and conversational agents\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10639-024-12775-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10639-024-12775-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Large Language Models like the Generative Pre-trained Transformer 4 (GPT-4), recognized for their exceptional performance on public datasets, are examined in this study for their potential adaptability and interactivity nature, which align well with the dynamic nature of entrepreneurship learning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1424283019\",\n",
      "            \"name\": \"Marsela Thanasi-Boe\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290624379\",\n",
      "            \"name\": \"Julian Hoxha\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\n",
      "        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\n",
      "        \"citationCount\": 42,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2403.01439\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2238201928\",\n",
      "            \"name\": \"Xin Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"94882716\",\n",
      "            \"name\": \"Dingkang Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284804294\",\n",
      "            \"name\": \"Wei Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284636478\",\n",
      "            \"name\": \"Xingkui Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290029091\",\n",
      "            \"name\": \"Yihan Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287848763\",\n",
      "            \"name\": \"Zhikang Zou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238115894\",\n",
      "            \"name\": \"Xiang Bai\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b67a6181fad5c5838945583ccdc7f39187e29332\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b67a6181fad5c5838945583ccdc7f39187e29332\",\n",
      "        \"title\": \"Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.20911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Mantis is a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations, leading the attacker's LLM to disrupt their own operations or even compromise the attacker's machine.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"50610174\",\n",
      "            \"name\": \"Dario Pasquini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2762279\",\n",
      "            \"name\": \"Evgenios M. Kornaropoulos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1700850\",\n",
      "            \"name\": \"G. Ateniese\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker's LLM to disrupt their own operations (passive defense) or even compromise the attacker's machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker's LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project_mantis\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2f274db9aa447a13c019114e327057d4b161b6d5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/2f274db9aa447a13c019114e327057d4b161b6d5\",\n",
      "        \"title\": \"LLM-controller: Dynamic robot control adaptation using large language models\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.robot.2024.104913?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.robot.2024.104913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-01-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"90182090\",\n",
      "            \"name\": \"Rasoul Zahedifar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1799503\",\n",
      "            \"name\": \"M. Baghshah\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273939584\",\n",
      "            \"name\": \"Alireza Taheri\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\n",
      "        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1485402830\",\n",
      "            \"name\": \"Xin Jin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40093162\",\n",
      "            \"name\": \"Cuiling Lan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1634494276\",\n",
      "            \"name\": \"Wenjun Zeng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31482866\",\n",
      "            \"name\": \"Zhibo Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415\",\n",
      "        \"title\": \"HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.19878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE is proposed, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-09-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2211429378\",\n",
      "            \"name\": \"Bingshen Mu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299944267\",\n",
      "            \"name\": \"Kun Wei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2061559378\",\n",
      "            \"name\": \"Qijie Shao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2323714781\",\n",
      "            \"name\": \"Yong Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249732546\",\n",
      "            \"name\": \"Lei Xie\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named HDMoLE, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixture of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bcac614f9774488447221ebb4f16f05e3975ec1e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/bcac614f9774488447221ebb4f16f05e3975ec1e\",\n",
      "        \"title\": \"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\",\n",
      "        \"citationCount\": 75,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2309.04669\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A well-designed visual tokenizer is introduced to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read, which empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-09\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2239056878\",\n",
      "            \"name\": \"Yang Jin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266735757\",\n",
      "            \"name\": \"Kun Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266735757\",\n",
      "            \"name\": \"Kun Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266419021\",\n",
      "            \"name\": \"Liwei Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239059653\",\n",
      "            \"name\": \"Chao Liao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239091862\",\n",
      "            \"name\": \"Jianchao Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2007771781\",\n",
      "            \"name\": \"Quzhe Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2230906921\",\n",
      "            \"name\": \"Bin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366079231\",\n",
      "            \"name\": \"Chenyi Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239069665\",\n",
      "            \"name\": \"An Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241686105\",\n",
      "            \"name\": \"Chengru Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238955477\",\n",
      "            \"name\": \"Xiaoqiang Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2228125963\",\n",
      "            \"name\": \"Di Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238953778\",\n",
      "            \"name\": \"Wenwu Ou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238953242\",\n",
      "            \"name\": \"Kun Gai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238953689\",\n",
      "            \"name\": \"Yadong Mu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\n",
      "        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2284692979\",\n",
      "            \"name\": \"Guangya Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284699718\",\n",
      "            \"name\": \"Yuqi Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317120126\",\n",
      "            \"name\": \"Jie Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316974967\",\n",
      "            \"name\": \"Sheng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"58700f3740105e3422eb030305372b6d8bc44986\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/58700f3740105e3422eb030305372b6d8bc44986\",\n",
      "        \"title\": \"Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours, and can serve as an orthogonal optimization for synergistic integration with existing speculative decoding.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2242179319\",\n",
      "            \"name\": \"H. Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2303652170\",\n",
      "            \"name\": \"Wayne Luk\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301711440\",\n",
      "            \"name\": \"Ka-Fai Cedric Yiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152153064\",\n",
      "            \"name\": \"Rui Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2303652428\",\n",
      "            \"name\": \"Konstantin Mishchenko\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115955596\",\n",
      "            \"name\": \"Stylianos I. Venieris\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"10001427\",\n",
      "            \"name\": \"Hongxiang Fan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\\\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"427c3a502d132b6e1cea2d5565460d284db6e3f7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/427c3a502d132b6e1cea2d5565460d284db6e3f7\",\n",
      "        \"title\": \"Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2507.00601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies and introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation is proposed.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-06-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2345186311\",\n",
      "            \"name\": \"Shuangquan Lyu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2353085449\",\n",
      "            \"name\": \"Yingnan Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2372425942\",\n",
      "            \"name\": \"Guiran Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2374351029\",\n",
      "            \"name\": \"Zhen Qi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2372322790\",\n",
      "            \"name\": \"Ruotong Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model's original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method's applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c0e24f98323c7114b9229ac17b8b63581e3e5914\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c0e24f98323c7114b9229ac17b8b63581e3e5914\",\n",
      "        \"title\": \"Dynamic and Adaptive Feature Generation with LLM\",\n",
      "        \"citationCount\": 30,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This research introduces a novel approach adopting large language models and feature-generating prompts to address challenges of explainability, applicability, and inflexible strategy and proposes a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2130031226\",\n",
      "            \"name\": \"XinHao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108045855\",\n",
      "            \"name\": \"Jinghan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1966492\",\n",
      "            \"name\": \"Banafsheh Rekabdar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2145108199\",\n",
      "            \"name\": \"Yuanchun Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301248160\",\n",
      "            \"name\": \"Pengfei Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293571072\",\n",
      "            \"name\": \"Kunpeng Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/4607a529dfb8b64a5767e53fd482bfccd23cfc20\",\n",
      "        \"title\": \"Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.04295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process and demonstrates measurable performance improvements compared to content-only optimization methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-02-06\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2344555074\",\n",
      "            \"name\": \"Yuanye Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257094139\",\n",
      "            \"name\": \"Jiahang Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274195530\",\n",
      "            \"name\": \"L. Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344193091\",\n",
      "            \"name\": \"Qi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2341721557\",\n",
      "            \"name\": \"Xuan Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344520491\",\n",
      "            \"name\": \"Yang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2339241318\",\n",
      "            \"name\": \"Zhongxin Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344097630\",\n",
      "            \"name\": \"Yuqing Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2296029993\",\n",
      "            \"name\": \"Peng Cheng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\n",
      "        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\n",
      "        \"citationCount\": 130,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2212.04145\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-12-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2142286278\",\n",
      "            \"name\": \"Yulu Gan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1387903470\",\n",
      "            \"name\": \"Xianzheng Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9920529\",\n",
      "            \"name\": \"Yihang Lou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145079192\",\n",
      "            \"name\": \"Yan Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115713503\",\n",
      "            \"name\": \"Renrui Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2194622137\",\n",
      "            \"name\": \"Nian Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2194260623\",\n",
      "            \"name\": \"Lin Luo\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e3e3aa5576de899b755100db211501bb405aba3e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e3e3aa5576de899b755100db211501bb405aba3e\",\n",
      "        \"title\": \"Firewalls to Secure Dynamic LLM Agentic Networks\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.01822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work identifies required properties for agent communication: proactivity, adaptability, privacy, privacy, and security, and proposes a practical design and protocol inspired by network security principles.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-02-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2343752567\",\n",
      "            \"name\": \"Sahar Abdelnabi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249532110\",\n",
      "            \"name\": \"Amr Gomaa\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"36103467\",\n",
      "            \"name\": \"Eugene Bagdasarian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237674591\",\n",
      "            \"name\": \"P. O. Kristensson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2346834097\",\n",
      "            \"name\": \"Reza Shokri\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ee552989a03693a441863af4c29dc594bfcd1ab5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ee552989a03693a441863af4c29dc594bfcd1ab5\",\n",
      "        \"title\": \"AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration\",\n",
      "        \"citationCount\": 21,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2503.18891?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2503.18891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": null\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2316787389\",\n",
      "            \"name\": \"Zhexuan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2306069252\",\n",
      "            \"name\": \"Yutong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256344322\",\n",
      "            \"name\": \"Xuebo Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46573238\",\n",
      "            \"name\": \"Liang Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2187384924\",\n",
      "            \"name\": \"Miao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348727938\",\n",
      "            \"name\": \"Jie Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2346352158\",\n",
      "            \"name\": \"Min Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout , which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\n",
      "        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2229108213\",\n",
      "            \"name\": \"Ding-Chu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2149135777\",\n",
      "            \"name\": \"Zhi Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276808245\",\n",
      "            \"name\": \"Yufeng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"273b2c64d675edd522cd6f679891756ad5207296\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/273b2c64d675edd522cd6f679891756ad5207296\",\n",
      "        \"title\": \"Exploring the Effectiveness of LLM Domain Adaptation for Business IT Machine Translation\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/2024.eamt-1.51, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is observed that while LLMs can translate on-par with SAPs MT models on general domain data, it is difficult to close the gap on SAPs domain-specific data, even with extensive training and carefully curated data.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2322393313\",\n",
      "            \"name\": \"Johannes Eschbach-Dymanus\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322400027\",\n",
      "            \"name\": \"Frank Essenberger\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1403814959\",\n",
      "            \"name\": \"Bianka Buschbeck-Wolf\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"70124681\",\n",
      "            \"name\": \"Miriam Exel\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7738e909d563d84fbd4ab5cb6aacf62c84fe2ab9\",\n",
      "        \"title\": \"ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities\",\n",
      "        \"citationCount\": 81,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is shown that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2283831590\",\n",
      "            \"name\": \"Jiarui Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315811087\",\n",
      "            \"name\": \"Thomas Holleis\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313695880\",\n",
      "            \"name\": \"Yizhe Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315810003\",\n",
      "            \"name\": \"Bernhard Aumayer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313640225\",\n",
      "            \"name\": \"Feng Nan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313910532\",\n",
      "            \"name\": \"Felix Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313694040\",\n",
      "            \"name\": \"Shuang Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313694042\",\n",
      "            \"name\": \"Shen Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315946702\",\n",
      "            \"name\": \"Mengyu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293171017\",\n",
      "            \"name\": \"Guoli Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313671930\",\n",
      "            \"name\": \"Zirui Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238621132\",\n",
      "            \"name\": \"Ruoming Pang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at https://github.com/apple/ToolSandbox\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"44b0d2e884efa5344e50424dbe2edf616981f201\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/44b0d2e884efa5344e50424dbe2edf616981f201\",\n",
      "        \"title\": \"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers\",\n",
      "        \"citationCount\": 55,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2303.00807\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.00807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work develops and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply and achieves substantially lower latency than standard reranking methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-03-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2127522115\",\n",
      "            \"name\": \"Jon Saad-Falcon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144112155\",\n",
      "            \"name\": \"O. Khattab\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50818255\",\n",
      "            \"name\": \"Keshav Santhanam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1707117\",\n",
      "            \"name\": \"Radu Florian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39038065\",\n",
      "            \"name\": \"M. Franz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1781292\",\n",
      "            \"name\": \"S. Roukos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2707234\",\n",
      "            \"name\": \"Avirup Sil\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2937809\",\n",
      "            \"name\": \"Md Arafat Sultan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144922861\",\n",
      "            \"name\": \"Christopher Potts\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9803d83bbb28d02fb01f00e0e05aa3c192a87255\",\n",
      "        \"title\": \"MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\",\n",
      "        \"citationCount\": 212,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.02490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing, is introduced, which effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-02\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2181120463\",\n",
      "            \"name\": \"Huiqiang Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1527099159\",\n",
      "            \"name\": \"Yucheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284970741\",\n",
      "            \"name\": \"Chengruidong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108728536\",\n",
      "            \"name\": \"Qianhui Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"13289447\",\n",
      "            \"name\": \"Xufang Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309738728\",\n",
      "            \"name\": \"Surin Ahn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2281867465\",\n",
      "            \"name\": \"Zhenhua Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309244780\",\n",
      "            \"name\": \"Amir H. Abdi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305587638\",\n",
      "            \"name\": \"Dongsheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257359863\",\n",
      "            \"name\": \"Chin-Yew Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2125051198\",\n",
      "            \"name\": \"Yuqing Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2160727304\",\n",
      "            \"name\": \"Lili Qiu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/34eedbb011e45d80045cadebaf1d01b2ddec22a1\",\n",
      "        \"title\": \"GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting\",\n",
      "        \"citationCount\": 91,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/30383/32447\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i21.30383?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i21.30383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously simultaneously, named GPT4MTS, and proposes a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2256985863\",\n",
      "            \"name\": \"Furong Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293923697\",\n",
      "            \"name\": \"Kevin Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257061490\",\n",
      "            \"name\": \"Yixiang Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"120783624\",\n",
      "            \"name\": \"Defu Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260272787\",\n",
      "            \"name\": \"Yan Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c395bb7b22e6153831fb6a81b0c0304dc7bd94f8\",\n",
      "        \"title\": \"Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in Zero-shot Anomaly Detection\",\n",
      "        \"citationCount\": 35,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.09654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM), enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2108678212\",\n",
      "            \"name\": \"Jiaqi Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51459472\",\n",
      "            \"name\": \"Shaofeng Cai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276607267\",\n",
      "            \"name\": \"Fang Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2188240935\",\n",
      "            \"name\": \"Bengchin Ooi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2296743990\",\n",
      "            \"name\": \"Junran Wu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\n",
      "        \"title\": \"FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.15813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-11-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2247164321\",\n",
      "            \"name\": \"Yu Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2948393\",\n",
      "            \"name\": \"Linchao Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3446334\",\n",
      "            \"name\": \"Hehe Fan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257587812\",\n",
      "            \"name\": \"Yi Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"162f33c7799683ca9b0f193275fe7eec5a0b973f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/162f33c7799683ca9b0f193275fe7eec5a0b973f\",\n",
      "        \"title\": \"EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.09618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"EasyRef is introduced, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt and surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1571400317\",\n",
      "            \"name\": \"Zhuofan Zong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293242031\",\n",
      "            \"name\": \"Dongzhi Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261489892\",\n",
      "            \"name\": \"Bingqi Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"12920342\",\n",
      "            \"name\": \"Guanglu Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2075457131\",\n",
      "            \"name\": \"Hao Shao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292263397\",\n",
      "            \"name\": \"Dazhong Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292207974\",\n",
      "            \"name\": \"Yu Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261394248\",\n",
      "            \"name\": \"Hongsheng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although the tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent elements within multiple images through the training process, it necessitates specific finetuning for each distinct image group. This paper introduces EasyRef, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and the text prompt. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM's representations into the diffusion process through adapters can easily generalize to unseen domains, mining the consistent visual elements within unseen data. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ca4f0d2c85cfe46b97ec42b38decda107780769d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ca4f0d2c85cfe46b97ec42b38decda107780769d\",\n",
      "        \"title\": \"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.10462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"DyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation, is proposed and experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2241468593\",\n",
      "            \"name\": \"Hossein Rajabzadeh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9200111\",\n",
      "            \"name\": \"Mojtaba Valipour\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284643707\",\n",
      "            \"name\": \"Tianshu Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1996315\",\n",
      "            \"name\": \"Marzieh S. Tahaei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241480742\",\n",
      "            \"name\": \"Hyock Ju Kwon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237425782\",\n",
      "            \"name\": \"Ali Ghodsi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237517964\",\n",
      "            \"name\": \"Boxing Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2066076226\",\n",
      "            \"name\": \"Mehdi Rezagholizadeh\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9a73effed8775962c86587feb0f9ef841fa2ff4c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9a73effed8775962c86587feb0f9ef841fa2ff4c\",\n",
      "        \"title\": \"LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2258550477\",\n",
      "            \"name\": \"Yash Shukla\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258751660\",\n",
      "            \"name\": \"Wenchang Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3379438\",\n",
      "            \"name\": \"Vasanth Sarathy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258715054\",\n",
      "            \"name\": \"Alvaro Velasquez\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258551993\",\n",
      "            \"name\": \"Robert Wright\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1715858\",\n",
      "            \"name\": \"Jivko Sinapov\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a propreitary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the LLM proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692\",\n",
      "        \"title\": \"TimeRAG: Boosting LLM Time Series Forecasting via Retrieval-Augmented Generation\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.16643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"TimeRAG is proposed, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW).\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2206558646\",\n",
      "            \"name\": \"Si-Nan Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337389407\",\n",
      "            \"name\": \"Dong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336916957\",\n",
      "            \"name\": \"Haoqi Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336884221\",\n",
      "            \"name\": \"Ruochun Jin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1e6325865e809670765bea9dadd3c40b2014eb6d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1e6325865e809670765bea9dadd3c40b2014eb6d\",\n",
      "        \"title\": \"VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The performance contribution of different prompt components is explored, the vision for future improvement in visual accessibility, and the way for LLMs in video anomaly detection and vision-language understanding is paved.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2256768778\",\n",
      "            \"name\": \"Hao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292146470\",\n",
      "            \"name\": \"Jiayou Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260725391\",\n",
      "            \"name\": \"Ashish Bastola\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2024833342\",\n",
      "            \"name\": \"Xiwen Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292183597\",\n",
      "            \"name\": \"John Suchanek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292143727\",\n",
      "            \"name\": \"Zihao Gong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064311884\",\n",
      "            \"name\": \"Abolfazl Razi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7f96bb27a8fca35b1f7d02ee319a64be04114809\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7f96bb27a8fca35b1f7d02ee319a64be04114809\",\n",
      "        \"title\": \"LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments\",\n",
      "        \"citationCount\": 26,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.08337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties, and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-13\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2265927745\",\n",
      "            \"name\": \"Maonan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2219695355\",\n",
      "            \"name\": \"Aoyu Pang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7592365\",\n",
      "            \"name\": \"Yuheng Kan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144305489\",\n",
      "            \"name\": \"Man-On Pun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292117616\",\n",
      "            \"name\": \"Chung Shue Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291077490\",\n",
      "            \"name\": \"Bo Huang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"650a24da1702beca7eb70011a26f1f3238efad4b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/650a24da1702beca7eb70011a26f1f3238efad4b\",\n",
      "        \"title\": \"Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.acl-srw.90?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.acl-srw.90, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The proposed Time-LlaMA framework achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2346152667\",\n",
      "            \"name\": \"Juyuan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334602368\",\n",
      "            \"name\": \"Jiechao Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2374971695\",\n",
      "            \"name\": \"Wenwen Ouyang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334718086\",\n",
      "            \"name\": \"Wei Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2321408906\",\n",
      "            \"name\": \"Hui Yi Leong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the models predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\n",
      "        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\n",
      "        \"citationCount\": 31,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2163833353\",\n",
      "            \"name\": \"Jiapu Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2220259324\",\n",
      "            \"name\": \"Kai Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238130759\",\n",
      "            \"name\": \"Linhao Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302892265\",\n",
      "            \"name\": \"Wei Wei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2140879677\",\n",
      "            \"name\": \"Yongli Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243282363\",\n",
      "            \"name\": \"Alan Wee-Chung Liew\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294378361\",\n",
      "            \"name\": \"Shirui Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239088112\",\n",
      "            \"name\": \"Baocai Yin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\n",
      "        \"title\": \"Domain Adaptation via Prompt Learning\",\n",
      "        \"citationCount\": 212,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2202.06687\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-02-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2130368185\",\n",
      "            \"name\": \"Chunjiang Ge\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Rui Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112817811\",\n",
      "            \"name\": \"Mixue Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51451501\",\n",
      "            \"name\": \"Zihang Lai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1760750\",\n",
      "            \"name\": \"Shiji Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Shuang Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115218570\",\n",
      "            \"name\": \"Gao Huang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b7f46c9f01d9f649d18b709e2e88b3b97bebd016\",\n",
      "        \"title\": \"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair\",\n",
      "        \"citationCount\": 188,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2176865575\",\n",
      "            \"name\": \"Islem Bouzenia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"114875459\",\n",
      "            \"name\": \"Prem Devanbu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260683361\",\n",
      "            \"name\": \"Michael Pradel\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27\",\n",
      "        \"title\": \"Token-Budget-Aware LLM Reasoning\",\n",
      "        \"citationCount\": 114,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem is proposed, offering a practical solution to balance efficiency and accuracy in LLM reasoning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2170360833\",\n",
      "            \"name\": \"Tingxu Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2154723145\",\n",
      "            \"name\": \"Zhenting Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239197945\",\n",
      "            \"name\": \"Chunrong Fang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110773055\",\n",
      "            \"name\": \"Shiyun Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2333472479\",\n",
      "            \"name\": \"Shiqing Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238950128\",\n",
      "            \"name\": \"Zhenyu Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"20843eaa59db5e2af416d7db47d51d0aab3de230\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/20843eaa59db5e2af416d7db47d51d0aab3de230\",\n",
      "        \"title\": \"Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue Resolution\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.11709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper analyzes 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution and identifies key textual and code-related heuristicsSpecificity, Contextual Richness, and Claritythat are associated with successful issue closure and help assess prompt quality.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-01-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2147154362\",\n",
      "            \"name\": \"Ramtin Ehsani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2341336471\",\n",
      "            \"name\": \"Sakshi Pathak\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9728244\",\n",
      "            \"name\": \"Preetha Chatterjee\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) have become essential in software development, especially for issue resolution. However, despite their widespread use, significant challenges persist in the quality of LLM responses to issue resolution queries. LLM interactions often yield incorrect, incomplete, or ambiguous information, largely due to knowledge gaps in prompt design, which can lead to unproductive exchanges and reduced developer productivity.In this paper, we analyze 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution. We identify four main knowledge gaps in developer prompts: Missing Context, Missing Specifications, Multiple Context, and Unclear Instructions. Assuming that conversations within closed issues contributed to successful resolutions while those in open issues did not, we find that ineffective conversations contain knowledge gaps in $44.6 \\\\%$ of prompts, compared to only $12.6 \\\\%$ in effective ones. Additionally, we observe seven distinct conversational styles, with Directive Prompting, Chain of Thought, and Responsive Feedback being the most prevalent. We find that knowledge gaps are present in all styles of conversations, with Missing Context being the most repeated challenge developers face in issue-resolution conversations.Based on our analysis, we identify key textual and code-related heuristicsSpecificity, Contextual Richness, and Claritythat are associated with successful issue closure and help assess prompt quality. These heuristics lay the foundation for an automated tool that can dynamically flag unclear prompts and suggest structured improvements. To test feasibility, we developed a lightweight browser extension prototype for detecting prompt gaps, that can be easily adapted to other tools within developer workflows.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"25cee84e3a1541697a7c97443d7526574127c344\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/25cee84e3a1541697a7c97443d7526574127c344\",\n",
      "        \"title\": \"Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\",\n",
      "        \"citationCount\": 158,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.00367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs, and proposes two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2114887261\",\n",
      "            \"name\": \"Shangbin Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254168375\",\n",
      "            \"name\": \"Weijia Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108853330\",\n",
      "            \"name\": \"Yike Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282214127\",\n",
      "            \"name\": \"Wenxuan Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143820870\",\n",
      "            \"name\": \"Vidhisha Balachandran\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249583325\",\n",
      "            \"name\": \"Yulia Tsvetkov\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d5342fce96175f83550cfae471a0a46d16401481\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d5342fce96175f83550cfae471a0a46d16401481\",\n",
      "        \"title\": \"ST-LLM: Large Language Models Are Effective Temporal Learners\",\n",
      "        \"citationCount\": 120,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM, and develops a dynamic masking strategy with tailor-made training objectives to address the overhead and stability issues introduced by uncompressed video tokens within LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"12287885\",\n",
      "            \"name\": \"Ruyang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256784925\",\n",
      "            \"name\": \"Chen Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294629231\",\n",
      "            \"name\": \"Haoran Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152988335\",\n",
      "            \"name\": \"Yixiao Ge\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265579883\",\n",
      "            \"name\": \"Ying Shan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294517847\",\n",
      "            \"name\": \"Ge Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at https://github.com/TencentARC/ST-LLM.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/12fbc6c8234f9d747e11bd95a88aae8eb5885dd3\",\n",
      "        \"title\": \"Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.17282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with, and performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-02-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2218970025\",\n",
      "            \"name\": \"Yi-Kai Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274937712\",\n",
      "            \"name\": \"De-Chuan Zhan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2151459740\",\n",
      "            \"name\": \"Han-Jia Ye\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"edfff0e15449f438a13a7341290c008bf6486afc\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/edfff0e15449f438a13a7341290c008bf6486afc\",\n",
      "        \"title\": \"MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning\",\n",
      "        \"citationCount\": 34,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.18035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism, which significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2327293591\",\n",
      "            \"name\": \"Jingfan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322997112\",\n",
      "            \"name\": \"Yi Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327694040\",\n",
      "            \"name\": \"Dan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293390999\",\n",
      "            \"name\": \"Xing Tian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2179528564\",\n",
      "            \"name\": \"Huanran Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322888603\",\n",
      "            \"name\": \"Wei Zhu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\n",
      "        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\n",
      "        \"citationCount\": 203,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2310.04948\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"120783624\",\n",
      "            \"name\": \"Defu Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256985863\",\n",
      "            \"name\": \"Furong Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2676352\",\n",
      "            \"name\": \"Sercan . Arik\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1945962\",\n",
      "            \"name\": \"Tomas Pfister\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257061490\",\n",
      "            \"name\": \"Yixiang Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256992266\",\n",
      "            \"name\": \"Wen Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257088730\",\n",
      "            \"name\": \"Yan Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ab4800a924508f49d644ced8ba236ec92f54f566\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ab4800a924508f49d644ced8ba236ec92f54f566\",\n",
      "        \"title\": \"Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2505.16552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach and enhances CoLaR through reinforcement learning (RL) that leverages the latent head's non-deterministic nature to explore diverse reasoning paths and exploit more compact ones.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-05-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2218557953\",\n",
      "            \"name\": \"Wenhui Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2362865102\",\n",
      "            \"name\": \"Jiaze Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317982861\",\n",
      "            \"name\": \"Jianzhong Ju\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2363405807\",\n",
      "            \"name\": \"Zhenbo Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317980688\",\n",
      "            \"name\": \"Jian Luan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290923147\",\n",
      "            \"name\": \"Ruihua Song\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head's non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"268e28f8d5235031dcd7bfae0f857439e27e8564\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/268e28f8d5235031dcd7bfae0f857439e27e8564\",\n",
      "        \"title\": \"SteP: Stacked LLM Policies for Web Actions\",\n",
      "        \"citationCount\": 44,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.03720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Stacked LLM Policies for Web Actions is proposed, an approach to dynamically compose policies to solve a diverse set of web tasks and is competitive with prior works while using significantly less data.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2023-10-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2254267756\",\n",
      "            \"name\": \"Paloma Sodhi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1741598\",\n",
      "            \"name\": \"S. Branavan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2066324938\",\n",
      "            \"name\": \"Yoav Artzi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254260284\",\n",
      "            \"name\": \"Ryan McDonald\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Performing tasks on the web presents fundamental challenges to large language models (LLMs), including combinatorially large open-world tasks and variations across web interfaces. Simply specifying a large prompt to handle all possible behaviors and states is extremely complex, and results in behavior leaks between unrelated behaviors. Decomposition to distinct policies can address this challenge, but requires carefully handing off control between policies. We propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically compose policies to solve a diverse set of web tasks. SteP defines a Markov Decision Process where the state is a stack of policies representing the control state, i.e., the chain of policy calls. Unlike traditional methods that are restricted to static hierarchies, SteP enables dynamic control that adapts to the complexity of the task. We evaluate SteP against multiple baselines and web environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP improves (14.9\\\\% to 33.5\\\\%) over SOTA that use GPT-4 policies, while on MiniWob++, SteP is competitive with prior works while using significantly less data. Our code and data are available at https://asappresearch.github.io/webagents-step.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1f9822022f586e375461660db792f23e891c7123\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1f9822022f586e375461660db792f23e891c7123\",\n",
      "        \"title\": \"Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems\",\n",
      "        \"citationCount\": 62,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2307.06187\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.06187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models, grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-07-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2658311\",\n",
      "            \"name\": \"N. Nascimento\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40761174\",\n",
      "            \"name\": \"Paulo Alencar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2149928782\",\n",
      "            \"name\": \"Donald D. Cowan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs' capabilities and indicating further research opportunities to assess LLMs' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3817cb7d991f3418c4fa00bbd1189fd4c44f1d73\",\n",
      "        \"title\": \"Bridging the Gulf of Envisioning: Cognitive Challenges in Prompt Based Interactions with LLMs\",\n",
      "        \"citationCount\": 91,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642754\",\n",
      "          \"status\": \"BRONZE\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work theorizes how end-users envision translating their goals into clear intentions and craft prompts to obtain the desired LLM response.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2301051721\",\n",
      "            \"name\": \"Hariharan Subramonyam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2246886383\",\n",
      "            \"name\": \"Roy D. Pea\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1825757380\",\n",
      "            \"name\": \"Christopher Pondoc\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1820412\",\n",
      "            \"name\": \"Maneesh Agrawala\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289103973\",\n",
      "            \"name\": \"Colleen M. Seifert\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Normans gulfs of execution and evaluation. To address this gap, we theorize how end-users envision translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments on not knowing: (1) what the task should be, (2) how to instruct the LLM to do the task, and (3) what to expect for the LLMs output in meeting the goal. Finally, we make recommendations to narrow the gulf of envisioning in human-LLM interactions.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"28d6411019f448f54834c2a5cff723cd350345b5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/28d6411019f448f54834c2a5cff723cd350345b5\",\n",
      "        \"title\": \"Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration\",\n",
      "        \"citationCount\": 60,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC) involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2289252454\",\n",
      "            \"name\": \"Haokun Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"8247318\",\n",
      "            \"name\": \"Yaonan Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110285301\",\n",
      "            \"name\": \"Kenji Kato\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307380233\",\n",
      "            \"name\": \"Atsushi Tsukahara\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282115338\",\n",
      "            \"name\": \"Izumi Kondo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1752849\",\n",
      "            \"name\": \"T. Aoyama\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237520520\",\n",
      "            \"name\": \"Yasuhisa Hasegawa\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This letter proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"77a9c310df0d7896d297da90fc4a1131819c341e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/77a9c310df0d7896d297da90fc4a1131819c341e\",\n",
      "        \"title\": \"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models\",\n",
      "        \"citationCount\": 45,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding, and proposes LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2293615241\",\n",
      "            \"name\": \"Mingxing Peng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293665950\",\n",
      "            \"name\": \"Xusen Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146413818\",\n",
      "            \"name\": \"Xianda Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241024418\",\n",
      "            \"name\": \"Meixin Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267078966\",\n",
      "            \"name\": \"Kehua Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293775145\",\n",
      "            \"name\": \"Hao Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258778041\",\n",
      "            \"name\": \"Xuesong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258755636\",\n",
      "            \"name\": \"Yinhai Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"coherence in extended LLM\": {\n",
      "    \"total\": 23,\n",
      "    \"offset\": 0,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"d0da372b4b6f422e13556ce272595a0c9002fe90\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d0da372b4b6f422e13556ce272595a0c9002fe90\",\n",
      "        \"title\": \"GPT-4 in Education: Evaluating Aptness, Reliability, and Loss of Coherence in Solving Calculus Problems and Grading Submissions\",\n",
      "        \"citationCount\": 21,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://link.springer.com/content/pdf/10.1007/s40593-024-00403-3.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40593-024-00403-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40593-024-00403-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2256969876\",\n",
      "            \"name\": \"Alberto Gandolfi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we initially investigate the capabilities of GPT-3 5 and GPT-4 in solving college-level calculus problems, an essential segment of mathematics that remains under-explored so far. Although improving upon earlier versions, GPT-4 attains approximately 65% accuracy for standard problems and decreases to 20% for competition-like scenarios. Overall, the models prove to be unreliable due to common arithmetic errors. Our primary contribution lies then in examining the use of ChatGPT for grading solutions to calculus exercises. Our objectives are to probe an in-context learning task with less emphasis over direct calculations; recognize positive applications of ChatGPT in educational contexts; highlight a potentially emerging facet of AI that could necessitate oversight; and introduce unconventional AI benchmarks, for which models like GPT are untrained. Pertaining to the latter, we uncover a tendency for loss of coherence in extended contexts. Our findings suggest that while the current ChatGPT exhibits comprehension of the grading task and often provides relevant outputs, the consistency of grading is marred by occasional loss of coherence and hallucinations. Intriguingly, GPT-4's overall scores, delivered in mere moments, align closely with human graders, although its detailed accuracy remains suboptimal. This work suggests that, when appropriately orchestrated, collaboration between human graders and LLMs like GPT-4 might combine their unique strengths while mitigating their respective shortcomings In this direction, it is imperative to consider implementing transparency, fairness, and appropriate regulations in the near future.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"49b7bd275a0386392769f5b33028500754dbc69d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/49b7bd275a0386392769f5b33028500754dbc69d\",\n",
      "        \"title\": \"AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE), and reveals that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2317040111\",\n",
      "            \"name\": \"Abhay Gupta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317010916\",\n",
      "            \"name\": \"Philip Meng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317007185\",\n",
      "            \"name\": \"Ece Yurtseven\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241351144\",\n",
      "            \"name\": \"Sean O'Brien\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2312105716\",\n",
      "            \"name\": \"Kevin Zhu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Detecting biases in natural language understanding (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language processing (NLP) systems. To address dialect-induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE). AAVENUE builds upon and extends existing benchmarks like VALUE, replacing deterministic syntactic and morphological transformations with a more flexible methodology leveraging LLM-based translation with few-shot prompting, improving performance across our evaluation metrics when translating key tasks from the GLUE and SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics including fluency, BARTScore, quality, coherence, and understandability. Additionally, we recruit fluent AAVE speakers to validate our translations for authenticity. Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e5968ff7af575e014a4cb76a75f1f0f4378d16d7\",\n",
      "        \"title\": \"DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM\",\n",
      "        \"citationCount\": 30,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://aclanthology.org/2023.findings-emnlp.606.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study introduces a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME), which proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2110546424\",\n",
      "            \"name\": \"Weijie Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261645232\",\n",
      "            \"name\": \"Wenxiang Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261413304\",\n",
      "            \"name\": \"Fanyou Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1757518\",\n",
      "            \"name\": \"Srinivasan H. Sengamedu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME's potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0b04776792c83ec1bf4b83078c4b7618d85bc76e\",\n",
      "        \"title\": \"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.04650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An evaluation framework for mental health chatbots is validated, proving its effectiveness in improving safety and reliability and highlighting the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-08-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2315914561\",\n",
      "            \"name\": \"Jung In Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2182148069\",\n",
      "            \"name\": \"Mahyar Abbasian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2241201441\",\n",
      "            \"name\": \"Iman Azimi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315810363\",\n",
      "            \"name\": \"Dawn Bounds\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315810053\",\n",
      "            \"name\": \"Angela Jun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315889398\",\n",
      "            \"name\": \"Jaesu Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315811390\",\n",
      "            \"name\": \"Robert McCarron\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297708916\",\n",
      "            \"name\": \"Jessica Borelli\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348273518\",\n",
      "            \"name\": \"Parmida Safavi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348305728\",\n",
      "            \"name\": \"Sanaz Mirbaha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315875066\",\n",
      "            \"name\": \"Jia Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315811652\",\n",
      "            \"name\": \"Mona Mahmoudi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315811354\",\n",
      "            \"name\": \"Carmen Wiedenhoeft\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2311169857\",\n",
      "            \"name\": \"Amir M. Rahmani\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Materials and Methods: We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. Results: The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Discussion: Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. Conclusion: The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/09cfbc5219c1ddfebcd2e14d1779cc909e3d8b49\",\n",
      "        \"title\": \"Asynchronous LLM Function Calling\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.07017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An in-context protocol for function calls and interrupts is designed, an fine-tuning strategy is provided to adapt LLMs to the interrupt semantics, and mechanisms to asynchronously notify the LLM in-flight when function calls return are implemented.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-09\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2265756791\",\n",
      "            \"name\": \"In Gim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118065368\",\n",
      "            \"name\": \"Seung-seob Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2323908057\",\n",
      "            \"name\": \"Lin Zhong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM's operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call's completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3cb98ad3293f1e8709aa5f52a1800e2eb07e34eb\",\n",
      "        \"title\": \"Prediction of neovascular age-related macular degeneration recurrence using optical coherence tomography images with a deep neural network\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://www.nature.com/articles/s41598-024-56309-6.pdf\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10925587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is suggested that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies, and demonstrating a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2260831947\",\n",
      "            \"name\": \"Juho Jung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152569226\",\n",
      "            \"name\": \"Jinyoung Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6777367\",\n",
      "            \"name\": \"J. Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2088247339\",\n",
      "            \"name\": \"Junseo Ko\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1677558107\",\n",
      "            \"name\": \"Jeewoo Yoon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39548326\",\n",
      "            \"name\": \"J. Hwang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265721342\",\n",
      "            \"name\": \"Ji In Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2054436718\",\n",
      "            \"name\": \"Gyudeok Hwang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290973453\",\n",
      "            \"name\": \"Jae Ho Jung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2003794778\",\n",
      "            \"name\": \"Daniel Duck-Jin Hwang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Neovascular age-related macular degeneration (nAMD) can result in blindness if left untreated, and patients often require repeated anti-vascular endothelial growth factor injections. Although, the treat-and-extend method is becoming popular to reduce vision loss attributed to recurrence, it may pose a risk of overtreatment. This study aimed to develop a deep learning model based on DenseNet201 to predict nAMD recurrence within 3 months after confirming dry-up 1 month following three loading injections in treatment-nave patients. A dataset of 1076 spectral domain optical coherence tomography (OCT) images from 269 patients diagnosed with nAMD was used. The performance of the model was compared with that of 6 ophthalmologists, using 100 randomly selected samples. The DenseNet201-based model achieved 53.0% accuracy in predicting nAMD recurrence using a single pre-injection image and 60.2% accuracy after viewing all the images immediately after the 1st, 2nd, and 3rd injections. The model outperformed experienced ophthalmologists, with an average accuracy of 52.17% using a single pre-injection image and 53.3% after examining four images before and after three loading injections. In conclusion, the artificial intelligence model demonstrated a promising ability to predict nAMD recurrence using OCT images and outperformed experienced ophthalmologists. These findings suggest that deep learning models can assist in nAMD recurrence prediction, thus improving patient outcomes and optimizing treatment strategies.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b2fe504e9f15de9438d22e7b632e81e57bfbbc06\",\n",
      "        \"title\": \"Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning\",\n",
      "        \"citationCount\": 42,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.03275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a hybrid representation of the reasoning process, where the initial reasoning steps are partially abstracted away using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-02-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2325888815\",\n",
      "            \"name\": \"DiJia Su\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2255310892\",\n",
      "            \"name\": \"Hanlin Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269737738\",\n",
      "            \"name\": \"Yingchen Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258657022\",\n",
      "            \"name\": \"Jiantao Jiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2285362895\",\n",
      "            \"name\": \"Yuandong Tian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326106870\",\n",
      "            \"name\": \"Qinqing Zheng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/27fc2de05f7910e563a11f7b4b6f73a6ed4cc9b1\",\n",
      "        \"title\": \"13.4 A 48GB 16-High 1280GB/s HBM3E DRAM with All-Around Power TSV and a 6-Phase RDQS Scheme for TSV Area Optimization\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISSCC49657.2024.10454440?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISSCC49657.2024.10454440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2287764300\",\n",
      "            \"name\": \"Jinhyung Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287959684\",\n",
      "            \"name\": \"Kyungjun Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109114249\",\n",
      "            \"name\": \"C. Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287956252\",\n",
      "            \"name\": \"Yeonho Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287119443\",\n",
      "            \"name\": \"Jae-Hyung Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286899300\",\n",
      "            \"name\": \"Su-Hyun Oh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1640393542\",\n",
      "            \"name\": \"Yucheon Ju\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3365606\",\n",
      "            \"name\": \"Chunseok Jeong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"123947284\",\n",
      "            \"name\": \"H. Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2198615241\",\n",
      "            \"name\": \"Jaeseung Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3376046\",\n",
      "            \"name\": \"T. Yun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292215366\",\n",
      "            \"name\": \"Jin Hee Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3375969\",\n",
      "            \"name\": \"Sangmuk Oh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1640397693\",\n",
      "            \"name\": \"J. Moon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110426052\",\n",
      "            \"name\": \"Y. Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291083018\",\n",
      "            \"name\": \"Hong-Seok Choi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159532637\",\n",
      "            \"name\": \"In-Keun Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286904054\",\n",
      "            \"name\": \"Seung Min Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286933095\",\n",
      "            \"name\": \"Sun-Yeol Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291078685\",\n",
      "            \"name\": \"Jaemin Jang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292140319\",\n",
      "            \"name\": \"Jinwook Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108644317\",\n",
      "            \"name\": \"S. Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286906698\",\n",
      "            \"name\": \"Younghyun Jeon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292172289\",\n",
      "            \"name\": \"Juhyung Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159571158\",\n",
      "            \"name\": \"Tae-Kyun Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"82375019\",\n",
      "            \"name\": \"D. Ka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159321700\",\n",
      "            \"name\": \"Sanghoon Oh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292143044\",\n",
      "            \"name\": \"Jinse Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159244890\",\n",
      "            \"name\": \"Junyeol Jeon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292173125\",\n",
      "            \"name\": \"Seonhong Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291075551\",\n",
      "            \"name\": \"Kyeong Tae Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2154855420\",\n",
      "            \"name\": \"Tae-Hwan Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291083403\",\n",
      "            \"name\": \"Hyeonjin Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291072681\",\n",
      "            \"name\": \"Dongju Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291084224\",\n",
      "            \"name\": \"Minseop Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30684992\",\n",
      "            \"name\": \"Heewoong Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291069292\",\n",
      "            \"name\": \"Dongwook Jang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287120235\",\n",
      "            \"name\": \"Junghyun Shin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287261607\",\n",
      "            \"name\": \"Hyunsik Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291041862\",\n",
      "            \"name\": \"Changki Baek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291023817\",\n",
      "            \"name\": \"Hajun Jeong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291081668\",\n",
      "            \"name\": \"Jongchan Yoon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1641325386\",\n",
      "            \"name\": \"SeungGyeon Lim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110630984\",\n",
      "            \"name\": \"Kyo Yun Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159499427\",\n",
      "            \"name\": \"Young Jun Koo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287107051\",\n",
      "            \"name\": \"Myeong-Jae Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2510417\",\n",
      "            \"name\": \"Joohwan Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291056079\",\n",
      "            \"name\": \"Jonghwan Kim\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"With the emergence of large-language models (LLM) and generative AI, which require an enormous amount of model parameters, the required memory bandwidth and capacity for high-end systems is on an unprecedented increase. To meet this need, we present an extended version of the high-bandwidth memory-3 (HBM3 DRAM), HBM3E, which achieves a 1280GB/s bandwidth with a cube density of 48GB. New design schemes and features, such as all-around power-through-silicon via (TSV), a 6-phase read-data-strobe (RDQS) scheme, a byte-mapping swap scheme, and a voltage-drift compensator for write data strobe (WDQS), are implemented to achieve extended bandwidth and capacity with enhanced reliability. The overall architecture and specifications, such as bump map footprint, the number of channel and I/Os, and the operation voltage, are identical to the latest HBM3 [1, 2]; therefore, backward compatibility is provided, avoiding system modification.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d\",\n",
      "        \"title\": \"NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.12766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper details the design and construction of NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"35504092\",\n",
      "            \"name\": \"Cunxiang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30819687\",\n",
      "            \"name\": \"Ruoxi Ning\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292184774\",\n",
      "            \"name\": \"Boqi Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292208424\",\n",
      "            \"name\": \"Tonghui Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3187768\",\n",
      "            \"name\": \"Qipeng Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292147095\",\n",
      "            \"name\": \"Cheng Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1993226927\",\n",
      "            \"name\": \"Guangsheng Bao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292261580\",\n",
      "            \"name\": \"Qian Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261496744\",\n",
      "            \"name\": \"Yue Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"dda8031682684655744c7001374e6cb88c9503bd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/dda8031682684655744c7001374e6cb88c9503bd\",\n",
      "        \"title\": \"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.10902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Soda-Eval is introduced, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, and the performance of several open-access instruction-tuned LLMs is studied, finding that dialogue evaluation remains challenging.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2007581062\",\n",
      "            \"name\": \"John Mendona\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268558660\",\n",
      "            \"name\": \"Isabel Trancoso\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1784914\",\n",
      "            \"name\": \"A. Lavie\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2d0b030d314a5aa8feaa03695e8471270130bdf9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9\",\n",
      "        \"title\": \"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\",\n",
      "        \"citationCount\": 100,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.07542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-01-13\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2155795167\",\n",
      "            \"name\": \"Chengzu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51198241\",\n",
      "            \"name\": \"Wenshan Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2339967968\",\n",
      "            \"name\": \"Huanyu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258547658\",\n",
      "            \"name\": \"Yan Xia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273419590\",\n",
      "            \"name\": \"Shaoguang Mao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294850817\",\n",
      "            \"name\": \"Li Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2339667880\",\n",
      "            \"name\": \"Ivan Vuli'c\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249539478\",\n",
      "            \"name\": \"Furu Wei\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"78875987dc674fc556873df037cf114f04932e80\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/78875987dc674fc556873df037cf114f04932e80\",\n",
      "        \"title\": \"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\",\n",
      "        \"citationCount\": 83,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2401.07764\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.07764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A split learning system for LLM agents in 6G networks is proposed, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1454018677\",\n",
      "            \"name\": \"Minrui Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1713586\",\n",
      "            \"name\": \"D. Niyato\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261731446\",\n",
      "            \"name\": \"Jiawen Kang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2943819\",\n",
      "            \"name\": \"Zehui Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237802924\",\n",
      "            \"name\": \"Shiwen Mao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2264568786\",\n",
      "            \"name\": \"Zhu Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2228302663\",\n",
      "            \"name\": \"Dong In Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269994509\",\n",
      "            \"name\": \"K. B. Letaief\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"807ba70b6dc5ce8104268ef8e579d6ff67051230\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/807ba70b6dc5ce8104268ef8e579d6ff67051230\",\n",
      "        \"title\": \"A Survey on Post-training of Large Language Models\",\n",
      "        \"citationCount\": 35,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-03-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2187039492\",\n",
      "            \"name\": \"Guiyao Tie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349394246\",\n",
      "            \"name\": \"Zeli Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2347232612\",\n",
      "            \"name\": \"Dingjie Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349375946\",\n",
      "            \"name\": \"Fuyang Wei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349949677\",\n",
      "            \"name\": \"Rong Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349402229\",\n",
      "            \"name\": \"Yurou Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349357634\",\n",
      "            \"name\": \"Wen Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"121937496\",\n",
      "            \"name\": \"Zhejian Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349498657\",\n",
      "            \"name\": \"Jiangyue Yan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2342866931\",\n",
      "            \"name\": \"Yao Su\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349400490\",\n",
      "            \"name\": \"Zhenhan Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2324567791\",\n",
      "            \"name\": \"Yifeng Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2211165440\",\n",
      "            \"name\": \"Yihan Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301109277\",\n",
      "            \"name\": \"Lichao Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2221116622\",\n",
      "            \"name\": \"Pan Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254874151\",\n",
      "            \"name\": \"Lifang He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280102292\",\n",
      "            \"name\": \"Hechang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349483665\",\n",
      "            \"name\": \"Yu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284983420\",\n",
      "            \"name\": \"Qingsong Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348862207\",\n",
      "            \"name\": \"Tianming Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249536787\",\n",
      "            \"name\": \"Neil Zhenqiang Gong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279062891\",\n",
      "            \"name\": \"Jiliang Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265549448\",\n",
      "            \"name\": \"Caiming Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2349551882\",\n",
      "            \"name\": \"Heng Ji\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293777434\",\n",
      "            \"name\": \"Philip S. Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288029761\",\n",
      "            \"name\": \"Jianfeng Gao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT's alignment strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ecb1002427e476ec76463e0a8b5a453471a1931f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ecb1002427e476ec76463e0a8b5a453471a1931f\",\n",
      "        \"title\": \"The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks are illustrated, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2283841922\",\n",
      "            \"name\": \"Ayo Adedeji\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283935118\",\n",
      "            \"name\": \"Sarita Joshi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283841541\",\n",
      "            \"name\": \"Brendan Doohan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a6856607fcdbb0675c3bcc02eb9ed6b1e8f2c3f6\",\n",
      "        \"title\": \"Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.03151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A thorough and up-to-date comparison of reasoning techniques in both textual and multimodal LLMs is offered, highlighting practical methods for post-training optimization and test-time inference.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-04-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2066776633\",\n",
      "            \"name\": \"Jing Bi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153545235\",\n",
      "            \"name\": \"Susan Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322454494\",\n",
      "            \"name\": \"Xiaofei Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279760245\",\n",
      "            \"name\": \"Pinxin Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2331365304\",\n",
      "            \"name\": \"Junjia Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2119309562\",\n",
      "            \"name\": \"Yunlong Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2242154602\",\n",
      "            \"name\": \"Luchuan Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2161012966\",\n",
      "            \"name\": \"Chao Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350866278\",\n",
      "            \"name\": \"Guangyu Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350999609\",\n",
      "            \"name\": \"Jinxi He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350428628\",\n",
      "            \"name\": \"Jiarui Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2354168235\",\n",
      "            \"name\": \"Shu Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266412651\",\n",
      "            \"name\": \"Daoan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350433568\",\n",
      "            \"name\": \"Chen Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152129821\",\n",
      "            \"name\": \"L. Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337241442\",\n",
      "            \"name\": \"Zhang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2320811177\",\n",
      "            \"name\": \"Jiebo Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293314762\",\n",
      "            \"name\": \"Chenliang Xu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5550118041a89121e9d7274f83aef420cd9ed487\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5550118041a89121e9d7274f83aef420cd9ed487\",\n",
      "        \"title\": \"Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.11849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": null\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-13\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2311451390\",\n",
      "            \"name\": \"Yinghao Aaron Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243118841\",\n",
      "            \"name\": \"Xilin Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2162961620\",\n",
      "            \"name\": \"Jordan Darefsky\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316835793\",\n",
      "            \"name\": \"Ge Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1686269\",\n",
      "            \"name\": \"N. Mesgarani\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end speech-to-speech conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and speech styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the speech, which is then played back to the user. While the response speech is being played, the input speech undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input speech. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and speech-to-speech baselines in terms of both dialogue naturalness and coherence while being more than 50% faster.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/afdc9b9c86f06db2b10816fac916e9f72d7b04ff\",\n",
      "        \"title\": \"What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.14622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This survey summarizes existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual, and proposes a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2302816545\",\n",
      "            \"name\": \"Dingyi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317013378\",\n",
      "            \"name\": \"Qin Jin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories. Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/bfe1057e5bdd2ed6d7b5564cf644a7a11fb8387a\",\n",
      "        \"title\": \"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.19846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Quest is introduced, a query-centric data synthesis method aggregating semantically relevant yet diverse documents that achieves superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2152644192\",\n",
      "            \"name\": \"Chaochen Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155226596\",\n",
      "            \"name\": \"Xing Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2176771084\",\n",
      "            \"name\": \"Qingfang Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257376973\",\n",
      "            \"name\": \"Songlin Hu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest's superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1fc9a4dacd756a8c0b5f1f3a19d361cdd05e3f52\",\n",
      "        \"title\": \"When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.13476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AnchorAttention is developed, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training, and reduces training time by over 50% compared to standard full attention mechanisms.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-11-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2267866973\",\n",
      "            \"name\": \"Haonan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284062049\",\n",
      "            \"name\": \"Qian Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325201427\",\n",
      "            \"name\": \"Chao Du\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291015783\",\n",
      "            \"name\": \"Tongyao Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325980280\",\n",
      "            \"name\": \"Cunxiao Du\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256995496\",\n",
      "            \"name\": \"Kenji Kawaguchi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"19201674\",\n",
      "            \"name\": \"Tianyu Pang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16's limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\\\% compared to standard full attention mechanisms, while preserving the original LLM's capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/38915a0a264cec25c99dcca6fc2cfe4eb7b46c5f\",\n",
      "        \"title\": \"Ultrafast imaging of coherent polariton propagation and interactions\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2143435931\",\n",
      "            \"name\": \"Ding Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"13643895\",\n",
      "            \"name\": \"Arkajit Mandal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2053177998\",\n",
      "            \"name\": \"James M. Baxter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2004406278\",\n",
      "            \"name\": \"Shangjun Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49805255\",\n",
      "            \"name\": \"I. Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1849913502\",\n",
      "            \"name\": \"Haowen (Vicky) Su\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144363449\",\n",
      "            \"name\": \"Song Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6834462\",\n",
      "            \"name\": \"D. Reichman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3895968\",\n",
      "            \"name\": \"Milan Delor\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/aecd4d3e12a53a93935ada40ed8b5fb1ee85958f\",\n",
      "        \"title\": \"Extrapolation-Based Video Retargeting With Backward Warping Using an Image-to-Warping Vector Generation Network\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LSP.2020.2977206?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LSP.2020.2977206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-02-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"46445100\",\n",
      "            \"name\": \"Sung In Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1739537\",\n",
      "            \"name\": \"Suk-ju Kang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Video retargeting is a technique used to transform a given video to a target aspect ratio. Current methods often cause severe visual distortion due to frequent temporal incoherence during the retargeting. In this study, we propose a new extrapolation-based video retargeting method using an image-to-warping vector generation network to maintain temporal coherence and prevent deformation of an input frame by extending the side area of an input frame. Backward warping-based extrapolation is performed using a displacement vector (DV) that is generated by a proposed convolutional neural network (CNN). The DV is defined as the displacement between the current hole to be filled in the extended area and a pixel in the input frame used to fill the hole. We also propose a technique to efficiently train the CNN including a method for ground-truth DV generation. After the extrapolation, we propose a technique for the maintenance of temporal coherence of the extended region and a distortion suppression scheme (DSC) for minimizing visual artifacts. The simulation results demonstrated that the proposed method improved bidirectional similarity (BDS) up to 3.69, which is a measure of the quality of video retargeting, compared with existing video retargeting methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0df45f6ab09cc6ddcaf6829c131c777732a73731\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0df45f6ab09cc6ddcaf6829c131c777732a73731\",\n",
      "        \"title\": \"Temporal Incoherence-Free Video Retargeting Using Foreground Aware Extrapolation\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2020.2977171?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2020.2977171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-03-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"46445100\",\n",
      "            \"name\": \"Sung In Cho\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1739537\",\n",
      "            \"name\": \"Suk-ju Kang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Video retargeting is a method of adjusting the aspect ratio of a given video to the target aspect ratio. However, temporal incoherence of video contents, which can occur frequently by video retargeting, is the most dominant factor that degrades the quality of retargeted videos. Current methods to maintain temporal coherence use the entire frames of the input videos; however, these methods cannot be implemented as on-time systems because of their tremendous computational complexity. As far as we know, there is no existing on-time video retargeting method that can avoid spatial distortion while perfectly maintaining temporal coherence. In this paper, we propose a novel on-time video retargeting method that can perfectly maintain temporal coherence and prevent the spatial distortion by using only two consecutive input frames. In our method, the maximum a posteriori-based foreground aware-block matching is used for the extrapolation that extends the side area of a given video to adjust its aspect ratio to the target. To maintain the temporal coherence of the extended area, the result of block matching for backward warping-based extrapolation of the start frame after the scene change occurs, is reused for the other frames until the next scene change occurs. In addition, we propose a scene scenario-adaptive fallback scheme to prevent severe distortions that can occur with reusing block matching results or extrapolation-based side extension. The simulation results showed that the proposed method greatly improved the bidirectional similarity value, which can measure the quality of video retargeting, by up to 10.26 compared with the existing on-time video retargeting methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/cc35548ca7f8b797402e9a95ff901b642af0b2b3\",\n",
      "        \"title\": \"Posterior-GRPO: Rewarding Reasoning Processes in Code Generation\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2508.05170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces a unified framework that can effectively incorporate the quality of the reasoning process during RL and introduces Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-08-07\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2310403349\",\n",
      "            \"name\": \"Lishui Fan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2375147757\",\n",
      "            \"name\": \"Yu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2125101083\",\n",
      "            \"name\": \"Mouxiang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2331676092\",\n",
      "            \"name\": \"Zhongxin Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model's internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"adaptive prompt generation\": {\n",
      "    \"total\": 13544,\n",
      "    \"offset\": 0,\n",
      "    \"next\": 50,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"ef399ed62fcc00e73b02f286012080351652693c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ef399ed62fcc00e73b02f286012080351652693c\",\n",
      "        \"title\": \"Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions, which is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which the authors call flocking.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2171964328\",\n",
      "            \"name\": \"Harry Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282555057\",\n",
      "            \"name\": \"Beidi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284063779\",\n",
      "            \"name\": \"Yuejie Chi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\times$ and 1.25$\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\n",
      "        \"title\": \"Prompt Expansion for Adaptive Text-to-Image Generation\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A human evaluation study shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-12-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2070966889\",\n",
      "            \"name\": \"Siddhartha Datta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276608298\",\n",
      "            \"name\": \"Alexander Ku\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275054270\",\n",
      "            \"name\": \"Deepak Ramachandran\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276610768\",\n",
      "            \"name\": \"Peter Anderson\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1d954600f17a7c3f16aa726a5eaa902d6851e808\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1d954600f17a7c3f16aa726a5eaa902d6851e808\",\n",
      "        \"title\": \"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\",\n",
      "        \"citationCount\": 52,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.12761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model's weights and incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"13563486\",\n",
      "            \"name\": \"Jaehong Yoon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2164249715\",\n",
      "            \"name\": \"Shoubin Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2061083016\",\n",
      "            \"name\": \"Vaidehi Patil\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267311471\",\n",
      "            \"name\": \"Huaxiu Yao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276608813\",\n",
      "            \"name\": \"Mohit Bansal\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model's weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a05b33072a8330413947eab0833ed2fd21de4963\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a05b33072a8330413947eab0833ed2fd21de4963\",\n",
      "        \"title\": \"Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.06401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-12-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2273557458\",\n",
      "            \"name\": \"Hao Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273559489\",\n",
      "            \"name\": \"Jun Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118764798\",\n",
      "            \"name\": \"Yizhuang Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273589717\",\n",
      "            \"name\": \"Jun Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2113457514\",\n",
      "            \"name\": \"Zhen Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274088311\",\n",
      "            \"name\": \"Xiangyu Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bc9f1025246efeed568650934b6e183415aca279\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/bc9f1025246efeed568650934b6e183415aca279\",\n",
      "        \"title\": \"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\",\n",
      "        \"citationCount\": 29,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2307.03214\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The proposed Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation, outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on the authors' main metrics for each task.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-07-06\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2221313264\",\n",
      "            \"name\": \"Jonathan Pei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1410652795\",\n",
      "            \"name\": \"Kevin Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"38666915\",\n",
      "            \"name\": \"D. Klein\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\n",
      "        \"title\": \"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\",\n",
      "        \"citationCount\": 115,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds, and shows that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1443432623\",\n",
      "            \"name\": \"Anselm Paulus\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3461866\",\n",
      "            \"name\": \"Arman Zharmagambetov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298951327\",\n",
      "            \"name\": \"Chuan Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298758184\",\n",
      "            \"name\": \"Brandon Amos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253746559\",\n",
      "            \"name\": \"Yuandong Tian\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\n",
      "        \"title\": \"WavLLM: Towards Robust and Adaptive Speech Large Language Model\",\n",
      "        \"citationCount\": 104,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"WavLLM is introduced, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2277450543\",\n",
      "            \"name\": \"Shujie Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2135918679\",\n",
      "            \"name\": \"Long Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107983441\",\n",
      "            \"name\": \"Shujie Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107970655\",\n",
      "            \"name\": \"Sanyuan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294360053\",\n",
      "            \"name\": \"Hongkun Hao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258308585\",\n",
      "            \"name\": \"Jing Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274190703\",\n",
      "            \"name\": \"Xunying Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280887661\",\n",
      "            \"name\": \"Jinyu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9075412\",\n",
      "            \"name\": \"S. Sivasankaran\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294832157\",\n",
      "            \"name\": \"Linquan Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277299355\",\n",
      "            \"name\": \"Furu Wei\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\url{aka.ms/wavllm}.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5cacb35e7bd86e64e5aca126e5011b64630007d8\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5cacb35e7bd86e64e5aca126e5011b64630007d8\",\n",
      "        \"title\": \"OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.18866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"OmniAvatar is introduced, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements, and introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-06-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2293272991\",\n",
      "            \"name\": \"Qijun Gan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294510159\",\n",
      "            \"name\": \"Ruizi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2314648986\",\n",
      "            \"name\": \"Jianke Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2370937157\",\n",
      "            \"name\": \"Shaofei Xue\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2370937932\",\n",
      "            \"name\": \"Steven Hoi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8e37dc1215681aa153a51c07078ba8befd6a6e01\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8e37dc1215681aa153a51c07078ba8befd6a6e01\",\n",
      "        \"title\": \"AdaPlanner: Adaptive Planning from Feedback with Language Models\",\n",
      "        \"citationCount\": 175,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.16653\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2118180896\",\n",
      "            \"name\": \"Haotian Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"8103389\",\n",
      "            \"name\": \"Yuchen Zhuang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2865034\",\n",
      "            \"name\": \"Lingkai Kong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2218437288\",\n",
      "            \"name\": \"Bo Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145657504\",\n",
      "            \"name\": \"Chao Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9689acb6cb760e8bc21c16f368368b37dee977f9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9689acb6cb760e8bc21c16f368368b37dee977f9\",\n",
      "        \"title\": \"Adaptive Machine Translation with Large Language Models\",\n",
      "        \"citationCount\": 106,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2301.13294\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.13294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is observed that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-01-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"9400076\",\n",
      "            \"name\": \"Yasmin Moslem\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1748844\",\n",
      "            \"name\": \"Rejwanul Haque\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144315616\",\n",
      "            \"name\": \"Andy Way\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7665642af9e682e012bec045102a4d009421067c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7665642af9e682e012bec045102a4d009421067c\",\n",
      "        \"title\": \"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\",\n",
      "        \"citationCount\": 98,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2311.17061\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An efficient yet effective framework that generates high-quality 3D humans with fine-grained geometry and realistic appearance, based on the insight that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-11-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2257708193\",\n",
      "            \"name\": \"Xian Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260342453\",\n",
      "            \"name\": \"Xiaohang Zhan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1397711601\",\n",
      "            \"name\": \"Jiaxiang Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260340529\",\n",
      "            \"name\": \"Ying Shan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2247995148\",\n",
      "            \"name\": \"Gang Zeng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258618427\",\n",
      "            \"name\": \"Dahua Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257370021\",\n",
      "            \"name\": \"Xihui Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249080787\",\n",
      "            \"name\": \"Ziwei Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5888f7aba5c601a668c290bf57addf79cc1518f1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1\",\n",
      "        \"title\": \"ChatUniTest: a ChatGPT-based automated unit test generation tool\",\n",
      "        \"citationCount\": 87,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.04764\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.04764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.04764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework that outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and generates assertions while utilizing mock objects and reection to achieve test objectives.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1387638000\",\n",
      "            \"name\": \"Zhuo-Qi Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300176046\",\n",
      "            \"name\": \"Yinghao Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064478633\",\n",
      "            \"name\": \"Chen Zhi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145590434\",\n",
      "            \"name\": \"Shuiguang Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2116398505\",\n",
      "            \"name\": \"Jianwei Yin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"836e3069a83f455f916114e7265e00187e511838\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/836e3069a83f455f916114e7265e00187e511838\",\n",
      "        \"title\": \"Locally Differentially Private Document Generation Using Zero Shot Prompting\",\n",
      "        \"citationCount\": 57,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.16111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1512255229\",\n",
      "            \"name\": \"Saiteja Utpala\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261493078\",\n",
      "            \"name\": \"Sara Hooker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261697074\",\n",
      "            \"name\": \"Pin Yu Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\% reduction in author identification F1 score against static attackers and a 26\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\n",
      "        \"title\": \"Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2261902040\",\n",
      "            \"name\": \"Hongbo Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153003087\",\n",
      "            \"name\": \"Xiangteng He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261798088\",\n",
      "            \"name\": \"Jiahuan Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143753918\",\n",
      "            \"name\": \"Yuxin Peng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"107dce4ffbdd8d83b75492216646269d8c037ab5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/107dce4ffbdd8d83b75492216646269d8c037ab5\",\n",
      "        \"title\": \"The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.09576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1847858\",\n",
      "            \"name\": \"Subhankar Maity\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144085844\",\n",
      "            \"name\": \"Aniket Deroy\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b013c9eb1284554ae696fba02bd4d7fc599890b6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b013c9eb1284554ae696fba02bd4d7fc599890b6\",\n",
      "        \"title\": \"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.00330?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.00330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"StyleCrafter is introduced, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image, and designs a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2269171464\",\n",
      "            \"name\": \"Gongye Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257035878\",\n",
      "            \"name\": \"Menghan Xia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257199953\",\n",
      "            \"name\": \"Yong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2149052351\",\n",
      "            \"name\": \"Haoxin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2087273800\",\n",
      "            \"name\": \"Jinbo Xing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253795356\",\n",
      "            \"name\": \"Xintao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3001727\",\n",
      "            \"name\": \"Yujiu Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257019659\",\n",
      "            \"name\": \"Ying Shan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0f78249a5ff64441cc51e55bbe5b97e28f801240\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0f78249a5ff64441cc51e55bbe5b97e28f801240\",\n",
      "        \"title\": \"Prompt Tuning for Generative Multimodal Pretrained Models\",\n",
      "        \"citationCount\": 37,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2208.02532\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2208.02532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work implements prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks, with a focus on generative multimodal pretrained models, instead of contrastive ones.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-08-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"50841357\",\n",
      "            \"name\": \"Han Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35996608\",\n",
      "            \"name\": \"Junyang Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"An Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155302144\",\n",
      "            \"name\": \"Peng Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144161025\",\n",
      "            \"name\": \"Chang Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"38385080\",\n",
      "            \"name\": \"Hongxia Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at \\\\url{https://github.com/OFA-Sys/OFA}\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"192b808eba1232ba3b1d1481230db122b22c97e4\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/192b808eba1232ba3b1d1481230db122b22c97e4\",\n",
      "        \"title\": \"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2312.01663\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.01663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-12-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2269468811\",\n",
      "            \"name\": \"Runze He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2052151521\",\n",
      "            \"name\": \"Shaofei Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269461105\",\n",
      "            \"name\": \"Xuecheng Nie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"151475424\",\n",
      "            \"name\": \"Tianrui Hui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1776665\",\n",
      "            \"name\": \"Luoqi Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108984\",\n",
      "            \"name\": \"Jiao Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269685669\",\n",
      "            \"name\": \"Jizhong Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269748083\",\n",
      "            \"name\": \"Guanbin Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269687302\",\n",
      "            \"name\": \"Si Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/817ff4cfbcd5d6c870947fa8129ee5598f03a765\",\n",
      "        \"title\": \"TaskCraft: Automated Generation of Agentic Tasks\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.10055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories, and empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-06-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2367198602\",\n",
      "            \"name\": \"Dingfeng Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366609463\",\n",
      "            \"name\": \"Jingyi Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2368654631\",\n",
      "            \"name\": \"Qianben Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2367090248\",\n",
      "            \"name\": \"Weichen Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366569457\",\n",
      "            \"name\": \"Weizhen Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366571583\",\n",
      "            \"name\": \"Hongxuan Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366522296\",\n",
      "            \"name\": \"Fangchen Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366567233\",\n",
      "            \"name\": \"Tianrui Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2368705239\",\n",
      "            \"name\": \"King Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283080546\",\n",
      "            \"name\": \"Minghao Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366695720\",\n",
      "            \"name\": \"Jian Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366560952\",\n",
      "            \"name\": \"Ge Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2182423032\",\n",
      "            \"name\": \"Jiaheng Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2351712181\",\n",
      "            \"name\": \"Changwang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366603044\",\n",
      "            \"name\": \"Jun Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2134457930\",\n",
      "            \"name\": \"Y. Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284803168\",\n",
      "            \"name\": \"Wangchunshu Zhou\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce \\\\textsc{TaskCraft}, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f373c5569b45bf580b7502729a83761a791ee209\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f373c5569b45bf580b7502729a83761a791ee209\",\n",
      "        \"title\": \"MagDiff: Multi-alignment Diffusion for High-Fidelity Video Generation and Editing\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work is the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing, and Experimental results on four benchmarks suggest that the method outperforms the previous method on each task.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-11-29\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2238430925\",\n",
      "            \"name\": \"Haoyu Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238207741\",\n",
      "            \"name\": \"Tianyi Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239092619\",\n",
      "            \"name\": \"Jiaxi Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238449354\",\n",
      "            \"name\": \"Xing Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2311476511\",\n",
      "            \"name\": \"Qingping Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3099139\",\n",
      "            \"name\": \"Zuxuan Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258963974\",\n",
      "            \"name\": \"Hang Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238451522\",\n",
      "            \"name\": \"Yu-Gang Jiang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The diffusion model is widely leveraged for either video generation or video editing. As each field has its task-specific problems, it is difficult to merely develop a single diffusion for completing both tasks simultaneously. Video diffusion sorely relying on the text prompt can be adapted to unify the two tasks. However, it lacks a high capability of aligning heterogeneous modalities between text and image, leading to various misalignment problems. In this work, we are the first to propose a unified Multi-alignment Diffusion, dubbed as MagDiff, for both tasks of high-fidelity video generation and editing. The proposed MagDiff introduces three types of alignments, including subject-driven alignment, adaptive prompts alignment, and high-fidelity alignment. Particularly, the subject-driven alignment is put forward to trade off the image and text prompts, serving as a unified foundation generative model for both tasks. The adaptive prompts alignment is introduced to emphasize different strengths of homogeneous and heterogeneous alignments by assigning different values of weights to the image and the text prompts. The high-fidelity alignment is developed to further enhance the fidelity of both video generation and editing by taking the subject image as an additional model input. Experimental results on four benchmarks suggest that our method outperforms the previous method on each task.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a17aee1261b56ee828f029b1caeef78033acea83\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a17aee1261b56ee828f029b1caeef78033acea83\",\n",
      "        \"title\": \"Adaptive Ship Detection From Optical to SAR Images\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LGRS.2023.3317321?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LGRS.2023.3317321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2243328855\",\n",
      "            \"name\": \"Yuxuan Yuan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2221128858\",\n",
      "            \"name\": \"Zhijie Rao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232100687\",\n",
      "            \"name\": \"Chuyang Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1950637\",\n",
      "            \"name\": \"Yue Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2713947\",\n",
      "            \"name\": \"Xinghao Ding\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advances in synthetic aperture radar (SAR) ship detection have witnessed remarkable success by using large-scale annotated datasets. However, the annotation of SAR images requires strong domain-specific expertise, significantly hindering the prompt adoption of modern object detectors in this regime. Compared to SAR data, optical data in geoscience are considerably easier to label. Motivated by this, we investigate a new and challenging problemadaptive ship detectionwith the goal of enhancing ship detection performance on SAR images by leveraging knowledge transferred from optical images. Considering the large distributional discrepancy between the source (optical) and target (SAR) domains, we present OmniAdapt, a novel framework that progressively narrows the distance between the two types of images at the pixel, feature, and classifier levels. Specifically, OmniAdapt consists of three main modules, target-like generation module (TLGM), multifeature alignment module (MFAM), and common specific decomposition module (CSDM). TLGM minimizes the visual disparity by infusing the target-domain style into the source domain. MFAM aligns local- and global-level feature representations in an adversarial manner. Finally, CSDM decomposes the classifier into two independent components, that is, the domain-common component and the domain-specific component, and promotes the recognition ability of the former via regularization learning. Experimental results demonstrate the effectiveness of the proposed method.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/94bbe80766824ca4163b2578e19daf6b9f2b1fd6\",\n",
      "        \"title\": \"Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.05256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate, is introduced, a reinforcement learning objective tailoring generation length to per-prompt solve rate that delivers higher accuracy on the hardest problems with higher cost.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-06-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2274104658\",\n",
      "            \"name\": \"Violet Xiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326300653\",\n",
      "            \"name\": \"Chase Blagden\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"102801230\",\n",
      "            \"name\": \"Rafael Rafailov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283848553\",\n",
      "            \"name\": \"nathan lile\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2366009773\",\n",
      "            \"name\": \"Sang Truong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284774407\",\n",
      "            \"name\": \"Chelsea Finn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274104149\",\n",
      "            \"name\": \"Nick Haber\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/44d6f053a43a8bc5c45fe807066656d8e34d3d27\",\n",
      "        \"title\": \"OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2504.10825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-04-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2190109518\",\n",
      "            \"name\": \"Dianbing Xi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2356794181\",\n",
      "            \"name\": \"Jiepeng Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337074199\",\n",
      "            \"name\": \"Yuanzhi Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336910859\",\n",
      "            \"name\": \"Xi Qiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3131188\",\n",
      "            \"name\": \"Yuchi Huo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325437281\",\n",
      "            \"name\": \"Rui Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336934367\",\n",
      "            \"name\": \"Chi Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336880377\",\n",
      "            \"name\": \"Xuelong Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff , aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. Our framework supports three key capabilities: (1) Text-conditioned video generation, where all modalities are jointly synthesized from a textual prompt; (2) Video understanding, where structural modalities are predicted from rgb inputs in a coherent manner; and (3) X-conditioned video generation, where video synthesis is guided by finegrained inputs such as depth, canny and segmentation. Extensive experiments demonstrate that OmniVDiff achieves state-of-the-art performance in video generation tasks and competitive results in video understanding. Its flexibility and scalability make it well-suited for downstream applications such as video-to-video translation, modality adaptation for visual tasks, and scene reconstruction.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"42117d01d498eb9f8c21b788c3565bc6855d620b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/42117d01d498eb9f8c21b788c3565bc6855d620b\",\n",
      "        \"title\": \"Learning to Transfer Prompts for Text Generation\",\n",
      "        \"citationCount\": 43,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2205.01543\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.01543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper improves this technique and proposes a novel prompt-based method (PTG) for text generation in a transferable setting that learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-05-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2018027\",\n",
      "            \"name\": \"Junyi Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1997234792\",\n",
      "            \"name\": \"Tianyi Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50204644\",\n",
      "            \"name\": \"J. Nie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153693432\",\n",
      "            \"name\": \"Ji-rong Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2542603\",\n",
      "            \"name\": \"Wayne Xin Zhao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a\",\n",
      "        \"title\": \"CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer\",\n",
      "        \"citationCount\": 1175,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions, and develops an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2303231681\",\n",
      "            \"name\": \"Zhuoyi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238205354\",\n",
      "            \"name\": \"Jiayan Teng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2163967642\",\n",
      "            \"name\": \"Wendi Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2055623340\",\n",
      "            \"name\": \"Ming Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305795673\",\n",
      "            \"name\": \"Shiyu Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2214082934\",\n",
      "            \"name\": \"Jiazheng Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315948290\",\n",
      "            \"name\": \"Yuanming Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2105844599\",\n",
      "            \"name\": \"Wenyi Hong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268628279\",\n",
      "            \"name\": \"Xiaohan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307077651\",\n",
      "            \"name\": \"Guanyu Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307075814\",\n",
      "            \"name\": \"Da Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290625851\",\n",
      "            \"name\": \"Xiaotao Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316099643\",\n",
      "            \"name\": \"Yuxuan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265518149\",\n",
      "            \"name\": \"Weihan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2306161782\",\n",
      "            \"name\": \"Yean Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315952736\",\n",
      "            \"name\": \"Ting Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288066971\",\n",
      "            \"name\": \"Bin Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243402027\",\n",
      "            \"name\": \"Yuxiao Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238207092\",\n",
      "            \"name\": \"Jie Tang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f37a5c2bf4266d031533d5e029b74b00b48ef038\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f37a5c2bf4266d031533d5e029b74b00b48ef038\",\n",
      "        \"title\": \"Latency-Aware Generative Semantic Communications With Pre-Trained Diffusion Models\",\n",
      "        \"citationCount\": 62,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2403.17256\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A latency-aware semantic communications framework with pre-trained generative models that designs a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2258983485\",\n",
      "            \"name\": \"Li Qiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3202702\",\n",
      "            \"name\": \"Mahdi Boloursaz Mashhadi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293693238\",\n",
      "            \"name\": \"Zhen Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1690137\",\n",
      "            \"name\": \"C. Foh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293392561\",\n",
      "            \"name\": \"Pei Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279548875\",\n",
      "            \"name\": \"Mehdi Bennis\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this letter, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"759b95f7f90addc4c526cd92557e486ab143fbec\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/759b95f7f90addc4c526cd92557e486ab143fbec\",\n",
      "        \"title\": \"Style Vectors for Steering Generative Large Language Models\",\n",
      "        \"citationCount\": 39,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.01618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is shown that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches, demonstrating the effectiveness of activation engineering using such style vectors.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-02\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2098959640\",\n",
      "            \"name\": \"Kai Konen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467369\",\n",
      "            \"name\": \"Sophie Jentzsch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274662002\",\n",
      "            \"name\": \"Diaoul Diallo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467364\",\n",
      "            \"name\": \"Peer Schutt\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467405\",\n",
      "            \"name\": \"Oliver Bensch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51185829\",\n",
      "            \"name\": \"Roxanne El Baff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467346\",\n",
      "            \"name\": \"Dominik Opitz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282467403\",\n",
      "            \"name\": \"Tobias Hecking\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/621ba0b763e89a4e82fac4b4e264ca8fb6c04fb6\",\n",
      "        \"title\": \"EdgeLLM: Fast On-Device LLM Inference With Speculative Decoding\",\n",
      "        \"citationCount\": 22,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMC.2024.3513457?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMC.2024.3513457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2238890503\",\n",
      "            \"name\": \"Daliang Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238575108\",\n",
      "            \"name\": \"Wangsong Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2306091156\",\n",
      "            \"name\": \"Hao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239060901\",\n",
      "            \"name\": \"Xin Jin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326531487\",\n",
      "            \"name\": \"Ying Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238910539\",\n",
      "            \"name\": \"Shiyun Wei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326083763\",\n",
      "            \"name\": \"Mengwei Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237080638\",\n",
      "            \"name\": \"Xuanzhe Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device's memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3 faster than existing engines.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"411b16add23976ffcdf6422f932453f6ebcca119\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/411b16add23976ffcdf6422f932453f6ebcca119\",\n",
      "        \"title\": \"EvoPrompting: Language Models for Code-Level Neural Architecture Search\",\n",
      "        \"citationCount\": 118,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2302.14838\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.14838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-02-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"13336152\",\n",
      "            \"name\": \"Angelica Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35363891\",\n",
      "            \"name\": \"David Dohan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48165870\",\n",
      "            \"name\": \"David R. So\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1a907cda901dc8039e1d04838ff217fdbcacd2e6\",\n",
      "        \"title\": \"Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.18950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The Adaptive Guided Erasure (AGE) method is proposed, which selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-01-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2007884558\",\n",
      "            \"name\": \"Anh-Vu Bui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299801919\",\n",
      "            \"name\": \"T. V\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"67329496\",\n",
      "            \"name\": \"Tung-Long Vuong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249909946\",\n",
      "            \"name\": \"Trung Le\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292198330\",\n",
      "            \"name\": \"Paul Montague\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2059248789\",\n",
      "            \"name\": \"Tamas Abraham\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275034108\",\n",
      "            \"name\": \"Junae Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1400659302\",\n",
      "            \"name\": \"Dinh Q. Phung\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a neutral concept or just an empty text prompt. In this paper, we demonstrate that this fixed-target strategy is suboptimal, as it fails to account for the impact of erasing one concept on the others. To address this limitation, we model the concept space as a graph and empirically analyze the effects of erasing one concept on the remaining concepts. Our analysis uncovers intriguing geometric properties of the concept space, where the influence of erasing a concept is confined to a local region. Building on this insight, we propose the Adaptive Guided Erasure (AGE) method, which \\\\emph{dynamically} selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects. Experimental results show that AGE significantly outperforms state-of-the-art erasure methods on preserving unrelated concepts while maintaining effective erasure performance. Our code is published at {https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/af06dc2be258419a3d7c5b38d446234e68b1a4b8\",\n",
      "        \"title\": \"StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.08503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A cross-modal Adaptive Instance Normalization mechanism is introduced for better integration of style and text features, enhancing alignment and a Style-based Classifier-Free Guidance approach is developed that enables selective control over stylistic elements, reducing irrelevant influences.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2334740500\",\n",
      "            \"name\": \"Mingkun Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334824597\",\n",
      "            \"name\": \"Xue Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2336265253\",\n",
      "            \"name\": \"Beier Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334818360\",\n",
      "            \"name\": \"Hao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2334822819\",\n",
      "            \"name\": \"Chi Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Text-driven style transfer aims to merge the style of a reference image with content described by a text prompt. Recent advancements in text-to-image models have improved the nuance of style transformations, yet significant challenges remain, particularly with overfitting to reference styles, limiting stylistic control, and misaligning with textual content. In this paper, we propose three complementary strategies to address these issues. First, we introduce a cross-modal Adaptive Instance Normalization (AdaIN) mechanism for better integration of style and text features, enhancing alignment. Second, we develop a Style-based Classifier-Free Guidance (SCFG) approach that enables selective control over stylistic elements, reducing irrelevant influences. Finally, we incorporate a teacher model during early generation stages to stabilize spatial layouts and mitigate artifacts. Our extensive evaluations demonstrate significant improvements in style transfer quality and alignment with textual prompts. Furthermore, our approach can be integrated into existing style transfer frameworks without fine-tuning.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5fbe002a3d1cd93b6b6fc1105fb716dd90bfc80c\",\n",
      "        \"title\": \"TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.16425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information, and designs a Dynamic Map Scaling mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-11-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2333781842\",\n",
      "            \"name\": \"Linqing Zhong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2105618628\",\n",
      "            \"name\": \"Chen Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2264574237\",\n",
      "            \"name\": \"Zihan Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325825544\",\n",
      "            \"name\": \"Yue Liao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325537018\",\n",
      "            \"name\": \"Si Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM's spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/4e5c310f6b04c2122de108cb5f742b7a3b83af1d\",\n",
      "        \"title\": \"A new rapid deflagration-to-detonation transition in a short smooth tube\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1063/5.0191500?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1063/5.0191500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"92832821\",\n",
      "            \"name\": \"Wandong Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290316859\",\n",
      "            \"name\": \"Caizhi Fan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1734241\",\n",
      "            \"name\": \"R. Deiterding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290237139\",\n",
      "            \"name\": \"Xiaokang Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"36072040\",\n",
      "            \"name\": \"Jianhan Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290198596\",\n",
      "            \"name\": \"Xiong Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Obtaining a rapid deflagration-to-detonation transition (DDT) within a short smooth tube is a challenging task. Here, an unconventional means of flame acceleration propagating upstream in subsonic and supersonic mixtures within a smooth tube was introduced to acquire a speedy DDT. The NavierStokes equations with an adaptive mesh refinement technique and a detailed hydrogenair chemistry reaction mechanism of 11 species and 27 steps were utilized to resolve the entire DDT characteristics. The effect of the initial Mach number on flame acceleration and DDT mechanism was revealed comprehensively. The results demonstrated that a prompt oblique shock wave (SW) occurs when the flame propagates upstream along the boundary walls due to the boundary layer influence. An intense coupling between the SW and the leading flame front is enhanced by increasing the initial Mach number of the mixture. The speedy generation of the oblique SW is formed at the incipient stage, mainly produced by the boundary layer influence and the coalescences of the compression waves. Consequently, the run-up time to detonation is shortened accordingly through a fierce reflected SW due to the intense leading SW after it reflects from the confined wall. Furthermore, three kinds of DDT evolution are revealed from the obtained results: (1) localized ignition in the upper boundary wall after the reflected and transverse shock waves propagate in the upper wall regions; (2) autoignition is formed in the confined wall corner after the reflected SW; and (3) direct detonation transition occurs at the end wall behind a strongly reflected SW in the supersonic case.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/815ed9c4970d34cd128a00f0a3fd4ea1aef1c9dd\",\n",
      "        \"title\": \"Towards Educator-Driven Tutor Authoring: Generative AI Approaches for Creating Intelligent Tutor Interfaces\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2405.14713\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Generative AI capabilities are introduced to assist educators in creating tutor interfaces that meet their needs while adhering to design principles and raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2170697820\",\n",
      "            \"name\": \"Tommaso Cal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257237899\",\n",
      "            \"name\": \"Christopher J. MacLellan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e8e619d906f1e3a4f9b715a9e553d996a935b4f0\",\n",
      "        \"title\": \"Transformer-Based Variable-Rate Image Compression with Region-of-Interest Control\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.10807\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.10807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a transformer-based learned image compression system that is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality while confirming its superiority over the other competing methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2140736758\",\n",
      "            \"name\": \"Chia-Hao Kao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1723619\",\n",
      "            \"name\": \"Ying Weng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2116613919\",\n",
      "            \"name\": \"Yi-Hsin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"37811787\",\n",
      "            \"name\": \"Wei-Chen Chiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"123608804\",\n",
      "            \"name\": \"Wenmin Peng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper proposes a transformer-based learned image compression system. It is capable of achieving variable-rate compression with a single model while supporting the region-of-interest (ROI) functionality. Inspired by prompt tuning, we introduce prompt generation networks to condition the transformer-based autoencoder of compression. Our prompt generation networks generate content-adaptive tokens according to the input image, an ROI mask, and a rate parameter. The separation of the ROI mask and the rate parameter allows an intuitive way to achieve variable-rate and ROI coding simultaneously. Extensive experiments validate the effectiveness of our proposed method and confirm its superiority over the other competing methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c7acf9250926100f531bcf46d63d7da06e73928e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c7acf9250926100f531bcf46d63d7da06e73928e\",\n",
      "        \"title\": \"MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.04399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work identifies that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images, and advances the cross-modality relation learning between the prompt and the generated images to better align the prompt and image content.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2238903147\",\n",
      "            \"name\": \"Yupeng Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"18119920\",\n",
      "            \"name\": \"Daquan Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238887924\",\n",
      "            \"name\": \"Zuo-Liang Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238889090\",\n",
      "            \"name\": \"Yaxing Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3298532\",\n",
      "            \"name\": \"Qibin Hou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"33221685\",\n",
      "            \"name\": \"Jiashi Feng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in diffusion models have showcased their impressive capacity to generate visually striking images. However, ensuring a close match between the generated image and the given prompt remains a persistent challenge. In this work, we identify that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images. To better align the prompt and image content, we advance the cross-attention with an adaptive mask, which is conditioned on the attention maps and the prompt embeddings, to dynamically adjust the contribution of each text token to the image features. This mechanism explicitly diminishes the ambiguity in the semantic information embedding of the text encoder, leading to a boost of text-to-image consistency in the synthesized images. Our method, termed MaskDiffusion, is training-free and hot-pluggable for popular pre-trained diffusion models. When applied to the latent diffusion models, our MaskDiffusion can largely enhance their capability to correctly generate objects and their attributes, with negligible computation overhead compared to the original diffusion models. Our project page is https://github.com/HVision-NKU/MaskDiffusion.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/66c5a0fbde7e06bf0ef179e660b6a211bcd80aac\",\n",
      "        \"title\": \"TEMPERA: Test-Time Prompting via Reinforcement Learning\",\n",
      "        \"citationCount\": 45,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2211.11890\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.11890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work designs a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers, and achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-11-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1993655237\",\n",
      "            \"name\": \"Tianjun Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275277634\",\n",
      "            \"name\": \"Xuezhi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"65855107\",\n",
      "            \"name\": \"Denny Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50319359\",\n",
      "            \"name\": \"D. Schuurmans\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49988044\",\n",
      "            \"name\": \"Joseph E. Gonzalez\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/88fda6e889c5a3db5f8e0c8ac708bf2c7f413ba6\",\n",
      "        \"title\": \"QA-CLIMS: Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.09883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Book\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2280039282\",\n",
      "            \"name\": \"Songhe Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279830536\",\n",
      "            \"name\": \"Wei Zhuo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2220635949\",\n",
      "            \"name\": \"Jinheng Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265520934\",\n",
      "            \"name\": \"Linlin Shen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model's ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d3c81cc6e98a2ba22ef0a5139876e229db16f214\",\n",
      "        \"title\": \"Immunosenescence, Inflammaging, and Lung Senescence in Asthma in the Elderly\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://www.mdpi.com/2218-273X/12/10/1456/pdf?version=1665574999\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9599177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Immunosenescences contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly is discussed and an overview of age-related features in the immune system and lung structure is presented.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Review\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-10-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"47522049\",\n",
      "            \"name\": \"T. Soma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46375440\",\n",
      "            \"name\": \"M. Nagata\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Prevalence of asthma in older adults is growing along with increasing global life expectancy. Due to poor clinical consequences such as high mortality, advancement in understanding the pathophysiology of asthma in older patients has been sought to provide prompt treatment for them. Age-related alterations of functions in the immune system and lung parenchyma occur throughout life. Alterations with advancing age are promoted by various stimuli, including pathobionts, fungi, viruses, pollutants, and damage-associated molecular patterns derived from impaired cells, abandoned cell debris, and senescent cells. Age-related changes in the innate and adaptive immune response, termed immunosenescence, includes impairment of phagocytosis and antigen presentation, enhancement of proinflammatory mediator generation, and production of senescence-associated secretory phenotype. Immnunosenescence could promote inflammaging (chronic low-grade inflammation) and contribute to late-onset adult asthma and asthma in the elderly, along with age-related pulmonary disease, such as chronic obstructive pulmonary disease and pulmonary fibrosis, due to lung parenchyma senescence. Aged patients with asthma exhibit local and systemic type 2 and non-type 2 inflammation, associated with clinical manifestations. Here, we discuss immunosenescences contribution to the immune response and the combination of type 2 inflammation and inflammaging in asthma in the elderly and present an overview of age-related features in the immune system and lung structure.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/94d2f525eed0cd7c337bd692f833f006d62dc3ab\",\n",
      "        \"title\": \"DAFT-E: Feature-Based Multivariate and Multi-Step-Ahead Wind Power Forecasting\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tste.2021.3130949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tste.2021.3130949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2022-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1978743481\",\n",
      "            \"name\": \"F. De Caro\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2142416821\",\n",
      "            \"name\": \"Jacopo De Stefani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"33858225\",\n",
      "            \"name\": \"A. Vaccaro\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1772497\",\n",
      "            \"name\": \"Gianluca Bontempi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Wind energy is one of the most promising resources for the mitigation of greenhouse gas emissions that contribute to anthropogenic global warming. However, the large proliferation of wind power generators is causing several critical issues in power systems due to their variable power generated profiles. For this reason, a large number of learning techniques, e.g. integrating Vector Auto-Regressive and Neural Network-based models, were proposed in the literature for mitigating wind power uncertainty issues. Unfortunately, these methodologies show several limitations, e.g. the huge number of parameters and/or the heavy computational cost, which hinder their deployment in modern power system operation, where prompt and reliable wide-area wind power generation forecasts are requested for supporting time-critical decision making on several time horizons. To try addressing this issue, this paper proposes the Dynamic Adaptive Feature-based Temporal Ensemble (DAFT-E) forecasting approach, which relies on an extensive feature engineering, a fast feature selection step and an ensemble of computationally inexpensive models to reduce the computational complexity of the forecasting task, while still preserving predictive accuracy. The experimental results, which benchmark DAFT-E against multivariate (VAR and deep learning) alternatives on two real case studies, show that the proposed approach outperforms state-of-the-art and representation learning models according to several forecasting accuracy metrics.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f6023556221018f779a63a60874973195aea8352\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f6023556221018f779a63a60874973195aea8352\",\n",
      "        \"title\": \"Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.09857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a novel framework named Attention-aware Self-adaptive Prompt (ASP), which prevents overfitting on base task and does not require enormous data in few-shot incremental tasks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2275745354\",\n",
      "            \"name\": \"Chenxi Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254008335\",\n",
      "            \"name\": \"Zhenyi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249155683\",\n",
      "            \"name\": \"Tianyi Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262968852\",\n",
      "            \"name\": \"Ruibo Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254326623\",\n",
      "            \"name\": \"Yihan Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275765198\",\n",
      "            \"name\": \"Junfeng Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261394090\",\n",
      "            \"name\": \"Heng Huang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensive experiments on three benchmark datasets validate that ASP consistently outperforms state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning new classes and mitigating forgetting.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c36cb4a41369369d837ea170397f7818d02150dd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c36cb4a41369369d837ea170397f7818d02150dd\",\n",
      "        \"title\": \"Soft Prompt Generation for Domain Generalization\",\n",
      "        \"citationCount\": 25,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG), which consists of a two-stage training phase and an inference phase, aiming to incorporate the generative model domain knowledge.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2274938328\",\n",
      "            \"name\": \"Shuanghao Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290728057\",\n",
      "            \"name\": \"Yuedi Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275025483\",\n",
      "            \"name\": \"Wanqi Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7778036\",\n",
      "            \"name\": \"Zhirong Luan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275030348\",\n",
      "            \"name\": \"Badong Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\n",
      "        \"title\": \"Adapting to Distribution Shift by Visual Domain Prompt Generation\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.02797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction and outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"35793956\",\n",
      "            \"name\": \"Zhixiang Chi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300096585\",\n",
      "            \"name\": \"Li Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300089295\",\n",
      "            \"name\": \"Tao Zhong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277793919\",\n",
      "            \"name\": \"Huan Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1787848\",\n",
      "            \"name\": \"Yuanhao Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277598061\",\n",
      "            \"name\": \"Konstantinos N. Plataniotis\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277695392\",\n",
      "            \"name\": \"Yang Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/fbceb7ffd77ca45eaba297f3f421f65df4feb5cf\",\n",
      "        \"title\": \"Query-Based Adversarial Prompt Generation\",\n",
      "        \"citationCount\": 44,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.12329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work improves on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2268494505\",\n",
      "            \"name\": \"Jonathan Hayase\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284689404\",\n",
      "            \"name\": \"Ema Borevkovic\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2483738\",\n",
      "            \"name\": \"Nicholas Carlini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2444919\",\n",
      "            \"name\": \"Florian Tramr\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3490923\",\n",
      "            \"name\": \"Milad Nasr\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"91b6158978b248e9a0e65d0d588bc1054e72bc16\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/91b6158978b248e9a0e65d0d588bc1054e72bc16\",\n",
      "        \"title\": \"MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization\",\n",
      "        \"citationCount\": 28,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://aclanthology.org/2023.findings-emnlp.215.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP and proposes a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2260750930\",\n",
      "            \"name\": \"Yuyan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260655625\",\n",
      "            \"name\": \"Zhihao Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260651904\",\n",
      "            \"name\": \"Ge Fan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273721608\",\n",
      "            \"name\": \"Zhengyu Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273816877\",\n",
      "            \"name\": \"Wei Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260908086\",\n",
      "            \"name\": \"Dayiheng Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243457917\",\n",
      "            \"name\": \"Zhixu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2163832089\",\n",
      "            \"name\": \"Bang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265724350\",\n",
      "            \"name\": \"Yanghua Xiao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/135afe1ddef0ca7d4e9404ec88d2a4691cfe8c19\",\n",
      "        \"title\": \"Federated Text-driven Prompt Generation for Vision-Language Models\",\n",
      "        \"citationCount\": 27,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2256983385\",\n",
      "            \"name\": \"Chen Qiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257324808\",\n",
      "            \"name\": \"Xingyu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"29359383\",\n",
      "            \"name\": \"Chaithanya Kumar Mummadi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144487556\",\n",
      "            \"name\": \"M. Ganesh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257091754\",\n",
      "            \"name\": \"Zhenzhen Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257130661\",\n",
      "            \"name\": \"Lu Peng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257132255\",\n",
      "            \"name\": \"Wan-Yi Lin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\n",
      "        \"title\": \"EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2024.3482175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2024.3482175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2145205189\",\n",
      "            \"name\": \"Yinsong Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266873357\",\n",
      "            \"name\": \"Jiaqi Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266387679\",\n",
      "            \"name\": \"Aidong Men\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266583142\",\n",
      "            \"name\": \"Qingchao Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1566d96346927ad4dced85de4d55356f6aee6fb6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1566d96346927ad4dced85de4d55356f6aee6fb6\",\n",
      "        \"title\": \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\n",
      "        \"citationCount\": 92,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A new approach to code generation by LLMs is proposed, which is called AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"72729733\",\n",
      "            \"name\": \"T. Ridnik\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279758170\",\n",
      "            \"name\": \"Dedy Kredo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"49668367\",\n",
      "            \"name\": \"Itamar Friedman\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"211e915b2e1e0753ddd581f10362fc82f28cc606\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/211e915b2e1e0753ddd581f10362fc82f28cc606\",\n",
      "        \"title\": \"DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.18597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"DiTCtrl is proposed, a training-free multi-prompt video generation method under MM-DiT architectures for the first time that achieves state-of-the-art performance without additional training and presents MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2337083029\",\n",
      "            \"name\": \"Minghong Cai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30176430\",\n",
      "            \"name\": \"Xiaodong Cun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257035102\",\n",
      "            \"name\": \"Xiaoyu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2308556703\",\n",
      "            \"name\": \"Wenze Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2303078452\",\n",
      "            \"name\": \"Zhaoyang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257199953\",\n",
      "            \"name\": \"Yong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268490605\",\n",
      "            \"name\": \"Ying Shan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316484241\",\n",
      "            \"name\": \"Xiangyu Yue\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer (MM-DiT) architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiTs attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training. Code is available at https://github.com/TencentARC/DiTCtrl.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7be9a0708300e42fd3c376b4f4027dd530f240af\",\n",
      "        \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2404.04095\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces the Prompt Auto-Editing (PAE) method, which employs an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2148661301\",\n",
      "            \"name\": \"Wenyi Mo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146332319\",\n",
      "            \"name\": \"Tianyu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2281418241\",\n",
      "            \"name\": \"Yalong Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2295513824\",\n",
      "            \"name\": \"Bing Su\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293310016\",\n",
      "            \"name\": \"Ji-Rong Wen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2281323801\",\n",
      "            \"name\": \"Qing Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"iterative context update\": {\n",
      "    \"total\": 9219,\n",
      "    \"offset\": 0,\n",
      "    \"next\": 50,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\n",
      "        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\n",
      "        \"citationCount\": 51,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2004.02194\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-04-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"144713153\",\n",
      "            \"name\": \"Dan Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46507139\",\n",
      "            \"name\": \"Haibo Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"5462268\",\n",
      "            \"name\": \"Hanwang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143962510\",\n",
      "            \"name\": \"Zhengjun Zha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47446553\",\n",
      "            \"name\": \"Meng Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\n",
      "        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\n",
      "        \"citationCount\": 288,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2303.06615\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-03-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2158317969\",\n",
      "            \"name\": \"Gangwei Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107956900\",\n",
      "            \"name\": \"Xianqi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2117436225\",\n",
      "            \"name\": \"Xiao-Hua Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2150441002\",\n",
      "            \"name\": \"Xin Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\n",
      "        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-04-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"15569181\",\n",
      "            \"name\": \"S. Cipolla\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51043392\",\n",
      "            \"name\": \"J. Gondzio\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to showusing a new rearrangement of the Schur complement which exploits regularizationthat general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\n",
      "        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\n",
      "          \"status\": \"BRONZE\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2021-06-29\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1490486462\",\n",
      "            \"name\": \"Jie Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1518268141\",\n",
      "            \"name\": \"Laiyan Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1516136186\",\n",
      "            \"name\": \"Rui Huang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\n",
      "        \"title\": \"Iterative Privileged Learning\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2116383294\",\n",
      "            \"name\": \"Xue Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145728792\",\n",
      "            \"name\": \"Bo Du\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2760404\",\n",
      "            \"name\": \"Yipeng Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Chang Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143719920\",\n",
      "            \"name\": \"D. Tao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\n",
      "        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\n",
      "        \"citationCount\": 211,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2286959603\",\n",
      "            \"name\": \"Zhimin Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301232669\",\n",
      "            \"name\": \"Jianwei Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301456725\",\n",
      "            \"name\": \"Qin Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291439620\",\n",
      "            \"name\": \"Jiangfeng Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301272172\",\n",
      "            \"name\": \"Yanxin Long\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291203760\",\n",
      "            \"name\": \"Xinchi Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301231346\",\n",
      "            \"name\": \"Yingfang Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2278003955\",\n",
      "            \"name\": \"Xingchao Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2162225343\",\n",
      "            \"name\": \"Minbin Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301268000\",\n",
      "            \"name\": \"Zedong Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301264655\",\n",
      "            \"name\": \"Dayou Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268375443\",\n",
      "            \"name\": \"Jiajun He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276668269\",\n",
      "            \"name\": \"Jiahao Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301265720\",\n",
      "            \"name\": \"Wenyue Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279868541\",\n",
      "            \"name\": \"Chen Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2194482010\",\n",
      "            \"name\": \"Rongwei Quan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301417687\",\n",
      "            \"name\": \"Jianxiang Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301265144\",\n",
      "            \"name\": \"Jiabin Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302916549\",\n",
      "            \"name\": \"Xiaoyan Yuan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282541108\",\n",
      "            \"name\": \"Xiao-Ting Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2238393614\",\n",
      "            \"name\": \"Yixuan Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290433340\",\n",
      "            \"name\": \"Jihong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256776298\",\n",
      "            \"name\": \"Chao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279065846\",\n",
      "            \"name\": \"Mengxi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290446584\",\n",
      "            \"name\": \"Jie Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2180527302\",\n",
      "            \"name\": \"Zheng Fang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280852403\",\n",
      "            \"name\": \"Weiyan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2067731583\",\n",
      "            \"name\": \"J. Xue\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267016579\",\n",
      "            \"name\": \"Yang-Dan Tao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2141509708\",\n",
      "            \"name\": \"Jianchen Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2278809013\",\n",
      "            \"name\": \"Kai Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2107932778\",\n",
      "            \"name\": \"Si-Da Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2373974424\",\n",
      "            \"name\": \"Yifu Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261466479\",\n",
      "            \"name\": \"Yun Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2195078271\",\n",
      "            \"name\": \"Dongdong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2393630397\",\n",
      "            \"name\": \"Mingtao Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297268600\",\n",
      "            \"name\": \"Zhichao Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290971075\",\n",
      "            \"name\": \"Xiao Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266017750\",\n",
      "            \"name\": \"Yan Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265797902\",\n",
      "            \"name\": \"Yuhong Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283172530\",\n",
      "            \"name\": \"Wei Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256286591\",\n",
      "            \"name\": \"Dingyong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284490220\",\n",
      "            \"name\": \"Yong Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280399696\",\n",
      "            \"name\": \"Jie Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268722917\",\n",
      "            \"name\": \"Qinglin Lu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\n",
      "        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\n",
      "        \"citationCount\": 39,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2279774134\",\n",
      "            \"name\": \"Yanlin Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279774134\",\n",
      "            \"name\": \"Yanlin Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2314374754\",\n",
      "            \"name\": \"Daya Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254800142\",\n",
      "            \"name\": \"Jiachi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305869100\",\n",
      "            \"name\": \"Ruikai Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305694096\",\n",
      "            \"name\": \"Yuchi Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267902535\",\n",
      "            \"name\": \"Zibin Zheng\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\n",
      "        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\n",
      "        \"citationCount\": 15,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-03-17\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2257320765\",\n",
      "            \"name\": \"Weiyu Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2347655949\",\n",
      "            \"name\": \"Ziyang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2347552474\",\n",
      "            \"name\": \"Shaoguang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2347022035\",\n",
      "            \"name\": \"Jianxiang He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294806563\",\n",
      "            \"name\": \"Yijie Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2348899671\",\n",
      "            \"name\": \"Jinhui Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257321422\",\n",
      "            \"name\": \"Ying Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2346984163\",\n",
      "            \"name\": \"Hui Xiong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\"finding a needle in a haystack.\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\n",
      "        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2310610834\",\n",
      "            \"name\": \"Alexey Kravets\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2310607749\",\n",
      "            \"name\": \"Vinay Namboodiri\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\n",
      "        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\n",
      "        \"citationCount\": 21,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2023-03-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2133908735\",\n",
      "            \"name\": \"B. M. Kessels\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"102993518\",\n",
      "            \"name\": \"R. Fey\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2246982557\",\n",
      "            \"name\": \"N. van de Wouw\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the systems entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\n",
      "        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\n",
      "        \"citationCount\": 31,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBYNCND\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-04-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"50823831\",\n",
      "            \"name\": \"Jonah Casebeer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"32125125\",\n",
      "            \"name\": \"Nicholas J. Bryan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1718742\",\n",
      "            \"name\": \"Paris Smaragdis\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against  all using a single general-purpose configuration of our approach.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\n",
      "        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"46532001\",\n",
      "            \"name\": \"Guohua Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30176488\",\n",
      "            \"name\": \"Qizhang Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2153095510\",\n",
      "            \"name\": \"Yanqi Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2049670966\",\n",
      "            \"name\": \"Xinjiang Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115387723\",\n",
      "            \"name\": \"Yang Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1731634\",\n",
      "            \"name\": \"W. Pedrycz\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\n",
      "        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\n",
      "        \"citationCount\": 28,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2021-02-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"26411201\",\n",
      "            \"name\": \"Xiaojun Mei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35250883\",\n",
      "            \"name\": \"Huafeng Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144922818\",\n",
      "            \"name\": \"J. Xian\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150041356\",\n",
      "            \"name\": \"Bowen Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\n",
      "        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2022-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2445322\",\n",
      "            \"name\": \"Guoqiu Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"27066021\",\n",
      "            \"name\": \"Huanqian Yan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2769710\",\n",
      "            \"name\": \"Xingxing Wei\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\% on average.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\n",
      "        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-12-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2765914\",\n",
      "            \"name\": \"Zhen Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2158172225\",\n",
      "            \"name\": \"Peng Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"10684484\",\n",
      "            \"name\": \"Yunpu Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1742501819\",\n",
      "            \"name\": \"Volker Tresp\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\n",
      "        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-05-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"40370059\",\n",
      "            \"name\": \"Ashok Bandi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153604607\",\n",
      "            \"name\": \"Bhavani Shankar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1760292\",\n",
      "            \"name\": \"S. Chatzinotas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"102896981\",\n",
      "            \"name\": \"B. Ottersten\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\n",
      "        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\n",
      "        \"citationCount\": 16,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-02-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"97740251\",\n",
      "            \"name\": \"Vijay Kakani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2380060\",\n",
      "            \"name\": \"Hakil Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2108780171\",\n",
      "            \"name\": \"Jongso Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34630628\",\n",
      "            \"name\": \"Choonwoo Ryu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"14653761\",\n",
      "            \"name\": \"Mahendar Kumbham\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\n",
      "        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-10-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"49916135\",\n",
      "            \"name\": \"Salman Habib\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"93037496\",\n",
      "            \"name\": \"Allison Beemer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064748249\",\n",
      "            \"name\": \"J. Kliewer\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\n",
      "        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\n",
      "        \"citationCount\": 40,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2170260327\",\n",
      "            \"name\": \"Zhangqian Bi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273871682\",\n",
      "            \"name\": \"Yao Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293395709\",\n",
      "            \"name\": \"Zheng Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273385018\",\n",
      "            \"name\": \"Hongyu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293312291\",\n",
      "            \"name\": \"Batu Guan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293769071\",\n",
      "            \"name\": \"Fangxin Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293399901\",\n",
      "            \"name\": \"Zili Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34296085\",\n",
      "            \"name\": \"Yulei Sui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1678835\",\n",
      "            \"name\": \"Xuanhua Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277587874\",\n",
      "            \"name\": \"Hai Jin\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\n",
      "        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract', 'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"151101398\",\n",
      "            \"name\": \"Lanyun Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261909262\",\n",
      "            \"name\": \"Tianrun Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2510538\",\n",
      "            \"name\": \"Jianxiong Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2250849553\",\n",
      "            \"name\": \"Simon See\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2321778822\",\n",
      "            \"name\": \"Jun Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\n",
      "        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\n",
      "        \"citationCount\": 30,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2306.09869\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-06-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"153118937\",\n",
      "            \"name\": \"Geon Yeong Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109216792\",\n",
      "            \"name\": \"Jeongsol Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3332270\",\n",
      "            \"name\": \"Beomsu Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152577026\",\n",
      "            \"name\": \"Sang Wan Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30547794\",\n",
      "            \"name\": \"Jong-Chul Ye\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fca9a4508863025d95a581ead47032d497825053\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/fca9a4508863025d95a581ead47032d497825053\",\n",
      "        \"title\": \"VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.02186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"VideoICL is a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach, improving OOD video understanding performance by extending effective context length without incurring high costs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-03\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2333526847\",\n",
      "            \"name\": \"Kangsan Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307075970\",\n",
      "            \"name\": \"Geon Park\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3445691\",\n",
      "            \"name\": \"Youngwan Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2119578055\",\n",
      "            \"name\": \"Woongyeong Yeo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265627157\",\n",
      "            \"name\": \"Sung Ju Hwang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in video large multimodal models (LMMs) have significantly improved their video understanding and reasoning capabilities. However, their performance drops on out-of-distribution (OOD) tasks that are underrepresented in training data. Traditional methods like fine-tuning on OOD datasets are impractical due to high computational costs. While In-context learning (ICL) with demonstration examples has shown promising generalization performance in language tasks and image-language tasks without fine-tuning, applying ICL to video-language tasks faces challenges due to the limited context length in Video LMMs, as videos require longer token lengths. To address these issues, we propose VideoICL, a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach. This allows to select the most relevant examples and rank them based on similarity, to be used for inference. If the generated response has low confidence, our framework selects new examples and performs inference again, iteratively refining the results until a high-confidence response is obtained. This approach improves OOD video understanding performance by extending effective context length without incurring high costs. The experimental results on multiple benchmarks demonstrate significant performance gains, especially in domain-specific scenarios, laying the groundwork for broader video comprehension applications. Code is released at https://github.com/KangsanKim07/VideoICL\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\n",
      "        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\n",
      "        \"citationCount\": 72,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-15\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2084609980\",\n",
      "            \"name\": \"Chengwei Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258754564\",\n",
      "            \"name\": \"Aston Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1627060158\",\n",
      "            \"name\": \"Anirudh Dagar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258719488\",\n",
      "            \"name\": \"Wenming Ye\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\n",
      "        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.13016\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2135964855\",\n",
      "            \"name\": \"Jiaxi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"151471590\",\n",
      "            \"name\": \"Binyuan Hui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144399900\",\n",
      "            \"name\": \"Min Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"66200440\",\n",
      "            \"name\": \"Binhua Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2087380523\",\n",
      "            \"name\": \"Fei Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1527090216\",\n",
      "            \"name\": \"Yongbin Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\n",
      "        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2309.13701\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2245381886\",\n",
      "            \"name\": \"Hosein Hasanbeig\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"20013278\",\n",
      "            \"name\": \"Hiteshi Sharma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"15449757\",\n",
      "            \"name\": \"Leo Betthauser\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1844283112\",\n",
      "            \"name\": \"F. Frujeri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2248289111\",\n",
      "            \"name\": \"Ida Momennejad\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\n",
      "        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\n",
      "        \"citationCount\": 12,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2245381886\",\n",
      "            \"name\": \"Hosein Hasanbeig\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258705787\",\n",
      "            \"name\": \"Microsoft Usa HITESHI SHARMA\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258705783\",\n",
      "            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258705381\",\n",
      "            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2248289111\",\n",
      "            \"name\": \"Ida Momennejad\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\n",
      "        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2118272992\",\n",
      "            \"name\": \"Zihan Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144986261\",\n",
      "            \"name\": \"D. Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2156506994\",\n",
      "            \"name\": \"Xinghuo Yu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/16749df89bb5d09c570fccad0fac3bbe7fc41b6c\",\n",
      "        \"title\": \"Single image deraining using scale constraint iterative update network\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2023.121339?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2023.121339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1390863714\",\n",
      "            \"name\": \"Yitong Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1591131546\",\n",
      "            \"name\": \"Yongjun Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150356192\",\n",
      "            \"name\": \"Zhongwei Cui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112674491\",\n",
      "            \"name\": \"Haoliang Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2210993430\",\n",
      "            \"name\": \"Ting Ouyang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\n",
      "        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\n",
      "        \"citationCount\": 188,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2299483768\",\n",
      "            \"name\": \"Yuxi Xie\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1996705\",\n",
      "            \"name\": \"Anirudh Goyal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289857220\",\n",
      "            \"name\": \"Wenyue Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257033898\",\n",
      "            \"name\": \"Min-Yen Kan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2542999\",\n",
      "            \"name\": \"T. Lillicrap\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266466003\",\n",
      "            \"name\": \"Kenji Kawaguchi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289844602\",\n",
      "            \"name\": \"Michael Shieh\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\%$ (+$5.9\\\\%$), $34.7\\\\%$ (+$5.8\\\\%$), and $76.4\\\\%$ (+$15.8\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\n",
      "        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\n",
      "        \"citationCount\": 366,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2305.15294\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-24\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"144485528\",\n",
      "            \"name\": \"Zhihong Shao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2171182\",\n",
      "            \"name\": \"Yeyun Gong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1752875\",\n",
      "            \"name\": \"Yelong Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1730108\",\n",
      "            \"name\": \"Minlie Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46429989\",\n",
      "            \"name\": \"Nan Duan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109136147\",\n",
      "            \"name\": \"Weizhu Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\n",
      "        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\n",
      "        \"citationCount\": 330,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://arxiv.org/pdf/2303.12570\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-03-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2158120018\",\n",
      "            \"name\": \"Fengji Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143876723\",\n",
      "            \"name\": \"B. Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2211964951\",\n",
      "            \"name\": \"Yue Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155352529\",\n",
      "            \"name\": \"Jin Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2134434187\",\n",
      "            \"name\": \"Daoguang Zan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145469202\",\n",
      "            \"name\": \"Yi Mao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153249455\",\n",
      "            \"name\": \"Jian-Guang Lou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109136147\",\n",
      "            \"name\": \"Weizhu Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\n",
      "        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2103.02022\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2021-03-02\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"104491233\",\n",
      "            \"name\": \"M. Flamarion\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1429030643\",\n",
      "            \"name\": \"R. Ribeiro-Jr\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB SchwarzChristoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\n",
      "        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\n",
      "        \"citationCount\": 37,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"38318287\",\n",
      "            \"name\": \"P. Amestoy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1760511\",\n",
      "            \"name\": \"A. Buttari\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1699285\",\n",
      "            \"name\": \"N. Higham\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1398545502\",\n",
      "            \"name\": \"J. LExcellent\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144615299\",\n",
      "            \"name\": \"Tho Mary\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1720746305\",\n",
      "            \"name\": \"Bastien Vieubl\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \". GMRES-based iterative renement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the ve-precision GMRES-based iterative renement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identies a small subset of relevant combinations. By choosing from within this subset one can achieve dierent levels of tradeo between cost and robustness, which allows for a ner choice of precisions depending on the problem diculty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\n",
      "        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\n",
      "        \"citationCount\": 70,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"47196237\",\n",
      "            \"name\": \"Zihao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"70097297\",\n",
      "            \"name\": \"Anji Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257447835\",\n",
      "            \"name\": \"Haowei Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290567624\",\n",
      "            \"name\": \"Jiaqi Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257636629\",\n",
      "            \"name\": \"Xiaojian Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257367774\",\n",
      "            \"name\": \"Yitao Liang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\n",
      "        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\n",
      "        \"citationCount\": 32,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"35361812\",\n",
      "            \"name\": \"Hongping Gan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260282205\",\n",
      "            \"name\": \"Xiaoyang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260279661\",\n",
      "            \"name\": \"Lijun He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260264613\",\n",
      "            \"name\": \"Jie Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\n",
      "        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2021-02-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1423730807\",\n",
      "            \"name\": \"Carsten Wiecher\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2909345\",\n",
      "            \"name\": \"Joel Greenyer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40500399\",\n",
      "            \"name\": \"Carsten Wolff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152617022\",\n",
      "            \"name\": \"H. Anacker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3261846\",\n",
      "            \"name\": \"R. Dumitrescu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\n",
      "        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\n",
      "        \"citationCount\": 11,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": \"CCBYNC\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2021-02-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"31183250\",\n",
      "            \"name\": \"I. Klein\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1834987\",\n",
      "            \"name\": \"S. Suraci\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2125043152\",\n",
      "            \"name\": \"Leonardo Castro de Oliveira\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"72414897\",\n",
      "            \"name\": \"V. F. Rofatto\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2059636056\",\n",
      "            \"name\": \"M. T. Matsuoka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1691074\",\n",
      "            \"name\": \"S. Baselga\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\n",
      "        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\n",
      "        \"citationCount\": 16,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2046956775\",\n",
      "            \"name\": \"Benita Nortmann\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267944139\",\n",
      "            \"name\": \"Andrea Monti\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1764871\",\n",
      "            \"name\": \"M. Sassano\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2695624\",\n",
      "            \"name\": \"T. Mylvaganam\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other's performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving humanrobot interaction.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\n",
      "        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"5691344\",\n",
      "            \"name\": \"Yujiao Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39271955\",\n",
      "            \"name\": \"Zhiwen Fang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153431402\",\n",
      "            \"name\": \"Shaofeng Yuan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30380386\",\n",
      "            \"name\": \"C. Zhan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2072728505\",\n",
      "            \"name\": \"Yanyan Xing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"10638646\",\n",
      "            \"name\": \"Joey Tianyi Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2115412895\",\n",
      "            \"name\": \"Feng Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\"bibr\\\" rid=\\\"ref1\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\"bibr\\\" rid=\\\"ref2\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\"bibr\\\" rid=\\\"ref3\\\">[3]</xref> and PH2 <xref ref-type=\\\"bibr\\\" rid=\\\"ref4\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\n",
      "        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\n",
      "        \"citationCount\": 44,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-09-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2158317969\",\n",
      "            \"name\": \"Gangwei Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289775621\",\n",
      "            \"name\": \"Xianqi Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2319408912\",\n",
      "            \"name\": \"Zhaoxing Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2049033628\",\n",
      "            \"name\": \"Junda Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2319622516\",\n",
      "            \"name\": \"Chunyuan Liao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265575132\",\n",
      "            \"name\": \"Xin Yang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\n",
      "        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\n",
      "        \"citationCount\": 48,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-06\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2028213158\",\n",
      "            \"name\": \"Zhenrui Yue\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39371343\",\n",
      "            \"name\": \"Honglei Zhuang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2324782053\",\n",
      "            \"name\": \"Aijun Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261281337\",\n",
      "            \"name\": \"Kai Hui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1886219\",\n",
      "            \"name\": \"R. Jagerman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2324910979\",\n",
      "            \"name\": \"Hansi Zeng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2099586642\",\n",
      "            \"name\": \"Zhen Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2325158655\",\n",
      "            \"name\": \"Dong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261356664\",\n",
      "            \"name\": \"Xuanhui Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1815447\",\n",
      "            \"name\": \"Michael Bendersky\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\n",
      "        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\n",
      "        \"citationCount\": 41,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-29\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"23111704\",\n",
      "            \"name\": \"Banghua Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273930444\",\n",
      "            \"name\": \"Michael I. Jordan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258657022\",\n",
      "            \"name\": \"Jiantao Jiao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\n",
      "        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2116395954\",\n",
      "            \"name\": \"Junbin Yuan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254058034\",\n",
      "            \"name\": \"Aiqing Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2575087\",\n",
      "            \"name\": \"Qingzhen Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2757120\",\n",
      "            \"name\": \"Kanoksak Wattanachote\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2240697074\",\n",
      "            \"name\": \"Yongyi Gong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\n",
      "        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\n",
      "        \"citationCount\": 36,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Book\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2305310515\",\n",
      "            \"name\": \"Zhouyu Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273904059\",\n",
      "            \"name\": \"Mengshu Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2303947668\",\n",
      "            \"name\": \"Lei Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290024603\",\n",
      "            \"name\": \"Zhiqiang Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\n",
      "        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\n",
      "        \"citationCount\": 2891,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1557386977\",\n",
      "            \"name\": \"Machel Reid\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2417003\",\n",
      "            \"name\": \"Nikolay Savinov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3035073\",\n",
      "            \"name\": \"Denis Teplyashin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150077954\",\n",
      "            \"name\": \"Dmitry Lepikhin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2542999\",\n",
      "            \"name\": \"T. Lillicrap\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2285263\",\n",
      "            \"name\": \"Jean-Baptiste Alayrac\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1737285\",\n",
      "            \"name\": \"Radu Soricut\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2672644\",\n",
      "            \"name\": \"Angeliki Lazaridou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273534960\",\n",
      "            \"name\": \"Orhan Firat\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4337102\",\n",
      "            \"name\": \"Julian Schrittwieser\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2460849\",\n",
      "            \"name\": \"Ioannis Antonoglou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1508890387\",\n",
      "            \"name\": \"Rohan Anil\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"148016269\",\n",
      "            \"name\": \"Sebastian Borgeaud\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273563615\",\n",
      "            \"name\": \"Andrew M. Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143434227\",\n",
      "            \"name\": \"Katie Millican\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180676\",\n",
      "            \"name\": \"Ethan Dyer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143471164\",\n",
      "            \"name\": \"Mia Glaese\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2070364520\",\n",
      "            \"name\": \"Thibault Sottiaux\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275292292\",\n",
      "            \"name\": \"Ben-jamin Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484786\",\n",
      "            \"name\": \"Fabio Viola\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47447264\",\n",
      "            \"name\": \"Malcolm Reynolds\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2145139570\",\n",
      "            \"name\": \"Yuanzhong Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2065370007\",\n",
      "            \"name\": \"James Molloy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249566095\",\n",
      "            \"name\": \"Jilin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2090818\",\n",
      "            \"name\": \"M. Isard\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152399055\",\n",
      "            \"name\": \"P. Barham\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146532222\",\n",
      "            \"name\": \"Tom Hennigan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176102\",\n",
      "            \"name\": \"Ross Mcilroy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275525680\",\n",
      "            \"name\": \"Melvin Johnson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1698491\",\n",
      "            \"name\": \"J. Schalkwyk\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181648\",\n",
      "            \"name\": \"Eli Collins\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143538252\",\n",
      "            \"name\": \"Eliza Rutherford\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185558\",\n",
      "            \"name\": \"Erica Moreira\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34122449\",\n",
      "            \"name\": \"Kareem W. Ayoub\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186741\",\n",
      "            \"name\": \"Megha Goel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1406288863\",\n",
      "            \"name\": \"Clemens Meyer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2005813\",\n",
      "            \"name\": \"Gregory Thornton\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275729730\",\n",
      "            \"name\": \"Zhen Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47407464\",\n",
      "            \"name\": \"H. Michalewski\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185143\",\n",
      "            \"name\": \"Zaheer Abbas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"74530494\",\n",
      "            \"name\": \"Nathan Schucher\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"12679121\",\n",
      "            \"name\": \"Ankesh Anand\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185509\",\n",
      "            \"name\": \"Richard Ives\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2058168486\",\n",
      "            \"name\": \"James Keeling\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3257286\",\n",
      "            \"name\": \"Karel Lenc\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40269586\",\n",
      "            \"name\": \"S. Haykal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2944868\",\n",
      "            \"name\": \"Siamak Shakeri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"67311962\",\n",
      "            \"name\": \"Pranav Shyam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2841893\",\n",
      "            \"name\": \"A. Chowdhery\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"81387328\",\n",
      "            \"name\": \"Roman Ring\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2135383313\",\n",
      "            \"name\": \"Stephen Spencer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1413718981\",\n",
      "            \"name\": \"Eren Sezener\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289035179\",\n",
      "            \"name\": \"Luke Vilnis\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186577\",\n",
      "            \"name\": \"Os-car Chang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288269273\",\n",
      "            \"name\": \"Nobuyuki Morioka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275183383\",\n",
      "            \"name\": \"George Tucker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276211400\",\n",
      "            \"name\": \"Ce Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186839\",\n",
      "            \"name\": \"Oliver Woodman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"80930649\",\n",
      "            \"name\": \"Nithya Attaluri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2367821\",\n",
      "            \"name\": \"Toms Kocisk\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187189\",\n",
      "            \"name\": \"Evgenii Eltyshev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275535939\",\n",
      "            \"name\": \"Xi Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188933\",\n",
      "            \"name\": \"Timothy Chung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1394635460\",\n",
      "            \"name\": \"Vittorio Selo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1791585\",\n",
      "            \"name\": \"Siddhartha Brahma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1737522\",\n",
      "            \"name\": \"Petko Georgiev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"133666998\",\n",
      "            \"name\": \"Ambrose Slone\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275539055\",\n",
      "            \"name\": \"Zhenkai Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266398107\",\n",
      "            \"name\": \"James Lottes\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275178766\",\n",
      "            \"name\": \"Siyuan Qiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484287\",\n",
      "            \"name\": \"Ben Caine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287841795\",\n",
      "            \"name\": \"Sebastian Riedel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176047\",\n",
      "            \"name\": \"Alex Tomala\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159545857\",\n",
      "            \"name\": \"Martin Chadwick\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253158807\",\n",
      "            \"name\": \"J Christopher Love\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2070068655\",\n",
      "            \"name\": \"Peter Choy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2073395505\",\n",
      "            \"name\": \"Sid Mittal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2815290\",\n",
      "            \"name\": \"N. Houlsby\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269752766\",\n",
      "            \"name\": \"Yunhao Tang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289446231\",\n",
      "            \"name\": \"Matthew Lamm\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275159462\",\n",
      "            \"name\": \"Libin Bai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2197671266\",\n",
      "            \"name\": \"Qiao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253917827\",\n",
      "            \"name\": \"Luheng He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275287219\",\n",
      "            \"name\": \"Yong Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186692\",\n",
      "            \"name\": \"Peter Humphreys\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275290025\",\n",
      "            \"name\": \"Yujia Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1786259\",\n",
      "            \"name\": \"Sergey Brin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51042571\",\n",
      "            \"name\": \"Albin Cassirer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283231534\",\n",
      "            \"name\": \"Ying-Qi Miao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1780245\",\n",
      "            \"name\": \"Luks Zilka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275189014\",\n",
      "            \"name\": \"Taylor Tobin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266735761\",\n",
      "            \"name\": \"Kelvin Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2161966573\",\n",
      "            \"name\": \"Lev Proleev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275175792\",\n",
      "            \"name\": \"Daniel Sohn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182383\",\n",
      "            \"name\": \"Al-berto Magni\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258347245\",\n",
      "            \"name\": \"L. Hendricks\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290513267\",\n",
      "            \"name\": \"Isabel Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2217756237\",\n",
      "            \"name\": \"Santiago Ontan'on\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275177720\",\n",
      "            \"name\": \"Oskar Bunyan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185667\",\n",
      "            \"name\": \"Nathan Byrd\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275537981\",\n",
      "            \"name\": \"Abhanshu Sharma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48335426\",\n",
      "            \"name\": \"Biao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290662953\",\n",
      "            \"name\": \"Mario Pinto\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275170606\",\n",
      "            \"name\": \"Rishika Sinha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"18138802\",\n",
      "            \"name\": \"Harsh Mehta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186531\",\n",
      "            \"name\": \"Dawei Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1413064976\",\n",
      "            \"name\": \"Sergi Caelles\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1991019030\",\n",
      "            \"name\": \"Albert Webson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275162692\",\n",
      "            \"name\": \"Alex Morris\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2080504963\",\n",
      "            \"name\": \"Becca Roelofs\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290634593\",\n",
      "            \"name\": \"Yifan Ding\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"86898863\",\n",
      "            \"name\": \"Robin Strudel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193471\",\n",
      "            \"name\": \"Xuehan Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39687627\",\n",
      "            \"name\": \"Marvin Ritter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256989598\",\n",
      "            \"name\": \"Mostafa Dehghani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1706980\",\n",
      "            \"name\": \"R. Chaabouni\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2078909017\",\n",
      "            \"name\": \"Abhijit Karmarkar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484705\",\n",
      "            \"name\": \"Guangda Lai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3468078\",\n",
      "            \"name\": \"Fabian Mentzer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290664586\",\n",
      "            \"name\": \"Bibo Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261797906\",\n",
      "            \"name\": \"YaGuang Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275534739\",\n",
      "            \"name\": \"Yujing Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40470211\",\n",
      "            \"name\": \"T. Paine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40034895\",\n",
      "            \"name\": \"Alex Goldin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3007442\",\n",
      "            \"name\": \"Behnam Neyshabur\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1734809439\",\n",
      "            \"name\": \"Kate Baumli\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6639036\",\n",
      "            \"name\": \"Anselm Levskaya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274104519\",\n",
      "            \"name\": \"Michael Laskin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193644\",\n",
      "            \"name\": \"Wenhao Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34269227\",\n",
      "            \"name\": \"Jack W. Rae\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268673324\",\n",
      "            \"name\": \"Kefan Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485493\",\n",
      "            \"name\": \"Antoine He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487747\",\n",
      "            \"name\": \"Skye Giordano\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307454258\",\n",
      "            \"name\": \"Lakshman Yagati\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143783339\",\n",
      "            \"name\": \"Jean-Baptiste Lespiau\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"122704930\",\n",
      "            \"name\": \"Paul Natsev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185831\",\n",
      "            \"name\": \"Sanjay Ganapathy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144097210\",\n",
      "            \"name\": \"Fangyu Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487616\",\n",
      "            \"name\": \"Danilo Martins\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249840944\",\n",
      "            \"name\": \"Nanxin Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275191526\",\n",
      "            \"name\": \"Yunhan Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180117\",\n",
      "            \"name\": \"Megan Barnes\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268760156\",\n",
      "            \"name\": \"Rhys May\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188533\",\n",
      "            \"name\": \"Arpi Vezer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275114643\",\n",
      "            \"name\": \"Junhyuk Oh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118834006\",\n",
      "            \"name\": \"Ken Franko\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273670422\",\n",
      "            \"name\": \"Sophie Bridgers\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275832693\",\n",
      "            \"name\": \"Ruizhe Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275291909\",\n",
      "            \"name\": \"Boxi Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40608942\",\n",
      "            \"name\": \"Basil Mustafa\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487447\",\n",
      "            \"name\": \"Sean Sechrist\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3166516\",\n",
      "            \"name\": \"Emilio Parisotto\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2598683\",\n",
      "            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487293\",\n",
      "            \"name\": \"Chris Larkin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275149073\",\n",
      "            \"name\": \"Chenjie Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186804\",\n",
      "            \"name\": \"Christina Sorokin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2048712\",\n",
      "            \"name\": \"M. Krikun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182203\",\n",
      "            \"name\": \"Alexey Guseynov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2065404873\",\n",
      "            \"name\": \"Jessica Landon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187147\",\n",
      "            \"name\": \"Romina Datta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1863250\",\n",
      "            \"name\": \"A. Pritzel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2151245633\",\n",
      "            \"name\": \"Phoebe Thacker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275801132\",\n",
      "            \"name\": \"Fan Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487874\",\n",
      "            \"name\": \"Kevin Hui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"119556335\",\n",
      "            \"name\": \"A.E. Hauth\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273556813\",\n",
      "            \"name\": \"C. Yeh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290481818\",\n",
      "            \"name\": \"David Barker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1423275766\",\n",
      "            \"name\": \"J. Mao-Jones\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2166051497\",\n",
      "            \"name\": \"Sophia Austin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2307453241\",\n",
      "            \"name\": \"Hannah Sheahan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2620528\",\n",
      "            \"name\": \"Parker Schuh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188153\",\n",
      "            \"name\": \"James Svensson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193365\",\n",
      "            \"name\": \"Rohan Jain\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"96641652\",\n",
      "            \"name\": \"V. Ramasesh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185833\",\n",
      "            \"name\": \"Anton Briukhov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180366\",\n",
      "            \"name\": \"D. Chung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51029932\",\n",
      "            \"name\": \"Tamara von Glehn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275166845\",\n",
      "            \"name\": \"Christina Butterfield\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184551\",\n",
      "            \"name\": \"Priya Jhakra\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275252154\",\n",
      "            \"name\": \"Matt Wiethoff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193725\",\n",
      "            \"name\": \"Justin Frye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275175432\",\n",
      "            \"name\": \"Jordan Grimstad\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2158369306\",\n",
      "            \"name\": \"Beer Changpinyo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153892869\",\n",
      "            \"name\": \"Charline Le Lan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181572\",\n",
      "            \"name\": \"Anna Bortsova\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275892922\",\n",
      "            \"name\": \"Yonghui Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2767859\",\n",
      "            \"name\": \"P. Voigtlaender\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279918122\",\n",
      "            \"name\": \"Tara N. Sainath\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275575378\",\n",
      "            \"name\": \"Charlotte Smith\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2191689971\",\n",
      "            \"name\": \"Will Hawkins\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275191626\",\n",
      "            \"name\": \"Kris Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186515\",\n",
      "            \"name\": \"James Besley\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2059763226\",\n",
      "            \"name\": \"S. Srinivasan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3175815\",\n",
      "            \"name\": \"Mark Omernick\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2160887964\",\n",
      "            \"name\": \"Colin Gaffney\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1956049835\",\n",
      "            \"name\": \"G. Surita\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484991\",\n",
      "            \"name\": \"Ryan Burnell\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2143374656\",\n",
      "            \"name\": \"Bogdan Damoc\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275220028\",\n",
      "            \"name\": \"Junwhan Ahn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2285740851\",\n",
      "            \"name\": \"Andrew Brock\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2146532125\",\n",
      "            \"name\": \"Mantas Pajarskas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187155\",\n",
      "            \"name\": \"Anastasia Petrushkina\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"30155667\",\n",
      "            \"name\": \"Seb Noury\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186192\",\n",
      "            \"name\": \"Lorenzo Blanco\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1754860\",\n",
      "            \"name\": \"Kevin Swersky\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185727\",\n",
      "            \"name\": \"Arun Ahuja\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261737895\",\n",
      "            \"name\": \"Thi Avrahami\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40055795\",\n",
      "            \"name\": \"Vedant Misra\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184736\",\n",
      "            \"name\": \"Raoul de Liedekerke\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181534\",\n",
      "            \"name\": \"Mariko Iinuma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144703404\",\n",
      "            \"name\": \"A. Polozov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"143981350\",\n",
      "            \"name\": \"Sarah York\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47568983\",\n",
      "            \"name\": \"George van den Driessche\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185732\",\n",
      "            \"name\": \"Paul Michel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2273650801\",\n",
      "            \"name\": \"Justin Chiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46901218\",\n",
      "            \"name\": \"Rory Blevins\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185661\",\n",
      "            \"name\": \"Zach Gleicher\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39257069\",\n",
      "            \"name\": \"Adri Recasens\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186093\",\n",
      "            \"name\": \"Alban Rrustemi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1980809\",\n",
      "            \"name\": \"E. Gribovskaya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275277736\",\n",
      "            \"name\": \"Au-rko Roy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487054\",\n",
      "            \"name\": \"Wiktor Gworek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186656\",\n",
      "            \"name\": \"Sbastien M. R. Arnold\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275291886\",\n",
      "            \"name\": \"Lisa Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267341862\",\n",
      "            \"name\": \"James Lee-Thorp\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2090812426\",\n",
      "            \"name\": \"M. Maggioni\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275183119\",\n",
      "            \"name\": \"Enrique Piqueras\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2051018967\",\n",
      "            \"name\": \"Kartikeya Badola\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2425230\",\n",
      "            \"name\": \"S. Vikram\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275585027\",\n",
      "            \"name\": \"Lucas Gonzalez\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186584\",\n",
      "            \"name\": \"Anirudh Baddepudi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268665228\",\n",
      "            \"name\": \"Evan Senter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261961752\",\n",
      "            \"name\": \"J. Devlin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47901308\",\n",
      "            \"name\": \"James Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275148073\",\n",
      "            \"name\": \"Michael Azzam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1994939814\",\n",
      "            \"name\": \"Maja Trebacz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35930544\",\n",
      "            \"name\": \"M. Polacek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485355\",\n",
      "            \"name\": \"Kashyap Krishnakumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275193337\",\n",
      "            \"name\": \"Shuo-Yiin Chang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176212\",\n",
      "            \"name\": \"Matthew Tung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187196\",\n",
      "            \"name\": \"Ivo Penchev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258551072\",\n",
      "            \"name\": \"Rishabh Joshi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180557\",\n",
      "            \"name\": \"Kate Olszewska\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184985\",\n",
      "            \"name\": \"Carrie Muir\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185968\",\n",
      "            \"name\": \"Mateo Wirth\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184113\",\n",
      "            \"name\": \"A. Hartman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2160888100\",\n",
      "            \"name\": \"Joshua Newlan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2252586080\",\n",
      "            \"name\": \"S. Kashem\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2218882489\",\n",
      "            \"name\": \"Vijay Bolina\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484418\",\n",
      "            \"name\": \"Elahe Dabir\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3038326\",\n",
      "            \"name\": \"Joost R. van Amersfoort\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275130349\",\n",
      "            \"name\": \"Zafarali Ahmed\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185511\",\n",
      "            \"name\": \"James Cobon-Kerr\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269391198\",\n",
      "            \"name\": \"Aishwarya B Kamath\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259962018\",\n",
      "            \"name\": \"A. M. Hrafnkelsson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274787555\",\n",
      "            \"name\": \"Le Hou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485798\",\n",
      "            \"name\": \"Ian Mackinnon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2156930381\",\n",
      "            \"name\": \"Alexandre Frechette\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51210148\",\n",
      "            \"name\": \"Eric Noland\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182246\",\n",
      "            \"name\": \"Xi-ance Si\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2779842\",\n",
      "            \"name\": \"Emanuel Taropa\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2350430090\",\n",
      "            \"name\": \"Dong Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275183277\",\n",
      "            \"name\": \"Phil Crone\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4478284\",\n",
      "            \"name\": \"Anmol Gulati\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180682\",\n",
      "            \"name\": \"S'ebastien Cevey\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275173231\",\n",
      "            \"name\": \"Jonas Adler\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275786213\",\n",
      "            \"name\": \"Ada Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185813\",\n",
      "            \"name\": \"David Silver\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"148152480\",\n",
      "            \"name\": \"Simon Tokumine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2067745837\",\n",
      "            \"name\": \"Richard Powell\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275280377\",\n",
      "            \"name\": \"Stephan Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275571997\",\n",
      "            \"name\": \"Michael B. Chang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275396476\",\n",
      "            \"name\": \"Samer Hassan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2007712128\",\n",
      "            \"name\": \"Diana Mincu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2064599701\",\n",
      "            \"name\": \"Antoine Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153898744\",\n",
      "            \"name\": \"Nir Levine\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186701\",\n",
      "            \"name\": \"Jenny Brennan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249764807\",\n",
      "            \"name\": \"Mingqiu Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265053608\",\n",
      "            \"name\": \"Sarah Hodkinson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144551262\",\n",
      "            \"name\": \"Jeffrey Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487597\",\n",
      "            \"name\": \"Josh Lipschultz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"20702300\",\n",
      "            \"name\": \"Aedan Pope\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275571997\",\n",
      "            \"name\": \"Michael B. Chang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275767067\",\n",
      "            \"name\": \"Cheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2121764\",\n",
      "            \"name\": \"Laurent El Shafey\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2264591527\",\n",
      "            \"name\": \"M. Paganini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269733876\",\n",
      "            \"name\": \"Sholto Douglas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266464503\",\n",
      "            \"name\": \"Bernd Bohnet\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274107421\",\n",
      "            \"name\": \"Fabio Pardo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182230\",\n",
      "            \"name\": \"Seth Odoom\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269541835\",\n",
      "            \"name\": \"Mihaela Roca\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267546965\",\n",
      "            \"name\": \"Cicero Nogueira dos Santos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185640\",\n",
      "            \"name\": \"Kedar Soparkar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35099444\",\n",
      "            \"name\": \"A. Guez\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187110\",\n",
      "            \"name\": \"Tom Hudson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188563\",\n",
      "            \"name\": \"Steven Hansen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"50844587\",\n",
      "            \"name\": \"Chulayuth Asawaroengchai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"104000494\",\n",
      "            \"name\": \"Ravichandra Addanki\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290486855\",\n",
      "            \"name\": \"Tianhe Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3448463\",\n",
      "            \"name\": \"Wojciech Stokowiec\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258793616\",\n",
      "            \"name\": \"Mina Khan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243002880\",\n",
      "            \"name\": \"Justin Gilmer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2253808003\",\n",
      "            \"name\": \"Jaehoon Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485108\",\n",
      "            \"name\": \"Carrie Grimes Bostock\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1996199677\",\n",
      "            \"name\": \"Keran Rong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2263289033\",\n",
      "            \"name\": \"Jonathan Caton\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184275\",\n",
      "            \"name\": \"Pedram Pejman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1696719\",\n",
      "            \"name\": \"Filip Pavetic\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259937157\",\n",
      "            \"name\": \"Geoff Brown\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290595918\",\n",
      "            \"name\": \"Vivek Sharma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2170162986\",\n",
      "            \"name\": \"Mario Luvci'c\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176205\",\n",
      "            \"name\": \"Rajku-mar Samuel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2941141\",\n",
      "            \"name\": \"J. Djolonga\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2063800905\",\n",
      "            \"name\": \"Amol Mandhane\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187845\",\n",
      "            \"name\": \"Lars Lowe Sjosund\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"118801223\",\n",
      "            \"name\": \"Elena Buchatskaya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275158927\",\n",
      "            \"name\": \"Elspeth White\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2201776471\",\n",
      "            \"name\": \"Natalie Clay\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260169185\",\n",
      "            \"name\": \"Jiepu Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275798209\",\n",
      "            \"name\": \"Hyeontaek Lim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"38637384\",\n",
      "            \"name\": \"Ross Hemsley\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184618\",\n",
      "            \"name\": \"Jane Labanowski\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"41019080\",\n",
      "            \"name\": \"Nicola De Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188258\",\n",
      "            \"name\": \"David Steiner\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3362306\",\n",
      "            \"name\": \"Sayed Hadi Hashemi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288056644\",\n",
      "            \"name\": \"Jacob Austin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2105841261\",\n",
      "            \"name\": \"Anita Gergely\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2221119859\",\n",
      "            \"name\": \"Tim Blyth\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275190309\",\n",
      "            \"name\": \"Joe Stanton\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2272718153\",\n",
      "            \"name\": \"K. Shivakumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9356387\",\n",
      "            \"name\": \"Aditya Siddhant\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39552848\",\n",
      "            \"name\": \"Anders Andreassen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279996944\",\n",
      "            \"name\": \"Carlos L. Araya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187415\",\n",
      "            \"name\": \"Nikhil Sethi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2934334\",\n",
      "            \"name\": \"Rakesh Shivanna\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275161833\",\n",
      "            \"name\": \"Steven Hand\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"12295226\",\n",
      "            \"name\": \"Ankur Bapna\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2402489\",\n",
      "            \"name\": \"A. Khodaei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"19200186\",\n",
      "            \"name\": \"Antoine Miech\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287809580\",\n",
      "            \"name\": \"Garrett Tanzer\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1394189636\",\n",
      "            \"name\": \"Andy Swing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"41037204\",\n",
      "            \"name\": \"S. Thakoor\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291169360\",\n",
      "            \"name\": \"Zhufeng Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"81408931\",\n",
      "            \"name\": \"Zachary Nado\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2218062983\",\n",
      "            \"name\": \"Stephanie Winkler\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256337021\",\n",
      "            \"name\": \"Dian Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144413479\",\n",
      "            \"name\": \"Mohammad Saleh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"108173905\",\n",
      "            \"name\": \"Lorenzo Maggiore\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2159207795\",\n",
      "            \"name\": \"Iain Barr\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187490\",\n",
      "            \"name\": \"Minh Giang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186582\",\n",
      "            \"name\": \"Thais Kagohara\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1841008\",\n",
      "            \"name\": \"Ivo Danihelka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176043\",\n",
      "            \"name\": \"Amit Marathe\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181199\",\n",
      "            \"name\": \"Vladimir Feinberg\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275176049\",\n",
      "            \"name\": \"Mohamed Elhawaty\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3404697\",\n",
      "            \"name\": \"Nimesh Ghelani\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48257711\",\n",
      "            \"name\": \"Dan Horgan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275121046\",\n",
      "            \"name\": \"Helen Miller\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184334\",\n",
      "            \"name\": \"Lexi Walker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1825728\",\n",
      "            \"name\": \"Richard Tanburn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275180099\",\n",
      "            \"name\": \"Mukarram Tariq\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275113487\",\n",
      "            \"name\": \"Disha Shrivastava\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487337\",\n",
      "            \"name\": \"Fei Xia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284761701\",\n",
      "            \"name\": \"Chung-Cheng Chiu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2333511945\",\n",
      "            \"name\": \"Zoe Ashwood\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290486431\",\n",
      "            \"name\": \"Khuslen Baatarsukh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2412073\",\n",
      "            \"name\": \"Sina Samangooei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275177971\",\n",
      "            \"name\": \"Fred Alcober\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2163521750\",\n",
      "            \"name\": \"Axel Stjerngren\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258235140\",\n",
      "            \"name\": \"P. Komarek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185589\",\n",
      "            \"name\": \"Katerina Tsihlas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"11167300\",\n",
      "            \"name\": \"Anudhyan Boral\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"89066101\",\n",
      "            \"name\": \"R. Comanescu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275275439\",\n",
      "            \"name\": \"Jeremy Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7247867\",\n",
      "            \"name\": \"Ruibo Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185808\",\n",
      "            \"name\": \"Dawn Bloxwich\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2182971260\",\n",
      "            \"name\": \"Charlie Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265240845\",\n",
      "            \"name\": \"Yanhua Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275173841\",\n",
      "            \"name\": \"Fangxi-aoyu Feng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2251517316\",\n",
      "            \"name\": \"M. Mauger\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1404332584\",\n",
      "            \"name\": \"Xerxes Dotiwalla\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297847306\",\n",
      "            \"name\": \"V. Hellendoorn\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184531\",\n",
      "            \"name\": \"Michael Sharman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187038\",\n",
      "            \"name\": \"Ivy Zheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256873459\",\n",
      "            \"name\": \"Krishna Haridasan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1403998955\",\n",
      "            \"name\": \"Gabriel Barth-Maron\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181554\",\n",
      "            \"name\": \"Craig Swanson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275184739\",\n",
      "            \"name\": \"Dominika Rogozi'nska\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290741315\",\n",
      "            \"name\": \"Alek Andreev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249760524\",\n",
      "            \"name\": \"P. Rubenstein\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275189194\",\n",
      "            \"name\": \"Ruoxin Sang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265528853\",\n",
      "            \"name\": \"Dan Hurt\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275189864\",\n",
      "            \"name\": \"Gamaleldin Elsayed\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290529512\",\n",
      "            \"name\": \"Ren-shen Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485332\",\n",
      "            \"name\": \"Dave Lacey\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279830514\",\n",
      "            \"name\": \"Anastasija Ili'c\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275112414\",\n",
      "            \"name\": \"Yao Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2215449616\",\n",
      "            \"name\": \"Woohyun Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257256357\",\n",
      "            \"name\": \"Lora Aroyo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275177173\",\n",
      "            \"name\": \"Chimezie Iwuanyanwu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"48942032\",\n",
      "            \"name\": \"Vitaly Nikolaev\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40627523\",\n",
      "            \"name\": \"Balaji Lakshminarayanan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290484919\",\n",
      "            \"name\": \"Sadegh Jazayeri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31713635\",\n",
      "            \"name\": \"Raphael Lopez Kaufman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2150348369\",\n",
      "            \"name\": \"Mani Varadarajan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"118505443\",\n",
      "            \"name\": \"Chetan Tekur\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187305\",\n",
      "            \"name\": \"Doug Fritz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2140488873\",\n",
      "            \"name\": \"Misha Khalman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257286979\",\n",
      "            \"name\": \"David Reitter\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487762\",\n",
      "            \"name\": \"Kingshuk Dasgupta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1658856741\",\n",
      "            \"name\": \"Shourya Sarcar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"103861813\",\n",
      "            \"name\": \"T. Ornduff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265527968\",\n",
      "            \"name\": \"Javier Snaider\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2174667321\",\n",
      "            \"name\": \"Fantine Huot\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275694953\",\n",
      "            \"name\": \"Johnson Jia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186180\",\n",
      "            \"name\": \"Rupert Kemp\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1702423\",\n",
      "            \"name\": \"Nejc Trdin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186554\",\n",
      "            \"name\": \"Anitha Vijayakumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290490486\",\n",
      "            \"name\": \"Lucy Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269460640\",\n",
      "            \"name\": \"Christof Angermueller\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485653\",\n",
      "            \"name\": \"Li Lao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275249023\",\n",
      "            \"name\": \"Tianqi Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290556567\",\n",
      "            \"name\": \"Haibin Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485958\",\n",
      "            \"name\": \"David Engel\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275190069\",\n",
      "            \"name\": \"Somer Greene\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275169305\",\n",
      "            \"name\": \"Anais White\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290488307\",\n",
      "            \"name\": \"Jessica Austin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290666013\",\n",
      "            \"name\": \"Lilly Taylor\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181309\",\n",
      "            \"name\": \"Shereen Ashraf\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290539499\",\n",
      "            \"name\": \"Dangyi Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280669236\",\n",
      "            \"name\": \"Maria Georgaki\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485789\",\n",
      "            \"name\": \"Irene Cai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275177999\",\n",
      "            \"name\": \"Yana Kulizhskaya\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2096063076\",\n",
      "            \"name\": \"Sonam Goenka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4125424\",\n",
      "            \"name\": \"Brennan Saeta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"4529644\",\n",
      "            \"name\": \"Kiran Vodrahalli\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290488254\",\n",
      "            \"name\": \"Christian Frank\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"47182967\",\n",
      "            \"name\": \"D. Cesare\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186837\",\n",
      "            \"name\": \"Brona Robenek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290487825\",\n",
      "            \"name\": \"Harry Richardson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186342\",\n",
      "            \"name\": \"Mah-moud Alnahlawi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187959\",\n",
      "            \"name\": \"Christo-pher Yew\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181434\",\n",
      "            \"name\": \"Priya Ponnapalli\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1749128\",\n",
      "            \"name\": \"M. Tagliasacchi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188906\",\n",
      "            \"name\": \"Alex Korchemniy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275999038\",\n",
      "            \"name\": \"Yelin Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275195333\",\n",
      "            \"name\": \"Dinghua Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2080520726\",\n",
      "            \"name\": \"B. Rosgen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186260\",\n",
      "            \"name\": \"Kyle Levin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187022\",\n",
      "            \"name\": \"Jeremy Wiesner\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187506\",\n",
      "            \"name\": \"Praseem Banzal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275182960\",\n",
      "            \"name\": \"Praveen Srinivasan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2254035020\",\n",
      "            \"name\": \"Hongkun Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275186671\",\n",
      "            \"name\": \"cCauglar Unlu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275179889\",\n",
      "            \"name\": \"David Reid\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9941702\",\n",
      "            \"name\": \"Zora Tung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2591720\",\n",
      "            \"name\": \"D. Finchelstein\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290629265\",\n",
      "            \"name\": \"Ravin Kumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2288791213\",\n",
      "            \"name\": \"A. Elisseeff\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290557316\",\n",
      "            \"name\": \"Jin Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290594698\",\n",
      "            \"name\": \"Ming Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2070271342\",\n",
      "            \"name\": \"Rui Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275185644\",\n",
      "            \"name\": \"Ricardo Aguilar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275181171\",\n",
      "            \"name\": \"Mai Gim'enez\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275552322\",\n",
      "            \"name\": \"Jiawei Xia\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2770149\",\n",
      "            \"name\": \"Olivier Dousse\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145556052\",\n",
      "            \"name\": \"W. Gierke\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1735318\",\n",
      "            \"name\": \"S. Yeganeh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290486731\",\n",
      "            \"name\": \"Damion Yates\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"153776147\",\n",
      "            \"name\": \"Komal Jalan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275716550\",\n",
      "            \"name\": \"Lu Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275189350\",\n",
      "            \"name\": \"Eri Latorre-Chimoto\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112293680\",\n",
      "            \"name\": \"D. D. Nguyen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275187515\",\n",
      "            \"name\": \"Ken Durden\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2561675\",\n",
      "            \"name\": \"Praveen Kallakuri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290524803\",\n",
      "            \"name\": \"Yaxin Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275221227\",\n",
      "            \"name\": \"Matthew Johnson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275175372\",\n",
      "            \"name\": \"Tomy Tsai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188417\",\n",
      "            \"name\": \"Alice Talbert\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275539011\",\n",
      "            \"name\": \"Jasmine Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40390373\",\n",
      "            \"name\": \"Alexander Neitz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2217508229\",\n",
      "            \"name\": \"C. Elkind\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269473701\",\n",
      "            \"name\": \"Marco Selvi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188903\",\n",
      "            \"name\": \"Mimi Jasarevic\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258550407\",\n",
      "            \"name\": \"Livio Baldini Soares\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7353832\",\n",
      "            \"name\": \"Livio Baldini Soares\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2164862499\",\n",
      "            \"name\": \"Pidong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290580195\",\n",
      "            \"name\": \"A. Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2181807096\",\n",
      "            \"name\": \"Xinyu Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2214770531\",\n",
      "            \"name\": \"Krystal Kallarackal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275188993\",\n",
      "            \"name\": \"Lucia Loher\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290486901\",\n",
      "            \"name\": \"Hoi Lam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290485721\",\n",
      "            \"name\": \"Josef Broder\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1404655176\",\n",
      "            \"name\": \"D. Holtmann-Rice\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275150753\",\n",
      "            \"name\": \"Nina Martin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257926827\",\n",
      "            \"name\": \"Bramandia Ramadhana\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1393948967\",\n",
      "            \"name\": \"Daniel Toyama\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290488378\",\n",
      "            \"name\": \"Mrinal Shukla\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266467648\",\n",
      "            \"name\": \"Sujoy Basu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290784246\",\n",
      "            \"name\": \"Abhi Mohan\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\n",
      "        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-05-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"144621045\",\n",
      "            \"name\": \"Adam Binch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145785386\",\n",
      "            \"name\": \"Gautham P. Das\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3291831\",\n",
      "            \"name\": \"J. P. Fentanes\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1728609\",\n",
      "            \"name\": \"Marc Hanheide\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\"black box\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\n",
      "        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\n",
      "        \"citationCount\": 59,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-09-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2275119437\",\n",
      "            \"name\": \"Wei Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1382573171\",\n",
      "            \"name\": \"Chengshuai Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266463492\",\n",
      "            \"name\": \"Jiaming Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302798001\",\n",
      "            \"name\": \"Aviv Rosenberg\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266819166\",\n",
      "            \"name\": \"Zhen Qin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2439765\",\n",
      "            \"name\": \"Daniele Calandriello\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2140488873\",\n",
      "            \"name\": \"Misha Khalman\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258551072\",\n",
      "            \"name\": \"Rishabh Joshi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1808897\",\n",
      "            \"name\": \"Bilal Piot\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316576401\",\n",
      "            \"name\": \"Mohammad Saleh\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2319960151\",\n",
      "            \"name\": \"Chi Jin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301173100\",\n",
      "            \"name\": \"Tong Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239381730\",\n",
      "            \"name\": \"Tianqi Liu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\n",
      "        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\n",
      "        \"citationCount\": 30,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-11\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2105550407\",\n",
      "            \"name\": \"Chen Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275119437\",\n",
      "            \"name\": \"Wei Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283877837\",\n",
      "            \"name\": \"Yuheng Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"35279146\",\n",
      "            \"name\": \"Hanze Dong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275167899\",\n",
      "            \"name\": \"Nan Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260473361\",\n",
      "            \"name\": \"Tong Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\n",
      "        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'tldr'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": null,\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2020-09-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2292169\",\n",
      "            \"name\": \"Ehab ElSalamouny\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1722055\",\n",
      "            \"name\": \"C. Palamidessi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\n",
      "        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2401.17842\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2218156728\",\n",
      "            \"name\": \"N. V. Stein\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"108119673\",\n",
      "            \"name\": \"Diederick Vermetten\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3160375\",\n",
      "            \"name\": \"Anna V. Kononova\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2237990304\",\n",
      "            \"name\": \"T. Back\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"thematic consistency LLM\": {\n",
      "    \"total\": 887,\n",
      "    \"offset\": 0,\n",
      "    \"next\": 50,\n",
      "    \"data\": [\n",
      "      {\n",
      "        \"paperId\": \"3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3bf29791bd995d17ef0b1e1b876260fa93f2a51b\",\n",
      "        \"title\": \"Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\n",
      "        \"citationCount\": 24,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2408.17017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting, is proposed.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2284692979\",\n",
      "            \"name\": \"Guangya Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284699718\",\n",
      "            \"name\": \"Yuqi Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317120126\",\n",
      "            \"name\": \"Jie Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316974967\",\n",
      "            \"name\": \"Sheng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/d2d96dc3bbf9d63c85f445e3fa08ad695457a532\",\n",
      "        \"title\": \"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\",\n",
      "        \"citationCount\": 25,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.02896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This experimental study seeks to lay the groundwork for the understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction?\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-05\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2282535211\",\n",
      "            \"name\": \"Ivar Frisch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"24068173\",\n",
      "            \"name\": \"Mario Giulianelli\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model (LLM) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between LLMs: Do persona-prompted LLMs show consistent personality and language use in interaction? We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"477a16bdbb43589e5feac3881b3370e3a4ab5624\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/477a16bdbb43589e5feac3881b3370e3a4ab5624\",\n",
      "        \"title\": \"Evaluating the Consistency of LLM Evaluators\",\n",
      "        \"citationCount\": 22,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.00543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Examining the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models demonstrates that strong proprietary models are not necessarily consistent evaluators.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-11-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2291076200\",\n",
      "            \"name\": \"Noah Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290955335\",\n",
      "            \"name\": \"Jiwoo Hong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290905396\",\n",
      "            \"name\": \"James Thorne\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) have shown potential as general evaluators along with the evident benefits of speed and cost. While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators. In this paper, we conduct extensive studies on the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models. Our comprehensive analysis demonstrates that strong proprietary models are not necessarily consistent evaluators, highlighting the importance of considering consistency in assessing the capability of LLM evaluators.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"75062b58398b6e9409e5fec855f6912534331eaf\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/75062b58398b6e9409e5fec855f6912534331eaf\",\n",
      "        \"title\": \"Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.17017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales, facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-08-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2284692979\",\n",
      "            \"name\": \"Guangya Wan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284699718\",\n",
      "            \"name\": \"Yuqi Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317120126\",\n",
      "            \"name\": \"Jie Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2316974967\",\n",
      "            \"name\": \"Sheng Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths,but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority voting, enabling more informed choices on when to halt sampling and which rationale to select. Our comprehensive experiments across diverse question-answering datasets demonstrate that RASC outperforms existing methods, reducing sample usage by approximately 70% while maintaining accuracy. Moreover, RASC facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs. Our approach effectively addresses the efficiency-accuracy trade-off in LLM reasoning tasks, offering a new perspective for more nuanced, faithful, and effective utilization of LLMs in resource-constrained environments.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3c38af66ef9254df723f99bb50e6fa20f479e0ef\",\n",
      "        \"title\": \"Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications\",\n",
      "        \"citationCount\": 37,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://doi.org/10.1186/s12911-024-02656-3\",\n",
      "          \"status\": \"GOLD\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11382443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare is emphasized, aiming to improve clinical outcomes ethically and effectively.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-09-09\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"145770274\",\n",
      "            \"name\": \"Tala Mirzaei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2320339088\",\n",
      "            \"name\": \"Leila Amini\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2574575\",\n",
      "            \"name\": \"Pouyan Esmaeilzadeh\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions. We analyzed 3049 posts and comments extracted from a self-identified clinician subreddit using unsupervised machine learning via Latent Dirichlet Allocation and a structured qualitative analysis methodology. Analysis uncovered 14 salient themes of ethical implications, which we further consolidated into 4 overarching domains reflecting ethical issues around various clinical applications of LLM in healthcare, LLM coding, algorithm, and data governance, LLMs role in health equity and the distribution of public health services, and the relationship between users (human) and LLM systems (machine). Mapping themes to ethical frameworks in literature illustrated multifaceted issues covering transparent LLM decisions, fairness, privacy, access disparities, user experiences, and reliability. This study emphasizes the need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare, aiming to improve clinical outcomes ethically and effectively.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"6b7c5fc0f6b401962153f68f8250951f75da929e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/6b7c5fc0f6b401962153f68f8250951f75da929e\",\n",
      "        \"title\": \"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.06503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper analyzes two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and finds that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-09\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2295732707\",\n",
      "            \"name\": \"Nathan Brake\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2295732451\",\n",
      "            \"name\": \"Thomas Schaaf\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e1770838ec0667cad48729a81764ed9964d6a8e6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e1770838ec0667cad48729a81764ed9964d6a8e6\",\n",
      "        \"title\": \"LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\",\n",
      "        \"citationCount\": 110,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL), which yields similar coding quality to that of human coders but reduces TA's labor and time demands.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-10-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3400291\",\n",
      "            \"name\": \"Shih-Chieh Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261362789\",\n",
      "            \"name\": \"Aiping Xiong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1746959\",\n",
      "            \"name\": \"Lun-Wei Ku\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA's labor and time demands.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"32426b96ff3c680125bde3b835bfa931288b8ade\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/32426b96ff3c680125bde3b835bfa931288b8ade\",\n",
      "        \"title\": \"Better Patching Using LLM Prompting, via Self-Consistency\",\n",
      "        \"citationCount\": 45,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2306.00108\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.00108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper describes an application of the $\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots, on the MODIT dataset.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-05-31\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3432275\",\n",
      "            \"name\": \"Toufique Ahmed\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"114875459\",\n",
      "            \"name\": \"Prem Devanbu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language models (LLMs) can be induced to solve non-trivial problems with few-shot prompts including illustrative problem-solution examples. Now if the few-shots also include chain of thought ($\\\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a explained solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\\\mathcal{S}-C$ (or even $\\\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e24424283c02fbe7f641e5b3490d7bb059f8355a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e24424283c02fbe7f641e5b3490d7bb059f8355a\",\n",
      "        \"title\": \"A Survey on LLM-as-a-Judge\",\n",
      "        \"citationCount\": 776,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.15594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built?\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-11-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2216587705\",\n",
      "            \"name\": \"Jiawei Gu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144267788\",\n",
      "            \"name\": \"Xuhui Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287881684\",\n",
      "            \"name\": \"Zhichao Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274159320\",\n",
      "            \"name\": \"Hexiang Tan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2332093190\",\n",
      "            \"name\": \"Xuehao Zhai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2250617116\",\n",
      "            \"name\": \"Chengjin Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2330714501\",\n",
      "            \"name\": \"Wei Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1944248313\",\n",
      "            \"name\": \"Yinghan Shen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2311556497\",\n",
      "            \"name\": \"Shengjie Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2332306096\",\n",
      "            \"name\": \"Honghao Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257058703\",\n",
      "            \"name\": \"Yuanzhuo Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284217200\",\n",
      "            \"name\": \"Jian Guo\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of\\\"LLM-as-a-Judge,\\\"where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0bf3a1867f7245b8a702093901c66b08b518eafc\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0bf3a1867f7245b8a702093901c66b08b518eafc\",\n",
      "        \"title\": \"Evaluating Very Long-Term Conversational Memory of LLM Agents\",\n",
      "        \"citationCount\": 166,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A machine-human pipeline is introduced to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs, and presents a comprehensive evaluation benchmark to measure long-term memory in models.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"8785371\",\n",
      "            \"name\": \"Adyasha Maharana\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266803131\",\n",
      "            \"name\": \"Dong-Ho Lee\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"145582202\",\n",
      "            \"name\": \"S. Tulyakov\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2285969697\",\n",
      "            \"name\": \"Mohit Bansal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266751000\",\n",
      "            \"name\": \"Francesco Barbieri\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267220081\",\n",
      "            \"name\": \"Yuwei Fang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7c04ab297b59d4fe29285f339350882a3120b27f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7c04ab297b59d4fe29285f339350882a3120b27f\",\n",
      "        \"title\": \"CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs\",\n",
      "        \"citationCount\": 205,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642773\",\n",
      "          \"status\": \"BRONZE\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.11314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions, is developed, revealing four design considerations for future educational AI assistants.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-20\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3136345\",\n",
      "            \"name\": \"Majeed Kazemitabaar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2184253123\",\n",
      "            \"name\": \"Runlong Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280281736\",\n",
      "            \"name\": \"Xiaoning Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280145055\",\n",
      "            \"name\": \"Austin Z Henley\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2243041721\",\n",
      "            \"name\": \"Paul Denny\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280145218\",\n",
      "            \"name\": \"Michelle Craig\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2280146888\",\n",
      "            \"name\": \"Tovi Grossman\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates students incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AIs unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/da9b51050a8574c4e03e5eb8e9e17cb640e27324\",\n",
      "        \"title\": \"Exploring and Evaluating Hallucinations in LLM-Powered Code Generation\",\n",
      "        \"citationCount\": 221,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation, and proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2290969862\",\n",
      "            \"name\": \"Fang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294504414\",\n",
      "            \"name\": \"Yang Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2295165194\",\n",
      "            \"name\": \"Lin Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294528116\",\n",
      "            \"name\": \"Houkun Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294510508\",\n",
      "            \"name\": \"Ruifeng Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2294664033\",\n",
      "            \"name\": \"Zhen Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290433096\",\n",
      "            \"name\": \"Li Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a06d3e9e90008c64c45a0029d580541d5f646771\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771\",\n",
      "        \"title\": \"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents\",\n",
      "        \"citationCount\": 108,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.00812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"An overview of the various benefits of integrating code into LLMs' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-01-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2277527247\",\n",
      "            \"name\": \"Ke Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"33456794\",\n",
      "            \"name\": \"Jiateng Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277421308\",\n",
      "            \"name\": \"John Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277597831\",\n",
      "            \"name\": \"Chaoqi Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"51135899\",\n",
      "            \"name\": \"Y. Fung\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262396117\",\n",
      "            \"name\": \"Sha Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277416897\",\n",
      "            \"name\": \"Zixuan Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344961610\",\n",
      "            \"name\": \"Xu Cao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144803999\",\n",
      "            \"name\": \"Xingyao Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277247982\",\n",
      "            \"name\": \"Yiquan Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2277409745\",\n",
      "            \"name\": \"Heng Ji\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261082008\",\n",
      "            \"name\": \"ChengXiang Zhai\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2\",\n",
      "        \"title\": \"PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference\",\n",
      "        \"citationCount\": 106,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.12532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The number of crucial keys and values that influence future generations decreases layer by layer and can be extracted by the consistency in attention weights, leading to PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2182432556\",\n",
      "            \"name\": \"Dongjie Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302683712\",\n",
      "            \"name\": \"Xiaodong Han\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302558089\",\n",
      "            \"name\": \"Yan Gao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302556666\",\n",
      "            \"name\": \"Yao Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302704855\",\n",
      "            \"name\": \"Shilin Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302545224\",\n",
      "            \"name\": \"Hai Zhao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"36b9dac525fc93100b18d8e489bd97460cd49a5e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/36b9dac525fc93100b18d8e489bd97460cd49a5e\",\n",
      "        \"title\": \"Thematic-LM: A LLM-based Multi-agent System for Large-scale Thematic Analysis\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3696410.3714595?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3696410.3714595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study applies Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2025-04-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2343740149\",\n",
      "            \"name\": \"Tingrui Qiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2343749049\",\n",
      "            \"name\": \"Caroline Walker\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2343746435\",\n",
      "            \"name\": \"Chris Cunningham\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2310725786\",\n",
      "            \"name\": \"Yun Sing Koh\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Thematic analysis (TA) is a widely used qualitative method for identifying underlying meanings within unstructured text. However, TA requires manual processes, which become increasingly labour-intensive and time-consuming as datasets grow. While large language models (LLMs) have been introduced to assist with TA on small-scale datasets, three key limitations hinder their effectiveness. First, current approaches often depend on interactions between an LLM agent and a human coder, a process that becomes challenging with larger datasets. Second, with feedback from the human coder, the LLM tends to mirror the human coder, which provides a narrower viewpoint of the data. Third, existing methods follow a sequential process, where codes are generated for individual samples without recalling previous codes and associated data, reducing the ability to analyse data holistically. To address these limitations, we propose Thematic-LM, an LLM-based multi-agent system for large-scale computational thematic analysis. Thematic-LM assigns specialised tasks to each agent, such as coding, aggregating codes, and maintaining and updating the codebook. We assign coder agents different identity perspectives to simulate the subjective nature of TA, fostering a more diverse interpretation of the data. We applied Thematic-LM to the Dreaddit dataset and the Reddit climate change dataset to analyse themes related to social media stress and online opinions on climate change. We evaluate the resulting themes based on trustworthiness principles in qualitative research. Our study reveals insights such as assigning different identities to coder agents promotes divergence in codes and themes.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/54ee5a4a19e70d18e79b6eb912d8aa72b0113f9a\",\n",
      "        \"title\": \"Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation\",\n",
      "        \"citationCount\": 88,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-22\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2285255408\",\n",
      "            \"name\": \"Jiawei Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"31279896\",\n",
      "            \"name\": \"Renhe Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"46962297\",\n",
      "            \"name\": \"Chuang Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2157765133\",\n",
      "            \"name\": \"Zengqing Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266396584\",\n",
      "            \"name\": \"Makoto Onizuka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239490643\",\n",
      "            \"name\": \"Ryosuke Shibasaki\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284717877\",\n",
      "            \"name\": \"Chuan Xiao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7944dc7e5437aa1318cc6887ba9bd43c9dc11e2f\",\n",
      "        \"title\": \"ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification\",\n",
      "        \"citationCount\": 75,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3660810?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3660810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions, and can effectively facilitate the practical application of LLMs in real-world development environments.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1994579604\",\n",
      "            \"name\": \"Fangwen Mu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305416699\",\n",
      "            \"name\": \"Lin Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259571951\",\n",
      "            \"name\": \"Song Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259613131\",\n",
      "            \"name\": \"Zhuohao Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259874187\",\n",
      "            \"name\": \"Binquan Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2259824656\",\n",
      "            \"name\": \"ChenXue Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260294248\",\n",
      "            \"name\": \"Shichao Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2157214565\",\n",
      "            \"name\": \"Qing Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"630c47372317164fc367153f938903e1d5b76059\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/630c47372317164fc367153f938903e1d5b76059\",\n",
      "        \"title\": \"LLM-Check: Investigating Detection of Hallucinations in Large Language Models\",\n",
      "        \"citationCount\": 50,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52202/079017-1077?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52202/079017-1077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work seeks to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1602820179\",\n",
      "            \"name\": \"Gaurang Sriramanan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344249306\",\n",
      "            \"name\": \"Siddhant Bharti\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"150333898\",\n",
      "            \"name\": \"Vinu Sankar Sadasivan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"152623528\",\n",
      "            \"name\": \"Shoumik Saha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305809801\",\n",
      "            \"name\": \"Priyatham Kattakinda\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"34389431\",\n",
      "            \"name\": \"S. Feizi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/038ed1e52bdf92a4db0c91d31d1db28b2c5051fb\",\n",
      "        \"title\": \"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges\",\n",
      "        \"citationCount\": 43,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt), which eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-27\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1966961\",\n",
      "            \"name\": \"Yanshen Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239274607\",\n",
      "            \"name\": \"Jianfeng He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293779433\",\n",
      "            \"name\": \"Limeng Cui\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3433489\",\n",
      "            \"name\": \"Shuo Lei\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2249846863\",\n",
      "            \"name\": \"Chang-Tien Lu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f11cbd0d24b0e3fb917fe14cfaf572e76402e5df\",\n",
      "        \"title\": \"Reasoning Runtime Behavior of a Program with LLM: How Far are We?\",\n",
      "        \"citationCount\": 49,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a framework, namely $\\\\boldsymbol{\\\\mathcal{R}}\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution, and utilizes existing code benchmarks and adapt them to new benchmarks within this framework.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2293350098\",\n",
      "            \"name\": \"Junkai Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276184077\",\n",
      "            \"name\": \"Zhiyuan Pan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2110049191\",\n",
      "            \"name\": \"Xing Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293350478\",\n",
      "            \"name\": \"Zhenhao Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2286413567\",\n",
      "            \"name\": \"Ge Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265241871\",\n",
      "            \"name\": \"Xin Xia\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models for code (i.e., code LLMs) have shown strong code understanding and generation capabilities. To evaluate the capabilities of code LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval and ClassEval). Code reasoning is one of the most essential abilities of code LLMs (i.e., predicting code execution behaviors such as program output and execution path), but existing benchmarks for code reasoning are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the reasoning. To address these problems, in this paper, we propose a framework, namely $\\\\boldsymbol{\\\\mathcal{R}}\\\\mathbf{Eval}$, for evaluating code reasoning abilities and consistency of code LLMs with program execution. We utilize existing code benchmarks and adapt them to new benchmarks within our framework. A large-scale empirical study is conducted and most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code LLMs reflect the urgent need for the community to strengthen the code reasoning capability of code LLMs. Our code, data and $\\\\boldsymbol{\\\\mathcal{R}}\\\\mathbf{Eval}$ leaderboard are available at https://r-eval.github.io.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/8e68b5d1808349fd53c372c0d952b6ebea9d7b9e\",\n",
      "        \"title\": \"Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization\",\n",
      "        \"citationCount\": 58,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.18120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper leverages the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics, they can be prompted to autoformalize informal mathematical statements into formal Isabelle code -- which provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2287813125\",\n",
      "            \"name\": \"Jin Peng Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2144884927\",\n",
      "            \"name\": \"Charles Staats\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293653101\",\n",
      "            \"name\": \"Wenda Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2574060\",\n",
      "            \"name\": \"Christian Szegedy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"7446832\",\n",
      "            \"name\": \"Kilian Q. Weinberger\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2287780080\",\n",
      "            \"name\": \"Yuhuai Wu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1b256fb2f9ac9857db996fa4f881f16e1345f8b1\",\n",
      "        \"title\": \"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models\",\n",
      "        \"citationCount\": 23,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.17578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"MM-Eval is introduced, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages designed with multilingual-specific challenges in mind and finds that evaluators are unfair and inconsistent when evaluating lower-resourced languages.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2321452295\",\n",
      "            \"name\": \"Guijin Son\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"29830817\",\n",
      "            \"name\": \"Dongkeun Yoon\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299329316\",\n",
      "            \"name\": \"Juyoung Suk\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301578911\",\n",
      "            \"name\": \"Javier Aula-Blasco\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327215494\",\n",
      "            \"name\": \"Mano Aslan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327216625\",\n",
      "            \"name\": \"Vu Trong Kim\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2232783785\",\n",
      "            \"name\": \"Shayekh Bin Islam\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327215436\",\n",
      "            \"name\": \"Jaume Prats-Cristi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2327217057\",\n",
      "            \"name\": \"Luca Tormo-Bauelos\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2184037220\",\n",
      "            \"name\": \"Seungone Kim\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\\\"meta-evaluation benchmarks\\\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f1366e505de4f1d0e901903e3c17471033758a96\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f1366e505de4f1d0e901903e3c17471033758a96\",\n",
      "        \"title\": \"Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based Chatbots\",\n",
      "        \"citationCount\": 18,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.11876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Rescriber is designed, built, and evaluated, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts, presenting a promising approach to address the privacy and trust challenges of AI.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2326261436\",\n",
      "            \"name\": \"Jijie Zhou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326116321\",\n",
      "            \"name\": \"Eryue Xu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326262935\",\n",
      "            \"name\": \"Yaoyao Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326228781\",\n",
      "            \"name\": \"Tianshi Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=Rescriber) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4o. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3ec06fe8d8764123490544ab5dc956143e84b443\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/3ec06fe8d8764123490544ab5dc956143e84b443\",\n",
      "        \"title\": \"Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents\",\n",
      "        \"citationCount\": 50,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.11977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise and the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Book\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2310565909\",\n",
      "            \"name\": \"Guangzhi Sun\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2089532795\",\n",
      "            \"name\": \"Xiao Zhan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2275248675\",\n",
      "            \"name\": \"Jose Such\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/dfbfe75ec8c2143e899897a3c054ee58d99ead43\",\n",
      "        \"title\": \"Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge\",\n",
      "        \"citationCount\": 39,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.07791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Evaluating position bias in LLM judges across pairwise and list-wise comparison settings confirms that position bias is not due to random chance and varies significantly across judges and tasks, and provides insights into the distribution of judging difficulty across the dataset.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-06-12\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2305925735\",\n",
      "            \"name\": \"Lin Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262963382\",\n",
      "            \"name\": \"Chiyu Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2330065663\",\n",
      "            \"name\": \"Wenhua Liang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2227771\",\n",
      "            \"name\": \"Weicheng Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1918441\",\n",
      "            \"name\": \"Soroush Vosoughi\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases - particularly position bias, the tendency to favor solutions based on their position within the prompt - compromise its reliability. This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 15 LLM judges across MTBench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"638d8d1f3865ebf065605535a7aa50727d5ffabe\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/638d8d1f3865ebf065605535a7aa50727d5ffabe\",\n",
      "        \"title\": \"TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.18919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation, significantly improves semantic and contextual consistency in synthesized images.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-29\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2298945483\",\n",
      "            \"name\": \"Junhao Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298032416\",\n",
      "            \"name\": \"Baiqiao Yin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2229014859\",\n",
      "            \"name\": \"Kaixin Cai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2162225343\",\n",
      "            \"name\": \"Minbin Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276604489\",\n",
      "            \"name\": \"Hanhui Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299161673\",\n",
      "            \"name\": \"Yuxin He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298943419\",\n",
      "            \"name\": \"Xi Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298043252\",\n",
      "            \"name\": \"Yue Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298926852\",\n",
      "            \"name\": \"Yifei Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298644153\",\n",
      "            \"name\": \"Yuhao Cheng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"144880586\",\n",
      "            \"name\": \"Yiqiang Yan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291389227\",\n",
      "            \"name\": \"Xiaodan Liang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a\\\"Screenwriter\\\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the\\\"Rehearsal\\\". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the\\\"Final Performance\\\". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4777305738fd1aa30243f96a1687d57d8f70fa5d\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/4777305738fd1aa30243f96a1687d57d8f70fa5d\",\n",
      "        \"title\": \"Improving Text-to-Image Consistency via Automatic Prompt Optimization\",\n",
      "        \"citationCount\": 59,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models and paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-26\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1796269096\",\n",
      "            \"name\": \"Oscar Maas\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274101827\",\n",
      "            \"name\": \"Pietro Astolfi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293590162\",\n",
      "            \"name\": \"Melissa Hall\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2256372432\",\n",
      "            \"name\": \"Candace Ross\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"39219656\",\n",
      "            \"name\": \"Jack Urbanek\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2293907712\",\n",
      "            \"name\": \"Adina Williams\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2801949\",\n",
      "            \"name\": \"Aishwarya Agrawal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1456285042\",\n",
      "            \"name\": \"Adriana Romero-Soriano\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3325894\",\n",
      "            \"name\": \"M. Drozdzal\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"275a3955a83867dd36a3683788e0e053e00f8a89\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/275a3955a83867dd36a3683788e0e053e00f8a89\",\n",
      "        \"title\": \"Collaborative Enhancement of Consistency and Accuracy in US Diagnosis of Thyroid Nodules Using Large Language Models.\",\n",
      "        \"citationCount\": 60,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1148/radiol.232255?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1148/radiol.232255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging and ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1678966811\",\n",
      "            \"name\": \"Shaohong Wu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2272038393\",\n",
      "            \"name\": \"Wen-juan Tong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2127991969\",\n",
      "            \"name\": \"Ming-De Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"28890237\",\n",
      "            \"name\": \"Hang-tong Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2187182790\",\n",
      "            \"name\": \"Xiao-zhou Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2185218332\",\n",
      "            \"name\": \"Ze-Rong Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290915880\",\n",
      "            \"name\": \"Xin-Xin Lin\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290897544\",\n",
      "            \"name\": \"Ruifang Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261915754\",\n",
      "            \"name\": \"Ming-De Lu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"6457299\",\n",
      "            \"name\": \"Li-da Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290866279\",\n",
      "            \"name\": \"Wei Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Background Large language models (LLMs) hold substantial promise for medical imaging interpretation. However, there is a lack of studies on their feasibility in handling reasoning questions associated with medical diagnosis. Purpose To investigate the viability of leveraging three publicly available LLMs to enhance consistency and diagnostic accuracy in medical imaging based on standardized reporting, with pathology as the reference standard. Materials and Methods US images of thyroid nodules with pathologic results were retrospectively collected from a tertiary referral hospital between July 2022 and December 2022 and used to evaluate malignancy diagnoses generated by three LLMs-OpenAI's ChatGPT 3.5, ChatGPT 4.0, and Google's Bard. Inter- and intra-LLM agreement of diagnosis were evaluated. Then, diagnostic performance, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), was evaluated and compared for the LLMs and three interactive approaches: human reader combined with LLMs, image-to-text model combined with LLMs, and an end-to-end convolutional neural network model. Results A total of 1161 US images of thyroid nodules (498 benign, 663 malignant) from 725 patients (mean age, 42.2 years  14.1 [SD]; 516 women) were evaluated. ChatGPT 4.0 and Bard displayed substantial to almost perfect intra-LLM agreement ( range, 0.65-0.86 [95% CI: 0.64, 0.86]), while ChatGPT 3.5 showed fair to substantial agreement ( range, 0.36-0.68 [95% CI: 0.36, 0.68]). ChatGPT 4.0 had an accuracy of 78%-86% (95% CI: 76%, 88%) and sensitivity of 86%-95% (95% CI: 83%, 96%), compared with 74%-86% (95% CI: 71%, 88%) and 74%-91% (95% CI: 71%, 93%), respectively, for Bard. Moreover, with ChatGPT 4.0, the image-to-text-LLM strategy exhibited an AUC (0.83 [95% CI: 0.80, 0.85]) and accuracy (84% [95% CI: 82%, 86%]) comparable to those of the human-LLM interaction strategy with two senior readers and one junior reader and exceeding those of the human-LLM interaction strategy with one junior reader. Conclusion LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging. ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5.  RSNA, 2024 Supplemental material is available for this article.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/b5cd6bc53343f2ad8a7c7830555d8c744e626245\",\n",
      "        \"title\": \"Mitigating LLM Hallucinations via Conformal Abstention\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.01563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": null\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-04\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"1388837087\",\n",
      "            \"name\": \"Yasin Abbasi-Yadkori\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3150458\",\n",
      "            \"name\": \"Ilja Kuzborskij\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2298902427\",\n",
      "            \"name\": \"David Stutz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305592785\",\n",
      "            \"name\": \"Andrs Gyrgy\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299943725\",\n",
      "            \"name\": \"Adam Fisch\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299943677\",\n",
      "            \"name\": \"Arnaud Doucet\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2299943992\",\n",
      "            \"name\": \"Iuliya Beloshapka\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2239098855\",\n",
      "            \"name\": \"Wei-Hung Weng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300022831\",\n",
      "            \"name\": \"Yao-Yuan Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2257346986\",\n",
      "            \"name\": \"Csaba Szepesv'ari\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"9235290\",\n",
      "            \"name\": \"Ali Taylan Cemgil\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2359197879\",\n",
      "            \"name\": \"Nenad Tomasev\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying\\\"I don't know\\\") in a general domain, instead of resorting to possibly\\\"hallucinating\\\"a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"023a98af94a3e7e8e538a6183da8ec05024fec56\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/023a98af94a3e7e8e538a6183da8ec05024fec56\",\n",
      "        \"title\": \"How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\",\n",
      "        \"citationCount\": 42,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.14805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work prompts different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions, and finds that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2307916998\",\n",
      "            \"name\": \"Julia Kharchenko\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284066307\",\n",
      "            \"name\": \"Tanya Roosta\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284065969\",\n",
      "            \"name\": \"Aman Chadha\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2234352974\",\n",
      "            \"name\": \"Chirag Shah\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs'cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a1849a77644ff411a03833b5aa7a65ff57158c50\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a1849a77644ff411a03833b5aa7a65ff57158c50\",\n",
      "        \"title\": \"CLLMs: Consistency Large Language Models\",\n",
      "        \"citationCount\": 50,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.00835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work develops a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory by refining the target LLM to consistently predict the fixed point given any state as input.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-28\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2258963117\",\n",
      "            \"name\": \"Siqi Kou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2258334187\",\n",
      "            \"name\": \"Lanxiang Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2116778591\",\n",
      "            \"name\": \"Zhe He\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2260296481\",\n",
      "            \"name\": \"Zhijie Deng\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2289837431\",\n",
      "            \"name\": \"Hao Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\\\times$ to 3.4$\\\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/acdb4b5a64c0872655379efd7889e692b5d7d7f6\",\n",
      "        \"title\": \"PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning\",\n",
      "        \"citationCount\": 25,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613905.3651008\",\n",
      "          \"status\": \"BRONZE\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.14227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded and thus highlight potential design improvements and considerations for peer agents in both roles.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Book\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-03-21\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2296401615\",\n",
      "            \"name\": \"Jiawen Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292671914\",\n",
      "            \"name\": \"Yuanyuan Yao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2283762773\",\n",
      "            \"name\": \"Pengcheng An\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301178822\",\n",
      "            \"name\": \"Qi Wang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In childrens collaborative learning, effective peer conversations can significantly enhance the quality of childrens collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster childrens creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5e317746c0f38d1149f33a19807af47d513bdd27\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/5e317746c0f38d1149f33a19807af47d513bdd27\",\n",
      "        \"title\": \"Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge\",\n",
      "        \"citationCount\": 22,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.12509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work introduces a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald's omega, and demonstrates the limitations of fixed randomness and the importance of considering multiple samples.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-17\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2328309355\",\n",
      "            \"name\": \"Kayla Schroeder\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1411379613\",\n",
      "            \"name\": \"Zach Wood-Doughty\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs. While deterministic settings can improve consistency, they do not guarantee reliability, as a single sample from the model's probability distribution can still be misleading. Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald's omega. We evaluate the reliability of LLMs when judging the outputs of other LLMs on standard single-turn and multi-turn benchmarks, simultaneously investigating the impact of temperature on reliability. By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implications for downstream applications. Our findings highlight the need for a nuanced understanding of LLM reliability and the potential risks associated with over-reliance on single-shot evaluations. This work provides a crucial step towards building more trustworthy and reliable LLM-based systems and applications.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"235a8bd57a6b53ecab756780a45bce6e4743cecd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/235a8bd57a6b53ecab756780a45bce6e4743cecd\",\n",
      "        \"title\": \"LLM-based Multi-Level Knowledge Generation for Few-shot Knowledge Graph Completion\",\n",
      "        \"citationCount\": 22,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": \"CLOSED\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2024/236?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2024/236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes a generation-based FKGC paradigm facilitated by LLM distillation, which achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-08-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2284861197\",\n",
      "            \"name\": \"Qian Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313599793\",\n",
      "            \"name\": \"Zhuo Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2052296239\",\n",
      "            \"name\": \"Cheng Ji\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2313658619\",\n",
      "            \"name\": \"Shiqi Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274552581\",\n",
      "            \"name\": \"Jianxin Li\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Knowledge Graphs (KGs) are pivotal in various NLP applications but often grapple with incompleteness, especially due to the long-tail problem where infrequent, unpopular relationships drastically reduce the KG completion performance. In this paper, we focus on Few-shot Knowledge Graph Completion (FKGC), a task addressing these gaps in long-tail scenarios. Amidst the rapid evolution of Large Language Models, we propose a generation-based FKGC paradigm facilitated by LLM distillation. Our MuKDC framework employs multi-level knowledge distillation for few-shot KG completion, generating supplementary knowledge to mitigate data scarcity in few-shot environments. MuKDC comprises two primary components: Multi-level Knowledge Generation, which enriches the KG at various levels, and Consistency Assessment, to ensure the coherence and reliability of the generated knowledge. Most notably, our method achieves SOTA results in both FKGC and multi-modal FKGC benchmarks, significantly advancing KG completion and enhancing the understanding and application of LLMs in structured knowledge generation and assessment.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/9f8da87ee4416d57a2cc044bdf8223c7728d74d7\",\n",
      "        \"title\": \"Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-23\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2144511530\",\n",
      "            \"name\": \"Xuan Liu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265432385\",\n",
      "            \"name\": \"Jie Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302819855\",\n",
      "            \"name\": \"Song Guo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302765389\",\n",
      "            \"name\": \"Haoyang Shang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302927286\",\n",
      "            \"name\": \"Chengxu Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2302915902\",\n",
      "            \"name\": \"Quanyan Zhu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"086046d38b3a7066aa39e3d350905e8065c8f1b5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/086046d38b3a7066aa39e3d350905e8065c8f1b5\",\n",
      "        \"title\": \"Efficiently Scaling LLM Reasoning with Certaindex\",\n",
      "        \"citationCount\": 19,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.20993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy, so this work introduces Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-12-30\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2282896192\",\n",
      "            \"name\": \"Yichao Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2279862923\",\n",
      "            \"name\": \"Junda Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317134948\",\n",
      "            \"name\": \"Siqi Zhu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337869460\",\n",
      "            \"name\": \"Zheyu Fu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2351053054\",\n",
      "            \"name\": \"Zhongdongming Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2152482391\",\n",
      "            \"name\": \"Yonghao Zhuang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2363671676\",\n",
      "            \"name\": \"Yian Ma\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2317112099\",\n",
      "            \"name\": \"Aurick Qiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2379937619\",\n",
      "            \"name\": \"Tajana Rosing\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2344601177\",\n",
      "            \"name\": \"Ion Stoica\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2337807823\",\n",
      "            \"name\": \"Hao Zhang\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50% compute savings and 3.3x higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"43fedc4430be030c083626b3e64b7093916b429a\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/43fedc4430be030c083626b3e64b7093916b429a\",\n",
      "        \"title\": \"Enhancing clinical reasoning skills for medical students: a qualitative comparison of LLM-powered social robotic versus computer-based virtual patients within rheumatology\",\n",
      "        \"citationCount\": 17,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://doi.org/10.1007/s00296-024-05731-0\",\n",
      "          \"status\": \"HYBRID\",\n",
      "          \"license\": \"CCBY\",\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11618132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform and shows promise in training CR skills, communication, and adaptive thinking.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-10-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2290951787\",\n",
      "            \"name\": \"Alexander Borg\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326093338\",\n",
      "            \"name\": \"Benjamin Jobs\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2164303442\",\n",
      "            \"name\": \"Viking Huss\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"23717264\",\n",
      "            \"name\": \"C. Gentline\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326093767\",\n",
      "            \"name\": \"Fabricio Espinosa\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2290722393\",\n",
      "            \"name\": \"Mini Ruiz\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2758537\",\n",
      "            \"name\": \"Samuel Edelbring\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2326094583\",\n",
      "            \"name\": \"Carina Georg\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"103081544\",\n",
      "            \"name\": \"G. Skantze\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"8637952\",\n",
      "            \"name\": \"Ioannis Parodis\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Virtual patients (VPs) are increasingly used in medical education to train clinical reasoning (CR) skills. However, optimal VP design for enhancing interactivity and authenticity remains unclear. Novel interactive modalities, such as large language model (LLM)-enhanced social robotic VPs might increase interactivity and authenticity in CR skill practice. To evaluate medical students perceptions of CR training using an LLM-enhanced social robotic VP platform compared with a conventional computer-based VP platform. A qualitative study involved 23 third-year medical students from Karolinska Institutet, who completed VP cases on an LLM-enhanced social robotic platform and a computer-based semi-linear platform. In-depth interviews assessed students self-perceived acquirement of CR skills using the two platforms. Thematic analysis was employed to identify themes and sub-themes. Three main themes were identified: authenticity, VP application, and strengths and limitations. Students found the social robotic platform more authentic and engaging. It enabled highly interactive communication and expressed emotions, collectively offering a realistic experience. It facilitated active learning, hypothesis generation, and adaptive thinking. Limitations included lack of physical examination options and, occasionally, mechanical dialogue. The LLM-enhanced social robotic VP platform offers a more authentic and interactive learning experience compared to the conventional computer-based platform. Despite some limitations, it shows promise in training CR skills, communication, and adaptive thinking. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations. An LLM-powered social robotic VP platform provides a more authentic and interactive learning experience compared to conventional computer-based VPs. Medical students undertaking clinical placements within rheumatology experienced that an LLM-enhanced social robotic platform can provide added value in training CR skills, particularly through realistic communication. Social robotic VPs may prove useful and safe learning environments for exposing medical students to diverse, highly interactive patient simulations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f04c25fcf3247ff4d8eca72d862b22090b884b75\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f04c25fcf3247ff4d8eca72d862b22090b884b75\",\n",
      "        \"title\": \"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\",\n",
      "        \"citationCount\": 33,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.06461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-10\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"49606614\",\n",
      "            \"name\": \"Junlin Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282521448\",\n",
      "            \"name\": \"Siddhartha Jain\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2305691523\",\n",
      "            \"name\": \"Dejiao Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2282366776\",\n",
      "            \"name\": \"Baishakhi Ray\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"40574366\",\n",
      "            \"name\": \"Varun Kumar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2304481349\",\n",
      "            \"name\": \"Ben Athiwaratkun\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often dont surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/7bef7f67019cd3b9ff81f715ea65628fac3291a5\",\n",
      "        \"title\": \"What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering\",\n",
      "        \"citationCount\": 70,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.12334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Two metrics for classification tasks, namely sensitivity and consistency, are introduced, which are complementary to task performance, and it is hoped that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-18\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2307085791\",\n",
      "            \"name\": \"Federico Errica\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2009237\",\n",
      "            \"name\": \"G. Siracusano\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3109801\",\n",
      "            \"name\": \"D. Sanvito\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269460793\",\n",
      "            \"name\": \"Roberto Bifulco\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs'inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely sensitivity and consistency, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/024c9fe5a0e00786683d64ec32d142aaaae55fa2\",\n",
      "        \"title\": \"PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency\",\n",
      "        \"citationCount\": 37,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2403.09732?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2403.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"A novel prompt representation, called reference-enhanced representation, is introduced, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries and proposes using cross-consistency across different LLMs rather than self-consistency within a particular LLM.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": null,\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2262448530\",\n",
      "            \"name\": \"Zhishuai Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2292059965\",\n",
      "            \"name\": \"Xiang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2291921433\",\n",
      "            \"name\": \"Jingjing Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262480162\",\n",
      "            \"name\": \"Sun Yang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2228059114\",\n",
      "            \"name\": \"Guoqing Du\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2267589674\",\n",
      "            \"name\": \"Xiaoru Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2315291036\",\n",
      "            \"name\": \"Bin Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2276235755\",\n",
      "            \"name\": \"Yuxiao Ye\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262543561\",\n",
      "            \"name\": \"Ziyue Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2263456785\",\n",
      "            \"name\": \"Rui Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2262446566\",\n",
      "            \"name\": \"Hangyu Mao\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": null\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0e314ddbf28514d92f2405b73941242c162ae0ba\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0e314ddbf28514d92f2405b73941242c162ae0ba\",\n",
      "        \"title\": \"AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators\",\n",
      "        \"citationCount\": 28,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.11073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AFaCTA (Automatic Factual Claim deTection Annotator) is introduced, a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs) and calibrates its annotation confidence with consistency along three predefined reasoning paths.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\",\n",
      "          \"Review\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-02-16\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2046974354\",\n",
      "            \"name\": \"Jingwei Ni\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2284947386\",\n",
      "            \"name\": \"Minjing Shi\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"146552774\",\n",
      "            \"name\": \"Dominik Stammbach\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2790926\",\n",
      "            \"name\": \"Mrinmaya Sachan\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2261279066\",\n",
      "            \"name\": \"Elliott Ash\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3073566\",\n",
      "            \"name\": \"Markus Leippold\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1c5a097b4e376897545f153370425cf7e0c2d8fd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/1c5a097b4e376897545f153370425cf7e0c2d8fd\",\n",
      "        \"title\": \"Explaining Length Bias in LLM-Based Preference Evaluations\",\n",
      "        \"citationCount\": 20,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.01085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"AdapAlpaca is proposed, a simple yet effective adjustment to win rate measurement that ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\n",
      "        },\n",
      "        \"publicationTypes\": null,\n",
      "        \"publicationDate\": \"2024-07-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": null,\n",
      "            \"name\": \"Zhengyu Hu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2322070046\",\n",
      "            \"name\": \"Linxin Song\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309191644\",\n",
      "            \"name\": \"Jieyu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2311315868\",\n",
      "            \"name\": \"Zheyuan Xiao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269687536\",\n",
      "            \"name\": \"Jingang Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309176938\",\n",
      "            \"name\": \"Zhenyu Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2309202283\",\n",
      "            \"name\": \"Jieyu Zhao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2269470756\",\n",
      "            \"name\": \"Hui Xiong\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"The use of large language models (LLMs) as judges, particularly in preference comparisons, has become widespread, but this reveals a notable bias towards longer responses, undermining the reliability of such evaluations. To better understand such bias, we propose to decompose the preference evaluation metric, specifically the win rate, into two key components: desirability and information mass, where the former is length-independent and related to trustworthiness such as correctness, toxicity, and consistency, and the latter is length-dependent and represents the amount of information in the response. We empirically demonstrated the decomposition through controlled experiments and found that response length impacts evaluations by influencing information mass. To derive a reliable evaluation metric that assesses content quality without being confounded by response length, we propose AdapAlpaca, a simple yet effective adjustment to win rate measurement. Specifically, AdapAlpaca ensures a fair comparison of response quality by aligning the lengths of reference and test model responses under equivalent length intervals.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"00ce8beee350a260395676490915d7ebfa7430d1\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/00ce8beee350a260395676490915d7ebfa7430d1\",\n",
      "        \"title\": \"Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation\",\n",
      "        \"citationCount\": 16,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.14088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Investigation of fine-tuned LLMs in D2T tasks in terms of model size reveals that increasing LLM size enhances readability andformativeness in D2T tasks, but larger LLMs may sacrifice \\\\textit{faithfulness}.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-07-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"8038450\",\n",
      "            \"name\": \"Joy Mahapatra\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2312204876\",\n",
      "            \"name\": \"U. Garain\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Data-to-text (D2T) generation aims to generate human-readable text from semi-structured data, such as tables and graphs. The recent success of D2T is largely attributed to advancements in LLMs. Despite the success of LLMs, no research has been conducted to illustrate the impact of model size on the performance of fine-tuned LLMs for D2T tasks. D2T model performance is typically assessed based on three key qualities: \\\\textit{readability} (indicates fluency and coherence), \\\\textit{informativeness} (measures content similarity), and \\\\textit{faithfulness} (assesses consistency of factual information). It is currently uncertain whether increasing the size of LLMs effectively improves performance in D2T tasks across these three qualities. The objective of this study is to investigate the performance of fine-tuned LLMs in D2T tasks in terms of model size. Through extensive comparative analysis, we aim to elucidate both the advantages and limitations of scaling model sizes across five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and twelve state-of-the-art LLMs with varying sizes from five different LLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all the three essential qualities of D2T models, we incorporate six widely recognized automatic metrics -- \\\\textsc{BLEU}, \\\\textsc{METEOR}, \\\\textsc{BERTScore}, \\\\textsc{MoverScore}, \\\\textsc{Parent}, and \\\\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance concerning model size in the presence of source-reference divergence, a critical aspect of D2T tasks. Our investigation reveals that increasing LLM size enhances \\\\textit{readability} and \\\\textit{informativeness} in D2T tasks, but larger (in terms of size) LLMs may sacrifice \\\\textit{faithfulness}. Moreover, small-sized LLMs show more resilience than larger ones when source-reference divergence is present.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"74908bc543e77b8995a6eebe32ab13cf0837949b\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/74908bc543e77b8995a6eebe32ab13cf0837949b\",\n",
      "        \"title\": \"LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment and demonstrates that, when using GPT-4, the proposed metric achieves evaluation consistency close to that of radiologists.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-01\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2294387070\",\n",
      "            \"name\": \"Zilong Wang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"13289447\",\n",
      "            \"name\": \"Xufang Luo\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268347004\",\n",
      "            \"name\": \"Xinyang Jiang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2268313028\",\n",
      "            \"name\": \"Dongsheng Li\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2160727304\",\n",
      "            \"name\": \"Lili Qiu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f33991c02f8f0ab8794dad020c648b37ccc53365\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/f33991c02f8f0ab8794dad020c648b37ccc53365\",\n",
      "        \"title\": \"CitaLaw: Enhancing LLM with Citations in Legal Domain\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.14556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This paper proposes CitaLaw, the first benchmark designed to evaluate LLMs' ability to produce legally sound responses with appropriate citations, and introduces syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\",\n",
      "          \"Conference\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-12-19\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2290124325\",\n",
      "            \"name\": \"Kepu Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2118684861\",\n",
      "            \"name\": \"Weijie Yu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2155892801\",\n",
      "            \"name\": \"Sunhao Dai\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2274965731\",\n",
      "            \"name\": \"Jun Xu\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0d5ccf0861b62223fe562e13369ab15746188251\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/0d5ccf0861b62223fe562e13369ab15746188251\",\n",
      "        \"title\": \"QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums\",\n",
      "        \"citationCount\": 13,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.05345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums, and applies QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-08\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2300845194\",\n",
      "            \"name\": \"Varun Nagaraj Rao\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300370478\",\n",
      "            \"name\": \"Eesha Agarwal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2300371225\",\n",
      "            \"name\": \"Samantha Dalal\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2265042713\",\n",
      "            \"name\": \"Dan Calacci\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2266397659\",\n",
      "            \"name\": \"Andr'es Monroy-Hern'andez\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methodologies used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting and human evaluation methodology. We applied this framework to analyze over one million comments from two of Reddit's rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33\",\n",
      "        \"title\": \"Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.13082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"TREACLE is proposed, a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints, and provides the user with the ability to gracefully trade off accuracy for cost.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-04-17\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2157197701\",\n",
      "            \"name\": \"Xuechen Zhang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297821531\",\n",
      "            \"name\": \"Zijian Huang\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2297769735\",\n",
      "            \"name\": \"Ege Onur Taga\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"1393650147\",\n",
      "            \"name\": \"Carlee Joe-Wong\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"3103394\",\n",
      "            \"name\": \"Samet Oymak\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2281075331\",\n",
      "            \"name\": \"Jiasi Chen\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE ($\\\\underline{T}$hrifty $\\\\underline{Rea}$soning via $\\\\underline{C}$ontext-Aware $\\\\underline{L}$LM and Prompt S$\\\\underline{e}$lection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c01bbc439164002f2c7326748f7939783e306a94\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/c01bbc439164002f2c7326748f7939783e306a94\",\n",
      "        \"title\": \"A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization\",\n",
      "        \"citationCount\": 14,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.09972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"It is found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a different level of rule understanding in the prompt, crucial for improving the accuracy and consistency of LLM-based evaluations.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-06-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"2264464750\",\n",
      "            \"name\": \"Kuanchao Chu\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2109381394\",\n",
      "            \"name\": \"Yi-Pei Chen\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301580436\",\n",
      "            \"name\": \"Hideki Nakayama\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This research investigates prompt designs of evaluating generated texts using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for open-ended text evaluation remains challenging due to model sensitivity and subjectivity in evaluation of text generation. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a different level of rule understanding in the prompt. An additional optimization may enhance scoring alignment if sufficient data is available. This insight is crucial for improving the accuracy and consistency of LLM-based evaluations.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/a03bdcf22b137ba220f0508768a5f3b4ff374bcd\",\n",
      "        \"title\": \"How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment\",\n",
      "        \"citationCount\": 135,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"https://arxiv.org/pdf/2309.14049\",\n",
      "          \"status\": \"GREEN\",\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.14049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"Insight into novice learners use of AI code generators in a self-paced learning environment is offered, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"Book\",\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2023-09-25\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"3136345\",\n",
      "            \"name\": \"Majeed Kazemitabaar\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2112801172\",\n",
      "            \"name\": \"Xinying Hou\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2063979470\",\n",
      "            \"name\": \"A. Henley\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"20937525\",\n",
      "            \"name\": \"B. Ericson\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2862077\",\n",
      "            \"name\": \"David Weintrop\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2666589\",\n",
      "            \"name\": \"Tovi Grossman\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\n",
      "        \"url\": \"https://www.semanticscholar.org/paper/221c89578778f872a9eb39c01a0b0b9b2f2a30f2\",\n",
      "        \"title\": \"PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles\",\n",
      "        \"citationCount\": 10,\n",
      "        \"openAccessPdf\": {\n",
      "          \"url\": \"\",\n",
      "          \"status\": null,\n",
      "          \"license\": null,\n",
      "          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\n",
      "        },\n",
      "        \"tldr\": {\n",
      "          \"model\": \"tldr@v2.0.0\",\n",
      "          \"text\": \"This work proposes to comprehensively address all subtasks together, and suggests employing a unique prompt-based in-context learning strategy to enhance error correction and error detection performance in medical systems where prediction errors can have grave consequences.\"\n",
      "        },\n",
      "        \"publicationTypes\": [\n",
      "          \"JournalArticle\"\n",
      "        ],\n",
      "        \"publicationDate\": \"2024-05-14\",\n",
      "        \"authors\": [\n",
      "          {\n",
      "            \"authorId\": \"9122885\",\n",
      "            \"name\": \"Kesav Gundabathula\"\n",
      "          },\n",
      "          {\n",
      "            \"authorId\": \"2301202434\",\n",
      "            \"name\": \"Sriram R Kolar\"\n",
      "          }\n",
      "        ],\n",
      "        \"abstract\": \"This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals. This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it. Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information. We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy. We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge. In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(list_of_papers, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d1b4c8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>methodology_overlap</th>\n",
       "      <th>problem_overlap</th>\n",
       "      <th>domain_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    methodology_overlap  problem_overlap  domain_overlap\n",
       "0                  0.85             0.85            0.70\n",
       "1                  0.84             0.82            0.78\n",
       "2                  0.83             0.82            0.80\n",
       "3                  0.70             0.70            0.75\n",
       "4                  0.78             0.75            0.76\n",
       "5                  0.72             0.68            0.65\n",
       "6                  0.80             0.80            0.80\n",
       "7                  0.72             0.70            0.70\n",
       "8                  0.66             0.66            0.70\n",
       "9                  0.68             0.70            0.72\n",
       "10                 0.81             0.82            0.79\n",
       "11                 0.73             0.74            0.72\n",
       "12                 0.77             0.76            0.75\n",
       "13                 0.80             0.78            0.79\n",
       "14                 0.72             0.74            0.72\n",
       "15                 0.70             0.70            0.75\n",
       "16                 0.64             0.66            0.68\n",
       "17                 0.66             0.66            0.70\n",
       "18                 0.58             0.62            0.60\n",
       "19                 0.68             0.70            0.72"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Parse the results\n",
    "result_content = json.loads(result_llm[\"messages\"][-1].content)\n",
    "\n",
    "# Create DataFrame\n",
    "papers_df = pd.DataFrame(result_content['papers'])\n",
    "\n",
    "papers_df[[\"methodology_overlap\",\"problem_overlap\",\"domain_overlap\"]]\n",
    "\n",
    "\n",
    "## NOTE\n",
    "## Wrong overlap calculation\n",
    "\n",
    "### Development Note\n",
    "## might be interesting to make the output of this papers analysis passed into \n",
    "## a GAN-like architecture (So not using overlapping score like the current one)\n",
    "## where one agent is argumenting why the idea is novel\n",
    "## and another agent is criticising why the idea is not novel, both based on the prior work\n",
    "## and then another agent act as a judge to decide which argument is stronger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c48acd",
   "metadata": {},
   "source": [
    "## Evaluation Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8839bf",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "-> since everything here will be based on the retrieved papers, we need to make sure that the papers are retrieved correctly.\n",
    "\n",
    "\n",
    "-> and also we need to limit the number of papers not to be too many that would make the system too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ead707",
   "metadata": {},
   "source": [
    "### Adversarial GAN-like Evaluation of Research Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ae5a3",
   "metadata": {},
   "source": [
    "#### Advocate Agent\n",
    "The goal of the advocate agent is to defend the research idea, grounded based on the retrieved papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d351c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "# Define the state for the adversarial debate\n",
    "\n",
    "class DebateState(TypedDict):\n",
    "    \"\"\"State for the adversarial debate graph\"\"\"\n",
    "    research_idea: str\n",
    "    retrieved_papers: str\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    debate_concluded: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialAgent(BaseModel):\n",
    "    \"\"\"\n",
    "    Base class for Adversarial Agents\n",
    "    All agents must provide evidence-based arguments citing retrieved papers.\n",
    "    \"\"\"\n",
    "    argument: str = Field(\n",
    "        description=\"The main argument presented by the agent\"\n",
    "    )\n",
    "    \n",
    "    supporting_papers: List[str] = Field(\n",
    "        description=\"List of paper IDs cited to support this argument\"\n",
    "    )\n",
    "    \n",
    "    key_points: List[str] = Field(\n",
    "        description=\"Main points made in this argument\"\n",
    "    )\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1be492cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "advocate_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are the ADVOCATE for the proposed research idea.\n",
    "Your goal is to defend the idea, highlight its novelty and feasibility.\n",
    "Use the provided retrieved papers to support your arguments.\n",
    "\n",
    "Focus on:\n",
    "1. Unique contributions\n",
    "2. How it improves upon existing methods (cite paper IDs)\n",
    "3. Why the potential impact outweighs risks\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Provide a strong, evidence-based argument citing specific papers.\n",
    "\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Automatically includes message history\n",
    "])\n",
    "\n",
    "# Create advocate agent that returns structured output\n",
    "class AdvocateResponse(AdversarialAgent):\n",
    "    \"\"\"Advocate's argument response\"\"\"\n",
    "    pass\n",
    "\n",
    "advocate_agent = advocate_prompt | llm.with_structured_output(AdvocateResponse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1797683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_advocate_agent(state: DebateState):\n",
    "    research_idea = state['research_idea']\n",
    "    retrieved_papers = state['retrieved_papers']\n",
    "    messages = state['messages']\n",
    "    \n",
    "    response = advocate_agent.invoke({\n",
    "        \"research_idea\": research_idea,\n",
    "        \"retrieved_papers\": retrieved_papers,\n",
    "        \"history\": messages  # Pass the messages list directly\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.argument)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad10e3",
   "metadata": {},
   "source": [
    "#### Critics Agent\n",
    "The goal of the critics agent is to challenge the idea and the advocate's argument by finding the gap or weakness in the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "skeptic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are the SKEPTIC of the proposed research idea.\n",
    "Your goal is to critique the idea, point out flaws, and question its novelty.\n",
    "Use the provided retrieved papers to show similarity to prior work or identify weaknesses.\n",
    "\n",
    "Focus on:\n",
    "1. Overlaps with existing work (cite specific papers).\n",
    "2. Potential technical challenges or flaws.\n",
    "3. Why the idea might not be as novel or impactful as claimed.\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Provide a critical, evidence-based counter-argument.\n",
    "\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Automatically includes message history\n",
    "])\n",
    "\n",
    "# Create skeptic agent that returns structured output\n",
    "class SkepticResponse(AdversarialAgent):\n",
    "    \"\"\"Skeptic's argument response\"\"\"\n",
    "    pass\n",
    "\n",
    "skeptic_agent = skeptic_prompt | llm.with_structured_output(SkepticResponse)\n",
    "\n",
    "def call_skeptic_agent(state: DebateState):\n",
    "    research_idea = state['research_idea']\n",
    "    retrieved_papers = state['retrieved_papers']\n",
    "    messages = state['messages']\n",
    "    \n",
    "    response = skeptic_agent.invoke({\n",
    "        \"research_idea\": research_idea,\n",
    "        \"retrieved_papers\": retrieved_papers,\n",
    "        \"history\": messages  # Pass the messages history directly\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.argument)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f307bae",
   "metadata": {},
   "source": [
    "#### Judge Agent\n",
    "The goal of the judge agent is to be a neutral and objective evaluator between the advocate and critic agents, and to find the final verdict of the research idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beea497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "moderator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are an EXPERT guiding a debate between an Advocate and a Skeptic about a research idea.\n",
    "Your goal is to synthesize the arguments, ask probing questions to clarify the idea, and ensure the discussion remains grounded in the literature.\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Current Iteration: {iteration} / {max_iterations}\n",
    "\n",
    "Task:\n",
    "1. Summarize the key points made by both sides so far.\n",
    "2. If the maximum iterations have been reached or if the discussion has converged, provide a FINAL VERDICT on the idea's novelty and feasibility. Start your response with \"VERDICT:\".\n",
    "3. If the discussion should continue, ask a specific, probing question to guide the next round of debate.\n",
    "\n",
    "OUTPUT (valid JSON only):\n",
    "\n",
    "\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Automatically includes message history\n",
    "])\n",
    "\n",
    "# Create moderator response that returns structured output\n",
    "class ModeratorResponse(AdversarialAgent):\n",
    "    \"\"\"Moderator's response\"\"\"\n",
    "    verdict: str = Field(\n",
    "        description=\"Final verdict if debate concluded (starts with 'VERDICT:')\",\n",
    "        default=None\n",
    "    )\n",
    "    next_question: Optional[str] = Field(\n",
    "        description=\"Question for next round if continuing\",\n",
    "        default=None\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "\n",
    "moderator_agent = moderator_prompt | llm.with_structured_output(ModeratorResponse)\n",
    "\n",
    "\n",
    "class VerdictResponse(BaseModel):\n",
    "    \"\"\"Final verdict structure\"\"\"\n",
    "    novelty_score: int = Field(description=\"Score 1-10 for novelty\", ge=1, le=10)\n",
    "    feasibility_score: int = Field(description=\"Score 1-10 for feasibility\", ge=1, le=10)\n",
    "    summary: str = Field(description=\"Summary of the debate\")\n",
    "    strengths: List[str] = Field(description=\"Key strengths identified\")\n",
    "    weaknesses: List[str] = Field(description=\"Key weaknesses identified\")\n",
    "    recommendation: str = Field(description=\"Accept/Revise/Reject with justification\")\n",
    "    \n",
    "    def to_verdict_text(self) -> str:\n",
    "        return f\"\"\"VERDICT: Research Idea Evaluation\n",
    "\n",
    "SCORES:\n",
    "- Novelty: {self.novelty_score}/10\n",
    "- Feasibility: {self.feasibility_score}/10\n",
    "\n",
    "SUMMARY:\n",
    "{self.summary}\n",
    "\n",
    "STRENGTHS:\n",
    "{chr(10).join(f'- {s}' for s in self.strengths)}\n",
    "\n",
    "WEAKNESSES:\n",
    "{chr(10).join(f'- {w}' for w in self.weaknesses)}\n",
    "\n",
    "RECOMMENDATION: {self.recommendation}\n",
    "\"\"\"\n",
    "\n",
    "# Update moderator to use VerdictResponse on final iteration\n",
    "def call_moderator_agent(state: DebateState):\n",
    "    research_idea = state['research_idea']\n",
    "    retrieved_papers = state['retrieved_papers']\n",
    "    messages = state['messages']\n",
    "    iteration = state.get('iteration', 0)\n",
    "    max_iterations = state.get('max_iterations', 3)\n",
    "    \n",
    "    \n",
    "    is_final_iteration = (iteration +1 >= max_iterations)\n",
    "    \n",
    "    if is_final_iteration:\n",
    "        # Create a verdict-specific prompt\n",
    "        verdict_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are the EXPERT providing the FINAL VERDICT for a research idea debate.\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Debate History:\n",
    "The Advocate and Skeptic have exchanged arguments over {iteration} rounds.\n",
    "\n",
    "YOUR TASK:\n",
    "Provide a comprehensive final verdict with structured scores.\n",
    "\n",
    "You MUST return a structured response with:\n",
    "- novelty_score: integer 1-10\n",
    "- feasibility_score: integer 1-10  \n",
    "- summary: string summarizing the debate\n",
    "- strengths: list of strings\n",
    "- weaknesses: list of strings\n",
    "- recommendation: \"Accept\" or \"Revise\" or \"Reject\" with justification\n",
    "\n",
    "Be decisive and evidence-based. \n",
    "START your response with \"VERDICT:\".\n",
    "\"\"\"\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "        ])\n",
    "        \n",
    "        # Use verdict-specific prompt and schema\n",
    "        verdict_agent = verdict_prompt | llm.with_structured_output(VerdictResponse)\n",
    "        response = verdict_agent.invoke({\n",
    "            \"research_idea\": research_idea,\n",
    "            \"retrieved_papers\": retrieved_papers,\n",
    "            \"iteration\": iteration,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"history\": messages\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=response.to_verdict_text())],\n",
    "            \"iteration\": iteration + 1\n",
    "        }\n",
    "    else:\n",
    "        # Regular moderator response\n",
    "        response = moderator_agent.invoke({\n",
    "            \"research_idea\": research_idea,\n",
    "            \"retrieved_papers\": retrieved_papers,\n",
    "            \"iteration\": iteration,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"history\": messages\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=response.argument)],\n",
    "            \"iteration\": iteration + 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a6653",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "\n",
    "\n",
    "# Define routing logic\n",
    "def should_continue(state: DebateState) -> str:\n",
    "    \"\"\"Determine if debate should continue or end\"\"\"\n",
    "    # Check if we've reached max iterations\n",
    "    if state['iteration'] >= state['max_iterations']:\n",
    "        return \"end\"\n",
    "    \n",
    "    # Check if moderator issued a verdict\n",
    "    last_message = state['messages'][-1].content\n",
    "    if isinstance(last_message, str) and last_message.startswith(\"VERDICT:\"):\n",
    "        return \"end\"\n",
    "    \n",
    "    # Continue the debate\n",
    "    return \"continue\"\n",
    "\n",
    "def route_after_moderator(state: DebateState) -> str:\n",
    "    \"\"\"Route after moderator's turn\"\"\"\n",
    "    result = should_continue(state)\n",
    "    if result == \"end\":\n",
    "        return END\n",
    "    return \"advocate\"\n",
    "\n",
    "# Build the debate workflow\n",
    "debate_workflow = StateGraph(DebateState)\n",
    "\n",
    "# Add nodes\n",
    "debate_workflow.add_node(\"advocate\", call_advocate_agent)\n",
    "debate_workflow.add_node(\"skeptic\", call_skeptic_agent)\n",
    "debate_workflow.add_node(\"moderator\", call_moderator_agent)\n",
    "\n",
    "# Add edges\n",
    "debate_workflow.add_edge(START, \"advocate\")\n",
    "debate_workflow.add_edge(\"advocate\", \"skeptic\")\n",
    "debate_workflow.add_edge(\"skeptic\", \"moderator\")\n",
    "\n",
    "# Add conditional edge from moderator\n",
    "debate_workflow.add_conditional_edges(\n",
    "    \"moderator\",\n",
    "    route_after_moderator,\n",
    "    {\n",
    "        \"advocate\": \"advocate\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "debate_graph = debate_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4cb2f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Visualize the debate graph\n",
    "display(Image(debate_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9255129e",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_backends/sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    208\u001b[39m     sock = socket.create_connection(\n\u001b[32m    209\u001b[39m         address,\n\u001b[32m    210\u001b[39m         timeout,\n\u001b[32m    211\u001b[39m         source_address=source_address,\n\u001b[32m    212\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Run the adversarial debate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m debate_result = \u001b[43mdebate_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearch_idea\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresearch_idea_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieved_papers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_papers_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miteration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_iterations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdebate_concluded\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Display the debate messages\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m debate_result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/main.py:3050\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3048\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3050\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/main.py:2633\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2632\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2643\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcall_advocate_agent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      3\u001b[39m retrieved_papers = state[\u001b[33m'\u001b[39m\u001b[33mretrieved_papers\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m messages = state[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43madvocate_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearch_idea\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresearch_idea\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieved_papers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_papers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhistory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the messages list directly\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [AIMessage(content=response.argument)]\n\u001b[32m     14\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/runnables/base.py:3129\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3127\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3129\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/runnables/base.py:5534\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5527\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5528\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5529\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5532\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5533\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5535\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5536\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5537\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_openai/chat_models/base.py:1356\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1355\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1359\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1360\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1361\u001b[39m ):\n\u001b[32m   1362\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_openai/chat_models/base.py:1324\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1321\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     raw_response = (\n\u001b[32m-> \u001b[39m\u001b[32m1324\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1327\u001b[39m     )\n\u001b[32m   1328\u001b[39m     response = raw_response.parse()\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/resources/chat/completions/completions.py:184\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    179\u001b[39m         response_format=response_format,\n\u001b[32m    180\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    181\u001b[39m         input_tools=chat_completion_tools,\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_base_client.py:1014\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1011\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1013\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\n\u001b[32m   1017\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1018\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     response.headers,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error.",
      "During task with name 'advocate' and id '08f38a9c-5c80-ce6b-5861-ad1e0f43b3d2'"
     ]
    }
   ],
   "source": [
    "# Extract research idea from initial user input\n",
    "research_idea_text = result_llm[\"messages\"][0].content\n",
    "\n",
    "# Extract and format retrieved papers\n",
    "papers_json = json.loads(result_llm[\"messages\"][-2].content)  # -2 because -1 is the analysis\n",
    "retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n",
    "\n",
    "# Run the adversarial debate\n",
    "debate_result = debate_graph.invoke({\n",
    "    \"research_idea\": research_idea_text,\n",
    "    \"retrieved_papers\": retrieved_papers_text,\n",
    "    \"messages\": [],\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": 4,\n",
    "    \"debate_concluded\": False\n",
    "})\n",
    "\n",
    "# Display the debate messages\n",
    "for msg in debate_result[\"messages\"]:\n",
    "    print(f\"\\n{'-'*80}\\n{msg.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561162f",
   "metadata": {},
   "source": [
    "##### Example Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a8720",
   "metadata": {},
   "source": [
    "gemini\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) operationalizes a three-phase, closed-loop prompting paradigm to directly address coherence and adaptability in long-form generation and multi-turn dialogue. Unlike fixed prompts or static few-shot prompts, DPA explicitly analyzes the evolving interaction history to extract themes, tone shifts, and narrative goals (Contextual Analysis), generates updated prompts that steer subsequent outputs toward new elements or clarified past responses (Adaptive Prompt Generation), and continually synthesizes the aggregate of prior exchanges to preserve global coherence (Iterative Context Update). This creates a lightweight, memory-efficient alternative to full fine-tuning while delivering sustained prompt-driven alignment across long sessions, multi-turn narratives, and dynamic topics. The approach is novel in its explicit three-phase cycle tailored for ongoing interactions, rather than single-shot adaptation, and it targets the core bottleneck of LLM coherence over time rather than surface-level accuracy alone. In addition, the framework is designed to be modular and cross-domain: it can pair with parameter-efficient adapters (e.g., DynaLoRA-style dynamics) to strike a favorable accuracy/compute trade-off as shown by dynamic-adapter+prompt-tuning work (Dynamic Adapter Meets Prompt Tuning) and Time-LlaMA-style dynamic adaptation, ensuring feasibility for real-world deployment with modest compute overhead. The plan also outlines concrete, ecologically valid experiments (storytelling and dynamic dialogue) using established coherence and engagement metrics, leveraging CTTA-inspired prompts to handle domain shifts and evolving contexts. This positions DPA as a practical, scalable solution with immediate impact for education, entertainment, and humanAI collaboration in creative writing and extended conversations.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) proposes a three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to maintain coherence and adaptability in extended LLM interactions. While appealing, the idea risks being incremental rather than novel: it essentially concatenates existing dynamic-prompt and prompt-tuning ideas (analysis-driven prompt updates, per-step prompt adaptation, and history-aware synthesis) that are already explored in the literature. Moreover, there are significant practical and evaluative risks (unstable prompts, drift over many turns, poor correlation of BLEU/ROUGE with narrative quality, data issues) that are not adequately addressed. The claimed noveltythree phases tailored for ongoing interactionsoverlaps substantially with prior dynamic-prompting and test-time adaptation work, and the experimental plan relies on metrics and datasets that have well-known limitations for evaluating long-form coherence and engagement. Without stronger theoretical grounding, rigorous baselines, and robust, human-centered evaluation, the proposal may yield modest, domain-limited impact despite substantial engineering effort.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Advocate argues that Dynamic Prompt Adaptation (DPA) offers a novel three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to sustain coherence and adaptability in long-form LLM interactions. It emphasizes explicit, history-aware prompting as a lightweight alternative to full fine-tuning, with modular compatibility alongside parameter-efficient methods (e.g., dynamic adapters, LoRA). Proponents claim novelty lies in the explicit three-phase cycle tailored for ongoing interactions and the potential for cross-domain impact in creative writing and extended conversations, supported by a concrete experiment plan and engagement metrics. Skeptic counterpoints stress that the core ideas resemble existing dynamic prompting and test-time adaptation paradigms, risk being incremental rather than groundbreaking, and rely on evaluation metrics (e.g., BLEU/ROUGE) and datasets (e.g., Reddit) that have known limitations for measuring long-range coherence and narrative quality. They caution about prompt drift, instability over many turns, lack of rigorous theoretical grounding, and potential biases in data and baselines. Overall, the debate centers on whether the proposed three-phase loop genuinely introduces a new methodological paradigm or re-packages established dynamic prompting concepts with potentially modest novelty and impact without stronger theoretical guarantees or robust human-centered evaluation.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) introduces a principled three-phase loopContextual Analysis, Adaptive Prompt Generation, and Iterative Context Updatethat actively maintains coherence and adaptability in long-form LLM interactions. This goes beyond fixed prompts, fixed few-shots, or single-turn CoT-style prompting by building a lightweight, memory-efficient mechanism that continually reasons about the evolving dialogue and fabricates targeted prompt updates to steer future generations. The novelty lies not just in prompting, but in the explicit three-phase orchestration that (i) extracts themes and tonal shifts from prior exchanges, (ii) generates updated prompts that integrate new elements or reframe past responses, and (iii) synthesizes the entire dialogue history to preserve global coherence over many turns. This closes the loop between memory-like context and prompt-driven control, enabling sustained narrative quality in story-telling, tutoring, conversational agents, and interactive media. Importantly, DPA remains highly feasible in practice by leveraging established parameter-efficient dynamics (e.g., dynamic adapters, LoRA-style modules, and memory-efficient prompt mechanisms) as shown in related work, ensuring deployment on commodity hardware without full fine-tuning.\n",
    "\n",
    "Key contributions and novelty beyond prior work:\n",
    "- Explicit three-phase loop tailored for ongoing interactions, not just one-shot adaptation or static prompting (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update). This targets long-range coherence and dynamic thematic evolution in dialogue and narrative tasks.\n",
    "- Modular, plugandplay design that can be paired with parameter-efficient adaptation techniques (e.g., Dynamic Adapters + Prompt Tuning; Time-LlaMAs dynamic LoRA selection) to balance accuracy and compute (see Dynamic Adapter Meets Prompt Tuning; Time-LlaMA) [papers cited].\n",
    "- Compatibility with multi-modal and cross-domain prompts via domain-aware prompting and meta-relabling strategies, enabling robust generalization across tasks and domains (DAPrompt and RADA-prompt style ideas) [papers cited].\n",
    "- A concrete experimental path that uses established storytelling and dialogue benchmarks (e.g., Story Cloze-like tasks and user-dialogue datasets) with multi-faceted metrics (engagement, coherence, user satisfaction, BLEU/ROUGE) to capture long-range narrative quality, moving beyond surface metrics.\n",
    "- A risk-mitigated deployment plan that embraces ensemble and adaptive prompting strategies to curb drift and overfitting, inspired by robust test-time prompting approaches.\n",
    "\n",
    "How DPA improves upon existing methods, with supporting evidence from the literature:\n",
    "- Dynamic prompt cores/CTTA-style adaptation: DPCore shows that dynamic prompt coresets, dedicated visual prompts, and a dynamic update mechanism can achieve robust continual test-time adaptation while drastically reducing trainable parameters and compute, illustrating the practicality and efficiency of dynamic prompting in changing contexts (DPCore; 246482d9758e93d0b349e2926996d887417174d8).\n",
    "- Parameter-efficient adaptation with prompts: Dynamic Adapter + Prompt Tuning (DAPT) demonstrates that freezing base models and adding dynamic adapters alongside internal prompts yields superior performance with dramatic reductions in trainable parameters and memory (6b533de65b3d2cee190415f5a2a2e6afe2ac7c78).\n",
    "- Dynamic, task-aware adaptation without full fine-tuning: Time-LlaMA introduces dynamic low-rank adaptation that selects LoRA modules per input to balance performance and inference efficiency, supporting the feasibility of input-dependent, prompt-related adaptation in large models (650a24da1702beca7eb70011a26f1f3238efad4b).\n",
    "- Prompt-based test-time adaptation and robust prompting: DAPrompt and related prompt-learning schemes show that ensembling and meta-prompting can mitigate biases and improve robustness during test-time adaptation, aligning with DPAs emphasis on robust, adaptive prompting during ongoing interactions (5db3cfc974c42bfa2d9518a8910762790; 759b5f58e58a76f79a7d845acd3169dc899d0ac2).\n",
    "- Retrieval-augmented and iterative prompting for long-horizon tasks: RAT and Iter-RetGen illustrate how iterative retrieval-generation loops can enhance reasoning and grounding in long-horizon tasks, informing DPAs design where prompts are updated based on retrieved dialogue history and prior prompts (dc256e179d4e8eff48879a40ddc414b15b0b2300; 8051818817a9a3815be6623a679d4a7f5a7b7964).\n",
    "- Encouraging coherent multi-turn generation with structured prompting: Works on narrative generation and multi-turn interactions (e.g., DialogGen, Agents Room, SCENECRAFT) demonstrate the value of structured, multi-agent or multi-stage prompting and planning to achieve coherence and narrative alignment; DPA operationalizes this mindset into a three-phase, repeatable loop suitable for dynamic contexts.\n",
    "\n",
    "Why the potential impact justifies the risks:\n",
    "- Long-form coherence is a central bottleneck in education, entertainment, and human-AI collaboration. By continuously aligning prompts with evolving context, DPA can sustain thematic consistency, maintain stylistic voice, and adapt to user feedback without costly fine-tuning (as demonstrated by dynamic prompting literature and test-time adaptation work cited above).\n",
    "- The approach is scalable and deployment-friendly. The literature shows that dynamic prompt mechanisms, domain prompts, and adaptive prompts can achieve notable gains with modest parameter overhead and without retraining core models (e.g., DAPrompt, DPCore, DAPT, Time-LlaMA) [papers cited].\n",
    "- Cross-domain applicability: The architecture supports applying adaptive prompting to a wide range of taskscreative writing, interactive gaming narratives, tutoring, and long-form content generationthrough its modular three-phase design and compatibility with dynamic adapters and memory-augmented prompts.\n",
    "- Risk management via ensemble and dynamic prompting strategies: The literature provides practical approaches to reduce overfitting and bias at test-time (e.g., ensemble prompts in robust TTA; meta-prompting in domain adaptation), which we can incorporate to mitigate drift and reliability concerns in extended interactions (ADAPROMPT; 5db3cfc9; 759b5f58).\n",
    "\n",
    "Feasibility and a concrete path forward:\n",
    "- Feasibility is supported by substantial precedent for dynamic prompting and adaptive prompting in large language models across modalities and domains, with concrete gains in efficiency and performance (see the cited papers). DPAs three-phase loop can be implemented with existing tooling: a lightweight contextual analyzer to extract themes/tones from the history, a prompt generator that produces updated prompts conditioned on the extracted signals, and a synthesis module that maintains an ongoing narrative memory to inform future updates.\n",
    "- Evaluation strategy is well-grounded: we will compare against static prompts and per-turn prompting baselines, using engagement and coherence metrics, human judgments, and standard text-generation metrics (BLEU, ROUGE), in datasets such as Story Cloze-like storytelling tasks and user-dialogue interactions on Reddit-like conversational data.\n",
    "- Additional safeguards: adopt ensemble prompting, confidence-based prompts, and selective memory buffers to curb drift, following best practices from robust test-time prompting literature (e.g., ensemble prompts in ADAPPROMPT; domain-prompt learning approaches; RAT-like iterative grounding). These measures address common failure modes in long-running prompts and support reliable deployment.\n",
    "\n",
    "In sum, DPA is a novel, feasible, and impactful framework that explicitly closes the loop between evolving dialogue context and prompt-level control, enabling robust coherence and dynamic adaptability across long interactions while leveraging and integrating proven, efficient prompting and adaptation techniques from the referenced literature.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "The Dynamic Prompt Adaptation (DPA) idea aims to continuously steer LLM outputs across long interactions by a threephase loop: Contextual Analysis, Adaptive Prompt Generation, and Iterative Context Update. While appealing, the proposal appears incremental and shows substantial overlap with a body of prior work on dynamic/adaptive prompting, continual test-time adaptation, and retrieval-grounded prompting. Several cited papers already demonstrate close ideas: dynamic prompts and memory-aware adaptation (DPCore), parameter-efficient prompt/adaptation (Dynamic Adapter Meets Prompt Tuning; Time-LlaMA), domain-aware and meta-prompting (Domain Prompt Tuning via Meta Relabeling; Decorate the Newcomers CTTA; ADAPROMPT family), and iterative grounding/retrieval loops (RAT, Iter-RetGen). Moreover, long-horizon coherence in narratives and dialogue has been explored via multi-agent/prompting pipelines (DialogGen, Agents Room, PANGeA, SCENECRAFT). Taken together, the core thrustaligning prompts to evolving context across turnshas already been explored in multiple orthogonal directions, making the three-phase framing less novel than claimed. The claim of a single, unified novel paradigm for dynamic, evolving prompts across domains is therefore questionable.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Advocate argued that Dynamic Prompt Adaptation (DPA) offers a novel three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to sustain coherence and adaptability in long-form LLM interactions, and that it can operate without full fine-tuning by leveraging dynamic prompts and modular adapters. Skeptic contended that the three-phase loop largely re-packages established dynamic prompting and test-time adaptation concepts, with substantial overlap to work on DPCore, Dynamic Adapter + Prompt Tuning, Time-LlaMA, and domain/prompts-based adaptation, raising concerns about novelty and impact. The subsequent moderator assessment reinforced the view that the core idea is incremental, citing multiple orthogonal lines of prior work (dynamic prompts, CTTA, retrieval-grounded prompting, and narrative/dialogue pipelines) and arguing that without stronger theoretical grounding or robust human-centric evaluation, the proposal risks limited novelty and practical payoff.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) offers a principled, three-phase loop that explicitly ties evolving dialogue context to prompt-level control, enabling sustained coherence and adaptability in long-form interactions without full model fine-tuning. The three phasesContextual Analysis (extracting themes, tonal shifts, and narrative goals from history), Adaptive Prompt Generation (producing updated prompts that extend past ideas or steer toward new elements), and Iterative Context Update (synthesizing prior exchanges to preserve global coherence)form a cohesive memory-prompting cycle that mirrors human creative and conversational workflows. This design yields several unique advantages:\n",
    "- Memory-efficient long-horizon coherence: by re-synthesizing context into targeted prompts rather than retraining or storing large state, DPA achieves durable coherence with modest compute overhead, which aligns with successful demonstrations of dynamic prompting and prompt-efficient adapters (e.g., Dynamic Adapter Meets Prompt Tuning; Time-LlaMA).\n",
    "- Modular, domain-agnostic applicability: the three-phase loop can be paired with parameter-efficient primitives (dynamic adapters, LoRA variants, internal prompts) to adapt to storytelling, tutoring, and interactive media without expensive finetuning (supported by DAPT and Time-LlaMA literature).\n",
    "- Robustness under domain shifts and evolving user feedback: the framework inherently accommodates shifting audience and themes, leveraging ideas from test-time adaptation and meta-prompting to maintain reliability across sessions (RAT-inspired grounding, ensemble prompting strategies in robust TTA).\n",
    "- Cross-modal and multi-turn potential: by integrating with domain prompts and meta-relabeling we can extend DPA to visual storytelling, interactive dialogue, and multimodal narrative generation, as supported by MaPLe and related multi-modal prompt work.\n",
    "- Concrete, testable impact on reader engagement and coherence: the proposed evaluation plan mirrors established storytelling and dialogue tasks and benefits from prior work showing that adaptive prompting yields improvements in coherence, engagement, and adaptability (RAT, Iter-RetGen, SCENECRAFT, DialogGen).\n",
    "Overall, DPA brings a coherent, scalable, and practically deployable paradigm that leverages the best of prompt-based adaptation while addressing the key pain point of long-context coherence in LLM-powered creative and conversational tasks.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "The Dynamic Prompt Adaptation (DPA) idea promises to maintain coherence across long interactions by a three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) and to do so without full fine-tuning. However, the contribution is largely incremental and overlaps with a broad set of prior dynamic/adaptive prompting and test-time adaptation works. There is insufficient theoretical grounding, and the evaluation plan relies on metrics and data that have well-known limitations for judging long-horizon coherence and engagement. The claimed noveltyan explicit three-phase cycle tailored for ongoing interactionsappears to be a re-packaging of existing concepts rather than a fundamentally new paradigm. Without stronger baselines, rigorous human evaluations, or a formal treatment of stability/guarantees over many turns, the work risks limited impact beyond narrow demonstrations in storytelling or dialogue tasks.\n",
    "\n",
    "Key overlaps include dynamic prompts and domain-adaptive prompting, dynamic adapters, and retrieval-grounded prompting from the literature (see DPCore; Dynamic Adapter Meets Prompt Tuning; Time-LlaMA; DAPrompt/ADT family; RAT/Iter-RetGen). The three-phase loop shares motifs with iterative grounding and prompt refinement seen in Retrieval-Augmented Thoughts (RAT), Iterative Retrieval-Generation (Iter-RetGen), and CTTA-style adaptation. In short, the core ideaprompts that adapt to evolving context across turnshas already been explored in multiple orthogonal directions; the proposed three-phase framing does not clearly establish a unique, cohesive methodological advance on top of these prior works.\n",
    "\n",
    "Supporting_papers IDs: [\"246482d9758e93d0b349e2926996d887417174d8\", \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\", \"650a24da1702beca7eb70011a26f1f3238efad4b\", \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\", \"5db3cfc974c42bfa2d9518a8910762790\", \"dc256e179d4e8eff48879a40ddc414b15b0b2300\", \"dc256e179d4e8eff48879a40ddc414b15b0b2300\", \"8051818817a9a3815be6623a679d4a7f5a7b7964\"]}\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Advocate argues Dynamic Prompt Adaptation (DPA) offers a novel three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to maintain coherence and adaptability across long interactions without full fine-tuning, with modular compatibility to existing parameter-efficient methods. Skeptic counters that the three-phase loop largely recycles established dynamic prompting and test-time adaptation ideas and overlaps with DPCore, dynamic adapters + prompt tuning, Time-LlaMA, RAT/Iter-RetGen, and Dialog/NPC-driven narrative pipelines, raising concerns about novelty and practical impact. The moderator notes substantial overlap across multiple orthogonal lines of prior work and emphasizes the need for stronger theoretical grounding and robust, human-centered evaluation to establish genuine novelty and generalizability beyond narrow storytelling tasks. The current stance favors caution: feasible to implement, but novelty and significance require stronger demonstration and clearer baselines.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) offers a principled threephase loopContextual Analysis, Adaptive Prompt Generation, and Iterative Context Updatethat continually aligns prompts with an evolving dialogue to sustain coherence and engagement without full model finetuning. The novelty lies in explicitly decoupling history understanding (Contextual Analysis) from prompt reconfiguration (Adaptive Prompt Generation) and global history synthesis (Iterative Context Update) into a repeatable, memoryefficient cycle that can operate across domains (storytelling, tutoring, conversational agents, interactive media). Crucially, DPA is designed to plug into existing parameterefficient pipelines (e.g., dynamic adapters and prompt tuning) to balance accuracy and compute, making it deployable on commodity hardware. The feasibility and impact are underpinned by a rich literature showing that dynamic prompts, adaptive prompts, and retrievalaugmented reasoning can yield gains with modest training or finetuning costs, and scale to longhorizon tasks:\n",
    "\n",
    "- Dynamic Prompt Coreset and continual testtime adaptation demonstrate robust performance across changing domains with dramatically reduced trainable parameters and compute (DPCore) (246482d9758e93d0b349e2926996d887417174d8).\n",
    "- Parameterefficient transfer with prompts and dynamic adapters shows substantial gains while freezing the backbone (Dynamic Adapter Meets Prompt Tuning) (6b533de65b3d2cee190415f5a2a2e6afe2ac7c78).\n",
    "- Dynamic lowrank adaptation selects task/instancespecific modules per input, enabling efficient, scalable adaptation without full finetuning (TimeLLaMA) (650a24da1702beca7eb70011a26f1f3238efad4b).\n",
    "- Domainaware and meta prompting approaches illustrate how prompts can embed domain semantics and adapt to shifts without retraining (Domain Prompt Tuning via Meta Relabeling; Decorate the Newcomers for CTTA) (eacb61136023a2f30c5a0313f222d50e5f63ac9b).\n",
    "- Robust testtime prompting and ensemble strategies mitigate biases and improve adaptation under distribution shift (ADAPROMPT family) (5db3cfc974c42bfa2d9518a8910762790b516037).\n",
    "- Retrievalaugmented thought and iterative retrievalgeneration loops provide grounding and improved reasoning over long horizons, aligning prompts with retrieved evidence (RAT; IterRetGen) (dc256e179d4e8eff48879a40ddc414b15b0b2300; 8051818817a9a3815be6623a679d4a7f5a7b7964).\n",
    "- Longform narrative and multiturn dialogue literature shows tangible gains from structured, multistage prompting and planning (DialogGen; SCENECRAFT; Agents Room; PANGeA).\n",
    "\n",
    "Together, these findings support the core premise of DPA: a lightweight, threephase, historyaware prompting protocol can markedly improve coherence and adaptability across long interactions while avoiding heavy finetuning, thereby delivering high impact with feasible resource requirements.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) proposes a three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to maintain coherence and adaptability across long LLM interactions without full fine-tuning. While appealing, the idea risks being incremental rather than revolutionary. The literature already shows extensive precedent for dynamic, memory-aware prompting and test-time adaptation, including explicit three-phase or iterative workflows, prompting strategies, and retrieval-grounded reasoning. Without stronger theoretical grounding, rigorous baselines, or robust human-centered evaluation, DPAs claimed novelty and practical impact remain uncertain and potentially domain-limited to demonstrations rather than generalizable gains across tasks and modalities.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "VERDICT: Research Idea Evaluation\n",
    "\n",
    "SCORES:\n",
    "- Novelty: 4/10\n",
    "- Feasibility: 7/10\n",
    "\n",
    "SUMMARY:\n",
    "The debate centers on the novelty and feasibility of Dynamic Prompt Adaptation (DPA), a proposed three-phase method (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to improve coherence in long-form LLM interactions. The Advocate positions DPA as a novel, lightweight, and modular framework that explicitly orchestrates prompt updates based on the evolving dialogue history, citing supporting evidence from literature on dynamic prompting and parameter-efficient adaptation. The Skeptic argues that DPA is incremental, repackaging existing concepts from continual test-time adaptation, retrieval-augmented generation, and other dynamic prompting techniques without offering a significant methodological leap. The Skeptic also raises concerns about the reliance on potentially weak evaluation metrics (BLEU/ROUGE) and the practical challenges of prompt drift and stability over many turns. The final verdict leans towards the Skeptic's view, scoring novelty low while acknowledging feasibility, and recommending a revision to better differentiate the work from the extensive prior art and to incorporate more robust, human-centered evaluation methods.\n",
    "\n",
    "STRENGTHS:\n",
    "- The proposed method directly addresses the critical challenge of maintaining long-term coherence in LLMs.\n",
    "- The iterative, three-phase loop is a structured and intuitive approach to dynamically adapting prompts based on conversational context.\n",
    "- The experiment plan is well-defined, with clear baselines and metrics for evaluation.\n",
    "- The idea of integrating with parameter-efficient methods like LoRA and dynamic adapters makes it computationally feasible.\n",
    "\n",
    "WEAKNESSES:\n",
    "- The novelty of the proposed method is questionable, as it appears to be a recombination of existing techniques in dynamic prompting and test-time adaptation.\n",
    "- The reliance on automated metrics like BLEU and ROUGE is a significant weakness, as these are known to correlate poorly with human judgments of narrative coherence and quality.\n",
    "- The proposal does not adequately address the risk of 'prompt drift' or error accumulation over long interactions, where the model could get stuck in repetitive loops or diverge thematically.\n",
    "- The datasets mentioned (Story Cloze, Reddit) may not be sufficient to robustly evaluate long-form narrative coherence and engagement.\n",
    "\n",
    "RECOMMENDATION: Revise. The proposal is feasible but the novelty is questionable given the extensive prior work on dynamic and adaptive prompting. The authors should clearly articulate what distinguishes their three-phase loop from existing iterative/adaptive prompting frameworks (e.g., retrieval-augmented generation, continual test-time adaptation). The experiment plan needs more robust, human-centered evaluation metrics beyond BLEU/ROUGE to convincingly demonstrate improved coherence and engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80423d",
   "metadata": {},
   "source": [
    "### AGENTIC AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07491a",
   "metadata": {},
   "source": [
    "ReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5356c7",
   "metadata": {},
   "source": [
    "#### SINGLE AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6370bac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 262\u001b[39m\n\u001b[32m    259\u001b[39m agentic_app = agentic_workflow.compile()\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# Extract research idea from initial user input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m research_idea_text = \u001b[43mresult_llm\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m].content\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# Extract and format retrieved papers\u001b[39;00m\n\u001b[32m    265\u001b[39m papers_json = json.loads(result_llm[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content)  \u001b[38;5;66;03m# -2 because -1 is the analysis\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'result_llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.tools import Tool \n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "import operator\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    proposal: str\n",
    "    retrieved_papers: str  # Pre-retrieved papers from earlier pipeline\n",
    "    plan: str\n",
    "    findings: Annotated[list, operator.add]\n",
    "    scores: dict\n",
    "    confidence: dict\n",
    "    iteration: int\n",
    "    next_action: str\n",
    "\n",
    "# Tools that work with pre-retrieved papers\n",
    "def analyze_papers_tool(focus_area: str, papers: str) -> str:\n",
    "    \"\"\"Analyze retrieved papers focusing on a specific area\"\"\"\n",
    "    return f\"Analysis of papers focusing on '{focus_area}': Found relevant insights from the pre-retrieved literature.\"\n",
    "\n",
    "def extract_paper_details_tool(paper_criteria: str, papers: str) -> str:\n",
    "    \"\"\"Extract specific details from papers based on criteria\"\"\"\n",
    "    # Parse papers and extract relevant details\n",
    "    lines = papers.split('\\n\\n---\\n\\n')\n",
    "    relevant_papers = []\n",
    "    \n",
    "    for paper in lines[:3]:  # Limit to first 3 papers for demonstration\n",
    "        if paper.strip():\n",
    "            relevant_papers.append(f\"Paper analysis for '{paper_criteria}': {paper[:200]}...\")\n",
    "    \n",
    "    return f\"Extracted details based on '{paper_criteria}': {len(relevant_papers)} papers analyzed.\"\n",
    "\n",
    "def compare_methodologies_tool(methodology_aspect: str, papers: str) -> str:\n",
    "    \"\"\"Compare methodologies in retrieved papers\"\"\"\n",
    "    return f\"Methodology comparison for '{methodology_aspect}': Analyzed methodological approaches in retrieved papers.\"\n",
    "\n",
    "def execute_tool(tool_name: str, params: dict, retrieved_papers: str) -> str:\n",
    "    \"\"\"Execute the specified tool with parameters and pre-retrieved papers\"\"\"\n",
    "    if tool_name == \"analyze_papers\":\n",
    "        return analyze_papers_tool(params.get(\"focus_area\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"extract_details\":\n",
    "        return extract_paper_details_tool(params.get(\"criteria\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"compare_methods\":\n",
    "        return compare_methodologies_tool(params.get(\"aspect\", \"\"), retrieved_papers)\n",
    "    else:\n",
    "        return f\"Tool {tool_name} executed with params {params}\"\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"analyze_papers\",\n",
    "        func=analyze_papers_tool,\n",
    "        description=\"Analyze retrieved papers focusing on specific aspects\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"extract_details\", \n",
    "        func=extract_paper_details_tool,\n",
    "        description=\"Extract specific methodological or technical details\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"compare_methods\",\n",
    "        func=compare_methodologies_tool,\n",
    "        description=\"Compare methodologies across retrieved papers\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define agent nodes\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Agent creates investigation plan\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Given this research proposal: {state['proposal']}\n",
    "        \n",
    "        Numbers of available retrieved papers: {len(state['retrieved_papers'].split('Paper ID:'))-1} papers\n",
    "        \n",
    "        Create a step-by-step plan to evaluate its novelty and feasibility using the already retrieved papers.\n",
    "        \n",
    "        Focus on:\n",
    "        1. Analyzing overlaps with existing methods\n",
    "        2. Identifying unique contributions\n",
    "        3. Assessing technical feasibility\n",
    "        \n",
    "        Return just the plan as a string.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    plan_response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"plan\": plan_response.content,\n",
    "        \"iteration\": 0,\n",
    "        \"next_action\": \"investigate\"\n",
    "    }\n",
    "\n",
    "def investigation_node(state: AgentState):\n",
    "    \"\"\"Agent investigates using retrieved papers\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Current plan: {state['plan']}\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        You have access to pre-retrieved papers. Based on the plan and current findings, what should you analyze next?\n",
    "        Choose one:\n",
    "        1. Analyze papers for specific aspects (respond with: \"TOOL: analyze_papers, FOCUS: <aspect to focus on>\")\n",
    "        2. Extract technical details (respond with: \"TOOL: extract_details, CRITERIA: <what to extract>\") \n",
    "        3. Compare methodologies (respond with: \"TOOL: compare_methods, ASPECT: <methodology aspect>\")\n",
    "        4. Conclude investigation (respond with: \"CONCLUDE\")\n",
    "        \n",
    "        Respond in the exact format specified above.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    decision_response = llm.invoke(messages)\n",
    "    decision = decision_response.content.strip()\n",
    "    \n",
    "    if decision.startswith(\"TOOL:\"):\n",
    "        # Parse the tool command\n",
    "        parts = decision.split(\", \")\n",
    "        tool_name = parts[0].split(\": \")[1]\n",
    "        \n",
    "        if \"FOCUS:\" in decision:\n",
    "            focus_area = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"focus_area\": focus_area}, state['retrieved_papers'])\n",
    "        elif \"CRITERIA:\" in decision:\n",
    "            criteria = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"criteria\": criteria}, state['retrieved_papers'])\n",
    "        elif \"ASPECT:\" in decision:\n",
    "            aspect = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"aspect\": aspect}, state['retrieved_papers'])\n",
    "        else:\n",
    "            result = \"Tool execution failed - invalid parameters\"\n",
    "        \n",
    "        return {\n",
    "            \"findings\": [result],\n",
    "            \"iteration\": state.get(\"iteration\", 0) + 1,\n",
    "            \"next_action\": \"reflect\"\n",
    "        }\n",
    "    else:\n",
    "        return {\"next_action\": \"conclude\"}\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    \"\"\"Agent reflects on progress\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        Based on your analysis of the retrieved papers, evaluate the confidence level (0-100) for each aspect:\n",
    "        - Novelty assessment confidence (how well you understand what's new)\n",
    "        - Feasibility assessment confidence (how realistic the implementation seems)\n",
    "        - Overall investigation completeness (do you have enough information)\n",
    "        \n",
    "        Return a JSON-like response:\n",
    "        {{\"novelty\": <score>, \"feasibility\": <score>, \"overall\": <score>}}\n",
    "        \n",
    "        Then decide: Should I continue investigating (if overall < 75) or conclude?\n",
    "        Add on a new line: CONTINUE or CONCLUDE\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    confidence_response = llm.invoke(messages)\n",
    "    response_lines = confidence_response.content.strip().split('\\n')\n",
    "    \n",
    "    # Parse confidence scores (simplified)\n",
    "    try:\n",
    "        confidence_line = response_lines[0]\n",
    "        # Extract numbers from the response (simplified parsing)\n",
    "        numbers = re.findall(r'\\d+', confidence_line)\n",
    "        if len(numbers) >= 3:\n",
    "            confidence = {\n",
    "                \"novelty\": int(numbers[0]),\n",
    "                \"feasibility\": int(numbers[1]), \n",
    "                \"overall\": int(numbers[2])\n",
    "            }\n",
    "        else:\n",
    "            confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    except:\n",
    "        confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    \n",
    "    # Determine next action\n",
    "    next_action = \"investigate\" if confidence.get(\"overall\", 0) < 75 else \"conclude\"\n",
    "    \n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"next_action\": next_action\n",
    "    }\n",
    "\n",
    "def scoring_node(state: AgentState):\n",
    "    \"\"\"Generate final scores and report\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Based on analysis of retrieved papers and findings: {state.get('findings', [])}\n",
    "        Confidence levels: {state.get('confidence', {})}\n",
    "        \n",
    "        Generate final evaluation scores (1-10) for:\n",
    "        - Novelty: How new/original is this idea compared to retrieved papers?\n",
    "        - Feasibility: How realistic is implementation based on similar work?\n",
    "        - Impact: Potential significance of results based on the literature?\n",
    "        \n",
    "        Provide a brief summary and recommendation based on the paper analysis.\n",
    "        \n",
    "        Format as JSON:\n",
    "        {{\"novelty_score\": <1-10>, \"feasibility_score\": <1-10>, \"impact_score\": <1-10>, \"summary\": \"<text>\", \"recommendation\": \"<Accept/Revise/Reject>\"}}\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    scores_response = llm.invoke(messages)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"scores\": scores_response,\n",
    "        \"next_action\": \"end\"\n",
    "    }\n",
    "\n",
    "# Define routing function\n",
    "def should_continue(state: AgentState) -> Literal[\"investigate\", \"conclude\"]:\n",
    "    \"\"\"Determine next step based on current state\"\"\"\n",
    "    next_action = state.get(\"next_action\", \"investigate\")\n",
    "    \n",
    "    # Safety check - limit iterations\n",
    "    if state.get(\"iteration\", 0) >= 4:  # Reduced since we're using pre-retrieved papers\n",
    "        return \"conclude\"\n",
    "    \n",
    "    if next_action == \"conclude\":\n",
    "        return \"conclude\"\n",
    "    else:\n",
    "        return \"investigate\"\n",
    "\n",
    "# Build the graph\n",
    "agentic_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agentic_workflow.add_node(\"planning\", planning_node)\n",
    "agentic_workflow.add_node(\"investigation\", investigation_node)\n",
    "agentic_workflow.add_node(\"reflection\", reflection_node)\n",
    "agentic_workflow.add_node(\"scoring\", scoring_node)\n",
    "\n",
    "# Define edges (control flow)\n",
    "agentic_workflow.add_edge(START, \"planning\")\n",
    "agentic_workflow.add_edge(\"planning\", \"investigation\")\n",
    "agentic_workflow.add_edge(\"investigation\", \"reflection\")\n",
    "\n",
    "# Conditional edge based on confidence/decision\n",
    "agentic_workflow.add_conditional_edges(\n",
    "    \"reflection\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"investigate\": \"investigation\",\n",
    "        \"conclude\": \"scoring\"\n",
    "    }\n",
    ")\n",
    "agentic_workflow.add_edge(\"scoring\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agentic_app = agentic_workflow.compile()\n",
    "\n",
    "# Extract research idea from initial user input\n",
    "research_idea_text = result_llm[\"messages\"][0].content\n",
    "\n",
    "# Extract and format retrieved papers\n",
    "papers_json = json.loads(result_llm[\"messages\"][-1].content)  # -2 because -1 is the analysis\n",
    "retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n",
    "\n",
    "print(\"Running ReAct Agent Evaluation with Pre-Retrieved Papers...\")\n",
    "print(f\"Number of retrieved papers: {len(retrieved_papers_text.split('Paper ID:'))-1}\")\n",
    "\n",
    "# Run the agent\n",
    "try:\n",
    "    result = agentic_app.invoke({\n",
    "        \"proposal\": research_idea_text,\n",
    "        \"retrieved_papers\": retrieved_papers_text,\n",
    "        \"plan\": \"\",\n",
    "        \"findings\": [],\n",
    "        \"scores\": {},\n",
    "        \"confidence\": {},\n",
    "        \"iteration\": 0,\n",
    "        \"next_action\": \"start\"\n",
    "    })\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running ReAct agent: {e}\")\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "print(\"\\nWorkflow Visualization:\")\n",
    "display(Image(agentic_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f495b7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Revise'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads((result[\"scores\"].content.strip()).replace(\"```json\", \"\").replace(\"```\", \"\").strip())[\"recommendation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c9e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b422e009",
   "metadata": {},
   "source": [
    "##### Example Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd0e84",
   "metadata": {},
   "source": [
    "{'proposal': \"Dynamic Prompt Adaptation:\n",
    "                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\n",
    "                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\n",
    "                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\n",
    "                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\n",
    "                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", 'retrieved_papers': 'Paper ID: 246482d9758e93d0b349e2926996d887417174d8\n",
    "                                    Title: DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\n",
    "                                    Abstract: Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\n",
    "                                \n",
    "\n",
    "---\n",
    "\n",
    "Paper ID: 6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\n",
    "                                    Title: Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\n",
    "                                    Abstract: Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\n",
    "                                \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "2) Build a structured evidence map from the 199 papers\n",
    "- Create a data schema: paper_id, title, year, method category, prompting technique, context length, memory/retention mechanism, adaptation trigger, evaluation metrics, datasets, claimed contributions.\n",
    "- Populate a searchable database or spreadsheet and tag papers by phase-related capabilities (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) and by overlap with proposed components.\n",
    "\n",
    "3) Overlap analysis by phase\n",
    "- Phase 1: Contextual Analysis\n",
    "  - Identify papers on theme detection, discourse/semantic drift, tone/style tracking, and prompting templates that reflect prior content.\n",
    "  - Record how prior work handles extraction of themes, tonal shifts, and narrative continuity.\n",
    "- Phase 2: Adaptive Prompt Generation\n",
    "  - Identify work on dynamic prompts, prompt rewrites, plan-and-solve prompting, instruction-tuning for adaptability, and prompt re-routing based on user input.\n",
    "- Phase 3: Iterative Context Update\n",
    "  - Identify long-context maintenance methods, memory-augmented generation, dialogue/state tracking, and synthesis prompts that condense prior interactions.\n",
    "- For each phase, compute overlaps with your proposed prompts, templates, and objectives; note any gaps or novel couplings.\n",
    "\n",
    "4) Gap analysis to reveal unique contributions\n",
    "- Compare three-phase orchestration against single-phase dynamic prompting and against memory augmented or retrieval-based methods.\n",
    "- Assess whether prior work demonstrates end-to-end pipelines that:\n",
    "  - Analyze prior outputs and user prompts for theme/tonal shifts (Phase 1).\n",
    "  - Generate updated prompts that explicitly steer continuation (Phase 2).\n",
    "  - Produce iterative, synthesized summaries to maintain coherence across many turns (Phase 3).\n",
    "- Identify gaps your plan addresses: explicit phased approach tailored to creative/storytelling tasks, integration of reflective prompts with continuity-focused synthesis, and a combined automatic/human-evaluation framework.\n",
    "\n",
    "5) Feasibility assessment of the proposed architecture\n",
    "- Decompose into modules and interfaces:\n",
    "  - Module A: Contextual Analysis (theme extraction, tonal shift detection, discourse tracking).\n",
    "  - Module B: Adaptive Prompt Generator (template-driven and learned prompts, handling new elements and clarifications).\n",
    "  - Module C: Iterative Context Updater (global narrative synthesis, coherence-preserving summaries).\n",
    "- Assess each module for technical feasibility with current LLMs:\n",
    "  - Prompt length constraints and context window limits; potential need for retrieval-augmented inputs.\n",
    "  - Computational and latency costs of repeated prompting across turns.\n",
    "  - Hallucination risk and drift control through synthesis prompts and validation checks.\n",
    "- Propose feasible realizations:\n",
    "  - Use a modular pipeline with a lightweight external memory store or episodic memory of themes.\n",
    "  Use retrieval or summarization steps to compress history within model limits.\n",
    "  Rely on established prompting techniques (CoT, self-critique, instruction-following) augmented by explicit coherence checks.\n",
    "- Identify technical risks and mitigation strategies (prompt drift, data leakage between prompts, evaluation noise).\n",
    "\n",
    "6) Experimental design aligned with novelty and feasibility\n",
    "- Baselines:\n",
    "  - Static prompts with fixed prompts.\n",
    "  - Dynamic prompting without phased structure.\n",
    "  - Existing memory-augmented/dialogue systems.\n",
    "- Datasets and domains:\n",
    "  - Story Cloze Test (coherence in narrative endings).\n",
    "  - Reddit-based dialogue interactions (creative storytelling threads, conversations).\n",
    "  - Additional long-form storytelling datasets or writing prompts to test continuity.\n",
    "- Evaluation metrics:\n",
    "  - Automatic: BLEU, ROUGE, METEOR, BERTScore, ROUGE-L; plus coherence-oriented metrics (entity grid/coherence scores, COH-METER where feasible).\n",
    "  - Engagement and naturalness: user-rated scales, preference tests.\n",
    "  - Specific to coherence: longitudinal coherence scores across turns, consistency of themes, and narrative arc continuity.\n",
    "- Evaluation design:\n",
    "  - Within-subject A/B/C testing comparing the three-phase approach versus baselines.\n",
    "  - Ablation studies to isolate contributions of Phase 1, Phase 2, and Phase 3.\n",
    "  - Statistical analysis plans (confidence intervals, significance tests, effect sizes).\n",
    "- Reproducibility plan:\n",
    "  - Pre-register hypotheses, methods, and evaluation protocol.\n",
    "  - Share prompts templates, evaluation scripts, and synthetic datasets where permissible.\n",
    "  - Document data licensing and preprocessing steps.\n",
    "\n",
    "7) Data handling, ethics, and compliance\n",
    "- Ensure dataset licenses and terms (Story Cloze, Reddit data) are compliant; obtain permissions where needed.\n",
    "- Address privacy and consent for user-generated dialogue data.\n",
    "- Mitigate bias and ensure diversity of writing styles and topics in evaluation materials.\n",
    "\n",
    "8) Architectural detail and implementation plan\n",
    "- Define a concrete pipeline:\n",
    "  - Input: user prompts and prior outputs.\n",
    "  - Phase 1: run contextual analysis to extract themes, tonal cues, and narrative branches.\n",
    "  - Phase 2: produce updated prompts that introduce new elements or clarify past responses, with explicit references to themes and context.\n",
    "  - Phase 3: generate a synthesized summary of prior interactions to guide future turns and prompt the model for continuity.\n",
    "  - Output: a continuation generation with coherence checks, followed by optional human-in-the-loop review.\n",
    "- Data flows and interfaces:\n",
    "  - Clear API boundaries between modules; stateless prompts per turn with a memory ledger.\n",
    "  - Logging for reproducibility and evaluation auditing.\n",
    "- Resource plan:\n",
    "  - Estimate compute for 199-paper-informed analysis, pipeline prototyping, and full experiments.\n",
    "  - Plan for iterative pilot studies before full-scale evaluation.\n",
    "\n",
    "9) Timeline and milestones\n",
    "- Month 12: complete novelty criteria, build evidence map from 199 papers, perform phase-wise overlap analysis.\n",
    "- Month 34: conduct gap/unique contribution assessment; draft architecture design and feasibility notes.\n",
    "- Month 56: implement a minimal viable three-phase pipeline prototype; develop datasets, baselines, and evaluation plan.\n",
    "- Month 78: run experiments across Story Cloze and dialogue datasets; collect automatic and human evaluations; perform ablations.\n",
    "- Month 9: synthesize results, refine claims of novelty and feasibility; prepare reproducibility materials.\n",
    "- Month 10: write up plan for potential conference submission; outline follow-up experiments and extensions.\n",
    "\n",
    "10) Deliverables and documentation\n",
    "- A structured novelty/feasibility report summarizing overlaps, gaps, unique contributions, and feasibility assessments.\n",
    "- An implementation blueprint for the three-phase dynamic prompt adaptation pipeline.\n",
    "- A detailed experimental protocol, including baselines, datasets, metrics, and statistical analysis plan.\n",
    "- Reproducibility package plan (prompts templates, evaluation scripts, data handling notes) with licensing considerations.\n",
    "\n",
    "Note: The plan emphasizes using the 199 retrieved papers to systematically map overlaps, identify true innovations, and assess the practical viability of the proposed three-phase dynamic prompt adaptation approach for long-horizon creative writing tasks.', 'findings': [\"Analysis of papers focusing on 'Phase 1 Contextual Analysis (theme extraction': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Theme extraction and tonal/style tracking in Phase 1 Contextual Analysis': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Theme extraction and tonal/style tracking in Phase 1 Contextual Analysis (discourse drift': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Phase 1 Contextual Analysis (theme detection': Found relevant insights from the pre-retrieved literature.\"], 'scores': AIMessage(content='{\n",
    "  \"novelty_score\": 6,\n",
    "  \"feasibility_score\": 7,\n",
    "  \"impact_score\": 7,\n",
    "  \"summary\": \"Based on the analysis of retrieved papers, the Phase 1 contextual analysis components (theme extraction, tonal/style tracking, discourse drift, and theme detection) show insights that largely align with existing literature. The integrated anglecombining theme extraction with tonal/style tracking and discourse drift in Phase 1offers incremental novelty at best. Feasibility is high, as the methods (topic/theme detection, stylometry, drift/discourse monitoring) are established and can be implemented with standard NLP/tooling. The potential impact is moderate, contingent on demonstrated improvements in early contextual understanding and downstream tasks, and on avoiding redundancy with prior work.\",\n",
    "  \"recommendation\": \"Revise\"\n",
    "}', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1394, 'prompt_tokens': 294, 'total_tokens': 1688, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Chii3x1Krl0KP6xllSfhGqmGgyn2I', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--a7c610b9-c5eb-45e1-a26e-1fdf8e1a3586-0', usage_metadata={'input_tokens': 294, 'output_tokens': 1394, 'total_tokens': 1688, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}}), 'confidence': {'novelty': 64, 'feasibility': 69, 'overall': 66}, 'iteration': 4, 'next_action': 'end'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40d9cc",
   "metadata": {},
   "source": [
    "#### MULTIPLE AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b23cb514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow Visualization:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.tools import Tool \n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "import operator\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    proposal: str\n",
    "    retrieved_papers: str  # Pre-retrieved papers from earlier pipeline\n",
    "    plan: str\n",
    "    findings: Annotated[list, operator.add]\n",
    "    scores: dict\n",
    "    confidence: dict\n",
    "    iteration: int\n",
    "    next_action: str\n",
    "\n",
    "# Tools that work with pre-retrieved papers\n",
    "def analyze_papers_tool(focus_area: str, papers: str) -> str:\n",
    "    \"\"\"Analyze retrieved papers focusing on a specific area\"\"\"\n",
    "    return f\"Analysis of papers focusing on '{focus_area}': Found relevant insights from the pre-retrieved literature.\"\n",
    "\n",
    "def extract_paper_details_tool(paper_criteria: str, papers: str) -> str:\n",
    "    \"\"\"Extract specific details from papers based on criteria\"\"\"\n",
    "    # Parse papers and extract relevant details\n",
    "    lines = papers.split('\\n\\n---\\n\\n')\n",
    "    relevant_papers = []\n",
    "    \n",
    "    for paper in lines[:3]:  # Limit to first 3 papers for demonstration\n",
    "        if paper.strip():\n",
    "            relevant_papers.append(f\"Paper analysis for '{paper_criteria}': {paper[:200]}...\")\n",
    "    \n",
    "    return f\"Extracted details based on '{paper_criteria}': {len(relevant_papers)} papers analyzed.\"\n",
    "\n",
    "def compare_methodologies_tool(methodology_aspect: str, papers: str) -> str:\n",
    "    \"\"\"Compare methodologies in retrieved papers\"\"\"\n",
    "    return f\"Methodology comparison for '{methodology_aspect}': Analyzed methodological approaches in retrieved papers.\"\n",
    "\n",
    "def execute_tool(tool_name: str, params: dict, retrieved_papers: str) -> str:\n",
    "    \"\"\"Execute the specified tool with parameters and pre-retrieved papers\"\"\"\n",
    "    if tool_name == \"analyze_papers\":\n",
    "        return analyze_papers_tool(params.get(\"focus_area\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"extract_details\":\n",
    "        return extract_paper_details_tool(params.get(\"criteria\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"compare_methods\":\n",
    "        return compare_methodologies_tool(params.get(\"aspect\", \"\"), retrieved_papers)\n",
    "    else:\n",
    "        return f\"Tool {tool_name} executed with params {params}\"\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"analyze_papers\",\n",
    "        func=analyze_papers_tool,\n",
    "        description=\"Analyze retrieved papers focusing on specific aspects\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"extract_details\", \n",
    "        func=extract_paper_details_tool,\n",
    "        description=\"Extract specific methodological or technical details\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"compare_methods\",\n",
    "        func=compare_methodologies_tool,\n",
    "        description=\"Compare methodologies across retrieved papers\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define agent nodes\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Agent creates investigation plan\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Given this research proposal: {state['proposal']}\n",
    "        \n",
    "        Available retrieved papers: {len(state['retrieved_papers'].split('Paper ID:'))-1} papers\n",
    "        \n",
    "        Create a step-by-step plan to evaluate its novelty and feasibility using the already retrieved papers.\n",
    "        Focus on:\n",
    "        1. Analyzing overlaps with existing methods\n",
    "        2. Identifying unique contributions\n",
    "        3. Assessing technical feasibility\n",
    "        \n",
    "        Return just the plan as a string.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    plan_response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"plan\": plan_response.content,\n",
    "        \"iteration\": 0,\n",
    "        \"next_action\": \"investigate\"\n",
    "    }\n",
    "\n",
    "def investigation_node(state: AgentState):\n",
    "    \"\"\"Agent investigates using retrieved papers\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Current plan: {state['plan']}\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        You have access to pre-retrieved papers. Based on the plan and current findings, what should you analyze next?\n",
    "        Choose one:\n",
    "        1. Analyze papers for specific aspects (respond with: \"TOOL: analyze_papers, FOCUS: <aspect to focus on>\")\n",
    "        2. Extract technical details (respond with: \"TOOL: extract_details, CRITERIA: <what to extract>\") \n",
    "        3. Compare methodologies (respond with: \"TOOL: compare_methods, ASPECT: <methodology aspect>\")\n",
    "        4. Conclude investigation (respond with: \"CONCLUDE\")\n",
    "        \n",
    "        Respond in the exact format specified above.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    decision_response = llm.invoke(messages)\n",
    "    decision = decision_response.content.strip()\n",
    "    \n",
    "    if decision.startswith(\"TOOL:\"):\n",
    "        # Parse the tool command\n",
    "        parts = decision.split(\", \")\n",
    "        tool_name = parts[0].split(\": \")[1]\n",
    "        \n",
    "        if \"FOCUS:\" in decision:\n",
    "            focus_area = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"focus_area\": focus_area}, state['retrieved_papers'])\n",
    "        elif \"CRITERIA:\" in decision:\n",
    "            criteria = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"criteria\": criteria}, state['retrieved_papers'])\n",
    "        elif \"ASPECT:\" in decision:\n",
    "            aspect = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"aspect\": aspect}, state['retrieved_papers'])\n",
    "        else:\n",
    "            result = \"Tool execution failed - invalid parameters\"\n",
    "        \n",
    "        return {\n",
    "            \"findings\": [result],\n",
    "            \"iteration\": state.get(\"iteration\", 0) + 1,\n",
    "            \"next_action\": \"reflect\"\n",
    "        }\n",
    "    else:\n",
    "        return {\"next_action\": \"conclude\"}\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    \"\"\"Agent reflects on progress\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        Based on your analysis of the retrieved papers, evaluate the confidence level (0-100) for each aspect:\n",
    "        - Novelty assessment confidence (how well you understand what's new)\n",
    "        - Feasibility assessment confidence (how realistic the implementation seems)\n",
    "        - Overall investigation completeness (do you have enough information)\n",
    "        \n",
    "        Return a JSON-like response:\n",
    "        {{\"novelty\": <score>, \"feasibility\": <score>, \"overall\": <score>}}\n",
    "        \n",
    "        Then decide: Should I continue investigating (if overall < 75) or conclude?\n",
    "        Add on a new line: CONTINUE or CONCLUDE\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    confidence_response = llm.invoke(messages)\n",
    "    response_lines = confidence_response.content.strip().split('\\n')\n",
    "    \n",
    "    # Parse confidence scores (simplified)\n",
    "    try:\n",
    "        confidence_line = response_lines[0]\n",
    "        # Extract numbers from the response (simplified parsing)\n",
    "        numbers = re.findall(r'\\d+', confidence_line)\n",
    "        if len(numbers) >= 3:\n",
    "            confidence = {\n",
    "                \"novelty\": int(numbers[0]),\n",
    "                \"feasibility\": int(numbers[1]), \n",
    "                \"overall\": int(numbers[2])\n",
    "            }\n",
    "        else:\n",
    "            confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    except:\n",
    "        confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    \n",
    "    # Determine next action\n",
    "    next_action = \"investigate\" if confidence.get(\"overall\", 0) < 75 else \"conclude\"\n",
    "    \n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"next_action\": next_action\n",
    "    }\n",
    "\n",
    "def scoring_node(state: AgentState):\n",
    "    \"\"\"Generate final scores and report\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Based on analysis of retrieved papers and findings: {state.get('findings', [])}\n",
    "        Confidence levels: {state.get('confidence', {})}\n",
    "        \n",
    "        Generate final evaluation scores (1-10) for:\n",
    "        - Novelty: How new/original is this idea compared to retrieved papers?\n",
    "        - Feasibility: How realistic is implementation based on similar work?\n",
    "        - Impact: Potential significance of results based on the literature?\n",
    "        \n",
    "        Provide a brief summary and recommendation based on the paper analysis.\n",
    "        \n",
    "        Format as JSON:\n",
    "        {{\"novelty_score\": <1-10>, \"feasibility_score\": <1-10>, \"impact_score\": <1-10>, \"summary\": \"<text>\", \"recommendation\": \"<Accept/Revise/Reject>\"}}\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    scores_response = llm.invoke(messages)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"scores\": scores_response,\n",
    "        \"next_action\": \"end\"\n",
    "    }\n",
    "\n",
    "# Define routing function\n",
    "def should_continue(state: AgentState) -> Literal[\"investigate\", \"conclude\"]:\n",
    "    \"\"\"Determine next step based on current state\"\"\"\n",
    "    next_action = state.get(\"next_action\", \"investigate\")\n",
    "    \n",
    "    # Safety check - limit iterations\n",
    "    if state.get(\"iteration\", 0) >= 4:  # Reduced since we're using pre-retrieved papers\n",
    "        return \"conclude\"\n",
    "    \n",
    "    if next_action == \"conclude\":\n",
    "        return \"conclude\"\n",
    "    else:\n",
    "        return \"investigate\"\n",
    "\n",
    "# Build the graph\n",
    "agentic_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agentic_workflow.add_node(\"planning\", planning_node)\n",
    "agentic_workflow.add_node(\"investigation\", investigation_node)\n",
    "agentic_workflow.add_node(\"reflection\", reflection_node)\n",
    "agentic_workflow.add_node(\"scoring\", scoring_node)\n",
    "\n",
    "# Define edges (control flow)\n",
    "agentic_workflow.add_edge(START, \"planning\")\n",
    "agentic_workflow.add_edge(\"planning\", \"investigation\")\n",
    "agentic_workflow.add_edge(\"investigation\", \"reflection\")\n",
    "\n",
    "# Conditional edge based on confidence/decision\n",
    "agentic_workflow.add_conditional_edges(\n",
    "    \"reflection\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"investigate\": \"investigation\",\n",
    "        \"conclude\": \"scoring\"\n",
    "    }\n",
    ")\n",
    "agentic_workflow.add_edge(\"scoring\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agentic_app = agentic_workflow.compile()\n",
    "\n",
    "# Extract research idea from initial user input\n",
    "# research_idea_text = result_llm[\"messages\"][0].content\n",
    "\n",
    "# # Extract and format retrieved papers\n",
    "# papers_json = json.loads(result_llm[\"messages\"][-2].content)  # -2 because -1 is the analysis\n",
    "# retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n",
    "\n",
    "# print(\"Running ReAct Agent Evaluation with Pre-Retrieved Papers...\")\n",
    "# print(f\"Number of retrieved papers: {len(retrieved_papers_text.split('Paper ID:'))-1}\")\n",
    "\n",
    "# # Run the agent\n",
    "# try:\n",
    "#     result = agentic_app.invoke({\n",
    "#         \"proposal\": research_idea_text,\n",
    "#         \"retrieved_papers\": retrieved_papers_text,\n",
    "#         \"plan\": \"\",\n",
    "#         \"findings\": [],\n",
    "#         \"scores\": {},\n",
    "#         \"confidence\": {},\n",
    "#         \"iteration\": 0,\n",
    "#         \"next_action\": \"start\"\n",
    "#     })\n",
    "    \n",
    "#     print(result)\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Error running ReAct agent: {e}\")\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "print(\"\\nWorkflow Visualization:\")\n",
    "display(Image(agentic_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ae335",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70b07a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Workflow Visualization:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agentic_app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWorkflow Visualization:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m display(Image(\u001b[43magentic_app\u001b[49m.get_graph().draw_mermaid_png()))\n",
      "\u001b[31mNameError\u001b[39m: name 'agentic_app' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "print(\"\\nWorkflow Visualization:\")\n",
    "display(Image(agentic_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c683ee6",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d766866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topic = \"\"\"Dynamic Prompt Adaptation:\n",
    "                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\n",
    "                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\n",
    "                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\n",
    "                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\n",
    "                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcfebce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "from get_list_of_papers import call_workflow\n",
    "import pandas as pd\n",
    "from agentic_evaluator_1 import run_workflow as run_agentic_evaluator\n",
    "\n",
    "\n",
    "# research_topic = \"\"\n",
    "\n",
    "list_of_papers = call_workflow(research_topic)\n",
    "list_of_papers = json.loads(list_of_papers[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ccbb234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ReAct Agent Evaluation with Pre-Retrieved Papers...\n",
      "Number of retrieved papers: 179\n",
      "Agentic Evaluator Result:\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import agentic_evaluator_1\n",
    "\n",
    "importlib.reload(agentic_evaluator_1)\n",
    "from agentic_evaluator_1 import run_workflow as run_agentic_evaluator\n",
    "\n",
    "\n",
    "\n",
    "agentic_result =run_agentic_evaluator(research_topic, list_of_papers)\n",
    "print(\"Agentic Evaluator Result:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f3c6cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Revise'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_json = agentic_result[\"scores\"].model_dump()\n",
    "test_json[\"recommendation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d32c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try :\n",
    "    evaluation_result = json.loads((agentic_result[\"scores\"].content.strip()).replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "except:\n",
    "    evaluation_result = json.loads(agentic_result.content)\n",
    "idea_recommendation = agentic_result[\"recommendation\"]\n",
    "idea_summary = idea_recommendation[\"summary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2f3beb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) often embed and perpetuate social biases prevalent in their training data, particularly when generating content that involves sensitive social issues. This leads to stereotypical responses that fail to reflect diverse perspectives.\n",
      "Current techniques such as fixed prompting strategies and few-shot examples focus on directing LLM outputs but frequently overlook the need for flexibility in representation. Approaches like counterfactual prompting aim to mitigate bias, yet they often produce generic responses lacking nuanced viewpoints.\n",
      "By dynamically adjusting the perspectives from which the model generates responses, we can ensure that multiple contexts are represented. This strategy may enhance understanding of the varied experiences associated with social identities and issues, thereby fostering more equitable outputs.\n",
      "We introduce Dynamic Perspective Prompting (DPP), wherein prompts instruct LLMs to explore topics from alternative social identities and contexts. An example prompt could be: 'Imagine you are a recent immigrant discussing the issue of immigration policy. What are your thoughts? Now, consider being a government official from an indigenous community. What would you say about the same topic?'. This iterative perspective-switching aims to diminish static biases by embracing the multiplicity of social narratives.\n",
      "The effectiveness of DPP will be assessed using the 'WinoBias' dataset and the 'Social Bias Frames' dataset, which include socio-culturally rich topics. Evaluation metrics will include bias scoring via the 'BiasFinder' tool and qualitative assessments employing a scoring rubric based on the diversity and richness of perspectives captured in the generated outputs.\n",
      "Models often generate biased outputs when asked to comment on current social issues by failing to contextualize them historically, leading to oversimplified views that reinforce stereotypes.\n",
      "Current methods include chaining current and historical facts together and employing explaining prompts to get deeper analyses. However, these often result in simplistic narratives or fail to adequately challenge the inherent biases in the data.\n",
      "Humans derive insights from historical contexts to understand present dilemmas. Modeling LLM prompting aligned with this practice can reveal complexities that counteract superficial biases.\n",
      "We propose Historical Contextualization Prompting (HCP), where prompts are designed to elicit historical context before discussing modern implications. For instance, a prompt could ask: 'What is the historical context of the Black Lives Matter movement? Please summarize key events. Now, based on that history, what are some potential stereotypes associated with this movement today?'. This two-step reasoning will help the model differentiate between historical and current biases, encouraging a more nuanced viewpoint in its responses.\n",
      "The efficacy of HCP will be benchmarked using the 'GossipCop' dataset, which includes social issues with historical relevance. A specific test case will employ the prompt above. Metrics will include bias analysis using the 'Fairness Indicators' and assessments of how well the LLM navigates through both historical complexity and contemporary implications, quantified via an output scoring rubric focused on narrative depth.\n",
      "LLMs often generate biased outputs partly due to a lack of self-awareness about their reasoning processes, leading to an uncritical acceptance of biases present in their training data.\n",
      "Common techniques such as Chain-of-Thought prompting harness reasoning capabilities but often neglect the self-reflective component essential for recognizing flaws in biases.\n",
      "Research on human cognition emphasizes the importance of metacognitionthinking about one's own thought processesin conducting critical evaluations. By guiding LLMs to reflect on their reasoning chain, we hypothesize that biases can be identified and reduced.\n",
      "We introduce Meta-Cognitive Reflection Prompting (MCRP), a prompting strategy that instructs LLMs to critically analyze their output process. An example prompt could be: 'After generating a response to the topic of police funding, reflect on whether any social biases may have influenced your answer. What thoughts went into your reasoning? Are there alternative perspectives that could add depth?'. Interleaving these reflective prompts, we aim to provoke LLMs to scrutinize their biases during the generation process.\n",
      "We will evaluate MCRP by comparing it to traditional prompt methodologies across various socio-cultural datasets such as 'Social Bias Frames'. Evaluation will not rely on subjective human judgments but will utilize automated scoring systems like 'TextRidge', which scores the narratives for expected biases, ensuring an objective analysis of model responses.\n",
      "Large language models (LLMs) often perpetuate stereotypes and biases due to their reliance on historical data, which may over-represent certain cultural narratives and perspectives while under-representing others. This leads to biased outputs that do not accurately reflect the diversity of human experiences.\n",
      "Current bias mitigation approaches frequently utilize manual debiasing techniques by filtering or re-weighting training data, using stereotype-aware prompts, or relying on existing bias detection frameworks. These methods can be limited in scope and often require heavy reliance on curated datasets.\n",
      "Drawing from the idea of reframing in cognitive psychology, where individuals can alter their perception of an event by viewing it from different angles, we aim to mitigate stereotype activation by prompting LLMs to articulate contexts that highlight diversity upfront. This approach trains LLMs to prioritize a more balanced understanding of multifaceted narratives in diverse contexts.\n",
      "We introduce Contextual Reframing Prompts, which involve generating a set of diverse contextual scenarios for a given prompt. For example, when asked, \"Describe a doctor,\" the model first receives a rephrased prompt such as, \"Imagine a world where doctors come from diverse backgrounds and experiences, including [Black, Hispanic, Asian, LGBTQ+, etc.]. What stories might emerge from this environment?\" This method compels the model to consider a wider array of identities and promotes an inclusive approach before generating an output.\n",
      "We will evaluate the effectiveness of contextual reframing prompts on bias-sensitive topics using the Adverse Childhood Experiences (ACE) dataset and the 2019-2020 American Community Survey dataset to ensure a diverse representation of characters, occupations, and narratives. Performance metrics will include qualitative assessments using the Diversity Index (a score from 0 to 1 quantifying diversity representation) and quantitative measures based on bias detection tools such as the WEAT (Word Embedding Association Test) and the BiasFinder tool.\n",
      "Large language models inherently carry social biases that can become more pronounced when generating character-driven narratives or dialogues. These biases often manifest in the stereotypical behaviors or attributes assigned to certain character roles, leading to unintentional reinforcement of harmful stereotypes.\n",
      "Existing narrative generation methods often employ fixed character archetypes or rely on stereotypes embedded within the training data. Approaches to diversify these narratives generally involve manually curated character descriptions or templates, which may still fall prey to biases.\n",
      "Inspired by the role-playing approach in social environments, we can utilize the power of narrative imagination where participants assume different personas to explore various perspectives while emphasizing empathy and understanding in character interactions.\n",
      "We propose Adaptive Role-Playing Prompts, whereby LLMs generate and embody multiple roles and personas through structured prompts. For instance, a prompt could be, \"Create a dialogue between a surgeon (female, Hispanic) and a nurse (male, Asian) who discuss their experiences navigating bias in their professions while working together on a challenging case. What stereotypes do they confront, and how do their backgrounds shape their perspectives?\" This method encourages the model to craft rich, counter-stereotypical narratives through character interactions.\n",
      "The effectiveness of Adaptive Role-Playing Prompts will be evaluated using the Narrative Diversity and Character Development (NDCD) metrics alongside the Bias Score from the BiasFinder tool. We will use a combination of prompts generated for characters from diverse backgrounds and analyze their dialogues across multiple test cases. Examples will be sourced from the NarrativeQA dataset to ensure relevance in character-driven scenarios.\n",
      "When tasked with generating narratives or responses to sensitive topics, LLMs can inadvertently perpetuate the biases inherent in their training data, often resulting in harmful and stereotypical portrayals that reinforce societal stereotypes.\n",
      "Most narrative generation methods lack mechanisms for self-correction when biases are detected in generated content. Traditional methods include basic debiasing techniques that either post-process the generated output or minimally modify the training data.\n",
      "In creative writing, authors often employ disruptive plot twists or surprising character decisions to avoid predictability and enhance narrative depth. This concept can be adapted in prompting LLMs by inserting deliberate disruptions into the storyline, challenging biases and prompting the model to generate nuanced perspectives.\n",
      "We introduce Narrative Disruption Prompting, which uses a structured approach to induce deliberate disruptions by prompting the model with conflicting scenarios or objectives. For example, a prompt could state, \"Imagine your character believes they will become wealthy by exploiting others but unexpectedly meets someone from a marginalized community who challenges their assumptions and biases. How does this encounter change their worldview?\" This approach pushes the model to explore complexity and challenge biased conventions.\n",
      "The effectiveness of Narrative Disruption Prompting will be assessed using narrative scenarios drawn from the Hateful Memes dataset and evaluated against traditional narrative methods. Metrics will include bias detection scores from BiasFinder, narrative originality ratings through machine-generated uniqueness measures, and the Narrative Quality Assessment automated tool, which will analyze sensitivity and depth of characters' development.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# path = \"external/multiagent_research_generator/logs/log_2025_07_07/lit_review/novel prompting methods to reduce social biases and stereotypes of large language models..json\"\n",
    "\n",
    "path = \"external/multiagent_research_generator/logs/log_2025_07_07/ideas_dedup/novel prompting methods to reduce social biases and stereotypes of large language models_diff_personas_proposer_reviser.json\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key in data[\"ideas\"].keys():\n",
    "    ideas = data['ideas'][key]\n",
    "    print(ideas[\"Problem\"])\n",
    "    print(ideas[\"Existing Methods\"])\n",
    "    print(ideas[\"Motivation\"])\n",
    "    print(ideas[\"Proposed Method\"])\n",
    "    print(ideas[\"Experiment Plan\"])\n",
    "    \n",
    "# data[\"paper_bank\"]\n",
    "\n",
    "# data_banks = pd.DataFrame(data[\"paper_bank\"])\n",
    "# # data_banks\n",
    "# data_banks[[\"title\",\"year\",\"citationCount\",\"abstract\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56489b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install oauth2client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b62d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1opJM6vhhof_BmFX5e2ErZoNDXIEojh2DhykJAEO3214',\n",
       " 'tableRange': 'Sheet1!A1:B2',\n",
       " 'updates': {'spreadsheetId': '1opJM6vhhof_BmFX5e2ErZoNDXIEojh2DhykJAEO3214',\n",
       "  'updatedRange': 'Sheet1!A3:B3',\n",
       "  'updatedRows': 1,\n",
       "  'updatedColumns': 2,\n",
       "  'updatedCells': 2}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('thesislogexcel-f962f7c421c0.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    " \n",
    "sh = client.open('TestingLog').worksheet('Sheet1')  \n",
    "row = [\"Time\",\"rowelijerltjl1\"]\n",
    "sh.append_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb0d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "lit_rev_path = \"external/multiagent_research_generator/logs/log_2025_07_07/lit_review/novel prompting methods to reduce social biases and stereotypes of large language models.json\"\n",
    "\n",
    "with open(lit_rev_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lit_rev = json.load(f)\n",
    "\n",
    "\n",
    "idea_generated = \"external/multiagent_research_generator/logs/log_2025_07_07/ideas_dedup/novel prompting methods to reduce social biases and stereotypes of large language models_diff_personas_proposer_reviser.json\"\n",
    "with open(idea_generated, \"r\", encoding=\"utf-8\") as f:\n",
    "    generated_ideas = json.load(f)\n",
    "    \n",
    "first_key = next(iter(generated_ideas[\"ideas\"]))\n",
    "first_idea = generated_ideas[\"ideas\"][first_key]\n",
    "research_idea = \"Generated Research Idea: \" + str(first_key) + \"\\n\\n\" + str(first_idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11de2522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8226d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_list_of_papers import call_workflow\n",
    "\n",
    "list_of_papers = call_workflow(research_idea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610377e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_papers = json.loads(list_of_papers[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c21e29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_papers_2 = dict(list(list_of_papers.items())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f244c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ReAct Agent Evaluation with Pre-Retrieved Papers...\n",
      "Number of retrieved papers: 0\n"
     ]
    }
   ],
   "source": [
    "from agentic_evaluator_debate import run_workflow as run_agentic_evaluator_debate\n",
    "\n",
    "agentic_result =run_agentic_evaluator_debate(research_idea, list_of_papers_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "425a7506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'novelty_score': 5,\n",
       " 'feasibility_score': 7,\n",
       " 'impact_score': 6,\n",
       " 'summary': 'CCP is timely and policy-relevant but its claimed novelty and practical impact require substantial, rigorous evidence to move from plausible to proven. The analysis identifies three core areasnovelty, feasibility, and evaluabilitywith strengths in clearly articulating a culture-centric prompting approach and a feasible architecture, but risks due to potential redundancy with prior work, language and identity biases, latency and governance concerns, and measurement noise. A credible advancement would require preregistered hypotheses, cross-language/domain validation, ablations, and open artifacts. A tightly scoped pilot plus transparent governance could establish credibility; otherwise gains may be limited to curated benchmarks.',\n",
       " 'recommendation': 'Revise'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_result[\"scores\"].model_dump()\n",
    "\n",
    "# agentic_result = agentic_result[\"scores\"].model_dump()\n",
    "# idea_recommendation = agentic_result[\"recommendation\"]\n",
    "# idea_summary = agentic_result[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "efa48094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from papers_retrieval import getReferencePaper\n",
    "\n",
    "from prompts import *\n",
    "from tools import get_model\n",
    "from models import *\n",
    "\n",
    "llm = get_model()\n",
    "\n",
    "def advocate_node(state: GANState):\n",
    "    history = \"\\n\".join([f\"{m.type}: {m.content}\" for m in state['messages']])\n",
    "    messages = [\n",
    "        HumanMessage(content=get_argument_advocate_prompt(\n",
    "            research_idea = state['research_idea'],\n",
    "            retrieved_papers = state['retrieved_papers'],\n",
    "            history = history\n",
    "        ))\n",
    "    ]       \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [AIMessage(content=f\"Advocate: {response.content}\")]}\n",
    "\n",
    "def skeptic_node(state: GANState):\n",
    "    history = \"\\n\".join([f\"{m.type}: {m.content}\" for m in state['messages']])\n",
    "    messages = [\n",
    "        HumanMessage(content=get_argument_skeptic_prompt(\n",
    "            research_idea = state['research_idea'],\n",
    "            retrieved_papers = state['retrieved_papers'],\n",
    "            history = history\n",
    "        ))\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [AIMessage(content=f\"Skeptic: {response.content}\")]}\n",
    "\n",
    "def moderator_node(state: GANState):\n",
    "    history = \"\\n\".join([f\"{m.type}: {m.content}\" for m in state['messages']])\n",
    "    messages = [HumanMessage(content = get_argument_moderator_prompt(\n",
    "                research_idea= state['research_idea'],\n",
    "                retrieved_papers= state['retrieved_papers'],\n",
    "                history=history,\n",
    "                iteration= state['iteration'],\n",
    "                max_iterations= state['max_iterations']\n",
    "                ))   \n",
    "                ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Moderator: {response.content}\")],\n",
    "        \"iteration\": state[\"iteration\"] + 1\n",
    "    }\n",
    "\n",
    "def should_continue(state: GANState):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1].content\n",
    "    iteration = state['iteration']\n",
    "    max_iterations = state['max_iterations']\n",
    "    \n",
    "    if \"VERDICT:\" in last_message or iteration >= max_iterations:\n",
    "        return \"conclude\"\n",
    "    return \"advocate\"\n",
    "\n",
    "def scoring_node(state: GANState):\n",
    "    messages = [\n",
    "        HumanMessage(content=get_argument_score_prompt(\n",
    "            findings = state['messages'][-1].content\n",
    "        ))\n",
    "        ]\n",
    "    response = llm.with_structured_output(Score_Agent).invoke(messages)\n",
    "    return {\"scores\": response}\n",
    "\n",
    "\n",
    "\n",
    "def compile_agentic_workflow():\n",
    "\n",
    "    gan_workflow = StateGraph(GANState)\n",
    "    gan_workflow.add_node(\"advocate\", advocate_node)\n",
    "    gan_workflow.add_node(\"skeptic\", skeptic_node)\n",
    "    gan_workflow.add_node(\"moderator\", moderator_node)\n",
    "    gan_workflow.add_node(\"scoring\", scoring_node)\n",
    "\n",
    "    gan_workflow.add_edge(START, \"advocate\")\n",
    "    gan_workflow.add_edge(\"advocate\", \"skeptic\")\n",
    "    gan_workflow.add_edge(\"skeptic\", \"moderator\")\n",
    "    gan_workflow.add_conditional_edges(\n",
    "        \"moderator\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"advocate\": \"advocate\",\n",
    "            \"conclude\": \"scoring\"\n",
    "        }\n",
    "    )\n",
    "    gan_workflow.add_edge(\"scoring\",END)\n",
    "\n",
    "    gan_app = gan_workflow.compile()\n",
    "    return gan_app\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "053bb548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ReAct Agent Evaluation with Pre-Retrieved Papers...\n",
      "Number of retrieved papers: 98\n"
     ]
    }
   ],
   "source": [
    "agentic_app = compile_agentic_workflow()\n",
    "\n",
    "retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(list_of_papers_2)\n",
    "\n",
    "print(\"Running ReAct Agent Evaluation with Pre-Retrieved Papers...\")\n",
    "print(f\"Number of retrieved papers: {len(retrieved_papers_text.split('Paper ID:'))-1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd56cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agentic_app.invoke({\n",
    "    \"research_idea\": idea_generated,\n",
    "    \"retrieved_papers\": retrieved_papers_text,\n",
    "    \"messages\": [],\n",
    "    \"scores\": {},\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\":3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9db0b8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'research_idea': 'external/multiagent_research_generator/logs/log_2025_07_07/ideas_dedup/novel prompting methods to reduce social biases and stereotypes of large language models_diff_personas_proposer_reviser.json',\n",
       " 'retrieved_papers': 'Paper ID: ae766548699f27e669932de14e1c0f47b2828536\\n                                    Title: Prompting for Multimodal Hateful Meme Classification\\n                                    Abstract: Hateful meme classification is a challenging multimodal task that requires complex reasoning and contextual background knowledge. Ideally, we could leverage an explicit external knowledge base to supplement contextual and cultural information in hateful memes. However, there is no known explicit external knowledge base that could provide such hate speech contextual information. To address this gap, we propose PromptHate, a simple yet effective prompt-based model that prompts pre-trained language models (PLMs) for hateful meme classification. Specifically, we construct simple prompts and provide a few in-context examples to exploit the implicit knowledge in the pre-trained RoBERTa language model for hateful meme classification. We conduct extensive experiments on two publicly available hateful and offensive meme datasets. Our experiment results show that PromptHate is able to achieve a high AUC of 90.96, outperforming state-of-the-art baselines on the hateful meme classification task. We also perform fine-grain analyses and case studies on various prompt settings and demonstrate the effectiveness of the prompts on hateful meme classification.\\n                                \\n\\n---\\n\\nPaper ID: 465239ff11b96a3e44ff7fdf457c63a477d5a1b6\\n                                    Title: Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection\\n                                    Abstract: Recent advances show that two-stream approaches have achieved outstanding performance in hateful meme detection. However, hateful memes constantly evolve as new memes emerge by fusing progressive cultural ideas, making existing methods obsolete or ineffective. In this work, we explore the potential of Large Multimodal Models (LMMs) for hateful meme detection. To this end, we propose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE) Prompting, by integrating the evolution attribute and in-context information of memes. Specifically, Evolver simulates the evolving and expressing process of memes and reasons through LMMs in a step-by-step manner. First, an evolutionary pair mining module retrieves the top-k most similar memes in the external curated meme set with the input meme. Second, an evolutionary information extractor is designed to summarize the semantic regularities between the paired memes for prompting. Finally, a contextual relevance amplifier enhances the in-context hatefulness information to boost the search for evolutionary processes. Extensive experiments on public FHM, MAMI, and HarM datasets show that CoE prompting can be incorporated into existing LMMs to improve their performance. More encouragingly, it can serve as an interpretive tool to promote the understanding of the evolution of social memes. [Homepage] (https://github.com/inFaaa/Evolver)\\n                                \\n\\n---\\n\\nPaper ID: 70530e508fcd981d3b3ed6dd29cb689930a1f5e5\\n                                    Title: See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: e81466aab95dfba46b27f5d24dd3d2860cad45cd\\n                                    Title: Empowering LLM-based Machine Translation with Cultural Awareness\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 60cc0b1a573e75732d54ac688b84b7bd4a021b89\\n                                    Title: NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly\\n                                    Abstract: Norm discovery is important for understanding and reasoning about the acceptable behaviors and potential violations in human communication and interactions. We introduce NormSage, a framework for addressing the novel task of conversation-grounded multi-lingual, multi-cultural norm discovery, based on language model prompting and self-verification. NormSAGE leverages the expressiveness and implicit knowledge of the pretrained GPT-3 language model backbone, to elicit knowledge about norms through directed questions representing the norm discovery task and conversation context. It further addresses the risk of language model hallucination with a self-verification mechanism ensuring that the norms discovered are correct and are substantially grounded to their source conversations. Evaluation results show that our approach discovers significantly more relevant and insightful norms for conversations on-the-fly compared to baselines (>10+% in Likert scale rating). The norms discovered from Chinese conversation are also comparable to the norms discovered from English conversation in terms of insightfulness and correctness (<3% difference). In addition, the culture-specific norms are promising quality, allowing for 80% accuracy in culture pair human identification. Finally, our grounding process in norm discovery self-verification can be extended for instantiating the adherence and violation of any norm for a given conversation on-the-fly, with explainability and transparency. NormSAGE achieves an AUC of 95.4% in grounding, with natural language explanation matching human-written quality.\\n                                \\n\\n---\\n\\nPaper ID: 5f4f0bf8e94fa3eccab4e0d18e9b4a41c8f25569\\n                                    Title: Cultural Stress and Substance Use Risk among Venezuelan Migrant Youth in the United States\\n                                    Abstract: Abstract Background Since 2015, more than four million Venezuelans have fled their once prosperous nation, prompting an ever-intensifying refugee crisis. Recent research with Venezuelan parents suggests that many are exposed to elevated migration-related stress, experience behavioral health problems, and express profound concern for their childrens post-migration wellbeing. We examine the relationships between stress, family functioning, and substance use risk with a cultural stress theoretical lens. Methods: Survey data were collected between November 2018 and June 2019 from 402 recently-arrived Venezuelan immigrant youth ages 1017. Outcomes include perceived discrimination, negative context of reception, family support/communication, and substance use intentions and normative beliefs. Structural equation modeling was used to examine the relationships between variables. Results: Structural equation modeling results indicated that negative context of reception was associated with permissive substance use norms (via family communication; B\\u2009=\\u20090.070, p\\u2009<\\u2009.01) and intentions to use (via family support; B\\u2009=\\u20090.051, p\\u2009<\\u2009.01). Discrimination was not mediated by family functioning, rather it exerted a direct effect on substance use norms (\\u2009=\\u20090.20, p\\u2009<\\u2009.01) and intentions (\\u2009=\\u20090.33, p\\u2009<\\u2009.001). Discussion: We see clear evidence that negative context of reception and discrimination are related to substance use risk, both directly (in the case of discrimination) and indirectly (in the case of negative context of reception). Given the manifold stressors faced by Venezuelan immigrants both prior to migration and in the process of resettling in the US, it is critical that practitioners and policymakers support this rapidly-growing population.\\n                                \\n\\n---\\n\\nPaper ID: d1e7d99f9c6fb03f0b0cbecab2ef51001a5a6ca3\\n                                    Title: La Influencia de la Globalizacin en la Contabilidad de Costos: Un Enfoque Cualitativo\\n                                    Abstract: Globalization has transformed cost accounting, prompting multinational companies to adapt their accounting practices to remain competitive. This study analyzes the influence of globalization through a qualitative literature review. It highlights the implementation of International Financial Reporting Standards (IFRS), the interoperability of globalized accounting systems, and the customization of accounting systems to specific cultural contexts. A comprehensive literature review was conducted, selecting relevant academic articles and case studies. The inclusion criteria covered studies on cost accounting in a context of globalization and the adoption of advanced technologies. The results indicate that IFRS have improved comparability and transparency, although cultural and regulatory challenges remain. The implementation of globalized accounting software, such as ERP systems, has improved financial data management. Companies must invest in advanced technologies, adapt local accounting practices, and understand cultural differences to effectively manage costs and remain competitive.\\n                                \\n\\n---\\n\\nPaper ID: 18bd959aaa8a83b5b2192282224d700da7459857\\n                                    Title: Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment\\n                                    Abstract: Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&A platform (Zhihu) and the existing SocialChemistry dataset as proxies for contrasting cultural axes, align social situations cross-culturally, and extract social norms from texts using in-context learning. Embedding Chain-of-Thought prompting in a human-AI collaborative framework, we build a high-quality dataset of 3,069 social norms aligned with social situations across Chinese and American cultures alongside corresponding free-text explanations. To test the ability of models to reason about social norms across cultures, we introduce the task of explainable social norm entailment, showing that existing models under 3B parameters have significant room for improvement in both automatic and human evaluation. Further analysis of cross-cultural norm differences based on our dataset shows empirical alignment with the social orientations framework, revealing several situational and descriptive nuances in norms across these cultures.\\n                                \\n\\n---\\n\\nPaper ID: 96ccff43791d88b4d84958e6f8faafb8c5b558e5\\n                                    Title: Evolutionary novelty in communication between the sexes\\n                                    Abstract: The diversity of signalling traits within and across taxa is vast and striking, prompting us to consider how novelty evolves in the context of animal communication. Sexual selection contributes to diversification, and here we endeavour to understand the initial conditions that facilitate the maintenance or elimination of new sexual signals and receiver features. New sender and receiver variants can occur through mutation, plasticity, hybridization and cultural innovation, and the initial conditions of the sender, the receiver and the environment then dictate whether a novel cue becomes a signal. New features may arise in the sender, the receiver or both simultaneously. We contend that it may be easier than assumed to evolve new sexual signals because sexual signals may be arbitrary, sexual conflict is common and receivers are capable of perceiving much more of the world than just existing sexual signals. Additionally, changes in the signalling environment can approximate both signal and receiver changes through a change in transmission characteristics of a given environment or the use of new environments. The Anthropocene has led to wide-scale disruption of the environment and may thus generate opportunity to directly observe the evolution of new signals to address questions that are beyond the reach of phylogenetic approaches.\\n                                \\n\\n---\\n\\nPaper ID: c1ad892d7f675abf76e9b9453ae66540c02ddb81\\n                                    Title: Debating translanguaging\\n                                    Abstract: \\nIn this article, we discuss the concept of translanguaging by showing how theoretically unhelpful it is to account for language dynamics among Indigenous speakers leading revitalization projects in the Southern Cone of Latin America. We show how clear-cut distinctions between Spanish and Indigenous languages are crucial for minority speakers socio-political struggles against Spanish cultural, political, and social hegemony.\\nWe open our discussion by reviewing the different definitions of translanguaging in sociolinguistics and applied linguistics. We examine how the term sometimes overlaps with other previously established concepts such as code-switching and code-mixing and show the importance of inscribing any concepts in the historical and socio-political context in which they are used. We illustrate how Indigenous peoples understanding of multilingualism challenges linguists discourse on translanguaging. Our analysis aims at prompting scholars to reflect on the ideologies and practices we describe here to understand and attend more responsibly to Indigenous peoples political concerns.\\n                                \\n\\n---\\n\\nPaper ID: 5f6fe4ad1468bd80a91b4a254f94d3b6cd7844e8\\n                                    Title: Whats in a yardscape? A case study of emergent ecosystem services and disservices within resident yardscape discourses in Minnesota\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 4dc04157322e996071d7a653f23875025a081848\\n                                    Title: VCP-CLIP: A visual context prompting model for zero-shot anomaly segmentation\\n                                    Abstract: Recently, large-scale vision-language models such as CLIP have demonstrated immense potential in zero-shot anomaly segmentation (ZSAS) task, utilizing a unified model to directly detect anomalies on any unseen product with painstakingly crafted text prompts. However, existing methods often assume that the product category to be inspected is known, thus setting product-specific text prompts, which is difficult to achieve in the data privacy scenarios. Moreover, even the same type of product exhibits significant differences due to specific components and variations in the production process, posing significant challenges to the design of text prompts. In this end, we propose a visual context prompting model (VCP-CLIP) for ZSAS task based on CLIP. The insight behind VCP-CLIP is to employ visual context prompting to activate CLIP\\'s anomalous semantic perception ability. In specific, we first design a Pre-VCP module to embed global visual information into the text prompt, thus eliminating the necessity for product-specific prompts. Then, we propose a novel Post-VCP module, that adjusts the text embeddings utilizing the fine-grained features of the images. In extensive experiments conducted on 10 real-world industrial anomaly segmentation datasets, VCP-CLIP achieved state-of-the-art performance in ZSAS task. The code is available at https://github.com/xiaozhen228/VCP-CLIP.\\n                                \\n\\n---\\n\\nPaper ID: 87907a45792875ca42c7ebf4471cf1f1ae6c2e6a\\n                                    Title: Artificial intelligence may affect diversity: architecture and cultural context reflected through ChatGPT, Midjourney, and Google Maps\\n                                    Abstract: This study aims to understand how widely used Artificial Intelligence (AI) tools reflect the cultural context through the built environment. This research explores how outputs obtained with ChatGPT-4o, Midjourneys bot on Discord and Google Maps represent the cultural context of Stockholm, Sweden. Cultural context is important because it shapes peoples identity, behaviour, and power dynamics. AI-generated recommendations and images of Stockholms cultural context were compared with real photographs, GIS demographic data and socio-economic information about the city. Results show how outputs written with ChatGPT-4o mostly listed museums and other venues popular among visitors, while Midjourneys bot mostly represented cafes, streets, and furniture, reflecting a cultural context heavily shaped by buildings, consumption and commercial interests. Google Maps shows commercial sites while also enabling users to directly add information about places, like opinions, photographs and the main features of a business. These AI perspectives on cultural context can broaden the understanding of the urban environment and facilitate a deeper insight into the prevailing ideas behind the data that train these algorithms. Results suggest that the generative AI systems analysed convey a narrow view of the cultural context, prioritising buildings and a sense of cultural context that is curated, exhibited and commercialised. Generative AI tools could jeopardise cultural diversity by prioritising some ideas and places as cultural, exacerbating power relationships and even aggravating segregation. Consequently, public institutions should promote further discussion and research on AI tools, and help users combine AI tools with other forms of knowledge. The providers of AI systems should ensure more inclusivity in AI training data, facilitate users writing of prompts and disclose the limitations of their data sources. Despite the current potential reduction of diversity of the cultural context, AI providers have a unique opportunity to produce more nuanced outputs, which promote more societal diversity and equality.\\n                                \\n\\n---\\n\\nPaper ID: 634a2de8ecb173aead3fcda596984378426acdc9\\n                                    Title: Veracity-Oriented Context-Aware Large Language Models-Based Prompting Optimization for Fake News Detection\\n                                    Abstract: Fake news detection (FND) is a critical task in natural language processing (NLP) focused on identifying and mitigating the spread of misinformation. Large language models (LLMs) have recently shown remarkable abilities in understanding semantics and performing logical inference. However, their tendency to generate hallucinations poses significant challenges in accurately detecting deceptive content, leading to suboptimal performance. In addition, existing FND methods often underutilize the extensive prior knowledge embedded within LLMs, resulting in less effective classification outcomes. To address these issues, we propose the CAPEFND framework, contextaware prompt engineering, designed for enhancing FND tasks. This framework employs unique veracityoriented contextaware constraints, background information, and analogical reasoning to mitigate LLM hallucinations and utilizes selfadaptive bootstrap prompting optimization to improve LLM predictions. It further refines initial LLM prompts through adaptive iterative optimization using a random search bootstrap algorithm, maximizing the efficacy of LLM prompting. Extensive zeroshot and fewshot experiments using GPT3.5turbo across multiple public datasets demonstrate the effectiveness and robustness of our CAPEFND framework, even surpassing advanced GPT4.0 and human performance in certain scenarios. To support further LLMbased FND, we have made our approachs code publicly available on GitHub (our CAPEFND code: https://github.com/albert-jin/CAPE-FND [Accessed on 2024.09]).\\n                                \\n\\n---\\n\\nPaper ID: 77b00c476948813f64124412adf51f2272d09023\\n                                    Title: ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\\n                                    Abstract: Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. One critical issue is bridging the gap between discrete entities in low-level observations and the abstract concepts required for effective planning. A common solution is building hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language. However, language suffers from the inability to communicate detailed spatial information. We propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from past observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, supported by real-time object tracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to tackle complex tasks that demand spatial reasoning. Experiments in Minecraft show that our approach enables agents to achieve previously unattainable tasks, with a 76% absolute improvement in open-world interaction performance. Codes are available at https://craftjarvis.github.io/ROCKET-1.\\n                                \\n\\n---\\n\\nPaper ID: dae634624c20ecb443cc96bd782a95a3e4b6892c\\n                                    Title: Visual in-Context Prompting\\n                                    Abstract: In-context prompting in large language models (LLMs) has become a prevalent approach to improve zero-shot capabilities, but this idea is less explored in the vision domain. Existing visual prompting methods focus on referring segmentation to segment the most relevant object, falling short of addressing many generic vision tasks like open-set segmentation and detection. In this paper, we introduce a universal visual in-context prompting framework for both tasks, as shown in Fig. 1. In particular, we build on top of an encoder-decoder architecture, and develop a versatile prompt encoder to support a variety of prompts like strokes, boxes, and points. We further enhance it to take an arbitrary number of reference image segments as the context. Our extensive explorations show that the proposed visual in-context prompting elicits extraordinary referring and generic segmentation capabilities to refer and detect, yielding competitive performance to close-set in-domain datasets and showing promising results on many open-set segmentation datasets. By joint training on COCO and SA-1B, DINOv achieves 57.7 PQ on COCO and 23.2 PQ on ADE20K. Code will be available at https://github.com/UX-Decoder/DINOv\\n                                \\n\\n---\\n\\nPaper ID: 447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e\\n                                    Title: Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models\\n                                    Abstract: We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to human intelligence. However, even the most advanced LLMs currently struggle with this form of reasoning. We examine this problem within the framework of in-context learning and find that demonstrating both foundational skills and compositional examples grounded in these skills within the same prompt context is crucial. We refer to this prompt structure as skills-in-context (SKiC). With as few as two exemplars, this in-context learning structure enables LLMs to tackle more challenging problems requiring innovative skill combinations, achieving near-perfect systematic generalization across a broad range of tasks. Intriguingly, SKiC also unlocks the latent potential of LLMs, allowing them to more actively utilize pre-existing internal skills acquired during earlier pretraining stages to solve complex reasoning problems. The SKiC structure is robust across different skill constructions and exemplar choices and demonstrates strong transferability to new tasks. Finally, inspired by our in-context learning study, we show that fine-tuning LLMs with SKiC-style data can elicit zero-shot weak-to-strong generalization, enabling the models to solve much harder problems directly with standard prompting.\\n                                \\n\\n---\\n\\nPaper ID: 4ba1c455b9efb2ae9f732da23717cca0caf1a634\\n                                    Title: Prompting large language model with context and pre-answer for knowledge-based VQA\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: e06510e3ad308d0ab3b7a581c75acb61ee9d2a07\\n                                    Title: Exploring the impact of cultural context on eye-tracking studies of architectural monuments in selected European cities: Sustainable heritage management\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 3d866c0056541f19f92fa080384d4036ce1dcccf\\n                                    Title: Cultural context shapes the selection and adaptiveness of interpersonal emotion regulation strategies.\\n                                    Abstract: In everyday life, we commonly experience, express, and regulate our emotions in interpersonal contexts. However, much of the existing research on utilizing others for modulating one\\'s emotions has focused on Western, individualistic cultures, leaving a significant gap in understanding how the selection and adaptiveness of interpersonal emotion regulation (IER) strategies vary across cultural contexts. This cross-national comparison study aims to bridge this gap by examining intrinsic IER in 1,187 participants from Turkey and Germany, which are characterized by different cultural norms, values, and socialization practices regarding emotional experience and expression. All participants completed measures of intrinsic IER strategies alongside measures of adaptive outcomes, including depression, anxiety, negative affect, and positive affect. The results revealed cross-national differences between Turkish and German individuals in terms of the intrinsic IER strategies most frequently selected and their associations with depression, anxiety, negative affect, and positive affect. These findings emphasize the significance of cultural context in intrinsic IER and offer insights into the conditions under which these strategies are linked to adaptive outcomes. By recognizing the cultural nuances in how people navigate their emotions via social interactions, clinicians and researchers can develop more culturally sensitive interventions tailored to the specific needs of individuals in diverse cultural contexts. (PsycInfo Database Record (c) 2024 APA, all rights reserved).\\n                                \\n\\n---\\n\\nPaper ID: 42b6852b2e5e8e50687a6aa34fb4655e413caa0c\\n                                    Title: CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting\\n                                    Abstract: As the utilization of large language models (LLMs) has proliferated world-wide, it is crucial for them to have adequate knowledge and fair representation for diverse global cultures. In this work, we uncover culture perceptions of three SOTA models on 110 countries and regions on 8 culture-related topics through culture-conditioned generations, and extract symbols from these generations that are associated to each culture by the LLM. We discover that culture-conditioned generation consist of linguistic\"markers\"that distinguish marginalized cultures apart from default cultures. We also discover that LLMs have an uneven degree of diversity in the culture symbols, and that cultures from different geographic regions have different presence in LLMs\\' culture-agnostic generation. Our findings promote further research in studying the knowledge and fairness of global culture perception in LLMs. Code and Data can be found here: https://github.com/huihanlhh/Culture-Gen/\\n                                \\n\\n---\\n\\nPaper ID: 8f48c6e1c7107dd19a55661a4d79fb5868d66e39\\n                                    Title: Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting\\n                                    Abstract: Socio-demographic prompting is a commonly employed approach to study cultural biases in LLMs as well as for aligning models to certain cultures. In this paper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo and GPT4) with prompts that are conditioned on culturally sensitive and non-sensitive cues, on datasets that are supposed to be culturally sensitive (EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models except GPT4 show significant variations in their responses on both kinds of datasets for both kinds of prompts, casting doubt on the robustness of the culturally-conditioned prompting as a method for eliciting cultural bias in models that are not sufficiently stable with respect to arbitrary prompting cues. Further, we also show that some of the supposedly culturally neutral datasets have a non-trivial fraction of culturally sensitive questions/tasks.\\n                                \\n\\n---\\n\\nPaper ID: 12c826f4195da172b212a529f8fcf10cc79e35da\\n                                    Title: Context-faithful Prompting for Large Language Models\\n                                    Abstract: Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs\\' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs\\' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator\\'s statement and inquire about the narrator\\'s opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.\\n                                \\n\\n---\\n\\nPaper ID: 3ec5f0da304a606c5989de5b00e1246ee64b3e46\\n                                    Title: kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference\\n                                    Abstract: In-Context Learning (ICL), which formulates target tasks as prompt completion conditioned on in-context demonstrations, has become the prevailing utilization of LLMs. In this paper, we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment. To address both challenges, we advocate a simple and effective solution, $k$NN Prompting, which first queries LLM with training data for distributed representations, then predicts test instances by simply referring to nearest neighbors. We conduct comprehensive experiments to demonstrate its two-fold superiority: 1) Calibration-Free: $k$NN Prompting does not directly align LLM output distribution with task-specific label space, instead leverages such distribution to align test and training instances. It significantly outperforms state-of-the-art calibration-based methods under comparable few-shot scenario. 2) Beyond-Context: $k$NN Prompting can further scale up effectively with as many training data as are available, continually bringing substantial improvements. The scaling trend holds across 10 orders of magnitude ranging from 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B to 30B. It successfully bridges data scaling into model scaling, and brings new potentials for the gradient-free paradigm of LLM deployment. Code is publicly available.\\n                                \\n\\n---\\n\\nPaper ID: 8ce6ad6d8a73757309d3b9f525cf15cb68e32397\\n                                    Title: Efficient Prompting via Dynamic In-Context Learning\\n                                    Abstract: The primary way of building AI applications is shifting from training specialist models to prompting generalist models. A common practice for prompting generalist models, often referred to as in-context learning, is to append a few examples (demonstrations) to the prompt to help the model better understand the task. While effective, in-context learning can be inefficient because it makes the input prompt much longer, consuming valuable space in the context window and leading to larger computational costs. In this paper, we propose DynaICL, a recipe for efficient prompting with black-box generalist models that dynamically allocate in-context examples according to the input complexity and the computational budget. To achieve this, we train a meta controller that predicts the number of in-context examples suitable for the generalist model to make a good prediction based on the performance-efficiency trade-off for a specific input. We then dynamically allocate the number of demonstrations for an input according to predictions from the meta controller and the given computation budget. Experimental results show that dynamic example allocation helps achieve a better performance-efficiency trade-off in two practical settings where computational resources or the required performance is constrained. Specifically, DynaICL saves up to 46% token budget compared to the common practice that allocates the same number of in-context examples to each input. We also find that a meta controller trained on a certain backbone model and tasks can successfully generalize to unseen models and tasks.\\n                                \\n\\n---\\n\\nPaper ID: ac8b81247858dac6f335a6fb7bcbb931136ea86c\\n                                    Title: EXPLORING THE IMPACT OF CULTURE ON LANGUAGE LEARNING: HOW UNDERSTANDING CULTURAL CONTEXT AND VALUES CAN DEEPEN LANGUAGE ACQUISITION\\n                                    Abstract: Language and culture are two intertwined elements that shape our understanding of the world. In the process of language learning, understanding the cultural context of a language can lead to a more comprehensive understanding of its nuances and intricacies. This article will delve into the crucial role of culture in language learning, highlighting how cultural values and beliefs influence language use and meaning. By exploring the interplay between language and culture, learners can gain a deeper appreciation of the language they are studying and become more proficient in their use of it. Through the lens of culture, this article aims to inspire learners to go beyond just learning the grammar and vocabulary of a language, and instead, to immerse themselves in the history, customs, and traditions of its speakers.\\n                                \\n\\n---\\n\\nPaper ID: f57fae177e4e6e0a2b306b7b6bef899821ce9545\\n                                    Title: Instruct Me More! Random Prompting for Visual In-Context Learning\\n                                    Abstract: Large-scale models trained on extensive datasets, have emerged as the preferred approach due to their high generalizability across various tasks. In-context learning (ICL), a popular strategy in natural language processing, uses such models for different tasks by providing instructive prompts but without updating model parameters. This idea is now being explored in computer vision, where an input-output image pair (called an in-context pair) is supplied to the model with a query image as a prompt to exemplify the desired output. The efficacy of visual ICL often depends on the quality of the prompts. We thus introduce a method coined Instruct Me More (InMeMo), which augments in-context pairs with a learnable perturbation (prompt), to explore its potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the current state-of-the-art performance. Specifically, compared to the baseline without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for foreground segmentation and single object detection tasks, respectively. Our findings suggest that InMeMo offers a versatile and efficient way to enhance the performance of visual ICL with lightweight training. Code is available at https://github.com/Jackieam/InMeMo.\\n                                \\n\\n---\\n\\nPaper ID: 74d245de70e9a9f11d6eaa72439004e5cc2fabaf\\n                                    Title: Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 02fdc8b8c977425635f0513832cdd49f3792e551\\n                                    Title: The Cultural Context of Plagiarism and Research Misconduct in the Asian Region\\n                                    Abstract: Plagiarism is one of the most frequent forms of research misconduct in South and East Asian countries. This narrative review examines the factors contributing to research misconduct, emphasizing plagiarism, particularly in South, East and Southeast Asian countries. We conducted a PubMed and Scopus search using the terms plagiarism, Asia, South Asia, East Asia, Southeast Asia, research misconduct and retractions in January of 2022. Articles with missing abstracts, incomplete information about plagiarism, publication dates before 2010, and those unrelated to South, East, and Southeast Asian countries were excluded. The retraction watch database was searched for articles retracted between 9th January 2020 to 9th January 2022. A total of 159 articles were identified, of which 21 were included in the study using the database search criteria mentioned above. The review of articles identified a lack of training in scientific writing and research ethics, publication pressure, permissive attitudes, and inadequate regulatory measures as the primary reasons behind research misconduct in scientific publications. Plagiarism remains a common cause of unethical publications and retractions in regions of Asia (namely South, East and Southeast). Researchers lack training in scientific writing, and substantial gaps exist in understanding various forms of plagiarism, which heavily contribute to the problem. There is an urgent need to foster high research ethics standards and adhere to journal policies. Providing appropriate training in scientific writing among researchers may help improve the knowledge of different types of plagiarism and promote the use of antiplagiarism software, leading to a substantial reduction in the problem.\\n                                \\n\\n---\\n\\nPaper ID: 6d1ef4436904de111c8b1975bbf25d3fe2f165f7\\n                                    Title: DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting\\n                                    Abstract: Recent progress has shown that large-scale pre-training using contrastive image-text pairs can be a promising alternative for high-quality visual representation learning from natural language supervision. Benefiting from a broader source of supervision, this new paradigm exhibits impressive transferability to downstream classification tasks and datasets. However, the problem of transferring the knowledge learned from image-text pairs to more complex dense prediction tasks has barely been visited. In this work, we present a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP. Specifically, we convert the original image-text matching problem in CLIP to a pixel-text matching problem and use the pixel-text score maps to guide the learning of dense prediction models. By further using the contextual information from the image to prompt the language model, we are able to facilitate our model to better exploit the pretrained knowledge. Our method is model-agnostic, which can be applied to arbitrary dense prediction systems and various pre-trained visual backbones including both CLIP models and ImageNet pre-trained models. Extensive experiments demonstrate the superior performance of our methods on semantic segmentation, object detection, and instance segmentation tasks. Code is available at https://github.com/raoyongming/DenseCLIP.\\n                                \\n\\n---\\n\\nPaper ID: 22a04f6d7d9dd84c478e3a76b7588fafd3da3ddd\\n                                    Title: What influences the purchase of virtual gifts in live streaming in China? A cultural contextsensitive model\\n                                    Abstract: China is one of the largest and fastestgrowing markets for live streaming, and the purchase of virtual gifts in live streaming is the core for streamers and live streaming platforms in China to survive and thrive. Compared to western countries, live streaming in China highlights the lively social atmosphere and heated social interactions among streamers and viewers. This study develops a cultural contextsensitive model that contextualises the purchase of virtual gifts in live streaming in China. Specifically, we focus on the viewer\\'s social experience and the social atmosphere in live streaming which have received limited attention yet. We introduce viewers\\' social perceptions with regard to the streamer and other viewers (ie, perceived proximity to the streamer and sense of belonging to the viewer crowd) and show how such social perceptions contribute to the development of flow experience, which subsequently leads to purchase intention. This study also reveals how such social perceptions can be shaped by the contextual setting consisting of the ITrelated factors of live streaming (ie, responsiveness, twoway communication, social presence, and selfpresentation) and the cultural characteristics of China (ie, social orientation and harmony). Our research offers both theoretical guidance for practitioners into cultivating viewers\\' purchase of virtual gifts in China\\'s live streaming.\\n                                \\n\\n---\\n\\nPaper ID: eecb45aa040064cbc0b37fd100706c02e7dc880e\\n                                    Title: Structured Prompting: Scaling In-Context Learning to 1, 000 Examples\\n                                    Abstract: Large language models have exhibited intriguing in-context learning capability, achieving promising zero- and few-shot performance without updating the parameters. However, conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. Specically, demonstration examples are separately encoded with well-designed position embeddings, and then they are jointly attended by the test example using a rescaled attention mechanism. So we can scale the number of exemplars with linear complexity instead of quadratic complexity with respect to length. Experimental results on a diverse set of tasks show that our approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases. Code has been released at https://aka.ms/structured-prompting .\\n                                \\n\\n---\\n\\nPaper ID: a612e4b3dfd06c600143b9b2d82ad755564089cd\\n                                    Title: Determinants of institutional trust: the role of cultural context\\n                                    Abstract: Abstract We examine the cultural context for individual\\'s trust in public institutions. To shed some light on possible cultural explanations from a more comparative perspective and cover a wider set of cultural aspects, we use indicators of cultural dimensions by Kaasa et al. (2014) based on Hofstede\\'s (1980) approach. Multilevel regression analysis is conducted with individual-level data from two waves of the European Social Survey (2008, 2010) and regional-level data from multiple sources. Confirmatory factor analysis is used to construct the indicators of social and institutional trust and corruption. Our results suggest that individuals tend to trust institutions less in regions with large power distance. Hence, an important key for governments being more successful in achieving their aims seems to be related to improving the sense of participation and civic responsibility.\\n                                \\n\\n---\\n\\nPaper ID: bd3d8a17988add79205f697465bbd6c2ef59e621\\n                                    Title: PromptRSVQA: Prompting visual context to a language model for Remote Sensing Visual Question Answering\\n                                    Abstract: Remote sensing visual question answering (RQA) was recently proposed with the aim of interfacing natural language and vision to ease the access of information contained in Earth Observation data for a wide audience, which is granted by simple questions in natural language. The traditional vision/language interface is an embedding obtained by fusing features from two deep models, one processing the image and another the question. Despite the success of early VQA models, it remains difficult to control the adequacy of the visual information extracted by its deep model, which should act as a context regularizing the work of the language model. We propose to extract this context information with a visual model, convert it to text and inject it, i.e. prompt it, into a language model. The language model is therefore responsible to process the question with the visual context, and extract features, which are useful to find the answer. We study the effect of prompting with respect to a black-box visual extractor and discuss the importance of training a visual model producing accurate context.\\n                                \\n\\n---\\n\\nPaper ID: b58a9c51c7dff371ff95b7eacc6deaa1129adaad\\n                                    Title: Improving patientcentred care through a tailored intervention addressing nursing clinical handover communication in its organizational and cultural context\\n                                    Abstract: Abstract Aims To increase the quality and safety of patient care, many hospitals have mandated that nursing clinical handover occur at the patient\\'s bedside. This study aims to improve the patientcentredness of nursing handover by addressing the communication challenges of bedside handover and the organizational and cultural practices that shape handover. Design Qualitative linguistic ethnographic design combining discourse analysis of actual handover interactions and interviews and focus groups before and after a tailored intervention. Methods Preintervention we conducted interviews with nursing, medical and allied health staff (n = 14) and focus groups with nurses and students (n = 13) in one hospital\\'s Rehabilitation ward. We recorded handovers (n = 16) and multidisciplinary team huddles (n = 3). An intervention of communication training and recommendations for organizational and cultural change was delivered to staff and championed by ward management. After the intervention we interviewed nurses and recorded and analyzed handovers. Data were collected from February to August 2020. Ward management collected hospitalacquired complication data. Results Notable changes postintervention included a shift to involve patients in bedside handovers, improved wardlevel communication and culture, and an associated decrease in reported hospitalacquired complications. Conclusions Effective change in handover practices is achieved through communication training combined with redesign of local practices inhibiting patientcentred handovers. Strong leadership to champion change, ongoing mentoring and reinforcement of new practices, and collaboration with nurses throughout the change process were critical to success. Impact Ineffective communication during handover jeopardizes patient safety and limits patient involvement. Our targeted, locally designed communication intervention significantly improved handover practices and patient involvement through the use of informational and interactional protocols, and redesigned handover tools and meetings. Our approach promoted a ward culture that prioritizes patientcentred care and patient safety. This innovative intervention resulted in an associated decrease in hospitalacquired complications. The intervention has been rolled out to a further five wards across two hospitals.\\n                                \\n\\n---\\n\\nPaper ID: 8e39df327bf3a09b8672c3b899416d04e56aea47\\n                                    Title: Suicide in cultural context: An ecosocial approach\\n                                    Abstract: This article introduces a thematic issue of Transcultural Psychiatry on suicide in cultural context. Developmental and social structural factors including exposure to violence, childhood abuse and privation, as well as intractable social problems that create psychic pain and a sense of entrapment have been shown to increase the risk of suicidal behavior. However, all of the major social determinants identified in suicide research are influenced or mediated by particular cultural meanings and contexts. To move beyond crude generalizations about suicide based on psychological theories developed mainly in Western contexts and culture-specific prototypes or exemplars, we need more fine-grained analysis of the experience of diverse populations. The articles in this issue provide clear illustrations of the impact of cultural and contextual factors in the causes of suicide, with implications for psychiatric research, theory, and practice. Cross-cultural research points to the possibility of developing a typology of social predicaments affecting specific sociodemographic groups and populations. This typology could be elaborated and applied in clinical and public health practice through an ecosocial approach that considers the ways that suicide is embodied and enacted in social systemic contexts.\\n                                \\n\\n---\\n\\nPaper ID: efdab3fe337fc1e511f3384063cd6aa2cc8ae880\\n                                    Title: Serious game design model for language learning in the cultural context\\n                                    Abstract: Digital learning environments have been gaining prominence during the last few years. In particular, the rising usage of mobile devices, including smartphones and tabs, has invited researchers to design and develop learning applications and games for such platforms. Mobile applications and games have been developed for learning languages like many other domains. However, most of these games are fun-based and lack a holistic design and development approach. Therefore, as a principal contribution, this research presents a theoretical model for designing language learning games in a cultural context. The proposed model combines the elements of sociocultural theory with the concepts and elements of gamification, keeping in view the requirements and educational settings, including level and mode of education, etc., to ensure the effectiveness and usability of the developed game. Subsequently, based on the proposed model, a Language Learning Game (LLG) has been designed and developed through a systematic process that involves game design, low-fidelity, and high-fidelity prototyping and its validation. The LLG has been evaluated comprehensively at different stages by incorporating standard methods. Whereby this research augments the existing set of heuristics by proposing a number of specialized heuristics for the evaluation of serious games to gauge their conformance to the cultural context. The evaluation results show that the game has overall usability scores of 90%. While the quasi-experiment-based pre-test and post-test have been conducted, the results reveal that the results obtained by LLG are statistically significantly better than adopted mobile application and traditional group.\\n                                \\n\\n---\\n\\nPaper ID: 886b40d68fb99865b06b898697489fa570ba701e\\n                                    Title: Socio-cultural context of adolescent sexuality and youth friendly service intervention in West Gojjam Zone, Northwest Ethiopia: a qualitative study\\n                                    Abstract: Background Recognizing that adolescents face barriers in accessing services, may feel embarrassed, face stigma on sexual matters, or have concerns about judgmental providers, youth-friendly service (YFS) has been introduced to deliver health services that meet the sexual and reproductive health (SRH) needs of young people. Evidences on the role of YFS in addressing the socio-cultural norms influence unmarried adolescent SRH behaviour are limited. Therefore, this study explore whether the socio-cultural norms influencing adolescent SRH behaviour vary between youth friendly service program and non Program areas in West Gojjam Zone, North West Ethiopia. Methods Qualitative case study design was employed to explore the socio-cultural context of adolescent sexuality. Purposive sampling was used to identify study participants. Data were collected from 112 participants both from YFS program and non-program areas using semi-structured in-depth interviews, key informants, and focus group discussions guides. A total of 18 key informant interviews, twelve FGDs and four in-depth interviews were conducted. Participants were comprised from unmarried adolescents, parents, religious leaders, community elders, health professionals, teachers, and unmarried adolescents who experienced SRH problem. Thematic analysis was used to summarized the data. Results The socio-cultural norms related to adolescent sexuality in both YFS program and non-program areas indicated that the community is intolerant to premarital sex, SRH service utilization (eg., contraceptive use) by unmarried adolescent; and discourage SRH communication with unmarried adolescents. According to the participants, premarital sex and SRH service use were not accepted by the community. Moreover, participants believed that, having communication on SRH issues with unmarried adolescents are equivalent to encouraging them to initiate sex, therefore, should not be practiced. Conclusion The socio-cultural norms influencing adolescent sexual behaviour were more or less the same between settings. In both areas, the socio-cultural context discourages YFS intervention like SRH communication and service use. Also, the YFS program does not modify the socio-cultural norm affecting adolescent sexuality. Therefore, the YFS interventions strategies should give due emphasis to the socially accepted sexual norms like sexual abstinence.\\n                                \\n\\n---\\n\\nPaper ID: 719cafa404e972da8a88e815b232dabe3021df18\\n                                    Title: The Historical, Psychosocial, and Cultural Context of Breastfeeding in the African American Community\\n                                    Abstract: Breastfeeding provides a range of benefits for the infant\\'s growth, immunity, and development. It also has health benefits for the mother, including a reduced risk of premenopausal breast cancer, earlier return to prepregnancy weight, reduction of postpartum bleeding, and reduced risk of osteoporosis. There are a number of complex factors that influence the decision to initiate and continue breastfeeding, including those external to women, such as cultural beliefs. The cultural context and environment of decision making are illuminated through the prism of traditions and historical and cultural events. The ideology and sentiment of breastfeeding have changed during the course of history and have evolved within the African American community. Throughout the evolution of infant feeding practices, historical aftermaths have contributed to the legacy and emotional context of infant feeding trends. The tradition of wet nursing for African American women is inherently linked to white supremacy, slavery, medical racism and the physical, emotional, and mental abuse that enslaved African American women endured. Thus, the decision to breastfeed and the act of breastfeeding may remain deeply affected by the generational trauma of wet nursing during slavery. The associated negative connotation of wet nursing, slavery, and medical exploitation is one of the many nuanced cultural barriers that denies Black women and infants the many health benefits of breastfeeding.\\n                                \\n\\n---\\n\\nPaper ID: 508c22a1ea2dd28e7c4424bf6d1def4f6c9bdc10\\n                                    Title: Self-efficacy, task values and growth mindset: what has the most predictive power for primary school students self-regulated learning in English writing and writing competence in an Asian Confucian cultural context?\\n                                    Abstract: ABSTRACT This study aimed to examine the relationships between motivational variables, self-regulated writing strategy use and writing competence with 511 fourth graders in Hong Kong. The high writing achievers reported a higher level of motivation, i.e. self-efficacy, task values (i.e. interest and utility) and growth mindset in English writing than the low writing achievers. The high achievers also used various self-regulated writing strategies more frequently than their low achieving counterparts. Self-efficacy and growth mindset were found to be almost equally important predictors of strategy use, while the impacts of interest and utility were weaker. The results indicated that self-regulated learning and competence in English writing can be improved through the promotion of motivation. This study highlights the influence of the social-cultural context on motivation. More importantly, growth mindset may emerge as a new research agenda in English as a Second/Foreign Language (ESL/EFL) writing.\\n                                \\n\\n---\\n\\nPaper ID: a5c3d87394477c4dc3c57198992de6f2b0b0a0f3\\n                                    Title: The impact of entrepreneurship education and cultural context on entrepreneurial intentions of Ukrainian students: the mediating role of attitudes and perceived control\\n                                    Abstract: PurposeThe purpose of the research is to test empirically whether the variables of personal attitudes towards entrepreneurship and perceived control mediate the relationship between entrepreneurship education and intentions of Ukrainian students to become entrepreneurs; to determine whether personal attitudes mediate the relationship between cultural context and entrepreneurial intentions.Design/methodology/approachThe research project carried out jointly by four Kharkiv universities used 349 survey responses from senior students majoring in business and economics and management and marketing. The data was analysed using Partial Least Squares regression.FindingsEducation and employed teaching methods, in particular, positively affect students\\' attitudes towards entrepreneurship, their perceived capability to start a business and indirectly influence entrepreneurial intentions. However, attendance of entrepreneurship-related courses itself does not enhance perceived control and has no significant effect on personal attitudes. The study also shows that cultural context has a positive influence on students\\' attitudes towards entrepreneurship and, therefore, indirectly impacts their intentions to become entrepreneurs.Research limitations/implicationsFirstly, the students participating in the study were from one country. And secondly, the paper deals with pre-educational entrepreneurial intentions.Practical implicationsThe study suggests that a practice-based approach to entrepreneurship education is a key to raising entrepreneurial awareness of young people in countries whose national cultures are built on collectivist values.Originality/valueThe results of the study are of value for teaching staff, who can actually influence students\\' entrepreneurial self-awareness, and for university management in the context of contemporary education reforms and the latest requirements to education process.\\n                                \\n\\n---\\n\\nPaper ID: 52969c11d91ea7b3ba13afc542122e2c7dea7391\\n                                    Title: The Intersection of Cultural Context and Research Encounter: Focus on Interviewing in Qualitative Research\\n                                    Abstract: This article discusses the influence of the cultural context on the interview process. With literature demonstrating the role of spatial context on interviews, the article contends that similar consideration should be given to cultural contexts of research studies. Focusing on the cultural context where the interview takes place and the interactions during the interview can help researchers understand and analyze interview material. Interview forms such as conversation/interview bombing emerged from the interaction of cultural context with the interview process. This points to the need for qualitative researchers to explore how the cultural context shapes their research encounter. Such focus will expand the literature on the forms of interview emerging from the intersection of cultural context and interviewing as well as research on spatiality and interview.\\n                                \\n\\n---\\n\\nPaper ID: d8aa897eafb647ed4c0c6286b5a9df973cfaaa33\\n                                    Title: A Study of Cultural Context in Chinese-English Translation\\n                                    Abstract: The cultural translation view considers translation as a cross-cultural communication activity. The paradigm and thinking of translation will also have profound changes in different cultural contexts. It can be seen from modern translation studies that the translation circle has paid full attention to the cultural differences between the two languages, and due to the profound influence of culture, translators have gradually formed their own unique and personalized cultural understanding and translation concepts in translation practice. Starting from the influence of cultural context on ChineseEnglish translation, this article explores the cultural context in Chinese-English translation combined with practical work experience, and the understanding and practice of translation activities under the cultural translation perspective.\\n                                \\n\\n---\\n\\nPaper ID: 231a113a80451b64352b02d9cac33dc11617ee4b\\n                                    Title: Effects and satisfaction of dignity therapy among patients with hematologic neoplasms in the Chinese cultural context: a randomized controlled trial\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 53427f07ce50e1ec8876726069ce80d75fee7a70\\n                                    Title: Women founders in a high-tech incubator: negotiating entrepreneurial identity in the Indian socio-cultural context\\n                                    Abstract: PurposeThis study aims to understand the socio-cultural context of Indian women\\'s high-tech entrepreneurial experience. Despite a small proportion of women entrepreneurs, and the traditional gender dynamics among the educated middle-classes that appears to be antithetical to female entrepreneurship; women-led high-tech start-ups are on the rise.Design/methodology/approachSemi-structured interviews were conducted with women founders at an academic incubator in an elite Indian Institute of Technology. The study was based on the post-structural feminist approach that women entrepreneurs are embedded in their socio-cultural and institutional context. During data collection, the Coronavirus lockdown provided a natural experiment, highlighting entrepreneurial response to unforeseen obstacles.FindingsIt finds that the context is significant in constructing opportunity, and in navigating challenges of gender and entrepreneurship. Further, in the process of construction of an entrepreneurial identity, women innovators not only reproduce, but also modify their context. Also, the experiences with academic incubator indicate positive results both for gender dynamics and enhancing an emergent entrepreneurial culture.Practical implicationsThe study highlights that women\\'s high-tech entrepreneurship has considerable potential for enhancing women\\'s status in society through the support of academic incubator. This has certain implications for policy.Originality/valueIt provides an insight in to the hitherto neglected issue of women\\'s high-tech entrepreneurship in India, and argues that a study of social embeddedness not only highlights constraints for women entrepreneurs unique to that context, but also the potential of women\\'s entrepreneurship in advancing women\\'s agency and gender equality.\\n                                \\n\\n---\\n\\nPaper ID: 0225907ef381543df7b4b1d8fc11fdc1a5c45e8f\\n                                    Title: Expanding the Transformative Explanatory Sequential Mixed Methods Design Archetype in a Cross-Cultural Context: The Polemics of African Refugee Livelihoods in Places of Resettlement\\n                                    Abstract: Transformative sequential mixed methods design in a cross-cultural context is seldom straightforward. Using a community-based participatory research approach as the transformative lens in an African refugee context in the southern United Status, we explored: (a) the intersection of culture, financial stress, and financial self-efficacy and (b) tested the efficacy of financial literacy as the focus of a culturally responsive solution grounded in community-identified priorities. Through a three-phased explanatory sequential mixed methods design, we demonstrate how the addition of a third phase of analysis that focuses on convergence and expansion of quantitative and qualitative data integration and cyclical processes of dissemination and action can strengthen the utility of transformative mixed methods research in a cross-cultural context. Our study offers a unique contribution to the long-standing methodological dialogue between the design elements of mixed methods research, community-based participatory research, and migration studies by expanding the transformative explanatory sequential design archetype in a cross-cultural context.\\n                                \\n\\n---\\n\\nPaper ID: f2a8ad62d936e807e463aa64a73a9f71b6b1bfb6\\n                                    Title: Self-Alignment: Improving Alignment of Cultural Values in LLMs via In-Context Learning\\n                                    Abstract: Improving the alignment of Large Language Models (LLMs) with respect to the cultural values that they encode has become an increasingly important topic. In this work, we study whether we can exploit existing knowledge about cultural values at inference time to adjust model responses to cultural value probes. We present a simple and inexpensive method that uses a combination of in-context learning (ICL) and human survey data, and show that we can improve the alignment to cultural values across 5 models that include both English-centric and multilingual LLMs. Importantly, we show that our method could prove useful in test languages other than English and can improve alignment to the cultural values that correspond to a range of culturally diverse countries.\\n                                \\n\\n---\\n\\nPaper ID: 081c97a6074084353bb0112133bbb8aa75d985d8\\n                                    Title: Autoethnographic Research to Explore Instructional Design Practices for Distance Teaching and Learning in a Cross-Cultural Context\\n                                    Abstract: The purpose of this autoethnography is to share my perspective of course design and delivery considerations based on my professional experience as an instructional designer for a virtual international exchange program during the COVID-19 pandemic. Due to the impact of the global pandemic, the partner institutions decided to take advantage of distance education to continue their exchange program. I was the instructional designer to support facultys transition to online instruction. Little research has been conducted to explore instructional design practices for distance teaching and learning in an intercultural context. Therefore, I applied an individual autoethnography to collect and analyze data from my narratives and artifacts to understand my perspective of instructional design and delivery practices within a cross-cultural distance learning environment. The findings showed course design in transnational distance contexts was more complicated than in distance education in a single cultural context. My role as an instructional designer was influenced by factors at both micro and macro levels.\\n                                \\n\\n---\\n\\nPaper ID: 741ab291a02f1cf6d1ab4aa2f417aafaac3a674e\\n                                    Title: What Motivates Consumers to Write Online Reviews? Qualitative Research in the Indian Cultural Context\\n                                    Abstract: Abstract The study explores and understands the antecedents that motivate the consumers to write online reviews on online consumer-opinion platforms in the Indian cultural context. This research employed the qualitative research approach using the in-depth interview method to understand the consumers motivation to write online reviews. The data collected in the form of interview narratives were analyzed using ATLAS.ti 8, a qualitative data analysis software. The study identified seven antecedents of writing online reviews based on the data analysis carried out in three phases. Further, based on the underlying meaning of these antecedents, a conceptual framework was proposed.\\n                                \\n\\n---\\n\\nPaper ID: 5b5ed68180564736ceaf558d21a5eb57260329ff\\n                                    Title: Adapting Pedagogy to Cultural Context\\n                                    Abstract: This paper argues that many pedagogical reform efforts falter because they fail to consider the cultural context of teacher and student behavior. Little guidance exists on how to adapt teaching practices to be compatible with culturally influenced behaviors and beliefs. We present evidence from three studies conducted as part of a large basic education program in Tanzania showing that some teaching activities are less effective or not well implemented because of culturally influenced behaviors in the classroom, namely childrens lack of confidence to speak up in class; a commitment to togetherness, fairness, and cooperation; avoidance of embarrassment; and age-graded authority. We propose ways teaching activities can be adapted to take these behaviors into account while still adhering to fundamental principles of effective learning, including student participation in their own learning, teaching at the right level, and monitoring students as a basis for adjusting instruction. Such adaptations may be made most effective by engaging teachers in co-creation of teaching activities.\\n                                \\n\\n---\\n\\nPaper ID: faea75f1f92ec7c888d409642658e0f96182522f\\n                                    Title: A Threshold Concept and Capability Approach to the Cross-Cultural Contextualization of Western Management Education\\n                                    Abstract: This article presents contextualization as a pedagogic response to the issues of cross-cultural relevance associated with Western management education in non-Western contexts, and with regard to the needs and expectations of non-Western students. Building on a synthesis of threshold concepts and threshold capabilities, this article demonstrates in principle how contextualization is a threshold concept that can help educators and students address the issue of relevance. Translation intelligence is introduced as a distinct threshold capability, which can enable the development of the knowledge handling skills necessary for students future management practice. This article posits that contextualization and translation intelligence are valuable to business schools and management educators because they address issues of cross-cultural relevance and by facilitating learning beyond content they equip students with skills which can be employed in their future management practice.\\n                                \\n\\n---\\n\\nPaper ID: 444af11263bc7dfda120af047887ebe174cef841\\n                                    Title: Development of the mental health cultural adaptation and contextualization for implementation (mhCACI) procedure: a systematic framework to prepare evidence-based psychological interventions for scaling\\n                                    Abstract: Abstract Background Because of the high burden of untreated mental illness in humanitarian settings and low- and middle-income countries, scaling-up effective psychological interventions require a cultural adaptation process that is feasible and acceptable. Our adaptation process incorporates changes into both content and implementation strategies, with a focus on local understandings of distress and treatment mechanisms of action. Methods Building upon the ecological validity model, we developed a 10-step process, the mental health Cultural Adaptation and Contextualization for Implementation (mhCACI) procedure, and piloted this approach in Nepal for Group Problem Management Plus (PM+), a task-sharing intervention, proven effective for adults with psychological distress in low-resource settings. Detailed documentation tools were used to ensure rigor and transparency during the adaptation process. Findings The mhCACI is a 10-step process: (1) identify mechanisms of action, (2) conduct a literature desk review for the culture and context, (3) conduct a training-of-trainers, (4) translate intervention materials, (5) conduct an expert read-through of the materials, (6) qualitative assessment of intervention population and site, (7) conduct practice rounds, (8) conduct an adaptation workshop with experts and implementers, (9) pilot test the training, supervision, and implementation, and (10) review through process evaluation. For Group PM+, key adaptations were harmonizing the mechanisms of action with cultural models of tension; modification of recruitment procedures to assure fit; and development of a skills checklist. Conclusion A 10-step mhCACI process could feasibly be implemented in a humanitarian setting to rapidly prepare a psychological intervention for widespread implementation.\\n                                \\n\\n---\\n\\nPaper ID: 723156908f9daa627d37feea8bbb666757200b60\\n                                    Title: Leveraging Temporal Contextualization for Video Action Recognition\\n                                    Abstract: We propose a novel framework for video understanding, called Temporally Contextualized CLIP (TC-CLIP), which leverages essential temporal information through global interactions in a spatio-temporal domain within a video. To be specific, we introduce Temporal Contextualization (TC), a layer-wise temporal information infusion mechanism for videos, which 1) extracts core information from each frame, 2) connects relevant information across frames for the summarization into context tokens, and 3) leverages the context tokens for feature encoding. Furthermore, the Video-conditional Prompting (VP) module processes context tokens to generate informative prompts in the text modality. Extensive experiments in zero-shot, few-shot, base-to-novel, and fully-supervised action recognition validate the effectiveness of our model. Ablation studies for TC and VP support our design choices. Our project page with the source code is available at https://github.com/naver-ai/tc-clip\\n                                \\n\\n---\\n\\nPaper ID: 7443dc4b146e5ce5ce733baec89f2e55388ea88e\\n                                    Title: Cultural tendencies in generative AI\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 8c2b92b3cd0da1a2abc359f1ef6d50608b067097\\n                                    Title: Evaluating ChatGPT performance in Arabic dialects: A comparative study showing defects in responding to Jordanian and Tunisian general health prompts\\n                                    Abstract: Background: The role of artificial intelligence (AI) is increasingly recognized to enhance digital health literacy. There is of particular importance with widespread availability and popularity of AI chatbots such as ChatGPT and its possible impact on health literacy. The involves the need to understand AI models performance across different languages, dialects, and cultural contexts. This study aimed to evaluate ChatGPT performance in response to prompting in two different Arabic dialects, namely Tunisian and Jordanian. \\nMethods: This descriptive study followed the METRICS checklist for the design and reporting of AI based studies in healthcare. Ten general health queries were translated into Tunisian and Jordanian dialects of Arabic by bilingual native speakers. The performance of two AI models, ChatGPT-3.5 and ChatGPT-4 in response to Tunisian, Jordanian, and English were evaluated using the CLEAR tool tailored for assessment of health information generated by AI models. \\nResults: ChatGPT-3.5 performance was categorized as average in Tunisian Arabic, with an overall CLEAR score of 2.83, compared to above average score of 3.40 in Jordanian Arabic. ChatGPT-4 showed a similar pattern with marginally better outcomes with a CLEAR score of 3.20 in Tunisian rated as average and above average performance in Jordanian with a CLEAR score of 3.53. The CLEAR components consistently showed superior performance in the Jordanian dialect for both models despite the lack of statistical significance. Using English content as a reference, the responses to both Tunisian and Jordanian dialects were significantly inferior (P<.001). \\nConclusion: The findings highlight a critical dialectical performance gap in ChatGPT, underlining the need to enhance linguistic and cultural diversity in AI models development, particularly for health-related content. Collaborative efforts among AI developers, linguists, and healthcare professionals are needed to improve the performance of AI models across different languages, dialects, and cultural contexts. Future studies are recommended to broaden the scope across an extensive range of languages and dialects, which would help in achieving equitable access to health information across various communities.\\n                                \\n\\n---\\n\\nPaper ID: 40cc89dff7f1f661e80ad2587fe8b5b42938956e\\n                                    Title: The Dynamic of Contextualization in Indonesian Qur\\'anic Tafsirs: A Comparative Study of Tafsir Al-Azhar and Tafsir Al-Mishbh on The Story of The Prophet Moses\\n                                    Abstract: The Indonesian Quranic tafsirs represent a contextual approach. Each tafsir scholar has different characteristics in contextualizing the meaning of the Quran so that it is relevant to contemporary Indonesian society. This article aims to show the dynamic character of Indonesian Quranic interpretation in contextualizing the meaning of the verses about Moses in the Indonesian context. This research uses a comparative method against the al-Azhar tafsir written by Haji Abdul Malik bin Karim Amrullah, also known as Hamka, and the Tafsir al-Mishbh by M. Quraish Shihab. The three aspects compared and the research questions include: How is the story of Prophet Moses narrated in the two tafsir books? What are the interpretation methods used in the verses about the story of Prophet Moses, and how can the lessons from the story of Prophet Moses align with the Indonesian cultural context? The research results show that both tafsir books have similarities in making the story of Moses relevant to Indonesian society. The difference is that Tafsir al-Mishbh appears more disciplined in following the textual meaning of the Quranic verses. In contrast, Tafsir al-Azhar is more flexible in its storytelling improvisation, making the story of Moses more cohesive and engaging. Shihab understands the story of Moses as Allahs plan to overthrow human arrogance (Pharaoh), with a narrative flow following Allahs laws. This retelling represents Shihabs traditionalist and pragmatic thinking. In contrast, Hamka is more rational and critical. He views the story of Moses as a struggle and heroism that teaches the importance of hard work and intelligence in facing lifes challenges. The findings of this research contribute to showing the dynamic nature of Indonesian contextual tafsir. It illustrates how the Quranic interpretation maintains its textual meaning while remaining relevant for contemporary readers.\\n                                \\n\\n---\\n\\nPaper ID: 3cdc639f9ac375e42a9d27039eb4560e62cef9e9\\n                                    Title: The Social Construction of Generative AI Prompts\\n                                    Abstract: As text-to-image AI tools grow in capability and widespread use, research has focused on studying individualistic user prompt crafting strategies. Recognizing that technologies are socially constructed, this paper examines prompt engineering through a social lens. We propose reframing prompt engineering as a socio-cultural practice shaped by collective knowledge building. Through qualitative analysis of 19 semi-structured interviews with members of the MidJourney community, a text-to-image generative AI tool, we identify four socio-engagement themes: proprietary/solitary, derivative, collaborative, and provocative prompting. These themes reveal a space of social engagement modes based on personal values and motivations from individual exploration to influencing the prompt community and highlight a fine line between being inspired by others prompts and maintaining creative ownership. We argue that understanding distinct social engagement preferences can inform the design of AI tools to facilitate transparent prompt reuse mechanisms, integrate collaborative features, or preserve ethical concerns about prompt sharing.\\n                                \\n\\n---\\n\\nPaper ID: a1fbf693554d07b23a29951dce5fe19afbd93ad8\\n                                    Title: Generating SPARQL Queries over CIDOC-CRM Using a Two-Stage Ontology Path Patterns Method in LLM Prompts\\n                                    Abstract: In this article, we focus on the task of exploiting the capabilities of Large Language Models (LLMs) to generate SPARQL Queries for answering natural questions over cultural Knowledge Graphs (KGs) expressed according to the ISO standard ontology CIDOC-CRM. Since CIDOC-CRM is an event-based model, usually we have to follow long paths for answering a question, thereby, the challenge is how to construct the prompt for aiding the LLM to produce the right SPARQL query. We propose and comparatively evaluate methods based on the creation of ontology path patterns of a configurable path radius (or length). Then, we construct a new dedicated benchmark that includes 100 natural questions and the corresponding SPARQL queries over two real KGs from the cultural domain describing artworks. Finally, we present comparative results about the effectiveness and efficiency over the benchmark by using ChatGPT-3.5. The most effective method follows a two-stage process that predicts and uses the most appropriate path patterns of \\\\(r\\\\leq 4\\\\) . This method achieves 3.5 \\\\(\\\\times\\\\) higher accuracy than the baseline method (0.66 versus 0.19), that includes in the prompt only the list of properties and classes of the KG. Benchmark: https://github.com/mountanton/CIDOC-QA-using-LLMs\\n                                \\n\\n---\\n\\nPaper ID: 16853f3f5f27b37449f51eb82bc0e8db8f95ca19\\n                                    Title: Prompts, privacy, and personalized learning: integrating AI into nursing educationa qualitative study\\n                                    Abstract: Generative artificial intelligence (GenAI) has emerged as a powerful tool in nursing education, offering novel ways to enhance clinical reasoning, critical thinking, and personalized learning. However, questions remain regarding the ethical use of AI-generated outputs, data privacy concerns, and limitations in recognizing emotional nuances. This study aims to explore how nursing students utilize GenAI tools to develop care plans, with a particular focus on the innovative role of prompt engineering. By identifying both challenges and opportunities, it seeks to provide actionable insights into seamlessly integrating GenAI into nursing education while safeguarding humanistic nursing skills. A qualitative design was adopted, involving semi-structured interviews with third-year undergraduate nursing students at a single institution. Participants worked with anonymized clinical cases and multiple GenAI tools, emphasizing the iterative design of prompts to optimize care-plan outputs. Data were analyzed thematically to capture detailed perspectives on AI-facilitated learning and ethical considerations. Findings indicate that GenAI tools enhanced efficiency and conceptual clarity, allowing students to focus more on higher-order clinical thinking. Prompt engineering significantly improved the accuracy and contextual relevance of AI-generated care plans. However, students expressed concerns about incomplete or imprecise responses, GenAIs limited emotional understanding, and privacy risks associated with sensitive healthcare data. When used with careful prompt refinement and critical evaluation, GenAI was viewed as a valuable supplement rather than a replacement for humanistic nursing competencies. This study highlights the transformative potential of GenAI in nursing education, underscoring the importance of structured prompt engineering and ethical safeguards. By balancing technological innovation with empathy, communication, and cultural sensitivity, nursing educators can harness AI to deepen clinical reasoning and prepare students for future AI-enhanced practice. Further research across diverse settings is needed to validate these findings and refine best practices for integrating GenAI into nursing curricula. Not applicable. This study did not involve a clinical trial.\\n                                \\n\\n---\\n\\nPaper ID: 21e54d78e355adb5f39fc0c5dc127741e2d899db\\n                                    Title: User-LLM: Efficient LLM Contextualization with User Embeddings\\n                                    Abstract: Large language models (LLMs) hold immense potential for personalized AI, but effectively incorporating user history for personalized responses remains challenging. Existing methods often convert user timelines into lengthy text descriptions, leading to high computational cost and potential loss of nuanced information. Inspired by the successful integration of LLMs with other modalities, such as images, we introduce USER-LLM, a novel framework that treats user timelines as a distinct modality and leverages user embeddings for efficient LLM contextualization. User embeddings, generated by a pretrained user encoder, capture latent user behaviors and interests from diverse interaction data. By integrating these embeddings with LLMs through cross-attention, USER-LLM enables LLMs to dynamically adapt their responses to individual user history. Our evaluation on three diverse datasets (MovieLens, Amazon Review, and Google Local Review) demonstrates that User-LLM achieves substantial computation reduction (up to 78.1X) compared to text-prompt-based methods, without sacrificing performance. Importantly, User-LLM maintains or even improves performance on tasks requiring deep user understanding, particularly with long user histories, highlighting its effectiveness in efficiently capturing and leveraging user information for personalized responses.\\n                                \\n\\n---\\n\\nPaper ID: 99129092aeeb63a008da1254f23faa41f14c87ff\\n                                    Title: Beyond Aesthetics: Cultural Competence in Text-to-Image Models\\n                                    Abstract: Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of cultural competence. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to evaluate cultural competence of T2I models. CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of high-quality prompts that enable the evaluation of cultural awareness, and 2) CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to evaluate cultural diversity. We also introduce cultural diversity as a novel T2I evaluation component, leveraging quality-weighted Vendi score. Our evaluations reveal significant gaps in the cultural awareness of existing models across countries and provide valuable insights into the cultural diversity of T2I outputs for under-specified prompts. Our methodology is extendable to other cultural regions and concepts, and can facilitate the development of T2I models that better cater to the global population.\\n                                \\n\\n---\\n\\nPaper ID: 497ccaa586c40991d6c6118c4739ed34b3375ce3\\n                                    Title: Will Artificial Intelligence Affect How Cultural Heritage Will Be Managed in the Future? Responses Generated by Four genAI Models\\n                                    Abstract: Generative artificial intelligence (genAI) language models have become firmly embedded in public consciousness. Their abilities to extract and summarise information from a wide range of sources in their training data have attracted the attention of many scholars. This paper examines how four genAI large language models (ChatGPT, GPT4, DeepAI, and Google Bard) responded to prompts, asking (i) whether artificial intelligence would affect how cultural heritage will be managed in the future (with examples requested) and (ii) what dangers might emerge when relying heavily on genAI to guide cultural heritage professionals in their actions. The genAI systems provided a range of examples, commonly drawing on and extending the status quo. Without a doubt, AI tools will revolutionise the execution of repetitive and mundane tasks, such as the classification of some classes of artifacts, or allow for the predictive modelling of the decay of objects. Important examples were used to assess the purported power of genAI tools to extract, aggregate, and synthesize large volumes of data from multiple sources, as well as their ability to recognise patterns and connections that people may miss. An inherent risk in the results presented by genAI systems is that the presented connections are artifacts of the system rather than being genuine. Since present genAI tools are unable to purposively generate creative or innovative thoughts, it is left to the reader to determine whether any text that is provided by genAI that is out of the ordinary is meaningful or nonsensical. Additional risks identified by the genAI systems were that some cultural heritage professionals might use AI systems without the required level of AI literacy and that overreliance on genAI systems might lead to a deskilling of general heritage practitioners.\\n                                \\n\\n---\\n\\nPaper ID: d676436e28525f4c6078bfb08e5fed62a53bdbfe\\n                                    Title: Case-based MCQ generator: A custom ChatGPT based on published prompts in the literature for automatic item generation\\n                                    Abstract: Abstract What is the Educational Challenge? A fundamental challenge in medical education is creating high-quality, clinically relevant multiple-choice questions (MCQs). ChatGPT-based automatic item generation (AIG) methods need well-designed prompts. However, the use of these prompts is hindered by the time-consuming process of copying and pasting, a lack of know-how among medical teachers, and the generalist nature of standard ChatGPT, which often lacks the medical context. What are the Proposed Solutions? The Case-based MCQ Generator, a custom GPT, addresses these challenges. It has been trained by using GPT Builder, which is a platform designed by OpenAI for customizing ChatGPT to meet specific needs, in order to allow users to generate case-based MCQs. By using this free tool for those who have ChatGPT Plus subscription, health professions educators can easily select a prompt, input a learning objective or item-specific test point, and generate clinically relevant questions. What are the Potential Benefits to a Wider Global Audience? It enhances the efficiency of MCQ generation and ensures the generation of contextually relevant questions, surpassing the capabilities of standard ChatGPT. It streamlines the MCQ creation process by integrating prompts published in medical education literature, eliminating the need for manual prompt input. What are the Next Steps? Future development aims at sustainability and addressing ethical and accessibility issues. It requires regular updates, integration of new prompts from emerging health professions education literature, and a supportive digital ecosystem around the tool. Accessibility, especially for educators in low-resource countries, is vital, demanding alternative access models to overcome financial barriers.\\n                                \\n\\n---\\n\\nPaper ID: ca94c924d8a3b77a2bd5b16ffc03b8723bce9c1f\\n                                    Title: Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study\\n                                    Abstract: The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like conversations. Given its usage by users from various nations and its training on a vast multilingual corpus that includes diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies.\\n                                \\n\\n---\\n\\nPaper ID: bce94fab36ca40661ac940c5c63a427c50baa7c0\\n                                    Title: Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring\\n                                    Abstract: Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM\\'s output through defining its prompt\\'s context. A group chat approach is developed to connect students with human mentors, either on demand or in cases that exceed the chatbot\\'s pre-defined tasks. We evaluate the chatbot with a user study, to provide a proof-of-concept and highlight the potential requirements and limitations of utilizing chatbots in conversational explainability.\\n                                \\n\\n---\\n\\nPaper ID: 372545a5bf4cec7ee4083199ccf4ad428e548fa4\\n                                    Title: HumanComputer Interaction (HCI) Advances to Re-Contextualize Cultural Heritage toward Multiperspectivity, Inclusion, and Sensemaking\\n                                    Abstract: Todays social and political movements against dominant Western narratives call for a re-contextualization of cultural heritage (CH) toward inclusivity, multiperspectivity, and sensemaking. Our work approaches this challenge from a HumanComputer Interaction (HCI) perspective, questioning how HCI approaches, tools and methods can contribute to CH re-contextualization. Through collaborative reflection on our research practice, we identified four diverging case studies highlighting the different roles of HCI and its increasing entanglement with CH. Case studies 13 focus on HCI as a medium for CH, case 4 on digital CH, and thereby on the HCICH entanglement. Our reflections contribute to CH re-contextualization by highlighting the need for co-design and slow design approaches, the role of HCI technologies in preserving, communicating, and shaping CH, and open questions and challenges related to the increasing HCICH convergence.\\n                                \\n\\n---\\n\\nPaper ID: 8b0977400aa0195a96cd79389e9947bcdc48aa1b\\n                                    Title: Partiality and Misconception: Investigating Cultural Representativeness in Text-to-Image Models\\n                                    Abstract: Text-to-image (T2I) models enable users worldwide to create high-definition and realistic images through text prompts, where the underrepresentation and potential misinformation of images have raised growing concerns. However, few existing works examine cultural representativeness, especially involving whether the generated content can fairly and accurately reflect global cultures. Combining automated and human methods, we investigate this issue in multiple dimensions quantificationally and conduct a set of evaluations on three prevailing T2I models (DALL-E v2, Stable Diffusion v1.5 and v2.1). Introducing attributes of cultural cluster and subject, we provide a fresh interdisciplinary perspective to bias analysis. The benchmark dataset UCOGC is presented, which encompasses authentic images of unique cultural objects from global clusters. Our results reveal that the culture of a disadvantaged country is prone to be neglected, some specified subjects often present a stereotype or a simple patchwork of elements, and over half of cultural objects are mispresented.\\n                                \\n\\n---\\n\\nPaper ID: cb9b9c9e3438e3b451290ab3217e8fffb9ee9e33\\n                                    Title: VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data\\n                                    Abstract: Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data. To address the issue, we introduce VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, we present an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, our framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised.\\n                                \\n\\n---\\n\\nPaper ID: 9a55b94a973fd159e93e756398d90a2298eacda3\\n                                    Title: Hey ChatGPT: an examination of ChatGPT prompts in marketing\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 430a95e3cbebddf8d727c874e007d32ab844f148\\n                                    Title: Having Beer after Prayer? Measuring Cultural Bias in Large Language Models\\n                                    Abstract: As the reach of large language models (LMs) expands globally, their ability to cater to diverse cultural contexts becomes crucial. Despite advancements in multilingual capabilities, models are not designed with appropriate cultural nuances. In this paper, we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. We introduce CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities spanning eight types that contrast Arab and Western cultures. CAMeL provides a foundation for measuring cultural biases in LMs through both extrinsic and intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance in Arabic of 16 different LMs on tasks such as story generation, NER, and sentiment analysis, where we find concerning cases of stereotyping and cultural unfairness. We further test their text-infilling performance, revealing the incapability of appropriate adaptation to Arab cultural contexts. Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment. We will make CAMeL publicly available at: https://github.com/tareknaous/camel\\n                                \\n\\n---\\n\\nPaper ID: 66dea8673da3af722d11bcdff767559af7320dbc\\n                                    Title: Enhancing User Interaction in ChatGPT: Characterizing and Consolidating Multiple Prompts for Issue Resolution\\n                                    Abstract: Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the models ability to extract contextually accurate responses. Thus, optimal prompt construction is essential for maximizing the utility and performance of ChatGPT. However, suboptimal prompt design may necessitate iterative refinement, as imprecise or ambiguous instructions can lead to undesired responses from ChatGPT. Existing studies explore several prompt patterns and strategies to improve the relevance of responses generated by ChatGPT. However, the exploration of constraints that necessitate the submission of multiple prompts is still an unmet attempt. In this study, our contributions are twofold. First, we attempt to uncover gaps in prompt design that demand multiple iterations. In particular, we manually analyze 686 prompts that were submitted to resolve issues related to Java and Python programming languages and identify eleven prompt design gaps (e.g., missing specifications). Such gap exploration can enhance the efficacy of single prompts in ChatGPT. Second, we attempt to reproduce the ChatGPT response by consolidating multiple prompts into a single one. We can completely consolidate prompts with four gaps (e.g., missing context) and partially consolidate prompts with three gaps (e.g., additional functionality). Such an effort provides concrete evidence to users to design more optimal prompts mitigating these gaps. Our study findings and evidence can  (a) save users time, (b) reduce costs, and (c) increase user satisfaction.CCS CONCEPTS Large Language Model  Prompt design; Prompt characterization.\\n                                \\n\\n---\\n\\nPaper ID: 9a290f3d364229c04d7ebfa8a87596d6d149e381\\n                                    Title: The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts\\n                                    Abstract: The rapid adoption of online chatbots represents a significant advancement in artificial intelligence. However, this convenience brings considerable privacy concerns, as prompts can inadvertently contain sensitive information exposed to large language models (LLMs). Limited by high computational costs, reduced task usability, and excessive system modifications, previous works based on local deployment, embedding perturbation, and homomorphic encryption are inapplicable to online prompt-based LLM applications. To address these issues, this paper introduces Prompt Privacy Sanitizer (i.e., ProSan), an end-to-end prompt privacy protection framework that can produce anonymized prompts with contextual privacy removed while maintaining task usability and human readability. It can also be seamlessly integrated into the online LLM service pipeline. To achieve high usability and dynamic anonymity, ProSan flexibly adjusts its protection targets and strength based on the importance of the words and the privacy leakage risk of the prompts. Additionally, ProSan is capable of adapting to diverse computational resource conditions, ensuring privacy protection even for mobile devices with limited computing power. Our experiments demonstrate that ProSan effectively removes private information across various tasks, including question answering, text summarization, and code generation, with minimal reduction in task performance.\\n                                \\n\\n---\\n\\nPaper ID: 4070023ec03e09ffaae25be0f715c2dcd76a081f\\n                                    Title: Optimizing Prompts Using In-Context Few-Shot Learning for Text-to-Image Generative Models\\n                                    Abstract: Recently, various text-to-image generative models have been released, demonstrating their ability to generate high-quality synthesized images from text prompts. Despite these advancements, determining the appropriate text prompts to obtain desired images remains challenging. The quality of the synthesized images heavily depends on the user input, making it difficult to achieve consistent and satisfactory results. This limitation has sparked the need for an effective prompt optimization method to generate optimized text prompts automatically for text-to-image generative models. Thus, this study proposes a prompt optimization method that uses in-context few-shot learning in a pretrained language model. The proposed approach aims to generate optimized text prompts to guide the image synthesis process by leveraging the available contextual information in a few text examples. The results revealed that synthesized images using the proposed prompt optimization method achieved a higher performance, at 18% on average, based on an evaluation metric that measures the similarity between the generated images and prompts for generation. The significance of this research lies in its potential to provide a more efficient and automated approach to obtaining high-quality synthesized images. The findings indicate that prompt optimization may offer a promising pathway for text-to-image generative models.\\n                                \\n\\n---\\n\\nPaper ID: 333613799d39af5d9ea880b0ecbeb904d9cc2b51\\n                                    Title: Using Prompts to Guide Large Language Models in Imitating a Real Person\\'s Language Style\\n                                    Abstract: Large language models (LLMs), such as GPT series and Llama series have demonstrated strong capabilities in natural language processing, contextual understanding, and text generation. In recent years, researchers are trying to enhance the abilities of LLMs in performing various tasks, and numerous studies have proved that well-designed prompts can significantly improve the performance of LLMs on these tasks. This study compares the language style imitation ability of three different large language models under the guidance of the same zero-shot prompt. It also involves comparing the imitation ability of the same large language model when guided by three different prompts individually. Additionally, by applying a Tree-of-Thoughts (ToT) Prompting method to Llama 3, a conversational AI with the language style of a real person was created. In this study, three evaluation methods were used to evaluate LLMs and prompts. The results show that Llama 3 performs best at imitating language styles, and that the ToT prompting method is the most effective to guide it in imitating language styles. Using a ToT framework, Llama 3 was guided to interact with users in the language style of a specific individual without altering its core parameters, thereby creating a text-based conversational AI that reflects the language style of the individual.\\n                                \\n\\n---\\n\\nPaper ID: a737c9bf121091bc3ba27dca04e1a0d368312e5f\\n                                    Title: Finding Wholes in the Metaverse: Posthuman Mystics as Agents of Evolutionary Contextualization\\n                                    Abstract: The Metaverse is a pervasive expression of technological culture whose impact will be global. First, through knowledge, then through social, and now through geo-spatial, AI (the foundation of the Metaverse) will connect all entities on Earth through digital means thereby creating a three-dimensional informational and experiential layer across the world dubbed the Metaverse. The Metaverse has four characteristics: augmented reality, lifelogging, mirror worlds, and virtual reality. From the standpoint of Christian cultural engagement, a contextual theology has yet to be developed. In the work that follows, the Metaverse is engaged through a combination of contextualization and wholemaking from the standpoint of posthumanism and mysticism. The study focuses on evolutionary wholemaking as identified by Teilhard/Delio, while being guided by Bevans five (early) models of contextualization. The method of contextual wholemaking enables new ways of seeing, embracing, communing, complexifying, and creating within the four spheres of the Metaverse. After exploring the nature of the Metaverse in the first half of the paper, insights were gathered from the dialogue between contextual theology and culture and discussed in the second half of the paper.\\n                                \\n\\n---\\n\\nPaper ID: 9e726f01e95e5b34b92e1eca22e1c04f1569ac09\\n                                    Title: Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs\\n                                    Abstract: Research on the cultural alignment of Large Language Models (LLMs) has emerged in response to growing interest in understanding representation across diverse stakeholders. Current approaches to evaluating cultural alignment through survey-based assessments that borrow from social science methodologies often overlook systematic robustness checks. We identify and test three assumptions behind current survey-based evaluation methods: (1) Stability: that cultural alignment is a property of LLMs rather than an artifact of evaluation design, (2) Extrapolability: that alignment with one culture on a narrow set of issues predicts alignment with that culture on others, and (3) Steerability: that LLMs can be reliably prompted to represent specific cultural perspectives. Through experiments examining both explicit and implicit preferences of leading LLMs, we find a high level of instability across presentation formats, incoherence between evaluated versus held-out cultural dimensions, and erratic behavior under prompt steering. We show that these inconsistencies can cause the results of an evaluation to be very sensitive to minor variations in methodology. Finally, we demonstrate in a case study on evaluation design that narrow experiments and a selective assessment of evidence can be used to paint an incomplete picture of LLMs cultural alignment properties. Overall, these results highlight significant limitations of current survey-based approaches to evaluating the cultural alignment of LLMs and highlight a need for systematic robustness checks and red-teaming for evaluation results. Data and code are available at https://doi.org/akhan02/cultural-dimension-cover-letters and https://doi.org/ariba-k/llm-cultural-alignment-evaluation, respectively.\\n                                \\n\\n---\\n\\nPaper ID: 5523669e80c3426bd47109706a7b4b0c265f1dc3\\n                                    Title: Developing effective prompts to improve communication with ChatGPT: a formula for higher education stakeholders\\n                                    Abstract: The escalating integration of artificial intelligence (AI) technologies, particularly the widespread use of ChatGPT in higher education, necessitates a profound exploration of effective communication strategies. This paper addresses the critical role of prompt development as a skill essential for university instructors engaging with ChatGPT. While emphasizing the practical implications for higher education, the study introduces a novel two-layered AI prompt formula, considering both components and elements. In methodology, the research synthesizes insights from existing models and proposes a tailored approach for ChatGPT, addressing its unique characteristics and the contextual elements within higher education. The results highlight the formulas flexibility and potential applications in diverse fields, from syllabus planning to assessment. Moreover, the study identifies limitations inherent in ChatGPT, emphasizing the need for instructors to exercise caution in its usage. In conclusion, the paper underscores the evolving landscape of AI in education, envisaging specialized versions of ChatGPT for academic settings and advocating for the proactive adoption of ethical frameworks in the use of AI in higher education. This study serves as a foundational contribution to the discourse on effective AI communication in educational settings.\\n                                \\n\\n---\\n\\nPaper ID: bc6633beccf8700db8af42f77222505916ffa77d\\n                                    Title: Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts\\n                                    Abstract: Visual question answering (VQA) is known as an AI-complete task as it requires understanding, reasoning, and inferring about the vision and the language content. Over the past few years, numerous neural architectures have been suggested for the VQA problem. However, achieving success in zero-shot VQA remains a challenge due to its requirement for advanced generalization and reasoning skills. This study explores the impact of incorporating image captioning as an intermediary process within the VQA pipeline. Specifically, we explore the efficacy of utilizing image captions instead of images and leveraging large language models (LLMs) to establish a zero-shot setting. Since image captioning is the most crucial step in this process, we compare the impact of state-of-the-art image captioning models on VQA performance across various question types in terms of structure and semantics. We propose a straightforward and efficient question-driven image captioning approach within this pipeline to transfer contextual information into the question-answering (QA) model. This method involves extracting keywords from the question, generating a caption for each image-question pair using the keywords, and incorporating the question-driven caption into the LLM prompt. We evaluate the efficacy of using general-purpose and question-driven image captions in the VQA pipeline. Our study highlights the potential of employing image captions and harnessing the capabilities of LLMs to achieve competitive performance on GQA under the zero-shot setting. Our code is available at https://github.com/ovguyo/captions-in-VQA.\\n                                \\n\\n---\\n\\nPaper ID: e7b9c75f488a4db2a45fd8f97e80163f254bfcf5\\n                                    Title: An experimental study of integrating fine-tuned large language models and prompts for enhancing mental health support chatbot system\\n                                    Abstract: Background : Conversational mental healthcare support plays a crucial role in aiding individuals with mental health concerns. Large language models (LLMs) like GBT and BERT show potential in enhancing chat bot-based therapy responses. Despite their potential, there are recognised limitations in directly deploying these LLMs for therapeutic interactions as they are trained in general context and knowledge data. The overarching aim of this study is to integrate the capabilities of both GBT and BERT with the use of specialised mental health dataset methodologies. Its goal is to enhance mental health conversations, limiting the risk and increasing quality. Methods : To achieve these aims, we will review existing chat bot methodologies from rule-based systems to advanced approaches based on cognitive behavioural therapy principles (CBT). The study introduces a unique method which integrates a fine-tuned DialoGBT model along with the real-time capabilities of the ChatGBT 3.5 API. This blended combination aims to leverage the contextual awareness of LLMs and the precision of mental health-focused training. The evaluation involves a case study whereby our hybrid model is compared to traditional and standalone LLM-based chat bots. The performance is assessed using metrics such as perplexity and BLEU (Bilingual Evaluation Understudy) scores, along with subjective evaluations from end-users and mental health carers. Results : Our combined\\n                                \\n\\n---\\n\\nPaper ID: be17a29b0425bd16339290207fd12aa73060ebf1\\n                                    Title: Using large language models to investigate cultural ecosystem services perceptions: A few-shot and prompt method\\n                                    Abstract: None\\n                                \\n\\n---\\n\\nPaper ID: 2e59b431a1b9092acefa7b70c07b741c6891a412\\n                                    Title: Contextualization of Psychological First Aid: An Integrative Literature Review.\\n                                    Abstract: PURPOSE\\nContextualization of psychological first aid (PFA) in different cultural, political, and socioeconomic contexts and in different population groups is essential. This review analyzes the efforts that have been made to contextualize PFA in different parts of the world for different disasters and emergencies.\\n\\n\\nDESIGN\\nIntegrative literature review.\\n\\n\\nMETHODS\\nThe major databases that were searched for related literature published until August 2019 included JBI, MEDLINE, Embase, the Cumulative Index to Nursing and Allied Health Literature (CINAHL), BIOSIS, ISI Web of Knowledge, Scopus, EBSCOhost, and PsycINFO. A total of 17 studies published in peer-reviewed journals were included. The review adhered to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) model, and the 6W3H tool was applied to synthesize the results.\\n\\n\\nFINDINGS\\nPFA has been adapted to various disasters and populations in different countries and regions. The organizations that administer PFA range from community level to national level. Professional or \"outside helpers\" who enter disaster-affected locations include psychologists, fire fighters, social workers, and nurses. \"Inside helpers,\" who live and work in the disaster-affected areas, include HR staff, teachers, and peer emergency personnel. Only a few studies have reported the exact number of first responders who administered PFA. Some studies revised PFA as group based, and a few reported the classification of groups of victims. Notably, all adaptations adhered to the basic principles of PFA, and the time at which PFA was administered ranged from a few days to months after an incident. PFA was conducted on site in all studies. The selection of the location depended on the type of disaster and local situation with due consideration of safety. Only a few studies specified the rationale for revising the PFA. None of these 17 studies reported the cost, cost-benefit, or cost-effectiveness of PFA.\\n\\n\\nCONCLUSIONS\\nPopulation-focused, context-specific, and group-based PFA is emerging worldwide. Nurses are actively playing a role in providing PFA. Research gaps exist in differentiating between the roles played by \"outside\" and \"inside\" responders, considering vulnerable age groups other than children, incorporating the major PFA concepts such as resilience, and evaluating the cost-effectiveness of PFA.\\n\\n\\nCLINICAL RELEVANCE\\nIt is imperative that nurses and other emergency staff consider the intersection of age, gender, cultural, political, social economic, and spiritual contexts when developing a context appropriate PFA.\\n                                \\n\\n---\\n\\nPaper ID: 1deac8b2bbd9fa40e033b3a403629622b436896c\\n                                    Title: Tell me more: integrating LLMs in a cultural heritage website for advanced information exploration support\\n                                    Abstract: Cultural Heritage websites capability to satisfy diverse information needs is limited by their high-quality but constrained knowledge bases. Thus, we investigate their extension with external large language models (LLMs), enriching the provision of cultural content by leveraging LLMs continuous collection and integration of information from heterogeneous data sources. This extension raises important challenges in synchronizing the LLMs behavior with the users browsing activity on the website to offer a unified interaction environment. To address these challenges, we propose a loosely coupled integration model that provides users with curated content and an assisted question-answering function to answer information needs that the systems knowledge base fails to cover. Our model is agnostic to the LLM and synchronizes its behavior with the users browsing activity through implicit prompt engineering. We tested a baseline website without LLM integration, one with free-text interaction with the LLM, and another that combines free-text interaction with the suggestion of context-dependent questions. In a user study involving 44 participants, we found that the LLM-powered website has higher usability and that context-dependent question suggestions further enhance user experience, especially for people with low curiosity levels (according to Curiosity and Exploration Inventory-II - CEI-II) who are guided in formulating effective questions. This shows the potential of LLMs to enrich engagement with existing Cultural Heritage websites.\\n                                \\n\\n---\\n\\nPaper ID: 705288166e516196bbab5998c82cbe3bd609fe19\\n                                    Title: An Evaluation of Cultural Value Alignment in LLM\\n                                    Abstract: LLMs as intelligent agents are being increasingly applied in scenarios where human interactions are involved, leading to a critical concern about whether LLMs are faithful to the variations in culture across regions. Several works have investigated this question in various ways, finding that there are biases present in the cultural representations of LLM outputs. To gain a more comprehensive view, in this work, we conduct the first large-scale evaluation of LLM culture assessing 20 countries\\' cultures and languages across ten LLMs. With a renowned cultural values questionnaire and by carefully analyzing LLM output with human ground truth scores, we thoroughly study LLMs\\' cultural alignment across countries and among individual models. Our findings show that the output over all models represents a moderate cultural middle ground. Given the overall skew, we propose an alignment metric, revealing that the United States is the best-aligned country and GLM-4 has the best ability to align to cultural values. Deeper investigation sheds light on the influence of model origin, prompt language, and value dimensions on cultural output. Specifically, models, regardless of where they originate, align better with the US than they do with China. The conclusions provide insight to how LLMs can be better aligned to various cultures as well as provoke further discussion of the potential for LLMs to propagate cultural bias and the need for more culturally adaptable models.\\n                                \\n\\n---\\n\\nPaper ID: be76594a206c40e8998f7722f8e5f0304e9f17bf\\n                                    Title: Multi-Prompts Learning with Cross-Modal Alignment for Attribute-based Person Re-Identification\\n                                    Abstract: The fine-grained attribute descriptions can significantly supplement the valuable semantic information for person image, which is vital to the success of person re-identification (ReID)\\ntask. However, current ReID algorithms typically failed to effectively leverage the rich contextual information available, primarily due to their reliance on simplistic and coarse utilization of image attributes. Recent advances in artificial intelligence generated content have made it possible to automatically generate plentiful fine-grained attribute descriptions and make full use of them. Thereby, this paper explores the potential of using the generated multiple person attributes as prompts in ReID tasks with off-the-shelf (large) models for more accurate retrieval results. To this end, we present a new framework called Multi-Prompts ReID (MP-ReID), based on prompt learning and language models, to fully dip fine attributes to assist ReID task. Specifically, MP-ReID first learns to hallucinate diverse, informative, and promptable sentences for describing the query images. This procedure includes (i) explicit prompts of which attributes a person has and furthermore (ii) implicit learnable prompts for adjusting/conditioning the criteria used towards this person identity matching. Explicit prompts are obtained by ensembling generation models, such as ChatGPT and VQA models. Moreover, an alignment module is designed to fuse multi-prompts (i.e., explicit and implicit ones) progressively and mitigate the cross-modal gap. Extensive experiments on the existing attribute-involved ReID datasets, namely, Market1501 and DukeMTMC-reID, demonstrate the effectiveness and rationality of the proposed MP-ReID solution.\\n                                \\n\\n---\\n\\nPaper ID: 21510620f0c92dde08741070a00593bcd1815d8c\\n                                    Title: Can LLMs Effectively Leverage Graph Structural Information through Prompts, and Why?\\n                                    Abstract: Large language models (LLMs) are gaining increasing attention for their capability to process graphs with rich text attributes, especially in a zero-shot fashion. Recent studies demonstrate that LLMs obtain decent text classification performance on common text-rich graph benchmarks, and the performance can be improved by appending encoded structural information as natural languages into prompts. We aim to understand why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs. First, we rule out the concern of data leakage by curating a novel leakage-free dataset and conducting a comparative analysis alongside a previously widely-used dataset. Second, as past work usually encodes the ego-graph by describing the graph structure in natural language, we ask the question: do LLMs understand the graph structure in accordance with the intent of the prompt designers? Third, we investigate why LLMs can improve their performance after incorporating structural information. Our exploration of these questions reveals that (i) there is no substantial evidence that the performance of LLMs is significantly attributed to data leakage; (ii) instead of understanding prompts as graph structures as intended by the prompt designers, LLMs tend to process prompts more as contextual paragraphs and (iii) the most efficient elements of the local neighborhood included in the prompt are phrases that are pertinent to the node label, rather than the graph structure.\\n                                \\n\\n---\\n\\nPaper ID: 1f79ec669e3b6701c814d0165ad281796a49bd13\\n                                    Title: Contextualized Soft Prompts for Extraction of Event Arguments\\n                                    Abstract: Event argument extraction (EAE) is a sub-task of event extraction where the goal is to identify roles of entity mentions for events in text. The current state-of-the-art approaches for this problem explore prompt-based meth-ods to prompt pre-trained language models for arguments over input context. However, existing prompt-based methods mainly rely on discrete and manually-designed prompts that cannot exploit specific context for each example to improve customization for optimal performance. In addition, the discrete nature of current prompts prevents the incorporation of relevant context from multiple external documents to enrich prompts for EAE. To this end, we propose a novel prompt-based method for EAE that introduces soft prompts to facilitate the encoding of individual example context and multiple relevant documents to boost EAE. We extensively evaluate the proposed method on benchmark datasets for EAE to demonstrate its benefits with state-of-the-art performance.\\n                                \\n\\n---\\n\\nPaper ID: e21713858033be63e7243d4bf20e5262dba60eb2\\n                                    Title: Spatially Covariant Image Registration With Text Prompts\\n                                    Abstract: Medical images are often characterized by their structured anatomical representations and spatially inhomogeneous contrasts. Leveraging anatomical priors in neural networks can greatly enhance their utility in resource-constrained clinical settings. Prior research has harnessed such information for image segmentation, yet progress in deformable image registration has been modest. Our work introduces textSCF, a novel method that integrates spatially covariant filters and textual anatomical prompts encoded by visual-language models, to fill this gap. This approach optimizes an implicit function that correlates text embeddings of anatomical regions to filter weights. textSCF not only boosts computational efficiency but can also retain or improve registration accuracy. By capturing the contextual interplay between anatomical regions, it offers impressive interregional transferability and the ability to preserve structural discontinuities during registration. textSCFs performance has been rigorously tested on intersubject brain magnetic resonance imaging (MRI) and abdominal computerized tomography (CT) registration tasks, outperforming existing state-of-the-art models in the MICCAI Learn2Reg 2021 challenge and leading the leaderboard. In abdominal registrations, textSCFs larger model variant improved the Dice score by 11.3% over the second-best model, while its smaller variant maintained similar accuracy but with an 89.13% reduction in network parameters and a 98.34% decrease in computational operations.\\n                                \\n\\n---\\n\\nPaper ID: 37108b0331deac85bfc87b3f784d8b882808466b\\n                                    Title: GeoSAM: Fine-tuning SAM with Multi-Modal Prompts for Mobility Infrastructure Segmentation\\n                                    Abstract: In geographical image segmentation, performance is often constrained by the limited availability of training data and a lack of generalizability, particularly for segmenting mobility infrastructure such as roads, sidewalks, and crosswalks. Vision foundation models like the Segment Anything Model (SAM), pre-trained on millions of natural images, have demonstrated impressive zero-shot segmentation performance, providing a potential solution. However, SAM struggles with geographical images, such as aerial and satellite imagery, due to its training being confined to natural images and the narrow features and textures of these objects blending into their surroundings. To address these challenges, we propose Geographical SAM (GeoSAM), a SAM-based framework that fine-tunes SAM using automatically generated multi-modal prompts. Specifically, GeoSAM integrates point prompts from a pre-trained task-specific model as primary visual guidance, and text prompts generated by a large language model as secondary semantic guidance, enabling the model to better capture both spatial structure and contextual meaning. GeoSAM outperforms existing approaches for mobility infrastructure segmentation in both familiar and completely unseen regions by at least 5\\\\% in mIoU, representing a significant leap in leveraging foundation models to segment mobility infrastructure, including both road and pedestrian infrastructure in geographical images. The source code can be found in this GitHub Repository: https://github.com/rafiibnsultan/GeoSAM.\\n                                \\n\\n---\\n\\nPaper ID: b1890367317f0657c08ed96be4c474035b34b485\\n                                    Title: Investigating Cultural Alignment of Large Language Models\\n                                    Abstract: The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.\\n                                \\n\\n---\\n\\nPaper ID: 1073c29b650815db80fe2804ab84338da967a7a9\\n                                    Title: CReBot: Exploring interactive question prompts for critical paper reading\\n                                    Abstract: Pre-compiled guidelines with a static question list can stimulate critical thinking while reading a scientific paper. However, they could be less engaging than taking live question prompts from others. In this paper, we develop CReBot that interactively asks section-level critical thinking questions and customize it for routine paper readers with prior research experience and novices new to research. Our first within-subjects study with 24 routine readers demonstrates CReBots engagement and usefulness over static guidelines. Then, with more teacher-like question-specific hints prepared for CReBot, we conduct another within-subjects study with 20 novices. The results, however, indicate that CReBot might not be better than static guidelines for beginners. Nevertheless, both user groups favor CReBots contextualized questions and interaction flexibility. We conclude with design implications for interactive tools to facilitate critical reading.\\n                                \\n\\n---\\n\\nPaper ID: 719dd2569f75f8a9a7c566b2c4fd75d601488948\\n                                    Title: Cultural and Contextual Adaptation of Digital Health Interventions: Narrative Review\\n                                    Abstract: Background Emerging evidence suggests that positive impacts can be generated when digital health interventions are designed to be responsive to the cultural and socioeconomic context of their intended audiences. Objective This narrative review aims to synthesize the literature about the cultural adaptation of digital health interventions. It examines how concepts of culture and context feature in design and development processes, including the methods, models, and content of these interventions, with the aim of helping researchers to make informed decisions about how to approach cultural adaptation in digital health. Methods Literature searches for this narrative review were conducted across 4 databases. Following full-text article screening by 2 authors, 16 studies of interventions predominantly focused on the self-management of health were selected based on their detailed focus on the process of cultural adaptation. Key considerations for cultural adaptation were identified and synthesized through a qualitative narrative approach, enabling an integrative and in-depth understanding of cultural adaptation. Results The literature demonstrates varying approaches and levels of cultural adaptation across stages of intervention development, involving considerations such as the research ethos orienting researchers, the methodologies and models used, and the resultant content adaptations. In relation to the latter, culturally appropriate and accessible user interface design and translation can be seen as particularly important in shaping the level of adaptation. Conclusions Optimizing cultural adaptation involves linking culture with other contextual factors such as economic conditions and social systems to ensure accessibility and the sustained use of digital health interventions. Culturally humble approaches that use the involvement of a broad range of participants, experts, and other stakeholders are demonstrated to spark vital insights for content development, implementation, and evaluation.\\n                                \\n\\n---\\n\\nPaper ID: 2ef23e8b3c07885a850164d86ba748303e335377\\n                                    Title: CultureLLM: Incorporating Cultural Differences into Large Language Models\\n                                    Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM significantly outperforms various counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with comparable performance to GPT-4 or even better. Our human study shows that the generated samples are semantically equivalent to the original samples, providing an effective solution for LLMs augmentation. Code is released at https://github.com/Scarelette/CultureLLM.\\n                                \\n\\n---\\n\\nPaper ID: 023a98af94a3e7e8e538a6183da8ec05024fec56\\n                                    Title: How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\\n                                    Abstract: Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user\\'s known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs\\'cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.\\n                                \\n\\n---\\n\\nPaper ID: 3daa3f34dbd43776ad41df533002dbdc2edecf70\\n                                    Title: Evaluating Cultural and Social Awareness of LLM Web Agents\\n                                    Abstract: As large language models (LLMs) expand into performing as agents for real-world applications beyond traditional NLP tasks, evaluating their robustness becomes increasingly important. However, existing benchmarks often overlook critical dimensions like cultural and social awareness. To address these, we introduce CASA, a benchmark designed to assess LLM agents\\' sensitivity to cultural and social norms across two web-based tasks: online shopping and social discussion forums. Our approach evaluates LLM agents\\' ability to detect and appropriately respond to norm-violating user queries and observations. Furthermore, we propose a comprehensive evaluation framework that measures awareness coverage, helpfulness in managing user queries, and the violation rate when facing misleading web content. Experiments show that current LLMs perform significantly better in non-agent than in web-based agent environments, with agents achieving less than 10% awareness coverage and over 40% violation rates. To improve performance, we explore two methods: prompting and fine-tuning, and find that combining both methods can offer complementary advantages -- fine-tuning on culture-specific datasets significantly enhances the agents\\' ability to generalize across different regions, while prompting boosts the agents\\' ability to navigate complex tasks. These findings highlight the importance of constantly benchmarking LLM agents\\' cultural and social awareness during the development cycle.\\n                                \\n\\n---\\n\\nPaper ID: c39c21f9cd9a85809cbc0dfc0ed6b004e6c8ef8b\\n                                    Title: Understanding Public Perceptions of AI Conversational Agents: A Cross-Cultural Analysis\\n                                    Abstract: Conversational Agents (CAs) have increasingly been integrated into everyday life, sparking significant discussions on social media. While previous research has examined public perceptions of AI in general, there is a notable lack in research focused on CAs, with fewer investigations into cultural variations in CA perceptions. To address this gap, this study used computational methods to analyze about one million social media discussions surrounding CAs and compared peoples discourses and perceptions of CAs in the US and China. We find Chinese participants tended to view CAs hedonically, perceived voice-based and physically embodied CAs as warmer and more competent, and generally expressed positive emotions. In contrat, US participants saw CAs more functionally, with an ambivalent attitude. Warm perception was a key driver of positive emotions toward CAs in both countries. We discussed practical implications for designing contextually sensitive and user-centric CAs to resonate with various users preferences and needs.\\n                                \\n\\n---\\n\\nPaper ID: 45b16f4bd9b5b7104c229c931b99d0813da2c863\\n                                    Title: Timestamps as Prompts for Geography-Aware Location Recommendation\\n                                    Abstract: Location recommendation plays a vital role in improving users\\' travel experience. The timestamp of the POI to be predicted is of great significance, since a user will go to different places at different times. However, most existing methods either do not use this kind of temporal information, or just implicitly fuse it with other contextual information. In this paper, we revisit the problem of location recommendation and point out that explicitly modeling temporal information is a great help when the model needs to predict not only the next location but also further locations. In addition, state-of-the-art methods do not make effective use of geographic information and suffer from the hard boundary problem when encoding geographic information by gridding. To this end, a Temporal Prompt-based and Geography-aware (TPG) framework is proposed. The temporal prompt is firstly designed to incorporate temporal information of any further check-in. A shifted window mechanism is then devised to augment geographic data for addressing the hard boundary problem. Via extensive comparisons with existing methods and ablation studies on five real-world datasets, we demonstrate the effectiveness and superiority of the proposed method under various settings. Most importantly, our proposed model has the superior ability of interval prediction. In particular, the model can predict the location that a user wants to go to at a certain time while the most recent check-in behavioral data is masked, or it can predict specific future check-in (not just the next one) at a given timestamp.\\n                                \\n\\n---\\n\\nPaper ID: ba73f425dc7e815991f9b547227c7a674e9e000a\\n                                    Title: Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas\\n                                    Abstract: The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users\\' diverse cultural backgrounds. We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment. Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits. Our analysis shows that specifying a person\\'s country of residence improves GPT-3.5\\'s alignment with their responses. In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance. These findings suggest that while direct nationality information enhances the model\\'s cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model\\'s effectiveness.\\n                                \\n\\n---\\n\\nPaper ID: 3fe351b19fb9d35f9d947cd93b5a378631ca3388\\n                                    Title: Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization\\n                                    Abstract: A key challenge of continual reinforcement learning (CRL) in dynamic environments is to promptly adapt the reinforcement learning (RL) agents behavior as the environment changes over its lifetime while minimizing the catastrophic forgetting of the learned information. To address this challenge, in this article, we propose DaCoRL, that is, dynamics-adaptive continual RL. DaCoRL learns a context-conditioned policy using progressive contextualization, which incrementally clusters a stream of stationary tasks in the dynamic environment into a series of contexts and opts for an expandable multihead neural network to approximate the policy. Specifically, we define a set of tasks with similar dynamics as an environmental context and formalize context inference as a procedure of online Bayesian infinite Gaussian mixture clustering on environment features, resorting to online Bayesian inference to infer the posterior distribution over contexts. Under the assumption of a Chinese restaurant process (CRP) prior, this technique can accurately classify the current task as a previously seen context or instantiate a new context as needed without relying on any external indicator to signal environmental changes in advance. Furthermore, we employ an expandable multihead neural network whose output layer is synchronously expanded with the newly instantiated context and a knowledge distillation regularization term for retaining the performance on learned tasks. As a general framework that can be coupled with various deep RL algorithms, DaCoRL features consistent superiority over existing methods in terms of stability, overall performance, and generalization ability, as verified by extensive experiments on several robot navigation and MuJoCo locomotion tasks.\\n                                ',\n",
       " 'messages': [AIMessage(content='Advocate: Executive summary\\n- The proposed research idea introduces a novel two-agent prompting framework for reducing social biases and stereotypes in large language models: a Proposer persona that generates diverse debiasing prompts from multiple cultural perspectives, and a Reviser persona that critiques and refines model outputs to root out biased or stereotypical content. Outputs are explicitly evaluated and improved through iterative prompting and self-verification.\\n- This approach is distinct from prior single-prompt debiasing or static alignment methods. It leverages multi-agent prompting, cross-cultural perspectives, and verification loops to produce fairer, more culturally aware generations.\\n- The work builds on and extends several strong, related strands in the literature: explicit bias/prompting for sensitive content (PromptHate), evolution-aware prompting for social dynamics (Evolver), culture-aware prompting and alignment (CultureLLM, CultureGEN, See It from My Perspective, CultureLLM-One), norm-grounded prompting and self-verification (NormSAGE), and cross-cultural evaluation frameworks (CASA, CAMeL, evaluating cultural alignment). Together these provide a solid foundation for a robust, evidence-based debiasing methodology and evaluation protocol.\\n- The potential impact is high: fairer AI interactions across languages and cultures, reduced propagation of stereotypes, improved user trust, and broader societal benefitespecially in education, public information, health, and governance contexts. Risks (e.g., introducing new prompts that unintentionally bias) can be mitigated by rigorous evaluation (CASA), cross-cultural testing (CAMeL, See It from My Perspective), and an explicit self-verification loop (NormSAGE).\\n\\nKey unique contributions\\n- Two-agent prompting with role separation\\n  - Proposer: generates a broad set of culturally diverse debiasing prompts, drawing on multiple cultural perspectives and normative frameworks.\\n  - Reviser: critiques and revises outputs from the model, flagging and correcting biased or stereotypical content before it reaches the user. This mirrors the multi-stage, reflective prompting paradigm that has shown promise in reducing bias and improving interpretability (for example, chain-of-evolution style prompting and self-check mechanisms in related work).\\n- Cross-cultural prompt design and verification\\n  - The Proposer leverages cross-cultural prompting idioms and normative knowledge to surface biases that may be invisible to monolingual or monocultural prompts, aligning with insights from CultureLLM and related culture-focused prompting work.\\n  - The Reviser enacts self-verification-style checks to confirm that outputs respect cross-cultural norms and do not rely on stereotypes, echoing NormSAGEs self-verification approach for norm-grounded reasoning.\\n- Iterative, evolution-aware debiasing\\n  - The framework can incorporate evolution-aware prompting (akin to Evolver) to adapt prompts as social memes and biases evolve, ensuring long-term robustness against shifting biases and stereotypes.\\n- Grounded evaluation strategy\\n  - An explicit evaluation pipeline drawing on CASA (cultural/social awareness of LLMs in web contexts) and CAMeL (cultural bias resources in Arabic contexts) to quantify cross-cultural bias and alignment, plus culture-aware robustness tests (e.g., See It from My Perspective) to diagnose perspective biases.\\n\\nHow it improves upon existing methods (with references)\\n- PromptHate shows that carefully crafted prompts can elicit strong, contextually informed judgments in hateful content tasks, outperforming baselines. Our Proposer/Reviser framework generalizes this idea to broad social biases by enabling a family of prompts that cover a wider cultural space and can be systematically verified (PromptHate as a blueprint for leveraging prompted reasoning to surface nuanced biases) [ae766548699f27e669932de14e1c0f47b2828536].\\n- Evolver demonstrates that chain-of-evolution prompting can guide LMMs to reason through evolving social memes, providing a mechanism for the Reviser to operate on multi-step, context-aware critiques rather than single-shot judgments. This supports robust bias detection and remediation over time [465239ff11b96a3e44ff7fdf457c63a477d5a1b6].\\n- See It from My Perspective and CultureLLM/CultureGEN show that cultural bias and cultural alignment in LLMs are real and tractable via perspective-taking, culture-aware prompts, and targeted data augmentation. Our approach formalizes this into a reusable, two-agent workflow that systematically explores multiple cultures and verifies outputs against cross-cultural norms [70530e508fcd981d3b3ed6dd29cb689930a1f5e5], [2e59b431a1b9092acefa7b70c07b741c6891a412], [430a95e3cbebddf8d727c874e007d32ab844f148], [42b6852b2e5e8e50687a6aa34fb4655e413caa0c], [66 or similar culture references].\\n- NormSAGE shows that asking targeted questions about norms and self-verification can dramatically improve ground-truth alignment in multi-lingual, multi-cultural norm discovery. Embedding a self-verification loop in the Reviser aligns with NormSAGEs strengths and provides a principled way to constrain outputs to culturally grounded norms [60cc0b1a573e75732d54ac688b84b7bd4a021b89].\\n- CASA and CAMeL provide robust, multi-country and multilingual evaluation frameworks for cultural and social alignment of LLMs, essential for validating the proposed two-agent debiasing approach across languages and cultural contexts [3daa3f34dbd43776ad41df533002dbdc2edecf70], [430a95e3cbebddf8d727c874e007d32ab844f148].\\n- VLMGuard and related safety-focused prompting work demonstrate that detection and mitigation of problematic prompts can be achieved without heavy labeling, supporting the feasibility of a classification-like Reviser component that flags biased or harmful outputs using unlabeled signals and internal checks [cb9b9c9e3438e3b451290ab3217e8fffb9ee9e33].\\n\\nWhy the potential impact justifies the risks\\n- Why it matters\\n  - Language models increasingly influence information access, education, health, and civic discourse. Social biases and stereotypes baked into LLM outputs can cause real-world harm, reinforce discrimination, and erode trust. The literature shows biases are pervasive across languages and cultures, not just English-language models, necessitating robust, cross-cultural debiasing strategies (See It from My Perspective; CAMeL; CultureLLM; Evaluating Cultural Alignment) [70530e508fcd981d3b3ed6dd29cb689930a1f5e5], [430a95e3cbebddf8d727c874e007d32ab844f148], [2e59b431a1b9092acefa7b70c07b741c6891a412], [705288166e516196bbab5998c82cbe3bd609fe19].\\n- Why the proposed approach offers a superior path\\n  - A two-agent prompting loop introduces explicit role separation of ideation (Proposer) and critique/verification (Reviser), which is more thorough than single-prompt debiasing and aligns with evidence that multi-perspective prompts reveal biases that single prompts miss (PromptHate; Evolver; CultureLLM; CASA) [ae766548699f27e669932de14e1c0f47b2828536], [465239ff11b96a3e44ff7fdf457c63a477d5a1b6], [2e59b431a1b9092acefa7b70c07b741c6891a412], [3daa3f34dbd43776ad41df533002dbdc2edecf70].\\n  - The approach is adaptable to evolving social biases (Evolver-style chain-of-prompt reasoning) and can be augmented with self-verification to reduce hallucinated or ungrounded bias claims (NormSAGE). This provides long-term resilience against shifts in social discourse and biases [465239ff11b96a3e44ff7fdf457c63a477d5a1b6], [60cc0b1a573e75732d54ac688b84b7bd4a021b89].\\n  - The framework can be evaluated across languages and cultures using established benchmarks and datasets for cultural alignment and fairness (CASA, CAMeL, CultureLLM, 20-country alignment study) to ensure broad applicability and accountability [3daa3f34dbd43776ad41df533002dbdc2edecf70], [430a95e3cbebddf8d727c874e007d32ab844f148], [705288166e516196bbab5998c82cbe3bd609fe19].\\n- Risk management and mitigation\\n  - Potential risk: prompts themselves could introduce new biases or conflicts between Proposer and Reviser. Mitigation: constrain prompts with explicit diversity goals, implement pre-registration of bias-check criteria, and continuously monitor outputs with CASA/CAMeL-style evaluation; use self-verification loops to catch inconsistencies (NormSAGE, CASA) [60cc0b1a573e75732d54ac688b84b7bd4a021b89], [3daa3f34dbd43776ad41df533002dbdc2edecf70], [430a95e3cbebddf8d727c874e007d32ab844f148].\\n  - Potential risk: increased compute from running multiple prompts and revisions. Mitigation: adopt efficient prompting strategies (kNN Prompting for calibration-free scaling; structured prompting to handle many exemplars efficiently) to keep costs reasonable while preserving debiasing quality [3ec5f0da304a606c5989de5b00e1246ee64b3e46], [eudab3fe337f... not exact, but reference to Structured Prompting].\\n  - Potential risk: cultural risk if the Proposer-reviser set overfits to certain cultures. Mitigation: use broad, multilingual prompts; evaluate with diverse cultural benchmarks (CASA, CAMeL, CultureGEN, CultureLLM) and ensure prompts are culturally inclusive and tested against multiple languages and regions [3daa3f34dbd43776ad41df533002dbdc2edecf70], [430a95e3cbebddf8d727c874e007d32ab844f148], [2e59b431a1b9092acefa7b70c07b741c6891a412], [bce94fab36ca40661ac940c5c63a427c50baa7c0].\\n\\nConcrete research plan (high level)\\n- Develop Proposer and Reviser personas with clearly defined cultural vantage points and debiasing objectives.\\n- Build a diverse prompt bank (e.g., cultural values prompts, stereotype probes, context counters, normative prompts) informed by CultureLLM, CASA, CAMeL, and See It from My Perspective.\\n- Implement iterative prompting loops: Proposer generates prompts, model outputs are evaluated by Reviser, outputs are revised, and a final post-processed output is produced.\\n- Integrate a self-verification stage (inspired by NormSAGE) to check outputs against explicit cross-cultural norms and grounding sources.\\n- Evaluate using multi-language, multi-cultural benchmarks:\\n  - Cross-cultural alignment and bias metrics from CAMeL and CultureLLM-style studies.\\n  - CASA-based evaluation of agent-level cultural and social awareness in web-like tasks.\\n  - Additional human-grounded evaluations across countries/languages to assess stereotype reduction.\\n- Compare against strong baselines: single-prompt debiasing, culture-focused fine-tuning, and existing debiasing prompts (e.g., CultureLLM, See It from My Perspective, PromptHate).\\n- Explore extensions with evolvable prompts (Evolver-style CoE prompting) to maintain robustness as social biases evolve.\\n\\nWhat this means for the field\\n- A practical, scalable method to reduce social biases that explicitly accounts for cross-cultural variability.\\n- A framework that can adapt over time to shifting societal norms and meme-like patterns of bias, while maintaining verifiable grounding in norms and context.\\n- An evidence-based, evaluable approach that leverages existing, peer-reviewed findings on culture-aware prompting, norm grounding, and cross-cultural evaluation to produce safer, fairer LLM outputs.\\n\\nRepresentative supporting literature (selected)\\n- Bias/debiasing via prompting and prompts-driven reasoning\\n  - PromptHate: simple prompts plus in-context examples yield strong hateful meme classification performance (AUC ~90.96) [ae766548699f27e669932de14e1c0f47b2828536].\\n  - Evolver: chain-of-evolution prompting enables LMMs to reason about evolving memes, improving hateful meme detection and adding interpretability [465239ff11b96a3e44ff7fdf457c63a477d5a1b6].\\n  - See It from My Perspective: diagnosing Western cultural bias in LLMs in image understanding; highlights need for perspective-aware prompting [70530e508fcd981d3b3ed6dd29cb689930a1f5e5].\\n- Culture-aware prompting and alignment\\n  - CultureLLM and CultureGEN: incorporating cultural differences via data augmentation or prompting to improve cultural alignment and coverage; demonstrates benefits and limitations of culture-aware LLMs [2e59b431a1b9092acefa7b70c07b741c6891a412], [41? see CultureGEN].\\n  - CultureLLM-One and related approaches show cost-effective, culture-sensitive model variants; relevant for scalable debiasing with limited resources [2e59b431a1b9092acefa7b70c07b741c6891a412].\\n  - Investigating Cultural Alignment of LLMs: large-scale country- and language-aware evaluation; demonstrates the value of cross-cultural benchmarks and the need for robust prompting strategies [705288166e516196bbab5998c82cbe3bd609fe19].\\n- Norm-grounded and self-verifying prompting\\n  - NormSAGE: framework for on-the-fly norm discovery with self-verification; demonstrates high grounding reliability in culturally grounded norms [60cc0b1a573e75732d54ac688b84b7bd4a021b89].\\n- Cross-cultural evaluation and safety in prompts\\n  - CASA: benchmark for cultural and social awareness in web agents; provides an evaluation framework for model behavior in cross-cultural contexts [3daa3f34dbd43776ad41df533002dbdc2edecf70].\\n  - CAMeL: resource for evaluating cultural biases in Arabic LLMs; demonstrates the importance of multilingual and cross-cultural evaluation [Paper 430a95e3cbebddf8d727c874e007d32ab844f148].\\n\\nIn sum\\n- The proposed idea offers a novel, principled, and scalable path to mitigate social biases in LLMs by leveraging a dual-role prompting workflow, grounded in a rich body of evidence from recent, peer-reviewed work on culture-aware prompting, iterative reasoning, norm grounding, and cross-cultural evaluation.\\n- It promises substantial practical impact by producing fairer, more contextually appropriate outputs across languages and cultures, while also providing a rigorous framework to monitor, validate, and adapt debiasing as society evolves. The approach is well-supported by the referenced literature and aligns with the current trajectory of research in culturally aware, trustworthy AI.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Skeptic: Critical counter-argument (evidence-based)\\n\\nExecutive takeaway\\n- The proposed idea of a two-agent prompting framework (Proposer + Reviser) to reduce social biases in LLMs is broadly incremental and has substantial overlaps with a large body of prior work. It mixes well-studied components (culture-aware prompting, self-verification, multi-turn prompting, and norm-grounding) rather than delivering a clearly novel core mechanism or durable improvement. Moreover, several technical and evaluative challenges threaten the effectiveness, scalability, and reliability of the approach as claimed.\\n\\n1) Overlaps with existing work (cite specific papers)\\n\\n- Culture-aware prompting and alignment\\n  - CultureLLM and CultureGEN show that surfacing and leveraging cultural differences via prompting or targeted data can improve cross-cultural alignment, but also reveal limits and instability across languages and cultures. This directly undercuts the novelty of a generic two-agent debiasing loop as a single-module improvement. Key references: CultureLLM (2e59b431a1b9092acefa7b70c07b741c6891a412), CultureGEN (CULTURE-GEN paper; 42b6852b2e5e8e50687a6aa34fb4655e413caa0c), See It from My Perspective (70530e508fcd981d3b3ed6dd29cb689930a1f5e5).\\n  - Large-scale evaluation of cultural alignment and cross-cultural prompts shows non-trivial variability and steerability risks, challenging the promise of robust debiasing from prompting alone. Key reference: What influences the cultural alignment of LLMs / Evaluating Cultural Alignment (705288166e516196bbab5998c82cbe3bd609fe19), and the broader cautionary work on alignment evaluation (Randomness, Not Representation) (9e726f01e95e5b34b92e1eca22e1c04f1569ac09).\\n\\n- Prompting for bias reduction and safe outputs\\n  - PromptHate demonstrates that carefully designed prompts plus in-context examples can yield strong performance on a sensitive classification task, illustrating the power and fragility of prompt-based debiasing. Evolver extends this by using chain-of-evolution prompting to address evolving memes. The Proposer/Reviser idea can be viewed as a generalization of these prompting patterns, not a fundamentally new mechanism. Key references: PromptHate (ae766548699f27e669932de14e1c0f47b2828536), Evolver (465239ff11b96a3e44ff7fdf457c63a477d5a1b6).\\n\\n- Norm-grounded/self-verification and contextual prompting\\n  - NormSAGE shows how norm discovery and self-verification can ground model outputs in culturally anchored norms. A Reviser that incorporates self-verification would not be novel in itself, given this lineage. Key reference: NormSAGE (60cc0b1a573e75732d54ac688b84b7bd4a021b89).\\n  - See It from My Perspective and related prompts demonstrate that framing outputs from alternative cultural standpoints can surface biases; your Proposer seems to operationalize this as a reusable two-agent workflow rather than presenting a new theoretical insight. Key reference: See It from My Perspective (70530e508fcd981d3b3ed6dd29cb689930a1f5e5).\\n\\n- Evaluation frameworks and cross-cultural datasets\\n  - CASA (Cultural and Social Awareness) and CAMeL provide concrete benchmarks for assessing cultural and social awareness/bias in LLMs; any debiasing method must be evaluated against these standards to claim robustness. The proposed plan aligns with these evaluation paradigms but relies on them as a baseline rather than delivering a new, independently verifiable evaluation framework. Key references: CASA (3daa3f34dbd43776ad41df533002dbdc2edecf70), CAMeL (430a95e3cbebddf8d727c874e007d32ab844f148).\\n\\n- Prompting techniques with multi-agent or multi-perspective flavor\\n  - While the exact Proposer/Reviser architecture is not widely published as a named two-agent prompting system, several papers explore multi-perspective prompting and iterative/reflective prompting (e.g., SKiC, CoE prompting, Debating/structured prompts). The field already contains many cross-cultural and normative prompting patterns that your idea would repackage rather than innovate. Representative works: SKiC (Skills-in-Context Prompting), CoE prompting (Evolutionary prompting with stepwise reasoning), structured prompts (Structured Prompting: Scaling ICL) and related reflective prompting papers; see Skills-in-Context Prompting (447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e), PromptHate/Evolver cited above, and Structured Prompting (eecb45aa040064cbc0b37fd100706c02e7dc880e).\\n\\nBottom line on overlaps: your idea uses a combination of established elements (culture-aware prompting, self-verification, iterative prompting) that have been explored separately in depth; the novelty of a dual-role Proposer/Reviser architecture is not clearly supported by the literature you cite, and the design space for truly novel contributions here is already crowded.\\n\\n2) Potential technical challenges or flaws\\n\\n- Evaluation fragility and measurement challenges\\n  - Cultural alignment is notoriously unstable to prompt phrasing, language, and model idiosyncrasies. The literature shows high variability and steerability; small changes in prompts can swing judgments, bias classifications, or perceived fairness. This undermines claims of robust debiasing and complicates replication across models and languages. Key reference: Randomness, Not Representation (9e726f01e95e...), and the broader critique of survey-based cultural alignment methods (_randomness/robustness concerns cited in 9e726f and related CASA/Camel work).\\n\\n- Scale, efficiency, and latency\\n  - A two-agent loop multiplies prompt generations, evaluations, and revisions. Even with efficiency techniques (Structured Prompting, kNN Prompting, DynaICL, etc.), the approach could incur prohibitive compute and latency costs in production, especially for multilingual/cross-cultural deployment. See Efficient Prompting via Dynamic In-Context Learning (8ce6ad6d8a73757309d3b9f525cf15cb68e32397) and Structured Prompting (eecb45aa040064cbc0b37fd100706c02e7dc880e) as evidence that scaling prompt supervision is non-trivial and costly.\\n\\n- Cultural nuance, overfitting, and stereotyping risk\\n  - Even with a Proposer, the set of prompts can encode dominant cultural frames and stereotypes unless carefully curated. Over-correction or homogenization across cultures is a real risk, given that many culture-focused prompts themselves rely on generalized cultural stereotypes or skewed datasets. The CAMeL and CultureLLM lines show both promise and fragility in cross-cultural prompts; overreliance on a fixed prompt bank can entrench biases rather than mitigate them.\\n\\n- Grounding and provenance of debiasing signals\\n  - A core weakness of prompting-based debiasing is that it often lacks durable grounding beyond the prompt surface. NormSAGE and related self-verification approaches address grounding, but the proposed framework must specify how the Revisers checks are anchored to external normative sources, evidence, or culturally diverse stakeholder input. Without explicit grounding, you risk superficial debiasing that vanishes under distribution shifts or adversarial prompts.\\n\\n- Potential for prompt-induced harms or new biases\\n  - A Proposer could surface prompts that themselves introduce new biases or misrepresent cultures. Without careful guardrails and continuous auditing (as emphasized by CASA and CAMeL), you may swap one set of biases for another. The risk of cascading bias in multi-agent prompting is non-trivial and underlines the need for external audits.\\n\\n- Data and language coverage gaps\\n  - Cross-cultural debiasing requires broad, multilingual coverage. The literature notes gaps and uneven performance across languages (CultureLLM, CAMeL, CAMeL-like studies). If your Proposer threads prompts primarily through high-resource languages or Western-centric frames, you will undercut the intended cross-cultural fairness gains.\\n\\n3) Why the idea might not be as novel or impactful as claimed\\n\\n- Not a cleanly novel architectural concept\\n  - A two-role prompting loop (Proposer + Reviser) is not, by itself, a new architectural idea. The field has explored multi-turn, multi-perspective, and reflective prompting, as well as explicit norm-grounding (NormSAGE), cross-cultural prompting (CultureLLM, See It from My Perspective, CAMeL), and iterative debiasing workflows (Evolver-style chain-of-evolution prompting). The proposed architecture appears to repackage these into a dual-role workflow rather than deliver a systems-level novelty with a proven empirical uplift. See PromptHate/Evolver for the closest conceptual lineage, and NormSAGE for self-verification grounding.\\n\\n- Impact depends on durable gains, which remain uncertain\\n  - The literature consistently shows that prompting-based debiasing and culture-aware prompting can yield improvements in specific tasks/datasets but often fail to generalize or endure under distribution shifts or adversarial prompts (Randomness paper; cross-cultural evaluation literature). Without strong, externally verifiable gains on broad, multilingual benchmarks (CASA, CAMeL, CultureLLM benchmarks) across real-world settings, the claimed broad impact is doubtful.\\n\\n- Evaluation plan risks circularity or insufficiency\\n  - If the evaluation leans heavily on model-internal prompts and human judgments within the same experimental paradigm, you risk overestimating robustness. The field emphasizes the need for robustness checks, red-teaming, and cross-cultural evaluation with diverse methodologies (Randomness, CASAs framework, CAMeLs multilingual analyses). The proposal should anticipate and address these concerns rather than rely on a single or narrow evaluation suite.\\n\\n- Practicality and deployment risk\\n  - The added complexity of a Proposer/Reviser loop could slow response times and complicate deployment in production settings. The literature on efficient prompting and scaling (Structured Prompting, DynaICL, kNN prompting) suggests that scaling up a multi-prompt, multi-agent approach will require careful engineering to be viable in latency-sensitive or cost-constrained contexts. Without a strong efficiency strategy, the practical impact may be limited.\\n\\nConcrete suggestions to strengthen (if you want to salvage plausibility)\\n\\n- Clarify novelty and boundaries\\n  - Explicitly position the Proposer/Reviser architecture as a systematic, reusable prompting workflow that unifies and extends existing culture-aware prompting and norm-grounding techniques, rather than claiming a fundamentally new debiasing paradigm.\\n\\n- Anchor in robust grounding\\n  - Build explicit grounding sources for the Reviser (cultural norms databases, diverse stakeholder input, external evidence) and document provenance. Align with NormSAGE-like self-verification and CASA-style multi-criteria evaluation to demonstrate reliable grounding.\\n\\n- Design rigorous, multi-faceted evaluations\\n  - Pre-register evaluation plans and include cross-language, cross-cultural, and cross-domain benchmarks, emphasizing robustness to prompt variations (per 9e726f01e95e) and distribution shifts. Include both automatic metrics (bias/fairness proxies) and human-grounded assessments across multiple cultures and languages.\\n\\n- Emphasize efficiency and scalability\\n  - Incorporate scalable prompting strategies (Structured Prompting, kNN Prompting, Dynamic In-Context Learning) to keep the system tractable as you scale prompts and languages (cite the efficiency papers when outlining the pipeline).\\n\\n- Include ablations and controls\\n  - Conduct ablations isolating the Proposer effect, the Reviser effect, and their interaction. Compare against strong baselines (single-prompt debiasing, culture-aware fine-tuning, and norm-grounded prompting) to quantify additive value.\\n\\nBottom line\\n- The core idea has clear ties to a number of existing lines of work (culture-aware prompting, norm grounding, self-verification, and evolving/debiasing prompts) and does not establish a clearly novel mechanism or demonstrably durable uplift by itself. The proposed two-agent architecture could be valuable in principle, but only if it is tightly grounded in robust evaluation, explicit provenance for debiasing signals, efficiency considerations, and demonstrated cross-cultural generalizability. Without these, the work risks being incremental at best and potentially brittle or even counterproductive at scale.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Moderator: VERDICT:\\n\\nSummary of key points so far\\n\\n- Advocate (pro debiasing framework):\\n  - Proposes a two-agent prompting system: Proposer (generates diverse, cross-cultural debiasing prompts) and Reviser (critiques and refines model outputs to root out bias/stereotypes). The process includes iterative prompting and a self-verification step.\\n  - Grounds the idea in a broad literature stream: culture-aware prompting and alignment (CultureLLM, CultureGEN, See It from My Perspective), norm-grounding and self-verification (NormSAGE), multi-turn/debat-style prompting (Evolver, SKiC, etc.), and cross-cultural evaluation benchmarks (CASA, CAMeL).\\n  - Expected impact: fairer, more culturally aware outputs across languages; applicability to education, health, governance contexts; potential resilience to shifting biases via evolution-aware prompting.\\n  - Risk management: rigorous, multi-benchmark evaluation (CASA, CAMeL, cross-cultural tests); explicit self-verification to ground outputs; mitigations for new biases in prompts; efficiency considerations acknowledged and partially addressed through references to efficient prompting methods.\\n\\n- Skeptic (critical counter-arguments):\\n  - Novelty concerns: the Proposer/Reviser architecture overlaps with a broad set of existing prompting and debiasing approaches (culture-aware prompting, multi-perspective prompts, norm grounding, self-verification). The core idea may be more repackaging than a fundamentally new mechanism with proven, durable uplift.\\n  - Evaluation fragility: cultural alignment and bias metrics are notoriously sensitive to prompt wording, language, and model idiosyncrasies; robustness across languages and distributions is not guaranteed (citing Randomness/Not Representation critiques and cross-cultural evaluation findings).\\n  - Practicality and scalability: a two-agent loop could incur substantial compute and latency; literature shows efficiency challenges and the need for scalable prompting strategies (Structured Prompting, kNN Prompting, Dynamic In-Context Learning).\\n  - Grounding and accountability gaps: without explicit external grounding sources for the Revisers checks, debiasing claims risk being surface-level or brittle under distribution shifts.\\n  - Risk of prompt-induced biases: prompts themselves could encode biased frames; overfitting to certain cultures or languages is a real risk; cross-cultural coverage requires careful, diverse data and ongoing auditing.\\n\\n- Synthesis and take-away:\\n  - There is clear alignment with a pressing need to address cross-cultural bias, but the claimed novelty and scalability require stronger positioning, demonstrated durable gains across languages/tasks, and explicit grounding.\\n  - The idea could be made more credible by clarifying novelty (as a unified, reusable prompting workflow with formal grounding), specifying external normative sources, and detailing a rigorous, multi-benchmark evaluation plan that includes ablations.\\n\\nIf you want the debate to continue, heres a precise question to guide the next round:\\n\\nQuestion for the next round\\n- How would you design the Proposers prompt bank and the Revisers grounding checks to ensure broad, non-Western cultural coverage without overfitting to any single culture? Specifically:\\n  - Which external cultural norms datasets or sources would you anchor the Reviser to (and how would you ensure provenance and multilingual coverage)?\\n  - What would be your minimal viable experimental setup to demonstrate a durable cross-cultural debiasing uplift across at least three languages and two tasks, including:\\n    - Task descriptions (e.g., safe/offensive content filtering, stereotype propensity evaluation, or cross-cultural reasoning prompts),\\n    - Languages and scripts (e.g., EN, ES, AR, ZH, HI),\\n    - Concrete evaluation metrics (automatic bias proxies, cross-cultural alignment scores, human judgments),\\n    - An ablation plan (Is the gain due to Proposer prompts, the Reviser checks, or their interaction?),\\n  - How would you address the efficiency/latency concerns in a real-world deployment (e.g., which efficient prompting techniques would you integrate and in what order)?\\n- Provide a compact, concrete outline (not more than 23 pages) of the experimental protocol, including data sources, prompts, evaluation rubrics, and preregistered hypotheses.\\n\\nWould you like to proceed with a round 2 focused on drafting that experimental protocol and grounding plan?', additional_kwargs={}, response_metadata={})],\n",
       " 'iteration': 1,\n",
       " 'max_iterations': 3}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idea_evaluation_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
