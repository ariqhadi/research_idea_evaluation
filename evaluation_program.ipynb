{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ideas_file = 'sample_ideas/agentic_ai_for_idea_generation_ideas.json'\n",
    "\n",
    "with open(ideas_file, 'r') as f:\n",
    "    ideas_data = json.load(f)\n",
    "\n",
    "for ideas in ideas_data['ideas']:\n",
    "    for idea, items in ideas.items():\n",
    "        print(idea)\n",
    "        for item, i in items.items():\n",
    "            print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7f575aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from typing import Literal\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e13a1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,  # Set the temperature for the model's responses\n",
    "    model_name=\"gpt-5-nano\",  # Specify the model name\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing LLM Call\n",
    "\n",
    "# \n",
    "# messages = [\n",
    "#     SystemMessage(content=\"You are a helpful assistant that speaks like a pirate.\"),\n",
    "#     HumanMessage(content=\"Tell me about Python programming.\")\n",
    "# ]\n",
    "# response = llm.invoke(messages)\n",
    "# print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6965b7c",
   "metadata": {},
   "source": [
    "## Retrieve Paper with SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7b750afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "class getReferencePaper():\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "\n",
    "\n",
    "    def query_search(self, query):\n",
    "        url=\"https://api.semanticscholar.org/graph/v1/paper/search/\"\n",
    "        \n",
    "        query_params = {\n",
    "            \"query\": query,\n",
    "            \"fields\": \"title,citationCount,tldr,url,publicationTypes,publicationDate,openAccessPdf,abstract\",\n",
    "            \"year\": \"2020-2025\",\n",
    "            \"limit\": 50,\n",
    "            \"sort\": \"relevance\",\n",
    "            \"minCitationCount\": 10\n",
    "        }\n",
    "        headers = {\"x-api-key\": self.api_key}\n",
    "        response = requests.get(url, params=query_params, headers=headers).json()\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def PaperDetails(self, paper_id, fields=\"title,year,abstract,authors,citationCount,venue,citations,references,tldr\"):\n",
    "        \n",
    "        url = \"https://api.semanticscholar.org/graph/v1/paper/\"\n",
    "        \n",
    "        paper_data_query_params = {\"fields\": fields}\n",
    "        headers = {\"x-api-key\": self.api_key}\n",
    "        response = requests.get(\n",
    "            url = url + paper_id, params=paper_data_query_params, headers=headers\n",
    "        )\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_papers_for_llm(list_of_papers):\n",
    "        unique_papers = {}\n",
    "\n",
    "        for query_string, query_data in list_of_papers.items():\n",
    "            for paper in query_data.get('data', []):\n",
    "                paper_id = paper.get('paperId')\n",
    "                if paper_id and paper_id not in unique_papers:  # Skips if paper_id is None\n",
    "                    paper_str = f\"\"\"Paper ID: {paper_id}\n",
    "                                    Title: {paper.get('title')}\n",
    "                                    Abstract: {paper.get('abstract')}\n",
    "                                \"\"\"\n",
    "                    unique_papers[paper_id] = paper_str\n",
    "                    \n",
    "        paper_list = list(unique_papers.values())\n",
    "                \n",
    "        papers_for_llm = \"\\n\\n---\\n\\n\".join(paper_list)\n",
    "        return papers_for_llm\n",
    "\n",
    " \n",
    "# query = 'Computing Machinery and Intelligence'\n",
    "\n",
    "# search_paper = getReferencePaper()\n",
    "# search_paper_response = search_paper.query_search(query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d15a9",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d63c6508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class DirectAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Intent Agent Answer\n",
    "    \"\"\"\n",
    "    direct_answer: str = Field(default=\"\", description=\"contains the general conversation answer to the user prompt\")\n",
    "\n",
    "\n",
    "# Define the WriterAnswer model for story or general answer responses\n",
    "class WriterAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Used to answer with a story or general answer\n",
    "    \"\"\"\n",
    "    story: Optional[str] = Field(default=\"\", description=\"contains exclusively the story, no introduction or conclusion\")\n",
    "    general_answer: Optional[str] = Field(default=\"\", description=\"contains the general answer if no story is provided\")\n",
    "\n",
    "    def to_list(self) -> list:\n",
    "        return [{\n",
    "            \"story\": self.story,\n",
    "            \"general_answer\": self.general_answer\n",
    "        }]\n",
    "    \n",
    "# Define the CriticAnswer model for story feedback responses\n",
    "class CriticAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Story Critic Agent Answer\n",
    "    \"\"\"\n",
    "    feedback: str = Field(default=\"\", description=\"contains your feedback on the story, the reasons why the story is not compliant\")\n",
    "    story_ok: bool = Field(default=\"\", description=\"True if you validated the story is correct, False otherwise\")\n",
    "\n",
    "    def to_list(self):\n",
    "        return [{\n",
    "            \"feedback\": self.feedback,\n",
    "            \"story_ok\": self.story_ok,\n",
    "        }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3b096",
   "metadata": {},
   "source": [
    "### Agent 1: Idea Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb69f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "\n",
    "class IdeaParser(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 1: Parse Idea from user input into structured format.\n",
    "    1. Extract ideas from the user's input.\n",
    "    \"\"\"\n",
    "    research_question: str = Field(\n",
    "        description=\"The main research question in one conscise sentence\"\n",
    "    )\n",
    "    \n",
    "    problem_domain: str = Field(\n",
    "        description=\"The specific problem domain or area of interest related to the research question. (e.g., natural language processing, computer vision, etc.)\"\n",
    "    )\n",
    "    \n",
    "    methodology_keywords: List[str] = Field(\n",
    "        description=\"A list of technical keywords mentioned in the methods\"\n",
    "    )\n",
    "    \n",
    "    key_concepts: List[str] = Field(\n",
    "        description=\"A list of key concepts or theories relevant to the research question and problem domain.\"\n",
    "    )\n",
    "    \n",
    "    existing_methods: List[str] = Field(\n",
    "        description=\"A list of existing methods or approaches mentioned in the user's input that are relevant to the research question and problem domain.\"\n",
    "    )\n",
    "    \n",
    "    claimed_novelty: List[str] = Field(\n",
    "        description=\"A list of claimed novel aspects or contributions inferred from the user's input.\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    @field_validator('key_concepts')\n",
    "    @classmethod\n",
    "    def validate_key_concepts_counts(cls, v):\n",
    "        \"\"\"Ensure that there are a reasonable number of key concepts extracted\"\"\"\n",
    "        if len(v) < 3 or len(v) > 15:\n",
    "            raise ValueError('At least 3 and at most 15 key concepts are required.')\n",
    "        return v\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "    def to_summary(self) -> str:\n",
    "        return f\"\"\"Research Question: {self.research_question}\\n\n",
    "                    Problem Domain: {self.problem_domain}\\n\n",
    "                    Key Concepts: {', '.join(self.key_concepts)}\\n\n",
    "                    Claimed Novelty: {', '.join(self.claimed_novelty)}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b5727ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "idea_parser_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a research analysis assistant that has a deep understanding of extracting structured information from research proposals.\n",
    "\n",
    "            INPUT:\n",
    "            You will receive a research idea description with these fields:\n",
    "            - Problem: The research problem being addressed\n",
    "            - Existing Methods: Current approaches and their limitations\n",
    "            - Motivation: Why this research is needed\n",
    "            - Proposed Method: The new approach being proposed\n",
    "            - Experiment Plan: How the approach will be evaluated\n",
    "\n",
    "            YOUR TASK:\n",
    "            Extract and structure the key information needed for finding similar work.\n",
    "\n",
    "            OUTPUT REQUIREMENTS:\n",
    "            Return ONLY valid JSON with NO additional text, markdown formatting, or code blocks.\n",
    "            Do not include ```json or ``` markers.\n",
    "            Your entire response must be parseable by JSON.parse().\n",
    "\n",
    "            CRITICAL: Be precise and specific in extraction. Extract actual technical terms, not generic descriptions.\n",
    "\n",
    "            OUTPUT SCHEMA:\n",
    "            {{\n",
    "            \"research_question\": \"string - The main research question in one concise sentence\",\n",
    "            \"problem_domain\": \"string - The specific research area/field (e.g., 'natural language processing', 'computer vision')\",\n",
    "            \"methodology_keywords\": [\n",
    "                \"string - Specific technical methods mentioned (e.g., 'reinforcement learning', 'transformer architecture')\"\n",
    "            ],\n",
    "            \"key_concepts\": [\n",
    "                \"string - Core concepts and techniques (e.g., 'prompt optimization', 'context window management')\"\n",
    "            ],\n",
    "            \"existing_methods_mentioned\": [\n",
    "                \"string - Baseline methods or prior work explicitly mentioned\"\n",
    "            ],\n",
    "            \"claimed_novelty_aspects\": [\n",
    "                \"string - What the proposal claims is novel (extract from Motivation and Proposed Method)\"\n",
    "            ]\n",
    "            }}\n",
    "\n",
    "            EXTRACTION RULES:\n",
    "            1. Be specific: Extract \"transformer architecture\" not \"neural network\"\n",
    "            2. Preserve technical terms exactly as written\n",
    "            3. For methodology_keywords: Include only actionable technical terms\n",
    "            4. For key_concepts: Include 5-8 most important concepts\n",
    "            5. For claimed_novelty_aspects: Extract 2-4 specific novel claims\n",
    "            6. If a field has no relevant information, use empty array [] or empty string \"\"\n",
    "\n",
    "            RESEARCH IDEA:\n",
    "            {research_idea}\n",
    "\n",
    "            OUTPUT (valid JSON only):\n",
    "            \"\"\",\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"messages\"),  # Placeholder for dynamic messages\n",
    "    ])\n",
    "\n",
    "\n",
    "# Create the writer agent by binding the prompt template and language model\n",
    "idea_parser_agent = idea_parser_prompt |  llm.with_structured_output(IdeaParser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a0f58820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_idea_parser(state: MessagesState):\n",
    "    user_message = state[\"messages\"][-1]  # HumanMessage\n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = idea_parser_agent.invoke({\n",
    "        \"research_idea\": user_message.content\n",
    "    })\n",
    "    # response is now an IdeaParser object\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941980b",
   "metadata": {},
   "source": [
    "### Agent 2: Literature Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d9626e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryGenerator(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 2: Generate Search Queries from parsed research idea.\n",
    "    1. Create effective search queries to find related work.\n",
    "    \"\"\"\n",
    "    query_string: str = Field(\n",
    "        description=\"Search query to be passed to the API\"\n",
    "    )\n",
    "    \n",
    "    rationale: str = Field(\n",
    "        description=\"Rationale for why this query will find relevant papers\"\n",
    "    )\n",
    "    \n",
    "    priority_concept: str = Field(\n",
    "    description=\"The most important concept to focus on in the search\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('query_string')\n",
    "    @classmethod\n",
    "    def validate_query_string_length(cls, v):\n",
    "        \"\"\"Ensure that the query is not too long\"\"\"\n",
    "        if len(v.split()) > 8:\n",
    "            raise ValueError('Query string must be less than 8 words.')\n",
    "        return v\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "class QueryGeneratorOutput(BaseModel):\n",
    "    \"\"\"Multiple search queries\"\"\"\n",
    "    queries: List[QueryGenerator] = Field(\n",
    "        description=\"List of 5 diverse search queries\"\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"queries\": [q.to_dict() for q in self.queries]\n",
    "        }\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_generator_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced professor with an established search query strategy skill for academic literature databases.\n",
    "\n",
    "            CONTEXT:\n",
    "            You have a parsed research idea and need to generate optimal search queries for Semantic Scholar API.\n",
    "            Your queries will retrieve papers to assess the novelty of the proposed research.\n",
    "\n",
    "            PARSED RESEARCH IDEA:\n",
    "            {parsed_idea_json}\n",
    "\n",
    "\n",
    "            YOUR TASK:\n",
    "            Generate 5 (five) diverse search queries that will find the most relevant existing work.\n",
    "\n",
    "\n",
    "            QUERY OPTIMIZATION RULES:\n",
    "            1. Keep queries SHORT: 2-6 words maximum for best results\n",
    "            2. Use technical terms, not natural language\n",
    "            3. Combine 2-3 concepts maximum per query\n",
    "            4. NO operators: Don't use \"AND\", \"OR\", \"-\", quotes, or \"site:\"\n",
    "            5. Prioritize precision over recall\n",
    "\n",
    "            OUTPUT REQUIREMENTS:\n",
    "            Return ONLY valid JSON with NO additional text or formatting.\n",
    "            Do not include ```json or ``` markers.\n",
    "\n",
    "            OUTPUT SCHEMA:\n",
    "            {{\n",
    "            \"queries\": [\n",
    "                {{\n",
    "                \"query_string\": \"string - The actual search query (2-6 words)\",\n",
    "                \"rationale\": \"string - Why this query will find relevant papers\",\n",
    "                \"priority_concepts\" : \"string - Top 3-5 concepts that should appear in similar papers\"\n",
    "                }}\n",
    "            ]\n",
    "            }}\n",
    "\n",
    "            EXAMPLES OF GOOD QUERIES:\n",
    "            - \"adaptive prompt dialogue coherence\"\n",
    "            - \"dynamic context management LLM\"\n",
    "            - \"conversational continuity language models\"\n",
    "            - \"iterative prompt optimization\"\n",
    "\n",
    "            EXAMPLES OF BAD QUERIES:\n",
    "            - \"papers about improving language model coherence\" (too natural language)\n",
    "            - \"dynamic AND adaptive OR iterative -static\" (operators not supported)\n",
    "            - \"comprehensive survey of prompt engineering techniques\" (too long/broad)\n",
    "\n",
    "            OUTPUT (valid JSON only):\"\"\",\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"messages\"),  # Placeholder for dynamic messages\n",
    "    ])\n",
    "\n",
    "\n",
    "query_generator_agent = query_generator_prompt |  llm.with_structured_output(QueryGeneratorOutput)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d42ce32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_query_generator(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  # HumanMessage\n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = query_generator_agent.invoke({\n",
    "        \"parsed_idea_json\":last_message.content #json.dumps(json.loads(last_message.content))\n",
    "    })\n",
    "    # response is now an IdeaParser object\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319927ef",
   "metadata": {},
   "source": [
    "#### AGENT 3 : search paper using semantic scholar api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_paper_search(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  # LastMessage\n",
    "    queries_json = json.loads(last_message.content)\n",
    "    all_search_results = {}\n",
    "    \n",
    "    search_paper = getReferencePaper()\n",
    "\n",
    "    \n",
    "    for search_query in queries_json['queries']:\n",
    "        query_string = search_query['query_string']\n",
    "        search_results = search_paper.query_search(query_string)\n",
    "        all_search_results[query_string] = search_results\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(all_search_results, indent=2))]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cfaeb",
   "metadata": {},
   "source": [
    "### Agent 4 : Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "04f52e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalyzer(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 2: Generate Search Queries from parsed research idea.\n",
    "    1. Create effective search queries to find related work.\n",
    "    \"\"\"\n",
    "    paper_id: str = Field(\n",
    "        description=\"semantic scholar paper ID to be analyzed\"\n",
    "    )\n",
    "    \n",
    "    title: str = Field(\n",
    "        description=\"Title of the paper to be analyzed\"\n",
    "    )\n",
    "    \n",
    "    overlap_score: float = Field(\n",
    "        description=\"float 0.0-1.0 - overlap similarity with proposed idea\"\n",
    "    )\n",
    "    \n",
    "    methodology_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Methodology overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )\n",
    "    \n",
    "    problem_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Problem overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )\n",
    "    \n",
    "    domain_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Domain overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )   \n",
    "    key_overlaps: List[str] = Field(\n",
    "        description=\"Specific overlapping aspects\"\n",
    "    )\n",
    "    \n",
    "    key_differences: List[str] = Field(\n",
    "        description=\"How proposed idea differs\"\n",
    "    )\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "class PaperAnalyzerOutput(BaseModel):\n",
    "    \"\"\"Multiple search queries\"\"\"\n",
    "    queries: List[PaperAnalyzer] = Field(\n",
    "        description=\"List of all analyzed papers\"\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"papers\": [q.to_dict() for q in self.queries]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "04b64098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prior_work_analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experience researcher with years of expertise in academic literature review and analysis.\n",
    "\n",
    "                PROPOSED RESEARCH IDEA:\n",
    "                {research_idea}\n",
    "                \n",
    "                LIST OF RETRIEVED PAPERS:\n",
    "                {paper_list}\n",
    "                \n",
    "                YOUR TASK:\n",
    "                For each paper, assess the degree of overlap with the proposed research idea.\n",
    "                \n",
    "                ANALYSIS CRITERIA:\n",
    "                Methodology overlap: Do they use similar approaches?\n",
    "                Problem overlap: Do they address the same problem?\n",
    "                Domain overlap: Same application area?\n",
    "                Overall score: The average of the three overlap scores.\n",
    "                \n",
    "                OUTPUT REQUIREMENTS:\n",
    "                Return ONLY valid JSON with NO additional text.\n",
    "                Do not include ```json or ``` markers.\n",
    "                \n",
    "                OUTPUT SCHEMA:\n",
    "                {{\n",
    "                \"paper_analyses\": [\n",
    "                   {{\n",
    "                      \"paper_id\": \"string - Semantic Scholar paper ID\",\n",
    "                      \"title\": \"string\",\n",
    "                      \"overlap_score\": \"float 0.0-1.0 - Overall similarity\",\n",
    "                      \"methodology_overlap\": \"float 0.0-1.0\",\n",
    "                      \"problem_overlap\": \"float 0.0-1.0\", \n",
    "                      \"domain_overlap\": \"float 0.0-1.0\",\n",
    "                      \"key_overlaps\": [\n",
    "                        \"string - Specific overlapping aspects\"\n",
    "                      ],\n",
    "                      \"key_differences\": [\n",
    "                        \"string - How proposed idea differs\"\n",
    "                      ]\n",
    "                    }}\n",
    "                  ]\n",
    "                }}\n",
    "                \n",
    "                SCORING GUIDELINES:\n",
    "                overlap_score 0.8-1.0: Nearly identical approach\n",
    "                overlap_score 0.6-0.8: High similarity, incremental difference\n",
    "                overlap_score 0.4-0.6: Moderate similarity, related work\n",
    "                overlap_score 0.2-0.4: Tangentially related\n",
    "                overlap_score 0.0-0.2: Different approach, same domain\n",
    "                \n",
    "                Be precise and evidence-based. Cite specific aspects from paper titles/abstracts.\n",
    "                \n",
    "                OUTPUT (valid JSON only):\n",
    "                \n",
    "            \"\"\",\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"messages\"),  # Placeholder for dynamic messages\n",
    "    ])\n",
    "\n",
    "prior_work_analysis_agent = prior_work_analysis_prompt |  llm.with_structured_output(PaperAnalyzerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "67fb8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_prior_work_analysis(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  # HumanMessage\n",
    "    initial_user_input = state[\"messages\"][0]  # Research idea\n",
    "    \n",
    "    \n",
    "    list_of_papers = getReferencePaper.prepare_papers_for_llm(\n",
    "        json.loads(last_message.content)\n",
    "    )\n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = prior_work_analysis_agent.invoke({\n",
    "        \"research_idea\":initial_user_input.content, #json.dumps(json.loads(last_message.content)),\n",
    "        \"paper_list\": list_of_papers\n",
    "    })\n",
    "    # response is now an IdeaParser object\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912274f2",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3afc5516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x11dbcd5b0>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"idea_parser\", call_idea_parser)\n",
    "workflow.add_node(\"search_query\", call_query_generator)\n",
    "workflow.add_node(\"search_paper\", call_paper_search)\n",
    "workflow.add_node(\"prior_work_analysis\", call_prior_work_analysis)\n",
    "\n",
    "workflow.add_edge(START, \"idea_parser\")\n",
    "workflow.add_edge(\"idea_parser\", \"search_query\")\n",
    "workflow.add_edge(\"search_query\", \"search_paper\")\n",
    "workflow.add_edge(\"search_paper\", \"prior_work_analysis\")\n",
    "workflow.add_edge(\"prior_work_analysis\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "969dfdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "graph = workflow.compile()\n",
    "# graph.get_graph().draw_mermaid_png(output_file_path='story.png')\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "823bc0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ad548c68-67b0-408a-9b14-9783c57b304a')]}\n",
      "\u001b[1m[updates]\u001b[0m {'idea_parser': {'messages': [AIMessage(content='{\\n  \"research_question\": \"How can Dynamic Prompt Adaptation improve coherence and engagement of large language models over extended interactions and creative narratives?\",\\n  \"problem_domain\": \"Natural Language Processing; dialogue systems; long-form text generation\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"prompt templating\",\\n    \"summarization of prior interactions\"\\n  ],\\n  \"key_concepts\": [\\n    \"coherence over extended interactions\",\\n    \"thematic consistency\",\\n    \"iterative dialogue\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"summarization/synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Three-phase Dynamic Prompt Adaptation framework (Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update) to maintain thematic coherence across extended interactions.\",\\n    \"Contextual Analysis extracts key themes and tonal shifts from prior outputs and prompts.\",\\n    \"Adaptive Prompt Generation creates updated prompts to introduce new elements or clarify past responses.\",\\n    \"Iterative Context Update synthesizes all prior interactions to maintain coherence.\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='ae974c28-bf76-4b15-a33d-b03c97254c5c')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ad548c68-67b0-408a-9b14-9783c57b304a'), AIMessage(content='{\\n  \"research_question\": \"How can Dynamic Prompt Adaptation improve coherence and engagement of large language models over extended interactions and creative narratives?\",\\n  \"problem_domain\": \"Natural Language Processing; dialogue systems; long-form text generation\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"prompt templating\",\\n    \"summarization of prior interactions\"\\n  ],\\n  \"key_concepts\": [\\n    \"coherence over extended interactions\",\\n    \"thematic consistency\",\\n    \"iterative dialogue\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"summarization/synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Three-phase Dynamic Prompt Adaptation framework (Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update) to maintain thematic coherence across extended interactions.\",\\n    \"Contextual Analysis extracts key themes and tonal shifts from prior outputs and prompts.\",\\n    \"Adaptive Prompt Generation creates updated prompts to introduce new elements or clarify past responses.\",\\n    \"Iterative Context Update synthesizes all prior interactions to maintain coherence.\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='ae974c28-bf76-4b15-a33d-b03c97254c5c')]}\n",
      "\u001b[1m[updates]\u001b[0m {'search_query': {'messages': [AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation\",\\n      \"rationale\": \"Targets studies of frameworks that adapt prompts in real time to sustain coherence over long dialogues.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, coherence, long-form dialogue\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Searches for methods that generate updated prompts to guide LLMs for consistent narrative and context handling.\",\\n      \"priority_concept\": \"adaptive prompt generation, context management, coherence\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Focuses on techniques that update system prompts/context iteratively to maintain dialogue continuity.\",\\n      \"priority_concept\": \"iterative context update, coherence, dialogue\"\\n    },\\n    {\\n      \"query_string\": \"summarization prior interactions\",\\n      \"rationale\": \"Looks for work on summarizing or synthesizing past interactions to preserve thematic coherence.\",\\n      \"priority_concept\": \"summarization, prior interactions, thematic coherence\"\\n    },\\n    {\\n      \"query_string\": \"iterative prompt templating\",\\n      \"rationale\": \"Explores templating strategies that evolve across iterations to embed new elements while preserving coherence.\",\\n      \"priority_concept\": \"iterative prompt templating, prompt engineering, coherence\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='9a9405f7-5b76-4099-897c-3852c95bb26a')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ad548c68-67b0-408a-9b14-9783c57b304a'), AIMessage(content='{\\n  \"research_question\": \"How can Dynamic Prompt Adaptation improve coherence and engagement of large language models over extended interactions and creative narratives?\",\\n  \"problem_domain\": \"Natural Language Processing; dialogue systems; long-form text generation\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"prompt templating\",\\n    \"summarization of prior interactions\"\\n  ],\\n  \"key_concepts\": [\\n    \"coherence over extended interactions\",\\n    \"thematic consistency\",\\n    \"iterative dialogue\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"summarization/synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Three-phase Dynamic Prompt Adaptation framework (Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update) to maintain thematic coherence across extended interactions.\",\\n    \"Contextual Analysis extracts key themes and tonal shifts from prior outputs and prompts.\",\\n    \"Adaptive Prompt Generation creates updated prompts to introduce new elements or clarify past responses.\",\\n    \"Iterative Context Update synthesizes all prior interactions to maintain coherence.\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='ae974c28-bf76-4b15-a33d-b03c97254c5c'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation\",\\n      \"rationale\": \"Targets studies of frameworks that adapt prompts in real time to sustain coherence over long dialogues.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, coherence, long-form dialogue\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Searches for methods that generate updated prompts to guide LLMs for consistent narrative and context handling.\",\\n      \"priority_concept\": \"adaptive prompt generation, context management, coherence\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Focuses on techniques that update system prompts/context iteratively to maintain dialogue continuity.\",\\n      \"priority_concept\": \"iterative context update, coherence, dialogue\"\\n    },\\n    {\\n      \"query_string\": \"summarization prior interactions\",\\n      \"rationale\": \"Looks for work on summarizing or synthesizing past interactions to preserve thematic coherence.\",\\n      \"priority_concept\": \"summarization, prior interactions, thematic coherence\"\\n    },\\n    {\\n      \"query_string\": \"iterative prompt templating\",\\n      \"rationale\": \"Explores templating strategies that evolve across iterations to embed new elements while preserving coherence.\",\\n      \"priority_concept\": \"iterative prompt templating, prompt engineering, coherence\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='9a9405f7-5b76-4099-897c-3852c95bb26a')]}\n",
      "\u001b[1m[updates]\u001b[0m {'search_paper': {'messages': [AIMessage(content='{\\n  \"dynamic prompt adaptation\": {\\n    \"total\": 5410,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 128,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 197,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"title\": \"Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/28759/29459\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i8.28759?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i8.28759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy to enable prompt adaptation to the evolving distribution of the dynamic graph.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9071547\",\\n            \"name\": \"Binwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2108814780\",\\n            \"name\": \"Pengkun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292591250\",\\n            \"name\": \"Yudong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108599981\",\\n            \"name\": \"Xu Wang\"\\n          },\\n          {\\n            \"authorId\": \"6231985\",\\n            \"name\": \"Zhengyang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2187108259\",\\n            \"name\": \"Lei Bai\"\\n          },\\n          {\\n            \"authorId\": \"46396284\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"With the progress of urban transportation systems, a significant amount of high-quality traffic data is continuously collected through streaming manners, which has propelled the prosperity of the field of spatial-temporal graph prediction. In this paper, rather than solely focusing on designing powerful models for static graphs, we shift our focus to spatial-temporal graph prediction in the dynamic scenario, which involves a continuously expanding and evolving underlying graph. To address inherent challenges, a decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy. Incorporating inductive biases of time-series structures, DSTG can interpret time dependencies into latent trend and seasonal terms. To enable prompt adaptation to the evolving distribution of the dynamic graph, our decoupling training strategy is devised to iteratively update these two types of patterns. Specifically, for learning seasonal patterns, we conduct thorough training for the model using a long time series (e.g., three months of data). To enhance the learning ability of the model, we also introduce the masked auto-encoding mechanism. During this period, we frequently update trend patterns to expand new information from dynamic graphs. Considering both effectiveness and efficiency, we develop a subnet sampling strategy to select a few representative nodes for fine-tuning the weights of the model. These sampled nodes cover unseen patterns and previously learned patterns. Experiments on dynamic spatial-temporal graph datasets further demonstrate the competitive performance, superior efficiency, and strong scalability of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"title\": \"Dynamics of Smallholder Farmers\\\\u2019 Livelihood Adaptation Decision-Making in Central Ethiopia\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2071-1050/12/11/4526/pdf?version=1591863876\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/su12114526?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/su12114526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2020-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13422136\",\\n            \"name\": \"D. Etana\"\\n          },\\n          {\\n            \"authorId\": \"4266116\",\\n            \"name\": \"D. Snelder\"\\n          },\\n          {\\n            \"authorId\": \"2051704940\",\\n            \"name\": \"C. V. van Wesenbeeck\"\\n          },\\n          {\\n            \"authorId\": \"3606491\",\\n            \"name\": \"T. de Cock Buning\"\\n          }\\n        ],\\n        \"abstract\": \"In previous studies mainly focusing on determinants of adaptation, evidence of the dynamic process of adaptation decision-making is negligible. The objective of this study was to investigate the effects of socio-cultural factors, changes in household characteristics, and climate variables on the transition from non-use to use of adaptation strategies. The study integrated primary data collected from households with secondary rainfall and temperature data. The quantitative and qualitative data were analysed using a dynamic random-effects probit model and a thematic approach, respectively. The result shows strong evidence of path dependence in which use of a strategy during the previous year significantly increases its current use. Climate-related risk perception and factual knowledge may not necessarily prompt adaptation action, whereas access to financial resources and farming-related trainings were consistent positive predictors of farmers\\\\u2019 adaptation decisions. The findings entail that economic capacity and the associated intrinsic motivation help few farmers to utilise robust and contesting adaptation strategies. For most households, economic problems and the consequent fatalistic attitude and risk-avoidance behaviour induce either non-use or use of responsive and accommodating strategies aimed at ensuring survival. Path dependence in non-use of adaptation strategies and sub-optimal adaptation actions demand effective institutional supports to address the behavioural and economic barriers of these households in order to build overall community resilience.\"\\n      },\\n      {\\n        \"paperId\": \"f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"title\": \"Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.08394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model, designs an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2309491187\",\\n            \"name\": \"Zhengbo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2288264614\",\\n            \"name\": \"Li Xu\"\\n          },\\n          {\\n            \"authorId\": \"2067913944\",\\n            \"name\": \"Duo Peng\"\\n          },\\n          {\\n            \"authorId\": \"2265553215\",\\n            \"name\": \"Hossein Rahmani\"\\n          },\\n          {\\n            \"authorId\": \"2309177751\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model. Our main idea is to leverage the rich knowledge encapsulated within the pre-trained diffusion model, such as the understanding of image semantics and structural information, to address unsupervised visual tracking. To this end, we design an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target. Furthermore, to facilitate dynamic adaptation of the prompt to the target\\'s movements, we propose an online prompt updater. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method, which also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"736275f05cbff41484ff4f0c90fd1401af85090f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/736275f05cbff41484ff4f0c90fd1401af85090f\",\\n        \"title\": \"A kaleidoscope of photosynthetic antenna proteins and their emerging roles\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1093/plphys/kiac175\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9237682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Some of the latest fundamental discoveries on the molecular mechanisms and regulation of light harvesting that can potentially be exploited for the optimization of photosynthesis are underline.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1811744891\",\\n            \"name\": \"Rameez Arshad\"\\n          },\\n          {\\n            \"authorId\": \"35279014\",\\n            \"name\": \"Francesco Saccon\"\\n          },\\n          {\\n            \"authorId\": \"1743932730\",\\n            \"name\": \"P. Bag\"\\n          },\\n          {\\n            \"authorId\": \"1498519841\",\\n            \"name\": \"Avratanu Biswas\"\\n          },\\n          {\\n            \"authorId\": \"36840944\",\\n            \"name\": \"C. Calvaruso\"\\n          },\\n          {\\n            \"authorId\": \"1791956159\",\\n            \"name\": \"Ahmad Farhan Bhatti\"\\n          },\\n          {\\n            \"authorId\": \"37543271\",\\n            \"name\": \"Steffen Grebe\"\\n          },\\n          {\\n            \"authorId\": \"1490962043\",\\n            \"name\": \"Vincenzo Mascoli\"\\n          },\\n          {\\n            \"authorId\": \"145679412\",\\n            \"name\": \"M. Mahbub\"\\n          },\\n          {\\n            \"authorId\": \"25484278\",\\n            \"name\": \"F. Muzzopappa\"\\n          },\\n          {\\n            \"authorId\": \"2042646658\",\\n            \"name\": \"Alexandros Polyzois\"\\n          },\\n          {\\n            \"authorId\": \"150064364\",\\n            \"name\": \"C. Schiphorst\"\\n          },\\n          {\\n            \"authorId\": \"68976915\",\\n            \"name\": \"M. Sorrentino\"\\n          },\\n          {\\n            \"authorId\": \"40827588\",\\n            \"name\": \"Simona Streckait\\\\u0117\"\\n          },\\n          {\\n            \"authorId\": \"6651561\",\\n            \"name\": \"H. van Amerongen\"\\n          },\\n          {\\n            \"authorId\": \"90940595\",\\n            \"name\": \"E. Aro\"\\n          },\\n          {\\n            \"authorId\": \"1881036\",\\n            \"name\": \"R. Bassi\"\\n          },\\n          {\\n            \"authorId\": \"4490696\",\\n            \"name\": \"E. Boekema\"\\n          },\\n          {\\n            \"authorId\": \"145609130\",\\n            \"name\": \"R. Croce\"\\n          },\\n          {\\n            \"authorId\": \"4296150\",\\n            \"name\": \"J. Dekker\"\\n          },\\n          {\\n            \"authorId\": \"3189715\",\\n            \"name\": \"R. van Grondelle\"\\n          },\\n          {\\n            \"authorId\": \"144071537\",\\n            \"name\": \"S. Jansson\"\\n          },\\n          {\\n            \"authorId\": \"5089855\",\\n            \"name\": \"D. Kirilovsky\"\\n          },\\n          {\\n            \"authorId\": \"4282160\",\\n            \"name\": \"R. Kou\\\\u0159il\"\\n          },\\n          {\\n            \"authorId\": \"145135253\",\\n            \"name\": \"S. Michel\"\\n          },\\n          {\\n            \"authorId\": \"5289431\",\\n            \"name\": \"C. Mullineaux\"\\n          },\\n          {\\n            \"authorId\": \"7175075\",\\n            \"name\": \"Kl\\\\u00e1ra Panzarov\\\\u00e1\"\\n          },\\n          {\\n            \"authorId\": \"145448524\",\\n            \"name\": \"B. Robert\"\\n          },\\n          {\\n            \"authorId\": \"2161625\",\\n            \"name\": \"A. Ruban\"\\n          },\\n          {\\n            \"authorId\": \"15201549\",\\n            \"name\": \"I. V. van Stokkum\"\\n          },\\n          {\\n            \"authorId\": \"114063537\",\\n            \"name\": \"E. Wientjes\"\\n          },\\n          {\\n            \"authorId\": \"3857871\",\\n            \"name\": \"C. B\\\\u00fcchel\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Photosynthetic light-harvesting antennae are pigment-binding proteins that perform one of the most fundamental tasks on Earth, capturing light and transferring energy that enables life in our biosphere. Adaptation to different light environments led to the evolution of an astonishing diversity of light-harvesting systems. At the same time, several strategies have been developed to optimize the light energy input into photosynthetic membranes in response to fluctuating conditions. The basic feature of these prompt responses is the dynamic nature of antenna complexes, whose function readily adapts to the light available. High-resolution microscopy and spectroscopic studies on membrane dynamics demonstrate the crosstalk between antennae and other thylakoid membrane components. With the increased understanding of light-harvesting mechanisms and their regulation, efforts are focusing on the development of sustainable processes for effective conversion of sunlight into functional bio-products. The major challenge in this approach lies in the application of fundamental discoveries in light-harvesting systems for the improvement of plant or algal photosynthesis. Here, we underline some of the latest fundamental discoveries on the molecular mechanisms and regulation of light harvesting that can potentially be exploited for the optimization of photosynthesis.\"\\n      },\\n      {\\n        \"paperId\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04095\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces the Prompt Auto-Editing (PAE) method, which employs an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148661301\",\\n            \"name\": \"Wenyi Mo\"\\n          },\\n          {\\n            \"authorId\": \"2146332319\",\\n            \"name\": \"Tianyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2281418241\",\\n            \"name\": \"Yalong Bai\"\\n          },\\n          {\\n            \"authorId\": \"2295513824\",\\n            \"name\": \"Bing Su\"\\n          },\\n          {\\n            \"authorId\": \"2293310016\",\\n            \"name\": \"Ji-Rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2281323801\",\\n            \"name\": \"Qing Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\"\\n      },\\n      {\\n        \"paperId\": \"ef9d13b78e418b05eba22563d460900dfb6acd2e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef9d13b78e418b05eba22563d460900dfb6acd2e\",\\n        \"title\": \"MedSAM-U: Uncertainty-Guided Auto Multi-Prompt Adaptation for Reliable MedSAM\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces MedSAM-U, an uncertainty-guided framework designed to automatically refine multi-prompt inputs for more reliable and precise medical image segmentation, and employs uncertainty-guided multi-prompt to effectively estimate the uncertainties associated with the prompts and their initial segmentation results.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295375184\",\\n            \"name\": \"Nan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"9258203\",\\n            \"name\": \"K. Zou\"\\n          },\\n          {\\n            \"authorId\": \"2295886187\",\\n            \"name\": \"Kai Ren\"\\n          },\\n          {\\n            \"authorId\": \"151501329\",\\n            \"name\": \"Mengting Luo\"\\n          },\\n          {\\n            \"authorId\": \"2308862755\",\\n            \"name\": \"Linchao He\"\\n          },\\n          {\\n            \"authorId\": \"2171627854\",\\n            \"name\": \"Meng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2199172643\",\\n            \"name\": \"Yidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2304326718\",\\n            \"name\": \"Yi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2287877133\",\\n            \"name\": \"Hu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2302371411\",\\n            \"name\": \"Huazhu Fu\"\\n          }\\n        ],\\n        \"abstract\": \"The Medical Segment Anything Model (MedSAM) has shown remarkable performance in medical image segmentation, drawing significant attention in the field. However, its sensitivity to varying prompt types and locations poses challenges. This paper addresses these challenges by focusing on the development of reliable prompts that enhance MedSAM\\'s accuracy. We introduce MedSAM-U, an uncertainty-guided framework designed to automatically refine multi-prompt inputs for more reliable and precise medical image segmentation. Specifically, we first train a Multi-Prompt Adapter integrated with MedSAM, creating MPA-MedSAM, to adapt to diverse multi-prompt inputs. We then employ uncertainty-guided multi-prompt to effectively estimate the uncertainties associated with the prompts and their initial segmentation results. In particular, a novel uncertainty-guided prompts adaptation technique is then applied automatically to derive reliable prompts and their corresponding segmentation outcomes. We validate MedSAM-U using datasets from multiple modalities to train a universal image segmentation model. Compared to MedSAM, experimental results on five distinct modal datasets demonstrate that the proposed MedSAM-U achieves an average performance improvement of 1.7\\\\\\\\% to 20.5\\\\\\\\% across uncertainty-guided prompts.\"\\n      },\\n      {\\n        \"paperId\": \"f3edc5a87051be2b4b249c4896c8ab14462c659c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f3edc5a87051be2b4b249c4896c8ab14462c659c\",\\n        \"title\": \"Knowledge-based dynamic prompt learning for multi-label disease diagnosis\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.knosys.2024.111395?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.knosys.2024.111395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2259963003\",\\n            \"name\": \"Jing Xie\"\\n          },\\n          {\\n            \"authorId\": \"2280033365\",\\n            \"name\": \"Xin Li\"\\n          },\\n          {\\n            \"authorId\": \"2279980846\",\\n            \"name\": \"Ye Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2153979217\",\\n            \"name\": \"Yi Guan\"\\n          },\\n          {\\n            \"authorId\": \"2259229\",\\n            \"name\": \"Jingchi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2279958972\",\\n            \"name\": \"Xitong Guo\"\\n          },\\n          {\\n            \"authorId\": \"2280535977\",\\n            \"name\": \"Xin Peng\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"81ca1ca9b81470988d29e0d0def9712320243e06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/81ca1ca9b81470988d29e0d0def9712320243e06\",\\n        \"title\": \"HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target Binding Affinity Prediction\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.17697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic prompts within a hybrid Graph-Transformer framework, and outperforms state-of-the-art DTA prediction methods in both prediction performance and generalization ability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2349232014\",\\n            \"name\": \"Xi Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2275183852\",\\n            \"name\": \"Wentao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2303777566\",\\n            \"name\": \"Jiacheng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2308231466\",\\n            \"name\": \"Lijing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2308228559\",\\n            \"name\": \"Gaofei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2303708410\",\\n            \"name\": \"Zhengji Li\"\\n          },\\n          {\\n            \"authorId\": \"2257021623\",\\n            \"name\": \"Tianyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2308674053\",\\n            \"name\": \"Min Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Drug target binding affinity (DTA) is a key criterion for drug screening. Existing experimental methods are time-consuming and rely on limited structural and domain information. While learning-based methods can model sequence and structural information, they struggle to integrate contextual data and often lack comprehensive modeling of drug-target interactions. In this study, we propose a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic prompts within a hybrid Graph-Transformer framework. Our method generates context-specific prompts for each drug-target pair, enhancing the model\\'s ability to capture unique interactions. The introduction of prompt tuning further optimizes the prediction process by filtering out irrelevant noise and emphasizing task-relevant information, dynamically adjusting the input features of the molecular graph. The proposed hybrid Graph-Transformer architecture combines structural information from Graph Convolutional Networks (GCNs) with sequence information captured by Transformers, facilitating the interaction between global and local information. Additionally, we adopted the multi-view feature fusion method to project molecular graph views and affinity subgraph views into a common feature space, effectively combining structural and contextual information. Experiments on two widely used public datasets, Davis and KIBA, show that HGTDP-DTA outperforms state-of-the-art DTA prediction methods in both prediction performance and generalization ability.\"\\n      },\\n      {\\n        \"paperId\": \"3967189742efab8859da542ce3953d4c72957aca\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3967189742efab8859da542ce3953d4c72957aca\",\\n        \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.23904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI), designed to design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275054108\",\\n            \"name\": \"Qinqian Lei\"\\n          },\\n          {\\n            \"authorId\": \"2313081973\",\\n            \"name\": \"Bo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256998291\",\\n            \"name\": \"Robby T. Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\"\\n      },\\n      {\\n        \"paperId\": \"1d24f2dfe942e2d67381d8fcfac3423145bd557f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d24f2dfe942e2d67381d8fcfac3423145bd557f\",\\n        \"title\": \"DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.10588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention that outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1390537791\",\\n            \"name\": \"Xiangpeng Yang\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2109074276\",\\n            \"name\": \"Xiaohan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-video retrieval is a critical multi-modal task to find the most relevant video for a text query. Although pretrained models like CLIP have demonstrated impressive potential in this area, the rising cost of fully finetuning these models due to increasing model size continues to pose a problem. To address this challenge, prompt tuning has emerged as an alternative. However, existing works still face two problems when adapting pretrained image-text models to downstream video-text tasks: (1) The visual encoder could only encode frame-level features and failed to extract global-level general video information. (2) Equipping the visual and text encoder with separated prompts failed to mitigate the visual-text modality gap. To this end, we propose DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention. In contrast to previous prompt tuning methods, we employ the shared latent space to generate local-level text and frame prompts that encourage inter-modal interaction. Furthermore, we propose modeling video in a global-local attention mechanism to capture global video information from the perspective of prompt tuning. Extensive experiments reveal that when only 0.67% parameters are tuned, our cross-modal prompt tuning strategy DGL outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets. Code will be available at https://github.com/knightyxp/DGL.\"\\n      },\\n      {\\n        \"paperId\": \"92d477bb89b43a73b6e8ca54ae2e1def9f3fe2dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/92d477bb89b43a73b6e8ca54ae2e1def9f3fe2dd\",\\n        \"title\": \"UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes the first universal CSR framework, dubbed UCIP, with dynamic prompt learning, intending to jointly support the CSR distortions of any compression codecs/modes, and introduces the novel MLP-like framework backbone for the UCIP.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2253011048\",\\n            \"name\": \"Xin Li\"\\n          },\\n          {\\n            \"authorId\": \"2289786700\",\\n            \"name\": \"Bingchen Li\"\\n          },\\n          {\\n            \"authorId\": \"2288532590\",\\n            \"name\": \"Yeying Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"2282209914\",\\n            \"name\": \"Hanxin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2232648762\",\\n            \"name\": \"Yulin Ren\"\\n          },\\n          {\\n            \"authorId\": \"2287374267\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Compressed Image Super-resolution (CSR) aims to simultaneously super-resolve the compressed images and tackle the challenging hybrid distortions caused by compression. However, existing works on CSR usually focuses on a single compression codec, i.e., JPEG, ignoring the diverse traditional or learning-based codecs in the practical application, e.g., HEVC, VVC, HIFIC, etc. In this work, we propose the first universal CSR framework, dubbed UCIP, with dynamic prompt learning, intending to jointly support the CSR distortions of any compression codecs/modes. Particularly, an efficient dynamic prompt strategy is proposed to mine the content/spatial-aware task-adaptive contextual information for the universal CSR task, using only a small amount of prompts with spatial size 1x1. To simplify contextual information mining, we introduce the novel MLP-like framework backbone for our UCIP by adapting the Active Token Mixer (ATM) to CSR tasks for the first time, where the global information modeling is only taken in horizontal and vertical directions with offset prediction. We also build an all-in-one benchmark dataset for the CSR task by collecting the datasets with the popular 6 diverse traditional and learning-based codecs, including JPEG, HEVC, VVC, HIFIC, etc., resulting in 23 common degradations. Extensive experiments have shown the consistent and excellent performance of our UCIP on universal CSR tasks. The project can be found in https://lixinustc.github.io/UCIP.github.io\"\\n      },\\n      {\\n        \"paperId\": \"dbad406ab82697b153d28c6654e7808187859701\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dbad406ab82697b153d28c6654e7808187859701\",\\n        \"title\": \"Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.07221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The essence of IDPT is to develop a dynamic prompt generation module to perceive semantic prior features of each point cloud instance and generate adaptive prompt tokens to enhance the model\\'s robustness, providing a promising solution to parameter-efficient learning for pre-trained point cloud models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-04-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2214515552\",\\n            \"name\": \"Yaohua Zha\"\\n          },\\n          {\\n            \"authorId\": \"2110174485\",\\n            \"name\": \"Jinpeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"151470385\",\\n            \"name\": \"Tao Dai\"\\n          },\\n          {\\n            \"authorId\": \"50461900\",\\n            \"name\": \"Bin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2108389544\",\\n            \"name\": \"Zhi Wang\"\\n          },\\n          {\\n            \"authorId\": \"3085483\",\\n            \"name\": \"Shutao Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained point cloud models have found extensive applications in 3D understanding tasks like object classification and part segmentation. However, the prevailing strategy of full fine-tuning in downstream tasks leads to large per-task storage overhead for model parameters, which limits the efficiency when applying large-scale pre-trained models. Inspired by the recent success of visual prompt tuning (VPT), this paper attempts to explore prompt tuning on pre-trained point cloud models, to pursue an elegant balance between performance and parameter efficiency. We find while instance-agnostic static prompting, e.g. VPT, shows some efficacy in downstream transfer, it is vulnerable to the distribution diversity caused by various types of noises in real-world point cloud data. To conquer this limitation, we propose a novel Instance-aware Dynamic Prompt Tuning (IDPT) strategy for pre-trained point cloud models. The essence of IDPT is to develop a dynamic prompt generation module to perceive semantic prior features of each point cloud instance and generate adaptive prompt tokens to enhance the model\\'s robustness. Notably, extensive experiments demonstrate that IDPT outperforms full finetuning in most tasks with a mere 7% of the trainable parameters, providing a promising solution to parameter-efficient learning for pre-trained point cloud models. Code is available at https://github.com/zyh16143998882/ICCV23-IDPT.\"\\n      },\\n      {\\n        \"paperId\": \"51ac2dceecc75431eab5b2c320822e081b282b27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51ac2dceecc75431eab5b2c320822e081b282b27\",\\n        \"title\": \"SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models\",\\n        \"citationCount\": 52,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed SwapPrompt is a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation and achieves state-of-the-art test-time adaptation performance on ImageNet and nine other datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2139700088\",\\n            \"name\": \"Xiaosong Ma\"\\n          },\\n          {\\n            \"authorId\": \"2265432385\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2277415299\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"50232004\",\\n            \"name\": \"Wenchao Xu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0911ae6be53e4e649eb8da3d8ae738f65e93993d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0911ae6be53e4e649eb8da3d8ae738f65e93993d\",\\n        \"title\": \"Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2402.18447\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.18447, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts and the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158676096\",\\n            \"name\": \"Deng Li\"\\n          },\\n          {\\n            \"authorId\": \"48352212\",\\n            \"name\": \"Aming Wu\"\\n          },\\n          {\\n            \"authorId\": \"2288039856\",\\n            \"name\": \"Yaowei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261890091\",\\n            \"name\": \"Yahong Han\"\\n          }\\n        ],\\n        \"abstract\": \"Single-domain generalization aims to learn a model from single source domain data attaining generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in crossdomain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and versatility of our proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"666cced301ad62e8cfb9a5ca02bb4f5320dbc0a5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/666cced301ad62e8cfb9a5ca02bb4f5320dbc0a5\",\\n        \"title\": \"Unsupervised Domain Adaptation Enhanced by Fuzzy Prompt Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TFUZZ.2024.3389705?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TFUZZ.2024.3389705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2297072145\",\\n            \"name\": \"Kuo Shi\"\\n          },\\n          {\\n            \"authorId\": \"2272678886\",\\n            \"name\": \"Jie Lu\"\\n          },\\n          {\\n            \"authorId\": \"2243460919\",\\n            \"name\": \"Zhen Fang\"\\n          },\\n          {\\n            \"authorId\": \"46266495\",\\n            \"name\": \"Guangquan Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) addresses the challenge of distribution shift between a labeled source domain and an unlabeled target domain by utilizing knowledge from the source. Traditional UDA methods mainly focus on single-modal scenarios, either vision or language, thus, not fully exploring the advantages of multimodal representations. Visionlanguage models utilize multimodal information, applying prompt learning techniques for addressing target domain tasks. Motivated by the recent advancements in pretrained visionlanguage models, this article expands the UDA framework to incorporate multimodal approaches using fuzzy techniques. The adoption of fuzzy techniques, preferred over conventional domain adaptation methods, is based on the following two key aspects: 1) the nature of prompt learning is intrinsically linked to fuzzy logic, and 2) the superior capability of fuzzy techniques in processing soft information and effectively utilizing inherent relationships both within and across domains. To this end, we propose UDA enhanced by fuzzy prompt learning (FUZZLE), a simple and effective method for aligning the source and target domains via domain-specific prompt learning. Specifically, we introduce a novel technique to enhance prompt learning in the target domain. This method integrates fuzzy C-means clustering and a novel instance-level fuzzy vector into the prompt learning loss function, minimizing the distance between prompt cluster centers and instance prompts, thereby, enhancing the prompt learning process. In addition, we propose a Kullback\\\\u2013Leibler (KL) divergence-based loss function with a fuzzification factor. This function is designed to minimize the distribution discrepancy in the classification of similar cross-domain data, aligning domain-specific prompts during the training process. We contribute an in-depth analysis to understand the effectiveness of FUZZLE. Extensive experiments demonstrate that our method achieves superior performance on standard UDA benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"5d48b087d3d4a16904c7e5442a4a815ff901cc0d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5d48b087d3d4a16904c7e5442a4a815ff901cc0d\",\\n        \"title\": \"CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17369, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoDA is proposed which instructs models to distinguish, focus, and learn from discrepancies within all adverse scenes at scene and image levels, and achieves SOTA performances on widely-used benchmarks under all adverse scenes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261751604\",\\n            \"name\": \"Ziyang Gong\"\\n          },\\n          {\\n            \"authorId\": \"2261889755\",\\n            \"name\": \"Fuhao Li\"\\n          },\\n          {\\n            \"authorId\": \"2262092854\",\\n            \"name\": \"Yupeng Deng\"\\n          },\\n          {\\n            \"authorId\": \"2293394749\",\\n            \"name\": \"Deblina Bhattacharjee\"\\n          },\\n          {\\n            \"authorId\": \"2293450075\",\\n            \"name\": \"Xiangwei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261751710\",\\n            \"name\": \"Zhenming Ji\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised Domain Adaptation (UDA) aims to adapt models from labeled source domains to unlabeled target domains. When adapting to adverse scenes, existing UDA methods fail to perform well due to the lack of instructions, leading their models to overlook discrepancies within all adverse scenes. To tackle this, we propose CoDA which instructs models to distinguish, focus, and learn from these discrepancies at scene and image levels. Specifically, CoDA consists of a Chain-of-Domain (CoD) strategy and a Severity-Aware Visual Prompt Tuning (SAVPT) mechanism. CoD focuses on scene-level instructions to divide all adverse scenes into easy and hard scenes, guiding models to adapt from source to easy domains with easy scene images, and then to hard domains with hard scene images, thereby laying a solid foundation for whole adaptations. Building upon this foundation, we employ SAVPT to dive into more detailed image-level instructions to boost performance. SAVPT features a novel metric Severity that divides all adverse scene images into low-severity and high-severity images. Then Severity directs visual prompts and adapters, instructing models to concentrate on unified severity features instead of scene-specific features, without adding complexity to the model architecture. CoDA achieves SOTA performances on widely-used benchmarks under all adverse scenes. Notably, CoDA outperforms the existing ones by 4.6%, and 10.3% mIoU on the Foggy Driving, and Foggy Zurich benchmarks, respectively. Our code is available at https://github.com/Cuzyoung/CoDA\"\\n      },\\n      {\\n        \"paperId\": \"8d531cb8cf51eec3b8f1106d189295fa3c81c02a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8d531cb8cf51eec3b8f1106d189295fa3c81c02a\",\\n        \"title\": \"Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing\",\\n        \"citationCount\": 83,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.15664\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.15664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Dynamic Prompt Learning (DPL) is proposed to force cross-attention maps to focus on correct noun words in the text prompt by updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses to achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2237135280\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"2115413065\",\\n            \"name\": \"Fei Yang\"\\n          },\\n          {\\n            \"authorId\": \"49080524\",\\n            \"name\": \"Shiqi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2248121772\",\\n            \"name\": \"Muhammad Atif Butt\"\\n          },\\n          {\\n            \"authorId\": \"2820687\",\\n            \"name\": \"Joost van de Weijer\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale text-to-image generative models have been a ground-breaking development in generative AI, with diffusion models showing their astounding ability to synthesize convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are susceptible to unintended modifications of regions outside the targeted area, such as on the background or on distractor objects which have some semantic or visual relationship with the targeted object. According to our experimental findings, inaccurate cross-attention maps are at the root of this problem. Based on this observation, we propose Dynamic Prompt Learning (DPL) to force cross-attention maps to focus on correct noun words in the text prompt. By updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses, we achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions. Our method DPL, based on the publicly available Stable Diffusion, is extensively evaluated on a wide range of images, and consistently obtains superior results both quantitatively (CLIP score, Structure-Dist) and qualitatively (on user-evaluation). We show improved prompt editing results for Word-Swap, Prompt Refinement, and Attention Re-weighting, especially for complex multi-object scenes.\"\\n      },\\n      {\\n        \"paperId\": \"b1b688cf506f64bc4a06e037bfcdfa8b22be51aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b1b688cf506f64bc4a06e037bfcdfa8b22be51aa\",\\n        \"title\": \"Cascade Prompt Learning for Vision-Language Model Adaptation\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.17805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel Cascade Prompt Learning CasPL framework to enable prompt learning to serve both generic and specific expertise simultaneously, which achieves a significantly better balance between performance and inference speed, which is especially beneficial for deploying smaller VLM models in resource-constrained environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-09-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2212236182\",\\n            \"name\": \"Ge Wu\"\\n          },\\n          {\\n            \"authorId\": \"2268132119\",\\n            \"name\": \"Xin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2146248526\",\\n            \"name\": \"Zheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2268797049\",\\n            \"name\": \"Zhaowei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2320744562\",\\n            \"name\": \"Jiajun Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284825678\",\\n            \"name\": \"Jian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2320925245\",\\n            \"name\": \"Xiang Li\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt learning has surfaced as an effective approach to enhance the performance of Vision-Language Models (VLMs) like CLIP when applied to downstream tasks. However, current learnable prompt tokens are primarily used for the single phase of adapting to tasks (i.e., adapting prompt), easily leading to overfitting risks. In this work, we propose a novel Cascade Prompt Learning CasPL framework to enable prompt learning to serve both generic and specific expertise (i.e., boosting and adapting prompt) simultaneously. Specifically, CasPL is a new learning paradigm comprising two distinct phases of learnable prompts: the first boosting prompt is crafted to extract domain-general knowledge from a senior larger CLIP teacher model by aligning their predicted logits using extensive unlabeled domain images. The second adapting prompt is then cascaded with the frozen first set to fine-tune the downstream tasks, following the approaches employed in prior research. In this manner, CasPL can effectively capture both domain-general and task-specific representations into explicitly different gradual groups of prompts, thus potentially alleviating overfitting issues in the target domain. It\\'s worth noting that CasPL serves as a plug-and-play module that can seamlessly integrate into any existing prompt learning approach. CasPL achieves a significantly better balance between performance and inference speed, which is especially beneficial for deploying smaller VLM models in resource-constrained environments. Compared to the previous state-of-the-art method PromptSRC, CasPL shows an average improvement of 1.85% for base classes, 3.44% for novel classes, and 2.72% for the harmonic mean over 11 image classification datasets. Code is publicly available at: https://github.com/megvii-research/CasPL.\"\\n      },\\n      {\\n        \"paperId\": \"3e565c544a8639cc9c7568833e484d7610f5e5d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4\",\\n        \"title\": \"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning\",\\n        \"citationCount\": 372,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2209.14610\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.14610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel approach is proposed, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example, which verifies its effectiveness in selecting in- context examples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-09-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2887562\",\\n            \"name\": \"Pan Lu\"\\n          },\\n          {\\n            \"authorId\": \"47659905\",\\n            \"name\": \"Liang Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2782886\",\\n            \"name\": \"Kai-Wei Chang\"\\n          },\\n          {\\n            \"authorId\": \"39092098\",\\n            \"name\": \"Y. Wu\"\\n          },\\n          {\\n            \"authorId\": \"145380991\",\\n            \"name\": \"Song-Chun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2590556\",\\n            \"name\": \"Tanmay Rajpurohit\"\\n          },\\n          {\\n            \"authorId\": \"48323507\",\\n            \"name\": \"Peter Clark\"\\n          },\\n          {\\n            \"authorId\": \"51043791\",\\n            \"name\": \"A. Kalyan\"\\n          }\\n        ],\\n        \"abstract\": \"Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.\"\\n      },\\n      {\\n        \"paperId\": \"fc31c17e27c23cc89f91c110acd68b9b66ee0d40\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fc31c17e27c23cc89f91c110acd68b9b66ee0d40\",\\n        \"title\": \"Dynamic prompt-based virtual assistant framework for BIM information search\",\\n        \"citationCount\": 82,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.autcon.2023.105067?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.autcon.2023.105067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2202987937\",\\n            \"name\": \"Junwen Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2113447170\",\\n            \"name\": \"M. Fischer\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"4f9766356478ddf606457af735f70682839194b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4f9766356478ddf606457af735f70682839194b1\",\\n        \"title\": \"VPA: Fully Test-Time Visual Prompt Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.15251\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.15251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents Visual Prompt Adaptation (VPA), the first framework that generalizes visual prompting with test-time adaptation, and evaluates VPA on multiple tasks, including out-of-distribution generalization, corruption robustness, and domain adaptation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-09-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135326251\",\\n            \"name\": \"Jiachen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2249088902\",\\n            \"name\": \"Mark Ibrahim\"\\n          },\\n          {\\n            \"authorId\": \"2248025897\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"22229139\",\\n            \"name\": \"I. Evtimov\"\\n          },\\n          {\\n            \"authorId\": \"3895596\",\\n            \"name\": \"Z. Morley Mao\"\\n          },\\n          {\\n            \"authorId\": \"66286536\",\\n            \"name\": \"Cris-tian Cant\\\\u00f3n Ferrer\"\\n          },\\n          {\\n            \"authorId\": \"3322806\",\\n            \"name\": \"C. Hazirbas\"\\n          }\\n        ],\\n        \"abstract\": \"Textual prompt tuning has demonstrated significant performance improvements in adapting natural language processing models to a variety of downstream tasks by treating hand-engineered prompts as trainable parameters. Inspired by the success of textual prompting, several studies have investigated the efficacy of visual prompt tuning. In this work, we present Visual Prompt Adaptation (VPA), the first framework that generalizes visual prompting with test-time adaptation. VPA introduces a small number of learnable tokens, enabling fully test-time and storage-efficient adaptation without necessitating source-domain information. We examine our VPA design under diverse adaptation settings, encompassing single-image, batched-image, and pseudo-label adaptation. We evaluate VPA on multiple tasks, including out-of-distribution (OOD) generalization, corruption robustness, and domain adaptation. Experimental results reveal that VPA effectively enhances OOD generalization by 3.3% across various models, surpassing previous test-time approaches. Furthermore, we show that VPA improves corruption robustness by 6.5% compared to strong baselines. Finally, we demonstrate that VPA also boosts domain adaptation performance by relatively 5.2%. Our VPA also exhibits marked effectiveness in improving the robustness of zero-shot recognition for vision-language models.\"\\n      },\\n      {\\n        \"paperId\": \"546caf796b0ae3578852a5de8fe34f19f2de0cba\",\\n        \"url\": \"https://www.semanticscholar.org/paper/546caf796b0ae3578852a5de8fe34f19f2de0cba\",\\n        \"title\": \"Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.11484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enabling efficient policy learning and adaptation upon a wide range of environmental and physical changes encountered by embodied agents.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287939397\",\\n            \"name\": \"Wonje Choi\"\\n          },\\n          {\\n            \"authorId\": \"2174282424\",\\n            \"name\": \"Woo Kyung Kim\"\\n          },\\n          {\\n            \"authorId\": \"2288614462\",\\n            \"name\": \"Seunghyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"35756158\",\\n            \"name\": \"Hong-Suh Woo\"\\n          }\\n        ],\\n        \"abstract\": \"For embodied reinforcement learning (RL) agents interacting with the environment, it is desirable to have rapid policy adaptation to unseen visual observations, but achieving zero-shot adaptation capability is considered as a challenging problem in the RL context. To address the problem, we present a novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enabling efficient policy learning and adaptation upon a wide range of environmental and physical changes encountered by embodied agents. Specifically, we devise a guided-attention-based ensemble approach with multiple visual prompts on the vision-language model to construct robust state representations. Each prompt is contrastively learned in terms of an individual domain factor that significantly affects the agent\\'s egocentric perception and observation. For a given task, the attention-based ensemble and policy are jointly learned so that the resulting state representations not only generalize to various domains but are also optimized for learning the task. Through experiments, we show that ConPE outperforms other state-of-the-art algorithms for several embodied agent tasks including navigation in AI2THOR, manipulation in egocentric-Metaworld, and autonomous driving in CARLA, while also improving the sample efficiency of policy learning and adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n        \"title\": \"Prompt-and-Transfer: Dynamic Class-Aware Enhancement for Few-Shot Segmentation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.10389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel and powerful prompt-driven scheme, called \\\\u201cPrompt and Transfer\\\\u201d (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216452880\",\\n            \"name\": \"Hanbo Bi\"\\n          },\\n          {\\n            \"authorId\": \"6962129\",\\n            \"name\": \"Yingchao Feng\"\\n          },\\n          {\\n            \"authorId\": \"2600667\",\\n            \"name\": \"Wenhui Diao\"\\n          },\\n          {\\n            \"authorId\": \"152702629\",\\n            \"name\": \"Peijin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2161856190\",\\n            \"name\": \"Yongqiang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2303405697\",\\n            \"name\": \"Kun Fu\"\\n          },\\n          {\\n            \"authorId\": \"2242774676\",\\n            \"name\": \"Hongqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2177485483\",\\n            \"name\": \"Xian Sun\"\\n          }\\n        ],\\n        \"abstract\": \"For more efficient generalization to unseen domains (classes), most Few-shot Segmentation (FSS) would directly exploit pre-trained encoders and only fine-tune the decoder, especially in the current era of large models. However, such fixed feature encoders tend to be class-agnostic, inevitably activating objects that are irrelevant to the target class. In contrast, humans can effortlessly focus on specific objects in the line of sight. This paper mimics the visual perception pattern of human beings and proposes a novel and powerful prompt-driven scheme, called \\\\u201cPrompt and Transfer\\\\u201d (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task. Three key points are elaborated to enhance the prompting: 1) Cross-modal linguistic information is introduced to initialize prompts for each task. 2) Semantic Prompt Transfer (SPT) that precisely transfers the class-specific semantics within the images to prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT to adaptively generate different but complementary part prompts for different individuals. Surprisingly, PAT achieves competitive performance on 4 different tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new state-of-the-arts on 11 benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yu-Feng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"bd46cb09425c1eb8e2d2e7dd612d839cdf4d0f39\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bd46cb09425c1eb8e2d2e7dd612d839cdf4d0f39\",\\n        \"title\": \"Prompt-based Distribution Alignment for Unsupervised Domain Adaptation\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.09553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper experimentally demonstrates that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA and proposes a Prompt-based Distribution Alignment method to incorporate the domain knowledge into prompt learning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2275570714\",\\n            \"name\": \"Min Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"122132048\",\\n            \"name\": \"Siteng Huang\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275032226\",\\n            \"name\": \"Donglin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, despite the unprecedented success of large pre-trained visual-language models (VLMs) on a wide range of downstream tasks, the real-world unsupervised domain adaptation (UDA) problem is still not well explored. Therefore, in this paper, we first experimentally demonstrate that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA. However, a major challenge for directly deploying such models on downstream UDA tasks is prompt engineering, which requires aligning the domain knowledge of source and target domains, since the performance of UDA is severely influenced by a good domain-invariant representation. We further propose a Prompt-based Distribution Alignment (PDA) method to incorporate the domain knowledge into prompt learning. Specifically, PDA employs a two-branch prompt-tuning paradigm, namely base branch and alignment branch. The base branch focuses on integrating class-related representation into prompts, ensuring discrimination among different classes. To further minimize domain discrepancy, for the alignment branch, we construct feature banks for both the source and target domains and propose image-guided feature tuning (IFT) to make the input attend to feature banks, which effectively integrates self-enhanced and cross-domain features into the model. In this way, these two branches can be mutually promoted to enhance the adaptation of VLMs for UDA. We conduct extensive experiments on three benchmarks to demonstrate that our proposed PDA achieves state-of-the-art performance. The code is available at https://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.\"\\n      },\\n      {\\n        \"paperId\": \"fa9d49f32440aff7417ce46419d1073239b58b5b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa9d49f32440aff7417ce46419d1073239b58b5b\",\\n        \"title\": \"API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.03987\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.03987, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERE), an API entity-relation joint extraction model based on the large pre-trained language model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2181288908\",\\n            \"name\": \"Qing Huang\"\\n          },\\n          {\\n            \"authorId\": \"2199853522\",\\n            \"name\": \"Yanbang Sun\"\\n          },\\n          {\\n            \"authorId\": \"3138980\",\\n            \"name\": \"Zhenchang Xing\"\\n          },\\n          {\\n            \"authorId\": \"2152596785\",\\n            \"name\": \"Mingming Yu\"\\n          },\\n          {\\n            \"authorId\": \"3087664\",\\n            \"name\": \"Xiwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2151674574\",\\n            \"name\": \"Qinghua Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.\"\\n      },\\n      {\\n        \"paperId\": \"aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n        \"url\": \"https://www.semanticscholar.org/paper/aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n        \"title\": \"Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2304.03119\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.03119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Qualitative and quantitative evaluations on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-04-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148900054\",\\n            \"name\": \"Jiayi Guo\"\\n          },\\n          {\\n            \"authorId\": \"2135747990\",\\n            \"name\": \"Chaofei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2115860363\",\\n            \"name\": \"You Wu\"\\n          },\\n          {\\n            \"authorId\": \"2176400796\",\\n            \"name\": \"Eric Zhang\"\\n          },\\n          {\\n            \"authorId\": \"37833805\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"101246501\",\\n            \"name\": \"Xingqian Xu\"\\n          },\\n          {\\n            \"authorId\": \"30619669\",\\n            \"name\": \"S. Song\"\\n          },\\n          {\\n            \"authorId\": \"48667025\",\\n            \"name\": \"Humphrey Shi\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, CLIP-guided image synthesis has shown appealing performance on adapting a pre-trained source-domain generator to an unseen target domain. It does not require any target-domain samples but only the textual domain labels. The training is highly efficient, e.g., a few minutes. However, existing methods still have some limitations in the quality of generated images and may suffer from the mode collapse issue. A key reason is that a fixed adaptation direction is applied for all cross-domain image pairs, which leads to identical supervision signals. To address this issue, we propose an Image-specific Prompt Learning (IPL) method, which learns specific prompt vectors for each source-domain image. This produces a more precise adaptation direction for every cross-domain image pair, endowing the target-domain generator with greatly enhanced flexibility. Qualitative and quantitative evaluations on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse. Moreover, IPL is independent of the structure of the generative model, such as generative adversarial networks or diffusion models. Code is available at https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"5a8541bb468afb45d68e16d2f83b327768f97c67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5a8541bb468afb45d68e16d2f83b327768f97c67\",\\n        \"title\": \"Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.07648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to apply an auxiliary video captioning objective to train the frame representations, which facilitates the learning of detailed video semantics by providing fine-grained guidance in the semantic space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41036094\",\\n            \"name\": \"Chaorui Deng\"\\n          },\\n          {\\n            \"authorId\": \"2115814744\",\\n            \"name\": \"Qi Chen\"\\n          },\\n          {\\n            \"authorId\": \"1937786\",\\n            \"name\": \"Pengda Qin\"\\n          },\\n          {\\n            \"authorId\": \"73286206\",\\n            \"name\": \"Dave Zhenyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2143599197\",\\n            \"name\": \"Qi Wu\"\\n          }\\n        ],\\n        \"abstract\": \"In text-video retrieval, recent works have benefited from the powerful learning capabilities of pre-trained text-image foundation models (e.g., CLIP) by adapting them to the video domain. A critical problem for them is how to effectively capture the rich semantics inside the video using the image encoder of CLIP. To tackle this, state-of-the-art methods adopt complex cross-modal modeling techniques to fuse the text information into video frame representations, which, however, incurs severe efficiency issues in large-scale retrieval systems as the video representations must be recomputed online for every text query. In this paper, we discard this problematic cross-modal fusion process and aim to learn semantically-enhanced representations purely from the video, so that the video representations can be computed offline and reused for different texts. Concretely, we first introduce a spatial-temporal \\\\\"Prompt Cube\\\\\" into the CLIP image encoder and iteratively switch it within the encoder layers to efficiently incorporate the global video semantics into frame representations. We then propose to apply an auxiliary video captioning objective to train the frame representations, which facilitates the learning of detailed video semantics by providing fine-grained guidance in the semantic space. With a naive temporal fusion strategy (i.e., mean-pooling) on the enhanced frame representations, we obtain state-of-the-art performances on three benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.\"\\n      },\\n      {\\n        \"paperId\": \"a4a26d7c6bc022f604a246c6ce6cc1cfbe618441\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a4a26d7c6bc022f604a246c6ce6cc1cfbe618441\",\\n        \"title\": \"APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.emnlp-main.567.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.emnlp-main.567?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.emnlp-main.567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Attention Prompt tuning method, namely AP ROMPT, which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2266712798\",\\n            \"name\": \"Qifan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2272672481\",\\n            \"name\": \"Yuning Mao\"\\n          },\\n          {\\n            \"authorId\": \"2109593338\",\\n            \"name\": \"Jingang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273361006\",\\n            \"name\": \"Hanchao Yu\"\\n          },\\n          {\\n            \"authorId\": \"35557488\",\\n            \"name\": \"Shaoliang Nie\"\\n          },\\n          {\\n            \"authorId\": \"2237101143\",\\n            \"name\": \"Sinong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2271382818\",\\n            \"name\": \"Fuli Feng\"\\n          },\\n          {\\n            \"authorId\": \"2273197501\",\\n            \"name\": \"Lifu Huang\"\\n          },\\n          {\\n            \"authorId\": \"2266062308\",\\n            \"name\": \"Xiaojun Quan\"\\n          },\\n          {\\n            \"authorId\": \"2273481670\",\\n            \"name\": \"Zenglin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2264378887\",\\n            \"name\": \"Dongfang Liu\"\\n          }\\n        ],\\n        \"abstract\": \"With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large pre-trained language models. However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement. In this work, we propose a novel Attention Prompt tuning method, namely AP ROMPT , for efficient adaptation of pre-trained language models. We first demonstrate that existing prompt tuning can be considered as a special case of attention prompt tuning. We then formally introduce AP ROMPT , which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning. Experimental results on the SuperGLUE benchmark consistently demonstrate that our proposed approach outperforms state-of-the-art baselines and full fine-tuning method with pre-trained models at different scales. In addition, a comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f849b873e94f28e1f2a3e1dc4d7bef17eb64adab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f849b873e94f28e1f2a3e1dc4d7bef17eb64adab\",\\n        \"title\": \"Towards Stable Test-Time Adaptation in Dynamic Wild World\",\\n        \"citationCount\": 372,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2302.12400\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.12400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1411039233\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"2239515454\",\\n            \"name\": \"Jiaxiang Wu\"\\n          },\\n          {\\n            \"authorId\": \"2108463891\",\\n            \"name\": \"Yifan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143976851\",\\n            \"name\": \"Z. Wen\"\\n          },\\n          {\\n            \"authorId\": \"1816749481\",\\n            \"name\": \"Yaofo Chen\"\\n          },\\n          {\\n            \"authorId\": \"144259957\",\\n            \"name\": \"P. Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2823637\",\\n            \"name\": \"Mingkui Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, \\\\\\\\ie, group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, \\\\\\\\ie, assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5c76e929496414ff9b31f100eaf72ff46ae4d8d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5c76e929496414ff9b31f100eaf72ff46ae4d8d7\",\\n        \"title\": \"Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.11808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyT is proposed, a novel approach to improve both parameter and inference efficiency for ViT adaptation by proposing a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292217857\",\\n            \"name\": \"Wangbo Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2284481794\",\\n            \"name\": \"Jiasheng Tang\"\\n          },\\n          {\\n            \"authorId\": \"40961502\",\\n            \"name\": \"Yizeng Han\"\\n          },\\n          {\\n            \"authorId\": \"2292187802\",\\n            \"name\": \"Yibing Song\"\\n          },\\n          {\\n            \"authorId\": \"2292214744\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"2249906537\",\\n            \"name\": \"Gao Huang\"\\n          },\\n          {\\n            \"authorId\": \"1716453\",\\n            \"name\": \"F. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2283134324\",\\n            \"name\": \"Yang You\"\\n          }\\n        ],\\n        \"abstract\": \"Existing parameter-efficient fine-tuning (PEFT) methods have achieved significant success on vision transformers (ViTs) adaptation by improving parameter efficiency. However, the exploration of enhancing inference efficiency during adaptation remains underexplored. This limits the broader application of pre-trained ViT models, especially when the model is computationally extensive. In this paper, we propose Dynamic Tuning (DyT), a novel approach to improve both parameter and inference efficiency for ViT adaptation. Specifically, besides using the lightweight adapter modules, we propose a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference. Additionally, we explore multiple design variants to find the best practice of DyT. Finally, inspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced adapter to further boost the adaptation performance. We validate DyT across various tasks, including image/video recognition and semantic segmentation. For instance, DyT achieves superior performance compared to existing PEFT methods while evoking only 71% of their FLOPs on the VTAB-1K benchmark.\"\\n      },\\n      {\\n        \"paperId\": \"64ffcf31b03aaae2905e06f9e24be58f51e2f776\",\\n        \"url\": \"https://www.semanticscholar.org/paper/64ffcf31b03aaae2905e06f9e24be58f51e2f776\",\\n        \"title\": \"Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2311.18363\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.18363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the pretrained model and proposes the Visual Prompt-based Test-Time Adaptation (VPTTA) method, which is lightweight with only a few parameters and can be effectively trained in a single iteration, and presents the low-frequency prompt, which is lightweight with only a few parameters and can be effectively trained in a single iteration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268795762\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2170220899\",\\n            \"name\": \"Yiwen Ye\"\\n          },\\n          {\\n            \"authorId\": \"143746093\",\\n            \"name\": \"Mengkang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2268798668\",\\n            \"name\": \"Yongsheng Pan\"\\n          },\\n          {\\n            \"authorId\": \"2256029665\",\\n            \"name\": \"Yong Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Distribution shift widely exists in medical images acquired from different medical centres and poses a significant obstacle to deploying the pretrained semantic segmentation model in real-world applications. Test-time adaptation has proven its effectiveness in tackling the cross-domain distribution shift during inference. However, most existing methods achieve adaptation by updating the pretrained models, rendering them susceptible to error accumulation and catastrophic forgetting when encountering a series of distribution shifts (i.e., under the continual test-time adaptation setup). To overcome these challenges caused by updating the models, in this paper, we freeze the pretrained model and propose the Visual Prompt-based Test-Time Adaptation (VPTTA) method to train a specific prompt for each test image to align the statistics in the batch normalization layers. Specifically, we present the low-frequency prompt, which is lightweight with only a few parameters and can be effectively trained in a single iteration. To enhance prompt initialization, we equip VPTTA with a memory bank to benefit the current prompt from previous ones. Additionally, we design a warm-up mechanism, which mixes source and target statistics to construct warm-up statistics, thereby facilitating the training process. Extensive experiments demonstrate the superiority of our VPTTA over other state-of-the-art methods on two medical image segmentation benchmark tasks. The code and weights of pretrained source models are available at https://github.com/Chen-Ziyang/VPTTA.\"\\n      },\\n      {\\n        \"paperId\": \"8125cc111e55b9d3e14bf3df9aba5d10c08417f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8125cc111e55b9d3e14bf3df9aba5d10c08417f1\",\\n        \"title\": \"A dynamic collaborative adversarial domain adaptation network for unsupervised rotating machinery fault diagnosis\",\\n        \"citationCount\": 76,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.ress.2024.110662?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.ress.2024.110662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299759595\",\\n            \"name\": \"Xin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256951279\",\\n            \"name\": \"Hongkai Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2266603243\",\\n            \"name\": \"Mingzhe Mu\"\\n          },\\n          {\\n            \"authorId\": \"2152989254\",\\n            \"name\": \"Yutong Dong\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"e6357834e6145b607c350b92238b5bc4d662e920\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e6357834e6145b607c350b92238b5bc4d662e920\",\\n        \"title\": \"Robust Test-Time Adaptation in Dynamic Scenarios\",\\n        \"citationCount\": 171,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.13899\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.13899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA, and presents a robust batch normalization scheme to estimate the normalization statistics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145235797\",\\n            \"name\": \"Longhui Yuan\"\\n          },\\n          {\\n            \"authorId\": \"1896719712\",\\n            \"name\": \"Binhui Xie\"\\n          },\\n          {\\n            \"authorId\": \"2165375715\",\\n            \"name\": \"Shuangliang Li\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time adaptation (TTA) intends to adapt the pretrained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distributions. However, these attempts may fail in dynamic scenarios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we explore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Extensive experiments prove that RoTTA enables continual test-time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA\"\\n      },\\n      {\\n        \"paperId\": \"b53604e693978aaa929136bd33d432fe69214d88\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b53604e693978aaa929136bd33d432fe69214d88\",\\n        \"title\": \"Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.01120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RAGPT is proposed, a novel Retrieval-AuGmented dynamic Prompt Tuning framework that consistently outperforms all competitive baselines in handling incomplete modality problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310823837\",\\n            \"name\": \"Jian Lang\"\\n          },\\n          {\\n            \"authorId\": \"2174885298\",\\n            \"name\": \"Zhangtao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"46456474\",\\n            \"name\": \"Ting Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2273625855\",\\n            \"name\": \"Fan Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Multimodal learning with incomplete modality is practical and challenging. Recently, researchers have focused on enhancing the robustness of pre-trained MultiModal Transformers (MMTs) under missing modality conditions by applying learnable prompts. However, these prompt-based methods face several limitations: (1) incomplete modalities provide restricted modal cues for task-specific inference, (2) dummy imputation for missing content causes information loss and introduces noise, and (3) static prompts are instance-agnostic, offering limited knowledge for instances with various missing conditions. To address these issues, we propose RAGPT, a novel Retrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three modules: (I) the multi-channel retriever, which identifies similar instances through a within-modality retrieval strategy, (II) the missing modality generator, which recovers missing information using retrieved contexts, and (III) the context-aware prompter, which captures contextual knowledge from relevant instances and generates dynamic prompts to largely enhance the MMT\\\\u2019s robustness. Extensive experiments conducted on three real-world datasets show that RAGPT consistently outperforms all competitive baselines in handling incomplete modality problems.\"\\n      },\\n      {\\n        \"paperId\": \"6f75404b0d01f9a09afe428f9efd5cbcd7825469\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6f75404b0d01f9a09afe428f9efd5cbcd7825469\",\\n        \"title\": \"Dynamic Prompting: A Unified Framework for Prompt Tuning\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.02909\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.02909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A theoretical analysis is provided, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-03-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145170944\",\\n            \"name\": \"Xianjun Yang\"\\n          },\\n          {\\n            \"authorId\": \"145859270\",\\n            \"name\": \"Wei Cheng\"\\n          },\\n          {\\n            \"authorId\": \"50879401\",\\n            \"name\": \"Xujiang Zhao\"\\n          },\\n          {\\n            \"authorId\": \"21038849\",\\n            \"name\": \"Linda Petzold\"\\n          },\\n          {\\n            \"authorId\": \"2145225543\",\\n            \"name\": \"Haifeng Chen\"\\n          }\\n        ],\\n        \"abstract\": \"It has been demonstrated that the art of prompt tuning is highly effective in efficiently extracting knowledge from pretrained foundation models, encompassing pretrained language models (PLMs), vision pretrained models, and vision-language (V-L) models. However, the efficacy of employing fixed soft prompts with a predetermined position for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. In this context, we provide a theoretical analysis, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Building upon our analysis, we present a unified dynamic prompt (DP) tuning strategy that dynamically determines different factors of prompts based on specific tasks and instances. To accomplish this, we employ a lightweight learning network with Gumble-Softmax, allowing us to learn instance-dependent guidance. Experimental results underscore the significant performance improvement achieved by dynamic prompt tuning across a wide range of tasks, including NLP tasks, vision recognition tasks, and vision-language tasks. Furthermore, we establish the universal applicability of our approach under full-data, few-shot, and multitask scenarios. Codes are available at https://github.com/Xianjun-Yang/DPT.\"\\n      },\\n      {\\n        \"paperId\": \"ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"title\": \"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.10462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation, is proposed and experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2241468593\",\\n            \"name\": \"Hossein Rajabzadeh\"\\n          },\\n          {\\n            \"authorId\": \"9200111\",\\n            \"name\": \"Mojtaba Valipour\"\\n          },\\n          {\\n            \"authorId\": \"2284643707\",\\n            \"name\": \"Tianshu Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1996315\",\\n            \"name\": \"Marzieh S. Tahaei\"\\n          },\\n          {\\n            \"authorId\": \"2241480742\",\\n            \"name\": \"Hyock Ju Kwon\"\\n          },\\n          {\\n            \"authorId\": \"2237425782\",\\n            \"name\": \"Ali Ghodsi\"\\n          },\\n          {\\n            \"authorId\": \"2237517964\",\\n            \"name\": \"Boxing Chen\"\\n          },\\n          {\\n            \"authorId\": \"2066076226\",\\n            \"name\": \"Mehdi Rezagholizadeh\"\\n          }\\n        ],\\n        \"abstract\": \"Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n      },\\n      {\\n        \"paperId\": \"74a68f4798a34c7254297babd1f3222408198799\",\\n        \"url\": \"https://www.semanticscholar.org/paper/74a68f4798a34c7254297babd1f3222408198799\",\\n        \"title\": \"Adversarial Domain Adaptation Network With Calibrated Prototype and Dynamic Instance Convolution for Hyperspectral Image Classification\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TGRS.2024.3387990?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TGRS.2024.3387990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2125738235\",\\n            \"name\": \"Yi Huang\"\\n          },\\n          {\\n            \"authorId\": \"2444202\",\\n            \"name\": \"Jiangtao Peng\"\\n          },\\n          {\\n            \"authorId\": \"2296669816\",\\n            \"name\": \"Genwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2282635307\",\\n            \"name\": \"Weiwei Sun\"\\n          },\\n          {\\n            \"authorId\": \"2118767432\",\\n            \"name\": \"Na Chen\"\\n          },\\n          {\\n            \"authorId\": \"2072520954\",\\n            \"name\": \"Q. Du\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, the adversarial domain adaptation (ADA) methods have been widely investigated and applied in cross-domain hyperspectral image (HSI) classification. However, most ADA algorithms aim to align the cross-domain distribution without focusing on the class separability of the aligned target features and the information of samples within the domain. To address these issues, a new ADA framework based on calibrated prototype and dynamic instance convolution (CPDIC) is proposed in this article for cross-domain HSI classification. The CPDIC is composed of a generator, a calibrated discriminator (CD), and a classifier. The generator includes a static 3-D convolutional network (SCN) and a dynamic instance convolutional network (DICN), where the SCN is used to extract coarse-grained features of HSI and the DICN can extract sample-specific fine-grained features using instance convolutions generated from dynamic instance convolution kernel generation (DCKG) module. As for the generator, the static and dynamic interactive feature extraction network extracts robust domain-invariant features with discriminability. The CD aligns the marginal distribution between domains and calibrate the predicted pseudo-labels of target domain. For classification, a calibrated prototype loss (CPL) is introduced to align the class distribution across domains. The results of three cross-domain HSI classification tasks show that the proposed CPDIC outperforms existing unsupervised domain adaptation (UDA) algorithms.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 208,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"fb2331ec0bb1efdfd023d69fdb1c2a15b30f1f87\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fb2331ec0bb1efdfd023d69fdb1c2a15b30f1f87\",\\n        \"title\": \"MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.09010\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.09010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders for DFER, and identifies main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72707603\",\\n            \"name\": \"K. Chumachenko\"\\n          },\\n          {\\n            \"authorId\": \"2248136895\",\\n            \"name\": \"Alexandros Iosifidis\"\\n          },\\n          {\\n            \"authorId\": \"1715200\",\\n            \"name\": \"M. Gabbouj\"\\n          }\\n        ],\\n        \"abstract\": \"Dynamic Facial Expression Recognition (DFER) has received significant interest in the recent years dictated by its pivotal role in enabling empathic and human-compatible technologies. Achieving robustness towards in-the-wild data in DFER is particularly important for real-world applications. One of the directions aimed at improving such models is multimodal emotion recognition based on audio and video data. Multimodal learning in DFER increases the model capabilities by leveraging richer, complementary data representations. Within the field of multimodal DFER, recent methods have focused on exploiting advances of self-supervised learning (SSL) for pre-training of strong multi-modal encoders [40]. Another line of research has focused on adapting pre-trained static models for DFER [8]. In this work, we propose a different perspective on the problem and investigate the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders. We identify main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation, and propose solutions to each of them. As a result, we demonstrate improvement over current state-of-the-art on two popular DFER benchmarks, namely DFEW [19] and MFAW [29].\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"1b4012f38daa8f09299e16771973c91ce9464ee2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b4012f38daa8f09299e16771973c91ce9464ee2\",\\n        \"title\": \"DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.09787\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.09787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a novel method called Dynamic Visual Prompt Tuning (DVPT), which efficiently adapts pre-trained models to the medical domain but also enhances data efficiency with limited labeled data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153245407\",\\n            \"name\": \"Along He\"\\n          },\\n          {\\n            \"authorId\": \"37833805\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"2216099246\",\\n            \"name\": \"Zhihong Wang\"\\n          },\\n          {\\n            \"authorId\": \"1840553660\",\\n            \"name\": \"Tao Li\"\\n          },\\n          {\\n            \"authorId\": \"1929093\",\\n            \"name\": \"H. Fu\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-training and fine-tuning have become popular due to the rich representations embedded in large pre-trained models, which can be leveraged for downstream medical tasks. However, existing methods typically either fine-tune all parameters or only task-specific layers of pre-trained models, overlooking the variability in input medical images. As a result, these approaches may lack efficiency or effectiveness. In this study, our goal is to explore parameter-efficient fine-tuning (PEFT) for medical image analysis. To address this challenge, we introduce a novel method called Dynamic Visual Prompt Tuning (DVPT). It can extract knowledge beneficial to downstream tasks from large models with only a few trainable parameters. First, the frozen features are transformed by a lightweight bottleneck layer to learn the domain-specific distribution of downstream medical tasks. Then, a few learnable visual prompts are employed as dynamic queries to conduct cross-attention with the transformed features, aiming to acquire sample-specific features. This DVPT module can be shared across different Transformer layers, further reducing the number of trainable parameters. We conduct extensive experiments with various pre-trained models on medical classification and segmentation tasks. We find that this PEFT method not only efficiently adapts pre-trained models to the medical domain but also enhances data efficiency with limited labeled data. For example, with only 0.5% additional trainable parameters, our method not only outperforms state-of-the-art PEFT methods but also surpasses full fine-tuning by more than 2.20% in Kappa score on the medical classification task. It can save up to 60% of labeled data and 99% of storage cost of ViT-B/16.\"\\n      },\\n      {\\n        \"paperId\": \"c28b7dfe341f1e13a5a98efbce7946ef795cf9b8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c28b7dfe341f1e13a5a98efbce7946ef795cf9b8\",\\n        \"title\": \"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer\",\\n        \"citationCount\": 309,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2022.acl-long.346.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2110.07904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that SPoT significantly boosts the performance of Prompt Tuning across many tasks, and an efficient retrieval approach is proposed that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144244743\",\\n            \"name\": \"Tu Vu\"\\n          },\\n          {\\n            \"authorId\": \"144104130\",\\n            \"name\": \"Brian Lester\"\\n          },\\n          {\\n            \"authorId\": \"40832517\",\\n            \"name\": \"Noah Constant\"\\n          },\\n          {\\n            \"authorId\": \"1388360943\",\\n            \"name\": \"Rami Al-Rfou\"\\n          },\\n          {\\n            \"authorId\": \"46724030\",\\n            \"name\": \"Daniel Matthew Cer\"\\n          }\\n        ],\\n        \"abstract\": \"There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks. Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks. More remarkably, across all model sizes, SPoT matches or outperforms standard Model Tuning (which fine-tunes all model parameters) on the SuperGLUE benchmark, while using up to 27,000\\\\u00d7 fewer task-specific parameters. To understand where SPoT is most effective, we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations, and demonstrate that many tasks can benefit each other via prompt transfer. Finally, we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.\"\\n      },\\n      {\\n        \"paperId\": \"81babe3cb42e620b537ecb5e6af540e3be2c3ca7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/81babe3cb42e620b537ecb5e6af540e3be2c3ca7\",\\n        \"title\": \"Visual Prompt Tuning for Test-time Domain Adaptation\",\\n        \"citationCount\": 105,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.04831\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.04831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work tackles the practical yet challenging test-time adaptation (TTA) problem, where a model adapts to the target domain without accessing the source data, with a simple recipe called Data-efficient Prompt Tuning (DePT).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282248267\",\\n            \"name\": \"Yunhe Gao\"\\n          },\\n          {\\n            \"authorId\": \"2110332219\",\\n            \"name\": \"Xingjian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2128816752\",\\n            \"name\": \"Yi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2359832\",\\n            \"name\": \"Hongya Wang\"\\n          },\\n          {\\n            \"authorId\": \"2986505\",\\n            \"name\": \"Zhiqiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"51453885\",\\n            \"name\": \"Xiong Zhou\"\\n          },\\n          {\\n            \"authorId\": \"1701799\",\\n            \"name\": \"Mu Li\"\\n          },\\n          {\\n            \"authorId\": \"1711560\",\\n            \"name\": \"Dimitris N. Metaxas\"\\n          }\\n        ],\\n        \"abstract\": \"Models should be able to adapt to unseen data during test-time to avoid performance drops caused by inevitable distribution shifts in real-world deployment scenarios. In this work, we tackle the practical yet challenging test-time adaptation (TTA) problem, where a model adapts to the target domain without accessing the source data. We propose a simple recipe called \\\\\\\\textit{Data-efficient Prompt Tuning} (DePT) with two key ingredients. First, DePT plugs visual prompts into the vision Transformer and only tunes these source-initialized prompts during adaptation. We find such parameter-efficient finetuning can efficiently adapt the model representation to the target domain without overfitting to the noise in the learning objective. Second, DePT bootstraps the source representation to the target domain by memory bank-based online pseudo-labeling. A hierarchical self-supervised regularization specially designed for prompts is jointly optimized to alleviate error accumulation during self-training. With much fewer tunable parameters, DePT demonstrates not only state-of-the-art performance on major adaptation benchmarks VisDA-C, ImageNet-C, and DomainNet-126, but also superior data efficiency, i.e., adaptation with only 1\\\\\\\\% or 10\\\\\\\\% data without much performance degradation compared to 100\\\\\\\\% data. In addition, DePT is also versatile to be extended to online or multi-source TTA settings.\"\\n      },\\n      {\\n        \"paperId\": \"0392d58335ce674a70f5e58ac8c438de296a0e6a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0392d58335ce674a70f5e58ac8c438de296a0e6a\",\\n        \"title\": \"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models\",\\n        \"citationCount\": 172,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2208.07852\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2208.07852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task, and then allows easy deployment of the newly created ad-hoc models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-08-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2879705\",\\n            \"name\": \"Hendrik Strobelt\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2285868436\",\\n            \"name\": \"Victor Sanh\"\\n          },\\n          {\\n            \"authorId\": \"2061407009\",\\n            \"name\": \"Benjamin Hoover\"\\n          },\\n          {\\n            \"authorId\": \"47273741\",\\n            \"name\": \"Johanna Beyer\"\\n          },\\n          {\\n            \"authorId\": \"143758236\",\\n            \"name\": \"H. Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2531268\",\\n            \"name\": \"Alexander M. Rush\"\\n          }\\n        ],\\n        \"abstract\": \"State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.\"\\n      }\\n    ]\\n  },\\n  \"adaptive prompt generation\": {\\n    \"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\",\\n    \"code\": \"429\"\\n  },\\n  \"iterative context update\": {\\n    \"total\": 8986,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 276,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 201,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\', \\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 182,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 359,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 322,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 68,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 47,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 40,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2801,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"title\": \"Spontaneous Reward Hacking in Iterative Self-Refinement\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Using an essay editing task, it is shown that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311112294\",\\n            \"name\": \"Jane Pan\"\\n          },\\n          {\\n            \"authorId\": \"2321875898\",\\n            \"name\": \"He He\"\\n          },\\n          {\\n            \"authorId\": \"2297768298\",\\n            \"name\": \"Samuel R. Bowman\"\\n          },\\n          {\\n            \"authorId\": \"2297816489\",\\n            \"name\": \"Shi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of human users, a second language model can be used as an evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator\\'s ratings improve while the generation quality remains stagnant or even decreases as judged by actual user preference. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive them to exploit shared vulnerabilities. Using an essay editing task, we show that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement. In addition, we study conditions under which reward hacking occurs and observe two factors that affect reward hacking severity: model size and context sharing between the generator and the evaluator.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      },\\n      {\\n        \"paperId\": \"f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"title\": \"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.01701?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      }\\n    ]\\n  },\\n  \"summarization prior interactions\": {\\n    \"total\": 16594,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a\",\\n        \"title\": \"TidyBot: Personalized Robot Assistance with Large Language Models\",\\n        \"citationCount\": 370,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2305.05658\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.05658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155142153\",\\n            \"name\": \"Jimmy Wu\"\\n          },\\n          {\\n            \"authorId\": \"39534622\",\\n            \"name\": \"Rika Antonova\"\\n          },\\n          {\\n            \"authorId\": \"2216606824\",\\n            \"name\": \"Adam Kan\"\\n          },\\n          {\\n            \"authorId\": \"10710717\",\\n            \"name\": \"Marion Lepert\"\\n          },\\n          {\\n            \"authorId\": \"38591293\",\\n            \"name\": \"Andy Zeng\"\\n          },\\n          {\\n            \"authorId\": \"3340170\",\\n            \"name\": \"Shuran Song\"\\n          },\\n          {\\n            \"authorId\": \"1775407\",\\n            \"name\": \"Jeannette Bohg\"\\n          },\\n          {\\n            \"authorId\": \"7723706\",\\n            \"name\": \"S. Rusinkiewicz\"\\n          },\\n          {\\n            \"authorId\": \"1807080\",\\n            \"name\": \"T. Funkhouser\"\\n          }\\n        ],\\n        \"abstract\": \"For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people\\'s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"0725debf3183589626823dbb64107bba8ed22448\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0725debf3183589626823dbb64107bba8ed22448\",\\n        \"title\": \"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.01596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids, is introduced and the value of integrating visual information from images to improve the creation of medically detailed summaries is demonstrated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275156796\",\\n            \"name\": \"Akash Ghosh\"\\n          },\\n          {\\n            \"authorId\": \"2273560136\",\\n            \"name\": \"Arkadeep Acharya\"\\n          },\\n          {\\n            \"authorId\": \"2165947844\",\\n            \"name\": \"Prince Jha\"\\n          },\\n          {\\n            \"authorId\": \"2277599094\",\\n            \"name\": \"Aniket Gaudgaul\"\\n          },\\n          {\\n            \"authorId\": \"2277600473\",\\n            \"name\": \"Rajdeep Majumdar\"\\n          },\\n          {\\n            \"authorId\": \"2243463123\",\\n            \"name\": \"Sriparna Saha\"\\n          },\\n          {\\n            \"authorId\": \"2275226689\",\\n            \"name\": \"Aman Chadha\"\\n          },\\n          {\\n            \"authorId\": \"2088137695\",\\n            \"name\": \"Raghav Jain\"\\n          },\\n          {\\n            \"authorId\": \"2275161937\",\\n            \"name\": \"Setu Sinha\"\\n          },\\n          {\\n            \"authorId\": \"2277651208\",\\n            \"name\": \"Shivani Agarwal\"\\n          }\\n        ],\\n        \"abstract\": \"In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient\\'s medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries. This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care. Our dataset, code, and pre-trained models will be made publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c1799bf28d1ae93e1631be5b59196ee1e568f538\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c1799bf28d1ae93e1631be5b59196ee1e568f538\",\\n        \"title\": \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\",\\n        \"citationCount\": 820,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GraphRAG is proposed, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text and leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2298275009\",\\n            \"name\": \"Darren Edge\"\\n          },\\n          {\\n            \"authorId\": \"2213073417\",\\n            \"name\": \"Ha Trinh\"\\n          },\\n          {\\n            \"authorId\": \"2298273973\",\\n            \"name\": \"Newman Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2298275805\",\\n            \"name\": \"Joshua Bradley\"\\n          },\\n          {\\n            \"authorId\": \"2298274563\",\\n            \"name\": \"Alex Chao\"\\n          },\\n          {\\n            \"authorId\": \"2210994342\",\\n            \"name\": \"Apurva N. Mody\"\\n          },\\n          {\\n            \"authorId\": \"2298273810\",\\n            \"name\": \"Steven Truitt\"\\n          },\\n          {\\n            \"authorId\": \"2298278846\",\\n            \"name\": \"Jonathan Larson\"\\n          }\\n        ],\\n        \"abstract\": \"The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as\\\\\"What are the main themes in the dataset?\\\\\", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.\"\\n      },\\n      {\\n        \"paperId\": \"4bf0c0f4aaf67f5138d5036c044a5d0de8938181\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4bf0c0f4aaf67f5138d5036c044a5d0de8938181\",\\n        \"title\": \"Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.13190\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.13190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel role interaction enhanced method that adopts cross attention and decoder self-attention interactions to interactively acquire other roles\\\\u2019 critical information for role-oriented dialogue summarization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152781993\",\\n            \"name\": \"Haitao Lin\"\\n          },\\n          {\\n            \"authorId\": \"24925751\",\\n            \"name\": \"Junnan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"145475526\",\\n            \"name\": \"Lu Xiang\"\\n          },\\n          {\\n            \"authorId\": \"2110631853\",\\n            \"name\": \"Yu Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2124819243\",\\n            \"name\": \"Jiajun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2064100826\",\\n            \"name\": \"Chengqing Zong\"\\n          }\\n        ],\\n        \"abstract\": \"Role-oriented dialogue summarization is to generate summaries for different roles in the dialogue, e.g., merchants and consumers. Existing methods handle this task by summarizing each role\\\\u2019s content separately and thus are prone to ignore the information from other roles. However, we believe that other roles\\\\u2019 content could benefit the quality of summaries, such as the omitted information mentioned by other roles. Therefore, we propose a novel role interaction enhanced method for role-oriented dialogue summarization. It adopts cross attention and decoder self-attention interactions to interactively acquire other roles\\\\u2019 critical information. The cross attention interaction aims to select other roles\\\\u2019 critical dialogue utterances, while the decoder self-attention interaction aims to obtain key information from other roles\\\\u2019 summaries. Experimental results have shown that our proposed method significantly outperforms strong baselines on two public role-oriented dialogue summarization datasets. Extensive analyses have demonstrated that other roles\\\\u2019 content could help generate summaries with more complete semantics and correct topic structures.\"\\n      },\\n      {\\n        \"paperId\": \"65c5e3e3be6e881c7d2feef4c0c3d07323f3aeff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/65c5e3e3be6e881c7d2feef4c0c3d07323f3aeff\",\\n        \"title\": \"Make The Most of Prior Data: A Solution for Interactive Text Summarization with Preference Feedback\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2204.05512\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.05512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new framework to train summarization models with preference feedback interactively with properly leveraging offline data and a novel reward model is introduced to improve the performance regarding ROUGE scores and sample-efficiency.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2153960542\",\\n            \"name\": \"Duy-Hung Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"38714383\",\\n            \"name\": \"Nguyen-Viet-Dung Nghiem\"\\n          },\\n          {\\n            \"authorId\": \"2140448395\",\\n            \"name\": \"Bao-Sinh Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2142143947\",\\n            \"name\": \"Dung Tien Le\"\\n          },\\n          {\\n            \"authorId\": \"2162186181\",\\n            \"name\": \"Shahab Sabahi\"\\n          },\\n          {\\n            \"authorId\": \"1789308\",\\n            \"name\": \"Minh Le Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2064728738\",\\n            \"name\": \"Hung Le\"\\n          }\\n        ],\\n        \"abstract\": \"For summarization, human preference is critical to tame outputs of the summarizer in favor of human interests, as ground-truth summaries are scarce and ambiguous. Practical settings require dynamic exchanges between human and AI agent wherein feedback is provided in an online manner, a few at a time. In this paper, we introduce a new framework to train summarization models with preference feedback interactively. By properly leveraging offline data and a novel reward model, we improve the performance regarding ROUGE scores and sample-efficiency. Our experiments on three various datasets confirm the benefit of the proposed framework in active, few-shot and online settings of preference learning.\"\\n      },\\n      {\\n        \"paperId\": \"9b86fc9596f5bc027a0a507414b2c4ecf60bcbbf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9b86fc9596f5bc027a0a507414b2c4ecf60bcbbf\",\\n        \"title\": \"Towards Clinical Encounter Summarization: Learning to Compose Discharge Summaries from Prior Notes\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2104.13498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Two new measures, faithfulness and hallucination rate, are introduced for evaluation in this task, which complement existing measures for fluency and informativeness.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-04-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46183205\",\\n            \"name\": \"Han-Chin Shing\"\\n          },\\n          {\\n            \"authorId\": \"1866532\",\\n            \"name\": \"Chaitanya P. Shivade\"\\n          },\\n          {\\n            \"authorId\": \"3158273\",\\n            \"name\": \"Nima Pourdamghani\"\\n          },\\n          {\\n            \"authorId\": \"144647318\",\\n            \"name\": \"Feng Nan\"\\n          },\\n          {\\n            \"authorId\": \"1680292\",\\n            \"name\": \"P. Resnik\"\\n          },\\n          {\\n            \"authorId\": \"1737250\",\\n            \"name\": \"Douglas W. Oard\"\\n          },\\n          {\\n            \"authorId\": \"50339091\",\\n            \"name\": \"Parminder Bhatia\"\\n          }\\n        ],\\n        \"abstract\": \"The records of a clinical encounter can be extensive and complex, thus placing a premium on tools that can extract and summarize relevant information. This paper introduces the task of generating discharge summaries for a clinical encounter. Summaries in this setting need to be faithful, traceable, and scale to multiple long documents, motivating the use of extract-then-abstract summarization cascades. We introduce two new measures, faithfulness and hallucination rate for evaluation in this task, which complement existing measures for fluency and informativeness. Results across seven medical sections and five models show that a summarization architecture that supports traceability yields promising results, and that a sentence-rewriting approach performs consistently on the measure used for faithfulness (faithfulness-adjusted $F_3$) over a diverse range of generated sections.\"\\n      },\\n      {\\n        \"paperId\": \"ba4548c01a8b29a406f269db80f7105b88ca9751\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ba4548c01a8b29a406f269db80f7105b88ca9751\",\\n        \"title\": \"FABLES: Evaluating faithfulness and content selection in book-length summarization\",\\n        \"citationCount\": 64,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper conducts the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books, and develops a typology of omission errors related to crucial narrative elements and identifies a systematic over-emphasis on events occurring towards the end of the book.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2294805305\",\\n            \"name\": \"Yekyung Kim\"\\n          },\\n          {\\n            \"authorId\": \"144455052\",\\n            \"name\": \"Yapei Chang\"\\n          },\\n          {\\n            \"authorId\": \"37796923\",\\n            \"name\": \"Marzena Karpinska\"\\n          },\\n          {\\n            \"authorId\": \"31099365\",\\n            \"name\": \"Aparna Garimella\"\\n          },\\n          {\\n            \"authorId\": \"2380898326\",\\n            \"name\": \"Varun Manjunatha\"\\n          },\\n          {\\n            \"authorId\": \"46258841\",\\n            \"name\": \"Kyle Lo\"\\n          },\\n          {\\n            \"authorId\": \"2253400779\",\\n            \"name\": \"Tanya Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2136562\",\\n            \"name\": \"Mohit Iyyer\"\\n          }\\n        ],\\n        \"abstract\": \"While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.\"\\n      },\\n      {\\n        \"paperId\": \"58b0923c25a721061f6a43574ed50846d637d0a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58b0923c25a721061f6a43574ed50846d637d0a9\",\\n        \"title\": \"Web-based online resources about adverse interactions or side effects associated with complementary and alternative medicine: a systematic review, summarization and quality assessment\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://bmcmedinformdecismak.biomedcentral.com/track/pdf/10.1186/s12911-020-01298-5\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7653751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study provides a summarized list of high quality, evidence-based, online resources about CAM and CAM-related adverse effects, and can thereby serve as a useful reference for clinicians, researchers, and patients.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-11-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8563902\",\\n            \"name\": \"J. Y. Ng\"\\n          },\\n          {\\n            \"authorId\": \"1840486037\",\\n            \"name\": \"Vanessa Munford\"\\n          },\\n          {\\n            \"authorId\": \"1572362560\",\\n            \"name\": \"Harmy Thakar\"\\n          }\\n        ],\\n        \"abstract\": \"Background Given an increased global prevalence of complementary and alternative medicine (CAM) use, healthcare providers commonly seek CAM-related health information online. Numerous online resources containing CAM-specific information exist, many of which are readily available/accessible, containing information shareable with their patients. To the authors\\\\u2019 knowledge, no study has summarized nor assessed the quality of content contained within these online resources for at least a decade, specifically pertaining to information about adverse effects or interactions. Methods This study provides summaries of web-based online resources that provide safety information on potential interactions or adverse effects of CAM. Specifically, clinicians are the intended users of these online resources containing patient information which they can then disseminate to their patients. All online resources were assessed for content quality using the validated rating tool, DISCERN. Results Of 21 articles identified in our previously published scoping review, 23 online resources were eligible. DISCERN assessments suggests that online resources containing CAM-specific information vary in quality. Summed DISCERN scores had a mean of 56.13 (SD\\\\u2009=\\\\u200910.25) out of 75. Online resources with the highest total DISCERN scores across all questions included Micromedex (68.50), Merck Manual (67.50) and Drugs.com (66.50). Online resources with the lowest total scores included Drug Information (33.00), Caremark Drug Interactions (42.50) and HIV Drug Interactions (43.00). The DISCERN questions that received the highest mean score across all online resources referred to whether the risks were described for each treatment (4.66), whether the aims were clear (4.58), whether the source achieved those aims (4.58), and whether the website referred to areas of uncertainty (4.58). The DISCERN questions that received the lowest mean score across all online resources assessed whether there was discussion about no treatment being used (1.29) and how treatment choices would affect quality of life (2.00). Conclusion This study provides a comprehensive list of online resources containing CAM-specific information. Informed by the appraisal of these resources, this study provides a summarized list of high quality, evidence-based, online resources about CAM and CAM-related adverse effects. This list of recommended resources can thereby serve as a useful reference for clinicians, researchers, and patients.\"\\n      },\\n      {\\n        \"paperId\": \"0b66b4398a0a768ca32ea23ae827a4b1fe9746ab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b66b4398a0a768ca32ea23ae827a4b1fe9746ab\",\\n        \"title\": \"Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies, and proposes a refined taxonomy of errors, coining the category of \\\\\"Circumstantial Inference\\\\\".\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"98806251\",\\n            \"name\": \"S. Ramprasad\"\\n          },\\n          {\\n            \"authorId\": \"2249355743\",\\n            \"name\": \"Elisa Ferracane\"\\n          },\\n          {\\n            \"authorId\": \"32219137\",\\n            \"name\": \"Zachary Chase Lipton\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems. However, they continue to face concerns about hallucinations. While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness. Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies. Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models. We propose a refined taxonomy of errors, coining the category of\\\\\"Circumstantial Inference\\\\\"to bucket these LLM behaviors and release the dataset. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors. To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying\\\\\"Circumstantial Inference.\\\\\"\"\\n      },\\n      {\\n        \"paperId\": \"fa253d232a933595ff8aa892730a49bcf63be4db\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa253d232a933595ff8aa892730a49bcf63be4db\",\\n        \"title\": \"Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.15496\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.15496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes an instruction fine-tuning model: Baichuan2-Sum, for role-oriented dialogue summarization, which shows an improvement in Rouge scores on both datasets compared to the previously best-performing model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282252656\",\\n            \"name\": \"Jianfei Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2281789572\",\\n            \"name\": \"Yancan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2148234209\",\\n            \"name\": \"Yimin Ou\"\\n          },\\n          {\\n            \"authorId\": \"2313881077\",\\n            \"name\": \"Hanyi Yu\"\\n          },\\n          {\\n            \"authorId\": \"2281897968\",\\n            \"name\": \"Yiyong Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) like LLaMA, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods foucus on small models (e.g BART and BERT). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented dialogue summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the desired summaries. Furthermore, we applied NEFTune technique to add suitable noise during training, improving the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. The Baichuan2-Sum model shows an improvement in Rouge scores on both datasets compared to the previously best-performing model. Notably, for the SAMSUM dataset, there is a 21% increase in the ROUGE-1 score, a 32% increase in the ROUGE-2 score, and a 9% increase in the ROUGE-L score. We have released our model and related codes to facilitate future studies in the dialogue summarization task.\"\\n      },\\n      {\\n        \"paperId\": \"2338d7c9ab07e6d0f4160335dce0e6e6a87c4749\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2338d7c9ab07e6d0f4160335dce0e6e6a87c4749\",\\n        \"title\": \"Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13412\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2143482843\",\\n            \"name\": \"Yiming Wang\"\\n          },\\n          {\\n            \"authorId\": \"3322871\",\\n            \"name\": \"Zhuosheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2151038501\",\\n            \"name\": \"Rui Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the \\\\u201cLasswell Communication Model\\\\u201d proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs\\\\u2019 zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.\"\\n      },\\n      {\\n        \"paperId\": \"d9046db67af64e78791099d646b5d22792fabc63\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d9046db67af64e78791099d646b5d22792fabc63\",\\n        \"title\": \"Overview of the MEDIQA-Chat 2023 Shared Tasks on the Summarization & Generation of Doctor-Patient Conversations\",\\n        \"citationCount\": 56,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.clinicalnlp-1.52.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://aclanthology.org/2023.clinicalnlp-1.52, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes the three MEDIQA-Chat 2023 tasks, the datasets, and the participants\\\\u2019 results and methods and hopes that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2127117221\",\\n            \"name\": \"Asma Ben Abacha\"\\n          },\\n          {\\n            \"authorId\": \"38845771\",\\n            \"name\": \"Wen-wai Yim\"\\n          },\\n          {\\n            \"authorId\": \"49496422\",\\n            \"name\": \"Griffin Adams\"\\n          },\\n          {\\n            \"authorId\": \"2370273\",\\n            \"name\": \"N. Snider\"\\n          },\\n          {\\n            \"authorId\": \"1398215463\",\\n            \"name\": \"Meliha Yetisgen-Yildiz\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic generation of clinical notes from doctor-patient conversations can play a key role in reducing daily doctors\\\\u2019 workload and improving their interactions with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective solutions through shared tasks on the automatic summarization of doctor-patient conversations and on the generation of synthetic dialogues from clinical notes for data augmentation. Seventeen teams participated in the challenge and experimented with a broad range of approaches and models. In this paper, we describe the three MEDIQA-Chat 2023 tasks, the datasets, and the participants\\\\u2019 results and methods. We hope that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes.\"\\n      },\\n      {\\n        \"paperId\": \"de546efe75ef22c5471e9e92c21cc093975ca9d6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/de546efe75ef22c5471e9e92c21cc093975ca9d6\",\\n        \"title\": \"Prompting Large Language Models with Audio for General-Purpose Speech Summarization\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2406.05968\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.05968, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An end-to-end system that combines an instruction-tuned LLM with an audio encoder that converts speech into token representations that the LLM can interpret that allows the LLM to process speech inputs in the same way as text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2113751566\",\\n            \"name\": \"Wonjune Kang\"\\n          },\\n          {\\n            \"authorId\": \"2305604637\",\\n            \"name\": \"Deb Roy\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, we introduce a framework for speech summarization that leverages the processing and reasoning capabilities of large language models (LLMs). We propose an end-to-end system that combines an instruction-tuned LLM with an audio encoder that converts speech into token representations that the LLM can interpret. Using a dataset with paired speech-text data, the overall system is trained to generate consistent responses to prompts with the same semantic information regardless of the input modality. The resulting framework allows the LLM to process speech inputs in the same way as text, enabling speech summarization by simply prompting the LLM. Unlike prior approaches, our method is able to summarize spoken content from any arbitrary domain, and it can produce summaries in different styles by varying the LLM prompting strategy. Experiments demonstrate that our approach outperforms a cascade baseline of speech recognition followed by LLM text processing.\"\\n      },\\n      {\\n        \"paperId\": \"5272b2e11769a0e4bcfb9ff7cdf4c58e648afa18\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5272b2e11769a0e4bcfb9ff7cdf4c58e648afa18\",\\n        \"title\": \"EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3640457.3688185\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.11441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EmbSum is introduced, a novel framework that enables offline pre-computations of users and candidate items while capturing the interactions within the user engagement history, surpassing state-of-the-art (SoTA) methods with higher accuracy and fewer parameters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-05-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50445559\",\\n            \"name\": \"Chiyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2284621464\",\\n            \"name\": \"Yifei Sun\"\\n          },\\n          {\\n            \"authorId\": \"2145209409\",\\n            \"name\": \"Minghao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2284643679\",\\n            \"name\": \"Jun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284594046\",\\n            \"name\": \"Jie Lei\"\\n          },\\n          {\\n            \"authorId\": \"2065312024\",\\n            \"name\": \"M. Abdul-Mageed\"\\n          },\\n          {\\n            \"authorId\": \"2284592470\",\\n            \"name\": \"Rong Jin\"\\n          },\\n          {\\n            \"authorId\": \"2302524479\",\\n            \"name\": \"Angli Liu\"\\n          },\\n          {\\n            \"authorId\": \"2302370397\",\\n            \"name\": \"Ji Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2284639887\",\\n            \"name\": \"Sem Park\"\\n          },\\n          {\\n            \"authorId\": \"2284591939\",\\n            \"name\": \"Ning Yao\"\\n          },\\n          {\\n            \"authorId\": \"2284595625\",\\n            \"name\": \"Bo Long\"\\n          }\\n        ],\\n        \"abstract\": \"Content-based recommendation systems play a crucial role in delivering personalized content to users in the digital world. In this work, we introduce EmbSum, a novel framework that enables offline pre-computations of users and candidate items while capturing the interactions within the user engagement history. By utilizing the pretrained encoder-decoder model and poly-attention layers, EmbSum derives User Poly-Embedding (UPE) and Content Poly-Embedding (CPE) to calculate relevance scores between users and candidate items. EmbSum actively learns the long user engagement histories by generating user-interest summary with supervision from large language model (LLM). The effectiveness of EmbSum is validated on two datasets from different domains, surpassing state-of-the-art (SoTA) methods with higher accuracy and fewer parameters. Additionally, the model\\\\u2019s ability to generate summaries of user interests serves as a valuable by-product, enhancing its usefulness for personalized content recommendations.\"\\n      },\\n      {\\n        \"paperId\": \"60e7545ec5280ad944d2d6e49be52419301aa063\",\\n        \"url\": \"https://www.semanticscholar.org/paper/60e7545ec5280ad944d2d6e49be52419301aa063\",\\n        \"title\": \"VSS-Net: Visual Semantic Self-Mining Network for Video Summarization\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3312325?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3312325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108326383\",\\n            \"name\": \"Yunzuo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2190800936\",\\n            \"name\": \"Yameng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2194582294\",\\n            \"name\": \"Weili Kang\"\\n          },\\n          {\\n            \"authorId\": \"2070824819\",\\n            \"name\": \"Ran Tao\"\\n          }\\n        ],\\n        \"abstract\": \"Video summarization, with the target to detect valuable segments given untrimmed videos, is a meaningful yet understudied topic. Previous methods primarily consider inter-frame and inter-shot temporal dependencies, which might be insufficient to pinpoint important content due to limited valuable information that can be learned. To address this limitation, we elaborate on a Visual Semantic Self-mining Network (VSS-Net), a novel summarization framework motivated by the widespread success of cross-modality learning tasks. VSS-Net initially adopts a two-stream structure consisting of a Context Representation Graph (CRG) and a Video Semantics Encoder (VSE). They are jointly exploited to establish the groundwork for further boosting the capability of content awareness. Specifically, CRG is constructed using an edge-set strategy tailored to the hierarchical structure of videos, enriching visual features with local and non-local temporal cues from temporal order and visual relationship perspectives. Meanwhile, by learning visual similarity across features, VSE adaptively acquires an instructive video-level semantic representation of the input video from coarse to fine. Subsequently, the two streams converge in a Context-Semantics Interaction Layer (CSIL) to achieve sophisticated information exchange across frame-level temporal cues and video-level semantic representation, guaranteeing informative representations and boosting the sensitivity to important segments. Eventually, importance scores are predicted utilizing a prediction head, followed by key shot selection. We evaluate the proposed framework and demonstrate its effectiveness and superiority against state-of-the-art methods on the widely used benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"a36658b26ea4ccb58f85d8a578f6ec6767446095\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a36658b26ea4ccb58f85d8a578f6ec6767446095\",\\n        \"title\": \"Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The authors\\' model with 770M parameters consistently outperforms strong baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT itself, including models distilled from ChatGPT, and sometimes, even ChatGPT itself.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2122355046\",\\n            \"name\": \"Jaehun Jung\"\\n          },\\n          {\\n            \"authorId\": \"119659229\",\\n            \"name\": \"Peter West\"\\n          },\\n          {\\n            \"authorId\": \"2112504145\",\\n            \"name\": \"Liwei Jiang\"\\n          },\\n          {\\n            \"authorId\": \"9252833\",\\n            \"name\": \"Faeze Brahman\"\\n          },\\n          {\\n            \"authorId\": \"50085131\",\\n            \"name\": \"Ximing Lu\"\\n          },\\n          {\\n            \"authorId\": \"33772445\",\\n            \"name\": \"Jillian R. Fisher\"\\n          },\\n          {\\n            \"authorId\": \"122436831\",\\n            \"name\": \"Taylor Sorensen\"\\n          },\\n          {\\n            \"authorId\": \"1699545\",\\n            \"name\": \"Yejin Choi\"\\n          }\\n        ],\\n        \"abstract\": \"We present Impossible Distillation, a novel framework for paraphrasing and sentence summarization, that distills a high-quality dataset and model from a low-quality teacher that itself cannot perform these tasks. Unlike prior works that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific architecture, we hypothesize and verify the paraphrastic proximity intrinsic to pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in the LM distribution. By identifying and distilling generations from these subspaces, Impossible Distillation produces a high-quality dataset and model even from GPT2-scale LMs. We evaluate our method on multiple benchmarks spanning unconstrained / syntax-controlled paraphrase generation and sentence summarization. Our model with 770M parameters consistently outperforms strong baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher diversity and fidelity than up to 13 times larger datasets.\"\\n      },\\n      {\\n        \"paperId\": \"c747c6ce74bf80f1c0af97fb3aeb37875415d238\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c747c6ce74bf80f1c0af97fb3aeb37875415d238\",\\n        \"title\": \"Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13693\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that not only do automated metrics fail to capture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31860505\",\\n            \"name\": \"Lucy Lu Wang\"\\n          },\\n          {\\n            \"authorId\": \"1881453937\",\\n            \"name\": \"Yulia Otmakhova\"\\n          },\\n          {\\n            \"authorId\": \"48727916\",\\n            \"name\": \"Jay DeYoung\"\\n          },\\n          {\\n            \"authorId\": \"153574160\",\\n            \"name\": \"Thinh Hung Truong\"\\n          },\\n          {\\n            \"authorId\": \"2003338023\",\\n            \"name\": \"Bailey Kuehl\"\\n          },\\n          {\\n            \"authorId\": \"2203427167\",\\n            \"name\": \"Erin Bransom\"\\n          },\\n          {\\n            \"authorId\": \"2111879324\",\\n            \"name\": \"Byron Wallace\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better automated evaluation methods for literature review MDS. We take advantage of community submissions to the Multi-document Summarization for Literature Review (MSLR) shared task to compile a diverse and representative sample of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical features of generated summaries, to other automated metrics including several we propose in this work, and to aspects of human-assessed summary quality. We find that not only do automated metrics fail to capture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators.\"\\n      },\\n      {\\n        \"paperId\": \"be658d4482299d7a77be324bdb9946bfcbcbad25\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be658d4482299d7a77be324bdb9946bfcbcbad25\",\\n        \"title\": \"Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16784\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16784, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2167123387\",\\n            \"name\": \"Dongqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2293409512\",\\n            \"name\": \"Yifan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293393732\",\\n            \"name\": \"Vera Demberg\"\\n          }\\n        ],\\n        \"abstract\": \"For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the \\\\u2018RSTformer\\\\u2019, a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.\"\\n      },\\n      {\\n        \"paperId\": \"5f6c67f3111ca652e6709f5ae3b6821c13838a80\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5f6c67f3111ca652e6709f5ae3b6821c13838a80\",\\n        \"title\": \"Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.12324\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.12324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs into figure captions, and shows that it can be more effectively tackled as a text summarization task in scientific documents.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1414028847\",\\n            \"name\": \"Huang Chieh-Yang\"\\n          },\\n          {\\n            \"authorId\": \"4500245\",\\n            \"name\": \"Ting-Yao Hsu\"\\n          },\\n          {\\n            \"authorId\": \"2066337266\",\\n            \"name\": \"Ryan A. Rossi\"\\n          },\\n          {\\n            \"authorId\": \"3115414\",\\n            \"name\": \"A. Nenkova\"\\n          },\\n          {\\n            \"authorId\": \"2109571021\",\\n            \"name\": \"Sungchul Kim\"\\n          },\\n          {\\n            \"authorId\": \"51192588\",\\n            \"name\": \"G. Chan\"\\n          },\\n          {\\n            \"authorId\": \"35910905\",\\n            \"name\": \"Eunyee Koh\"\\n          },\\n          {\\n            \"authorId\": \"145157784\",\\n            \"name\": \"C. Lee Giles\"\\n          },\\n          {\\n            \"authorId\": \"2109026884\",\\n            \"name\": \"Ting Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., \\\\u201cFigure 3 shows...\\\\u201d) into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and data are available at: https://github.com/Crowd-AI-Lab/Generating-Figure-Captions-as-a-Text-Summarization-Task.\"\\n      },\\n      {\\n        \"paperId\": \"6b9b1409c013ed3fca1a2bc697ad442c337de462\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b9b1409c013ed3fca1a2bc697ad442c337de462\",\\n        \"title\": \"Abstractive Financial News Summarization via Transformer-BiLSTM Encoder and Graph Attention-Based Decoder\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASLP.2023.3304473?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASLP.2023.3304473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1509369579\",\\n            \"name\": \"Haozhou Li\"\\n          },\\n          {\\n            \"authorId\": \"144135237\",\\n            \"name\": \"Qinke Peng\"\\n          },\\n          {\\n            \"authorId\": \"2005209281\",\\n            \"name\": \"Xu Mou\"\\n          },\\n          {\\n            \"authorId\": \"2155516032\",\\n            \"name\": \"Ying Wang\"\\n          },\\n          {\\n            \"authorId\": \"2150468905\",\\n            \"name\": \"Zeyuan Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2131541300\",\\n            \"name\": \"Muhammad Fiaz Bashir\"\\n          }\\n        ],\\n        \"abstract\": \"Financial news summarization (FNS) has been an attractive research problem in recent years, which aims to generate a shorter highlight of the news article while preserving key factual aspects, emotions, and opinions, providing significant assistance in stock trading and investment decision-making. However, FNS faces two challenges compared to the common domain. Firstly, financial news involves professional qualitative and quantitative information and salient content always scatters across long-range interactions. Secondly, financial news contains latent causal relationships, where historical information in the early generated sequence can significantly affect the subsequent decoding process. To address these difficulties, we propose an enhanced Seq2Seq model named TLGA, where the hierarchical Transformer-BiLSTM encoder can capture long-range interactions and sequential semantics while the Graph Attention-based decoder can fully utilize the historical information of decoded tokens and capture key causal relations. Moreover, we propose history-enhanced attention to concentrate on salient input content based on history semantics, guiding our decoder to generate the summary around the corresponding contents. It is also the first attempt to reuse history information of previously generated summary sequences in FNS using the idea of the Graph Attention Mechanism. Additionally, we construct the LCFNS dataset with 430,820 news-summary pairs for the lack of large-scale high-quality datasets in FNS. Experimental results on two financial datasets and two benchmark datasets indicate that our model outperforms other baselines.\"\\n      },\\n      {\\n        \"paperId\": \"2b5e411e664a521bcc223329562e14ece650b6bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2b5e411e664a521bcc223329562e14ece650b6bf\",\\n        \"title\": \"Mr. HiSum: A Large-scale Dataset for Video Highlight Detection and Summarization\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents Mr. HiSum, a large-scale dataset for video highlight detection and summarization, containing 31,892 videos and reliable labels aggregated over 50,000+ users per video, and empirically prove reliability of the labels as frame importance by cross-dataset transfer and user study.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2231980663\",\\n            \"name\": \"Jinhwan Sul\"\\n          },\\n          {\\n            \"authorId\": \"2288035283\",\\n            \"name\": \"Jihoon Han\"\\n          },\\n          {\\n            \"authorId\": \"2288343910\",\\n            \"name\": \"Joonseok Lee\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"17330c07a59b9dfb6cb01612c4b8be8841735396\",\\n        \"url\": \"https://www.semanticscholar.org/paper/17330c07a59b9dfb6cb01612c4b8be8841735396\",\\n        \"title\": \"Leveraging Summary Guidance on Medical Report Summarization\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.04001\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.04001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5878432\",\\n            \"name\": \"Yunqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1860612\",\\n            \"name\": \"Xuebing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2204743673\",\\n            \"name\": \"Yuanyuan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2108167911\",\\n            \"name\": \"Wensheng Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"This study presents three deidentified large medical text datasets, named DISCHARGE, ECHO and RADIOLOGY, which contain 50 K, 16 K and 378 K pairs of report and summary that are derived from MIMIC-III, respectively. We implement convincing baselines of automated abstractive summarization on the created datasets with pre-trained encoder-decoder language models, including BERT2BERT, BERTShare, RoBERTaShare, Pegasus, ProphetNet, T5-large, BART and GSUM. Further, based on the BART model, we leverage the sampled summaries from the training set as prior knowledge guidance, for encoding additional contextual representations of the guidance with the encoder and enhancing the decoding representations in the decoder. The experimental results confirm the improvement of ROUGE scores and BERTScore made by the proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"0ff8c04c8bdbf93b39b49582c9195cf3fc894d03\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0ff8c04c8bdbf93b39b49582c9195cf3fc894d03\",\\n        \"title\": \"Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)\",\\n        \"citationCount\": 154,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3597503.3639183\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.06815, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that adding semantic facts to the code in the prompt actually does help and this approach improves performance in several different settings suggested by prior work, including for three different Large Language Models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-04-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2214523312\",\\n            \"name\": \"K. Pai\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"1757975\",\\n            \"name\": \"Earl T. Barr\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLM) are a new class of computation engines, \\\\u201cprogrammed\\\\u201d via prompt engineering. Researchers are still learning how to best \\\\u201cprogram\\\\u201d these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of \\\\u201ccode analysis\\\\u201d and extracting such information, while processing code: but are they, really? If they aren\\'t, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM\\'s prompt with semantic facts explicitly. actually helps. Prior work shows that LLM performance on code summarization benefits from embedding a few code & summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU 11Scores of 30\\\\u201340 BLEU are considered \\\\u201cGood\\\\u201d to \\\\u201cUnderstandable\\\\u201d for natural language translation; see https://cloud.google.com/translate/automl/docs/evaluate.. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs\\' line completion performance.\"\\n      },\\n      {\\n        \"paperId\": \"c91b2c65da5e50df129b877da57ec07bb7f9c363\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c91b2c65da5e50df129b877da57ec07bb7f9c363\",\\n        \"title\": \"DragAPart: Learning a Part-Level Motion Prior for Articulated Objects\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.15382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces DragAPart, a method that, given an image and a set of drags as input, generates a new image of the same object that responds to the action of the drags, and demonstrates much better part-level motion understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2213346491\",\\n            \"name\": \"Ruining Li\"\\n          },\\n          {\\n            \"authorId\": \"2272947462\",\\n            \"name\": \"Chuanxia Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275252600\",\\n            \"name\": \"Christian Rupprecht\"\\n          },\\n          {\\n            \"authorId\": \"2258587368\",\\n            \"name\": \"Andrea Vedaldi\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce DragAPart, a method that, given an image and a set of drags as input, generates a new image of the same object that responds to the action of the drags. Differently from prior works that focused on repositioning objects, DragAPart predicts part-level interactions, such as opening and closing a drawer. We study this problem as a proxy for learning a generalist motion model, not restricted to a specific kinematic structure or object category. We start from a pre-trained image generator and fine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce. Combined with a new encoding for the drags and dataset randomization, the model generalizes well to real images and different categories. Compared to prior motion-controlled generators, we demonstrate much better part-level motion understanding.\"\\n      },\\n      {\\n        \"paperId\": \"eaa0078838310afb0e9a92e929e85a274d696dbb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eaa0078838310afb0e9a92e929e85a274d696dbb\",\\n        \"title\": \"G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.12383\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.12383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category, represents a first approach that allows jointly generating both hand and object.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9653518\",\\n            \"name\": \"Yufei Ye\"\\n          },\\n          {\\n            \"authorId\": \"2240431852\",\\n            \"name\": \"Abhinav Gupta\"\\n          },\\n          {\\n            \"authorId\": \"2297185992\",\\n            \"name\": \"Kris Kitani\"\\n          },\\n          {\\n            \"authorId\": \"2757335\",\\n            \"name\": \"Shubham Tulsiani\"\\n          }\\n        ],\\n        \"abstract\": \"We propose G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category. To learn a 3D spatial diffusion model that can capture this joint distribution, we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object. We show that this hand-object prior can then serve as generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis. We believe that our model, trained by aggregating seven diverse real-world interaction datasets spanning across 155 cate-gories, represents a first approach that allows jointly generating both hand and object. Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis, outperforming current task-specific baselines.\"\\n      },\\n      {\\n        \"paperId\": \"68571a8c3598888035c5c4f0a99793434ad61ef4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/68571a8c3598888035c5c4f0a99793434ad61ef4\",\\n        \"title\": \"LoRaLay: A Multilingual and Multimodal Dataset for Long Range and Layout-Aware Summarization\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.11312\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.11312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LoRaLay is presented, a collection of datasets for long-range summarization with accompanying visual/layout information and four novel datasets \\\\u2013 consistently built from scholar resources \\\\u2013 covering French, Spanish, Portuguese, and Korean languages are proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-01-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2150610138\",\\n            \"name\": \"Laura Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"90745780\",\\n            \"name\": \"Thomas Scialom\"\\n          },\\n          {\\n            \"authorId\": \"1703777\",\\n            \"name\": \"Benjamin Piwowarski\"\\n          },\\n          {\\n            \"authorId\": \"1767493\",\\n            \"name\": \"Jacopo Staiano\"\\n          }\\n        ],\\n        \"abstract\": \"Text Summarization is a popular task and an active area of research for the Natural Language Processing community. By definition, it requires to account for long input texts, a characteristic which poses computational challenges for neural models. Moreover, real-world documents come in a variety of complex, visually-rich, layouts. This information is of great relevance, whether to highlight salient content or to encode long-range interactions between textual passages. Yet, all publicly available summarization datasets only provide plain text content. To facilitate research on how to exploit visual/layout information to better capture long-range dependencies in summarization models, we present LoRaLay, a collection of datasets for long-range summarization with accompanying visual/layout information. We extend existing and popular English datasets (arXiv and PubMed) with layout information and propose four novel datasets \\\\u2013 consistently built from scholar resources \\\\u2013 covering French, Spanish, Portuguese, and Korean languages. Further, we propose new baselines merging layout-aware and long-range models \\\\u2013 two orthogonal approaches \\\\u2013 and obtain state-of-the-art results, showing the importance of combining both lines of research.\"\\n      },\\n      {\\n        \"paperId\": \"7ed835ecf9f3f6a222ec830e7d8ee40fd809dae5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7ed835ecf9f3f6a222ec830e7d8ee40fd809dae5\",\\n        \"title\": \"Context-aware Decoding Reduces Hallucination in Query-focused Summarization\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.14335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A large-scale reproducibility study on one recently proposed decoding method -- Context-aware Decoding (CAD), which improves QFS quality by reducing factuality errors/hallucinations while also at a cost of increased inference-time FLOPs and reduced decoding speed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-12-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284763197\",\\n            \"name\": \"Zhichao Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Query-focused summarization (QFS) aims to provide a summary of a single document/multi documents that can satisfy the information needs of a given query. It is useful for various real-world applications, such as abstractive snippet generation or more recent retrieval augmented generation (RAG). A prototypical QFS pipeline consists of a retriever (sparse or dense retrieval) and a generator (usually a large language model). However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs. There has been growing interest in developing new decoding methods to improve generation quality and reduce hallucination. In this work, we conduct a large-scale reproducibility study on one recently proposed decoding method -- Context-aware Decoding (CAD). In addition to replicating CAD\\'s experiments on news summarization datasets, we include experiments on QFS datasets, and conduct more rigorous analysis on computational complexity and hyperparameter sensitivity. Experiments with eight different language models show that performance-wise, CAD improves QFS quality by (1) reducing factuality errors/hallucinations while (2) mostly retaining the match of lexical patterns, measured by ROUGE scores, while also at a cost of increased inference-time FLOPs and reduced decoding speed. The code implementation based on Huggingface Library is made available https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs\"\\n      },\\n      {\\n        \"paperId\": \"4e444759dc61f26dee627fbb909a15a5ad32b184\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4e444759dc61f26dee627fbb909a15a5ad32b184\",\\n        \"title\": \"AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.09521\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.09521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AMRFact is proposed, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs) and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261278256\",\\n            \"name\": \"Haoyi Qiu\"\\n          },\\n          {\\n            \"authorId\": \"1956008001\",\\n            \"name\": \"Kung-Hsiang Huang\"\\n          },\\n          {\\n            \"authorId\": \"2266841207\",\\n            \"name\": \"Jingnong Qu\"\\n          },\\n          {\\n            \"authorId\": \"2261277442\",\\n            \"name\": \"Nanyun Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate our approach significantly outperforms previous systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating factuality of abstractive summarization.\"\\n      },\\n      {\\n        \"paperId\": \"143b5ef2b4b8f80923e0b46a09dc6b3fac7e3575\",\\n        \"url\": \"https://www.semanticscholar.org/paper/143b5ef2b4b8f80923e0b46a09dc6b3fac7e3575\",\\n        \"title\": \"A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.08503\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.08503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work enforces hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task, and outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152867918\",\\n            \"name\": \"Chenhui Shen\"\\n          },\\n          {\\n            \"authorId\": \"123962152\",\\n            \"name\": \"Liying Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2147330214\",\\n            \"name\": \"Yang You\"\\n          },\\n          {\\n            \"authorId\": \"1996394\",\\n            \"name\": \"Lidong Bing\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained language models (PLMs) have achieved outstanding achievements in abstractive single-document summarization (SDS). However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex. Previous works either design new MDS architectures or apply PLMs bluntly with concatenated source documents as a reformulated SDS task. While the former does not utilize previous pre-training efforts and may not generalize well across different domains, the latter may not sufficiently attend to the intricate cross-document relationships unique to MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task. Across 10 MDS benchmarks from various domains, our method outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters. It outperforms its corresponding PLM backbone by up to 3 Rouge-L and is favored by humans.\"\\n      },\\n      {\\n        \"paperId\": \"12f881588e6b461b86b34209c30a2bdfe67f904d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/12f881588e6b461b86b34209c30a2bdfe67f904d\",\\n        \"title\": \"Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.14981\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.14981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that using a contrastive learning framework with the refined candidate summaries leads to significant gains on both factuality and similarity-based metrics, and a ranking strategy in which it effectively combine two metrics, thereby preventing any conflict during training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2126503480\",\\n            \"name\": \"Tanay Dixit\"\\n          },\\n          {\\n            \"authorId\": \"47939052\",\\n            \"name\": \"Fei Wang\"\\n          },\\n          {\\n            \"authorId\": \"1998918\",\\n            \"name\": \"Muhao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose {pasted macro \\\\u2018MODEL\\\\u2019}name (i.e. Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.\"\\n      },\\n      {\\n        \"paperId\": \"c014d501abdeb23c6c1c5075e66989a72d3f6f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c014d501abdeb23c6c1c5075e66989a72d3f6f69\",\\n        \"title\": \"Generic protein\\\\u2013ligand interaction scoring by integrating physical prior knowledge and data augmentation modelling\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s42256-024-00849-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s42256-024-00849-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EquiScore, which utilizes a heterogeneous graph neural network to integrate physical prior knowledge and characterize protein\\\\u2013ligand interactions in equivariant geometric space, is introduced and is shown to have robust capabilities when applied to unseen protein targets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2220518044\",\\n            \"name\": \"Duanhua Cao\"\\n          },\\n          {\\n            \"authorId\": \"2175243828\",\\n            \"name\": \"Geng Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305221348\",\\n            \"name\": \"Jiaxin Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2155427582\",\\n            \"name\": \"Jie Yu\"\\n          },\\n          {\\n            \"authorId\": \"2219042185\",\\n            \"name\": \"Runze Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2276034718\",\\n            \"name\": \"Mingan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2256596397\",\\n            \"name\": \"Wei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2109298404\",\\n            \"name\": \"Lifan Chen\"\\n          },\\n          {\\n            \"authorId\": \"51135036\",\\n            \"name\": \"Feisheng Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2220532589\",\\n            \"name\": \"Yingying Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2220589745\",\\n            \"name\": \"Chenghao Lu\"\\n          },\\n          {\\n            \"authorId\": \"47056625\",\\n            \"name\": \"Xutong Li\"\\n          },\\n          {\\n            \"authorId\": \"2261477527\",\\n            \"name\": \"Xiaomin Luo\"\\n          },\\n          {\\n            \"authorId\": \"2145313647\",\\n            \"name\": \"Sulin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2114135568\",\\n            \"name\": \"Mingyue Zheng\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"ee1ef7b70dc34adcc90c42cc28168165ea56501f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ee1ef7b70dc34adcc90c42cc28168165ea56501f\",\\n        \"title\": \"SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization\",\\n        \"citationCount\": 435,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00453/1987014/tacl_a_00453.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2111.09525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work revisits the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-11-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46180754\",\\n            \"name\": \"Philippe Laban\"\\n          },\\n          {\\n            \"authorId\": \"48839531\",\\n            \"name\": \"Tobias Schnabel\"\\n          },\\n          {\\n            \"authorId\": \"144609235\",\\n            \"name\": \"Paul N. Bennett\"\\n          },\\n          {\\n            \"authorId\": \"1716902\",\\n            \"name\": \"Marti A. Hearst\"\\n          }\\n        ],\\n        \"abstract\": \"In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work.\"\\n      },\\n      {\\n        \"paperId\": \"1e2b6cd7247d9066c97a5f7befa5c4f890075f7b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e2b6cd7247d9066c97a5f7befa5c4f890075f7b\",\\n        \"title\": \"Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2407.12192\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.12192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A feature-oriented workflow for systematic prompt evaluation of summarization prompts is introduced and found that it helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and has the potential to generalize to other NLG and image-generation tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-07-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2289789049\",\\n            \"name\": \"Sam Yu-Te Lee\"\\n          },\\n          {\\n            \"authorId\": \"2311699054\",\\n            \"name\": \"Aryaman Bahukhandi\"\\n          },\\n          {\\n            \"authorId\": \"2312133175\",\\n            \"name\": \"Dongyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2239167039\",\\n            \"name\": \"Kwan-Liu Ma\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.\"\\n      },\\n      {\\n        \"paperId\": \"1b95053af03b5a06809a4967c6cf5ca137bbcde4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b95053af03b5a06809a4967c6cf5ca137bbcde4\",\\n        \"title\": \"Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.09177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a first multi-turn approach that leverages benign preliminary questions to interact with the LLM, and focuses on a new attack form, called Contextual Interaction Attack, which is black-box and can also transfer across LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282098821\",\\n            \"name\": \"Yixin Cheng\"\\n          },\\n          {\\n            \"authorId\": \"34291068\",\\n            \"name\": \"Markos Georgopoulos\"\\n          },\\n          {\\n            \"authorId\": \"1678641\",\\n            \"name\": \"V. Cevher\"\\n          },\\n          {\\n            \"authorId\": \"2140285545\",\\n            \"name\": \"Grigorios G. Chrysos\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired from Chomsky\\'s transformational-generative grammar theory and human practices of indirect context to elicit harmful information, we focus on a new attack form, called Contextual Interaction Attack. We contend that the prior context\\\\\\\\u2014the information preceding the attack query\\\\\\\\u2014plays a pivotal role in enabling strong Jailbreaking attacks. Specifically, we propose a first multi-turn approach that leverages benign preliminary questions to interact with the LLM. Due to the autoregressive nature of LLMs, which use previous conversation rounds as context during generation, we guide the model\\'s question-response pair to construct a context that is semantically aligned with the attack query to execute the attack. We conduct experiments on seven different LLMs and demonstrate the efficacy of this attack, which is black-box and can also transfer across LLMs. We believe this can lead to further developments and understanding of security in LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"ab36a9ced3ade6ca6e48055b4579c4ee8bed52f5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ab36a9ced3ade6ca6e48055b4579c4ee8bed52f5\",\\n        \"title\": \"InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion\",\\n        \"citationCount\": 162,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2308.16905\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.16905, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes InterDiff, a framework comprising two key steps: interaction diffusion, where a diffusion model is leverage to encode the distribution of future human-object interactions; and interaction correction, where a physics-informed predictor is introduced to correct denoised HOIs in a diffusion step.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8775961\",\\n            \"name\": \"Sirui Xu\"\\n          },\\n          {\\n            \"authorId\": \"2221242078\",\\n            \"name\": \"Zhengyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2236683644\",\\n            \"name\": \"Yu-Xiong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2587808\",\\n            \"name\": \"Liangyan Gui\"\\n          }\\n        ],\\n        \"abstract\": \"This paper addresses a novel task of anticipating 3D human-object interactions (HOIs). Most existing research on HOI synthesis lacks comprehensive whole-body interactions with dynamic objects, e.g., often limited to manipulating small or static objects. Our task is significantly more challenging, as it requires modeling dynamic objects with various shapes, capturing whole-body motion, and ensuring physically valid interactions. To this end, we propose InterDiff, a framework comprising two key steps: (i) interaction diffusion, where we leverage a diffusion model to encode the distribution of future human-object interactions; (ii) interaction correction, where we introduce a physics-informed predictor to correct denoised HOIs in a diffusion step. Our key insight is to inject prior knowledge that the interactions under reference with respect to contact points follow a simple pattern and are easily predictable. Experiments on multiple human-object interaction datasets demonstrate the effectiveness of our method for this task, capable of producing realistic, vivid, and remarkably longterm 3D HOI predictions.\"\\n      },\\n      {\\n        \"paperId\": \"59982844ae41696577f3fe4c396a8569f7d0fbd3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/59982844ae41696577f3fe4c396a8569f7d0fbd3\",\\n        \"title\": \"Calibration of Large Language Models on Code Summarization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151266129\",\\n            \"name\": \"Yuvraj Virk\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          }\\n        ],\\n        \"abstract\": \"A brief, fluent, and relevant summary can be helpful during program comprehension; however, such a summary does require significant human effort to produce. Often, good summaries are unavailable in software projects, which makes maintenance more difficult. There has been a considerable body of research into automated AI-based methods, using Large Language models (LLMs), to generate summaries of code; there also has been quite a bit of work on ways to measure the performance of such summarization methods, with special attention paid to how closely these AI-generated summaries resemble a summary a human might have produced. Measures such as BERTScore and BLEU have been suggested and evaluated with human-subject studies. However, LLM-generated summaries can be inaccurate, incomplete, etc: generally, too dissimilar to one that a good developer might write. Given an LLM-generated code summary, how can a user rationally judge if a summary is sufficiently good and reliable? Given just some input source code, and an LLM-generated summary, existing approaches can help judge brevity, fluency and relevance of the summary; however, it\\\\u2019s difficult to gauge whether an LLM-generated summary sufficiently resembles what a human might produce, without a \\\\u201cgolden\\\\u201d human-produced summary to compare against. Prior research indicates that human-produced summaries are generally preferred by human-raters, so we explore this issue in this paper. We study this resemblance question as a calibration problem: given just the code & the summary from an LLM, can we compute a confidence measure, that provides a reliable indication of whether the summary sufficiently resembles what a human would have produced in this situation? We examine this question using several LLMs, for several languages, and in several different settings. Our investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code.\"\\n      },\\n      {\\n        \"paperId\": \"c4960e3cd29f0af3c9120a4173bf681ed2131814\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c4960e3cd29f0af3c9120a4173bf681ed2131814\",\\n        \"title\": \"FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations\",\\n        \"citationCount\": 63,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2204.06508\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.06508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"FactGraph is proposed, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation and improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-04-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"10430740\",\\n            \"name\": \"Leonardo F. R. Ribeiro\"\\n          }\\n        ],\\n        \"abstract\": \"Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.\"\\n      },\\n      {\\n        \"paperId\": \"3e9b4218f6b943de8e57fe70f903911db11d24b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e9b4218f6b943de8e57fe70f903911db11d24b6\",\\n        \"title\": \"Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling\",\\n        \"citationCount\": 53,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.12378\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.12378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes to generate hard, representative synthetic examples of non-factual summaries through infilling language models to improve factual consistency inAbstractive summarization models and shows that this approach vastly outperforms prior methods in correcting erroneous summaries.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"143820870\",\\n            \"name\": \"Vidhisha Balachandran\"\\n          },\\n          {\\n            \"authorId\": \"2548384\",\\n            \"name\": \"Hannaneh Hajishirzi\"\\n          },\\n          {\\n            \"authorId\": \"2058480371\",\\n            \"name\": \"W. Cohen\"\\n          },\\n          {\\n            \"authorId\": \"2073587169\",\\n            \"name\": \"Yulia Tsvetkov\"\\n          }\\n        ],\\n        \"abstract\": \"Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets\\\\u2014 CNN/DM and XSum\\\\u2014we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model\\\\u2014FactEdit\\\\u2014improves factuality scores by over ~11 points on CNN/DM and over ~31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality.\"\\n      },\\n      {\\n        \"paperId\": \"4746b30f271a4acc472930dde079aeffa2e47367\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4746b30f271a4acc472930dde079aeffa2e47367\",\\n        \"title\": \"Evolution of coordination in pairwise and multi-player interactions via prior commitments\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://journals.sagepub.com/doi/pdf/10.1177/1059712321993166\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2009.11727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Whether prior commitment would be a viable evolutionary mechanism for enhancing coordination and the overall population social welfare strongly depends on the collective benefit and severity of competition, and more importantly, how asymmetric benefits are resolved in a commitment deal.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2164337059\",\\n            \"name\": \"Ndidi Bianca Ogbo\"\\n          },\\n          {\\n            \"authorId\": \"12078870\",\\n            \"name\": \"A. Elragig\"\\n          },\\n          {\\n            \"authorId\": \"2244073417\",\\n            \"name\": \"The Anh Han\"\\n          }\\n        ],\\n        \"abstract\": \"Upon starting a collective endeavour, it is important to understand your partners\\\\u2019 preferences and how strongly they commit to a common goal. Establishing a prior commitment or agreement in terms of posterior benefits and consequences from those engaging in it provides an important mechanism for securing cooperation. Resorting to methods from Evolutionary Game Theory (EGT), here we analyse how prior commitments can also be adopted as a tool for enhancing coordination when its outcomes exhibit an asymmetric payoff structure, in both pairwise and multi-party interactions. Arguably, coordination is more complex to achieve than cooperation since there might be several desirable collective outcomes in a coordination problem (compared to mutual cooperation, the only desirable collective outcome in cooperation dilemmas). Our analysis, both analytically and via numerical simulations, shows that whether prior commitment would be a viable evolutionary mechanism for enhancing coordination and the overall population social welfare strongly depends on the collective benefit and severity of competition, and more importantly, how asymmetric benefits are resolved in a commitment deal. Moreover, in multi-party interactions, prior commitments prove to be crucial when a high level of group diversity is required for optimal coordination. The results are robust for different selection intensities. Overall, our analysis provides new insights into the complexity and beauty of behavioural evolution driven by humans\\\\u2019 capacity for commitment, as well as for the design of self-organised and distributed multi-agent systems for ensuring coordination among autonomous agents.\"\\n      },\\n      {\\n        \"paperId\": \"903ce14de7226276a66e1bd3e3d3d97934d2dfd2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/903ce14de7226276a66e1bd3e3d3d97934d2dfd2\",\\n        \"title\": \"Mapping the Design Space of Human-AI Interaction in Text Summarization\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2206.14863\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2206.14863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work conducts a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions, and designed text summarization prototypes for each interaction.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2022-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"117397243\",\\n            \"name\": \"Ruijia Cheng\"\\n          },\\n          {\\n            \"authorId\": \"1405364873\",\\n            \"name\": \"Alison Smith-Renner\"\\n          },\\n          {\\n            \"authorId\": \"152645158\",\\n            \"name\": \"Kecheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1739099\",\\n            \"name\": \"Joel R. Tetreault\"\\n          },\\n          {\\n            \"authorId\": \"144633617\",\\n            \"name\": \"A. Jaimes\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans\\\\u2019 roles, experience, and needs when interacting with or being assisted by AI. From a human-centered perspective, we map the design opportunities and considerations for human-AI interaction in text summarization and broader text generation tasks. We first conducted a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions. We designed text summarization prototypes for each interaction. We then interviewed 16 users, aided by the prototypes, to understand their expectations, experience, and needs regarding efficiency, control, and trust with AI in text summarization and propose design considerations accordingly.\"\\n      },\\n      {\\n        \"paperId\": \"64c1f37f75d08902b460ff2c670293d903a74067\",\\n        \"url\": \"https://www.semanticscholar.org/paper/64c1f37f75d08902b460ff2c670293d903a74067\",\\n        \"title\": \"Expanding the coverage of regulons from high-confidence prior knowledge for accurate estimation of transcription factor activities\",\\n        \"citationCount\": 146,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://academic.oup.com/nar/advance-article-pdf/doi/10.1093/nar/gkad841/52174356/gkad841.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10639077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that the signed CollecTRI-derived regulons outperform other public collections of regulatory interactions in accurately inferring changes in TF activities in perturbation experiments and enable the accurate and comprehensive estimation of TF activities and thereby help to interpret transcriptomics data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2139664698\",\\n            \"name\": \"S. M\\\\u00fcller-Dott\"\\n          },\\n          {\\n            \"authorId\": \"1397165866\",\\n            \"name\": \"Eirini Tsirvouli\"\\n          },\\n          {\\n            \"authorId\": \"145019636\",\\n            \"name\": \"M. V\\\\u00e1zquez\"\\n          },\\n          {\\n            \"authorId\": \"1716214602\",\\n            \"name\": \"R. O. Ramirez Flores\"\\n          },\\n          {\\n            \"authorId\": \"1820842454\",\\n            \"name\": \"Pau Badia-i-Mompel\"\\n          },\\n          {\\n            \"authorId\": \"2213560713\",\\n            \"name\": \"Robin Fallegger\"\\n          },\\n          {\\n            \"authorId\": \"3124875\",\\n            \"name\": \"A. L\\\\u00e6greid\"\\n          },\\n          {\\n            \"authorId\": \"1400885451\",\\n            \"name\": \"J. Saez-Rodriguez\"\\n          }\\n        ],\\n        \"abstract\": \"Gene regulation plays a critical role in the cellular processes that underlie human health and disease. The regulatory relationship between transcription factors (TFs), key regulators of gene expression, and their target genes, the so called TF regulons, can be coupled with computational algorithms to estimate the activity of TFs. However, to interpret these findings accurately, regulons of high reliability and coverage are needed. In this study, we present and evaluate a collection of regulons created using the CollecTRI meta-resource containing signed TF-gene interactions for 1,183 TFs. In this context, we introduce a workflow to integrate information from multiple resources and assign the sign of regulation to TF-gene interactions that could be applied to other comprehensive knowledge bases. We find that the signed CollecTRI-derived regulons outperform other public collections of regulatory interactions in accurately inferring changes in TF activities in perturbation experiments. Furthermore, we showcase the value of the regulons by investigating hallmarks of TF activity profiles inferred from the transcriptomes of three different cancer types. Overall, the CollecTRI-derived TF regulons enable the accurate and comprehensive estimation of TF activities and thereby help to interpret transcriptomics data. GRAPHICAL ABSTRACT\"\\n      },\\n      {\\n        \"paperId\": \"ae1e48a74cb2f313e8e99c82f0aa4487b0805002\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ae1e48a74cb2f313e8e99c82f0aa4487b0805002\",\\n        \"title\": \"Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2204.13761\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.13761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that by utilizing an external knowledge base, one can improve the faithfulness of summaries without simply making them more extractive, and additionally, that external knowledge bases linked from the source can benefit the factuality of generated summaries.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-04-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49265991\",\\n            \"name\": \"Yue Dong\"\\n          },\\n          {\\n            \"authorId\": \"1771118\",\\n            \"name\": \"J. Wieting\"\\n          },\\n          {\\n            \"authorId\": \"2986975\",\\n            \"name\": \"Pat Verga\"\\n          }\\n        ],\\n        \"abstract\": \"Despite recent advances in abstractive summarization, current summarization systems still suffer from content hallucinations where models generate text that is either irrelevant or contradictory to the source document. However, prior work has been predicated on the assumption that any generated facts not appearing explicitly in the source are undesired hallucinations. Methods have been proposed to address this scenario by ultimately improving `faithfulness\\' to the source document, but in reality, there is a large portion of entities in the gold reference targets that are not directly in the source. In this work, we show that these entities are not aberrations, but they instead require utilizing external world knowledge to infer reasoning paths from entities in the source. We show that by utilizing an external knowledge base, we can improve the faithfulness of summaries without simply making them more extractive, and additionally, we show that external knowledge bases linked from the source can benefit the factuality of generated summaries.\"\\n      },\\n      {\\n        \"paperId\": \"81c83519b924bffca465f5a40183c62e711c0a69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/81c83519b924bffca465f5a40183c62e711c0a69\",\\n        \"title\": \"Toward Expanding the Scope of Radiology Report Summarization to Multiple Anatomies and Modalities\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.acl-short.41.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.08584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A dataset involving three new modalities and seven new anatomies based on the MIMIC-III and MIMic-CXR datasets is proposed and the performance of models both within and across modality-anatomy pairs in MIM IC-RRS is evaluated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-11-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35935570\",\\n            \"name\": \"Jean-Benoit Delbrouck\"\\n          },\\n          {\\n            \"authorId\": \"145859954\",\\n            \"name\": \"M. Varma\"\\n          },\\n          {\\n            \"authorId\": \"1584627064\",\\n            \"name\": \"C. Langlotz\"\\n          }\\n        ],\\n        \"abstract\": \"Radiology report summarization (RRS) is a growing area of research. Given the Findings section of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. However, RRS currently faces essential limitations.First, many prior studies conduct experiments on private datasets, preventing reproduction of results and fair comparisons across different systems and solutions. Second, most prior approaches are evaluated solely on chest X-rays. To address these limitations, we propose a dataset (MIMIC-RRS) involving three new modalities and seven new anatomies based on the MIMIC-III and MIMIC-CXR datasets. We then conduct extensive experiments to evaluate the performance of models both within and across modality-anatomy pairs in MIMIC-RRS. In addition, we evaluate their clinical efficacy via RadGraph, a factual correctness metric.\"\\n      },\\n      {\\n        \"paperId\": \"cbc05849bd04058e4aee09738dd73d9ad6621964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cbc05849bd04058e4aee09738dd73d9ad6621964\",\\n        \"title\": \"Dual-Prior Augmented Decoding Network for Long Tail Distribution in HOI Detection\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/27949/27918\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i3.27949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i3.27949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a dual-prior augmented decoding network by decomposing the HOI task into two sub-tasks: human-object pair detection and interaction recognition and leverages external knowledge to enhance the model\\'s ability at a finer granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293390597\",\\n            \"name\": \"Jiayi Gao\"\\n          },\\n          {\\n            \"authorId\": \"2582309\",\\n            \"name\": \"Kongming Liang\"\\n          },\\n          {\\n            \"authorId\": \"2240049431\",\\n            \"name\": \"Tao Wei\"\\n          },\\n          {\\n            \"authorId\": \"2256715874\",\\n            \"name\": \"Wei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268678016\",\\n            \"name\": \"Zhanyu Ma\"\\n          },\\n          {\\n            \"authorId\": \"2273915241\",\\n            \"name\": \"Jun Guo\"\\n          }\\n        ],\\n        \"abstract\": \"Human object interaction detection aims at localizing human-object pairs and recognizing their interactions. Trapped by the long-tailed distribution of the data, existing HOI detection methods often have difficulty recognizing the tail categories. Many approaches try to improve the recognition of HOI tasks by utilizing external knowledge (e.g. pre-trained visual-language models). However, these approaches mainly utilize external knowledge at the HOI combination level and achieve limited improvement in the tail categories. In this paper, we propose a dual-prior augmented decoding network by decomposing the HOI task into two sub-tasks: human-object pair detection and interaction recognition. For each subtask, we leverage external knowledge to enhance the model\\'s ability at a finer granularity. Specifically, we acquire the prior candidates from an external classifier and embed them to assist the subsequent decoding process. Thus, the long-tail problem is mitigated from a coarse-to-fine level with the corresponding external knowledge. Our approach outperforms existing state-of-the-art models in various settings and significantly boosts the performance on the tail HOI categories. The source code is available at https://github.com/PRIS-CV/DP-ADN.\"\\n      },\\n      {\\n        \"paperId\": \"18f86a1f9896795069d8bf7d5098e3debf2fce81\",\\n        \"url\": \"https://www.semanticscholar.org/paper/18f86a1f9896795069d8bf7d5098e3debf2fce81\",\\n        \"title\": \"Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2203.08257\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.08257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work designs a two-step approach: extractive summarization followed by abstractive summarizations of radiology report summarization, and additionally breaks down the extractive part into two independent tasks: extraction of salient sentences and keywords.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-03-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34112357\",\\n            \"name\": \"Sanjeev Kumar Karn\"\\n          },\\n          {\\n            \"authorId\": \"47695637\",\\n            \"name\": \"Ning Liu\"\\n          },\\n          {\\n            \"authorId\": \"144418438\",\\n            \"name\": \"Hinrich Sch\\\\u00fctze\"\\n          },\\n          {\\n            \"authorId\": \"2211973\",\\n            \"name\": \"Oladimeji Farri\"\\n          }\\n        ],\\n        \"abstract\": \"The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist\\\\u2019s reasoning and conclusions, and it also aids the referring physician in confirming or excluding certain diagnoses. A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report. These tasks include acquisition of salient content from the report and generation of a concise, easily consumable IMPRESSIONS section. Prior research on radiology report summarization has focused on single-step end-to-end models \\\\u2013 which subsume the task of salient content acquisition. To fully explore the cascade structure and explainability of radiology report summarization, we introduce two innovations. First, we design a two-step approach: extractive summarization followed by abstractive summarization. Second, we additionally break down the extractive part into two independent tasks: extraction of salient (1) sentences and (2) keywords. Experiments on a publicly available radiology report dataset show our novel approach leads to a more precise summary compared to single-step and to two-step-with-single-extractive-process baselines with an overall improvement in F1 score of 3-4%.\"\\n      },\\n      {\\n        \"paperId\": \"a9dc25d11fadca961b762c173f5b27a445f645d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a9dc25d11fadca961b762c173f5b27a445f645d7\",\\n        \"title\": \"SARS-CoV-2 biology and host interactions\",\\n        \"citationCount\": 117,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s41579-023-01003-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s41579-023-01003-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An updated summary of the SARS-CoV-2 life cycle, gene function and virus\\\\u2013host interactions, including recent landmark findings on general aspects of coronavirus biology and newly discovered host factors necessary for virus replication are provided.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40397706\",\\n            \"name\": \"Silvio Steiner\"\\n          },\\n          {\\n            \"authorId\": \"1580760257\",\\n            \"name\": \"A. Kratzel\"\\n          },\\n          {\\n            \"authorId\": \"2322491040\",\\n            \"name\": \"G. Barut\"\\n          },\\n          {\\n            \"authorId\": \"2054345681\",\\n            \"name\": \"Reto M. Lang\"\\n          },\\n          {\\n            \"authorId\": \"1398526410\",\\n            \"name\": \"Etori Aguiar Moreira\"\\n          },\\n          {\\n            \"authorId\": \"2007902383\",\\n            \"name\": \"Lisa Thomann\"\\n          },\\n          {\\n            \"authorId\": \"152841375\",\\n            \"name\": \"Jenna N. Kelly\"\\n          },\\n          {\\n            \"authorId\": \"2262194481\",\\n            \"name\": \"Volker Thiel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"e9bc2a727d54ad514cfcacae01f65a6a4f0d0159\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e9bc2a727d54ad514cfcacae01f65a6a4f0d0159\",\\n        \"title\": \"Faithful Abstractive Summarization via Fact-aware Consistency-constrained Transformer\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3511808.3557319?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3511808.3557319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel fact-aware abstractive summarization model, named Entity-Relation Pointer Generator Network (ERPGN), which attempts to formalize the facts in original document as a factual knowledge graph, and then generates the high-quality summary via directly modeling consistency between summary and the factualknowledge graph.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2187857206\",\\n            \"name\": \"Yuanjie Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2118043140\",\\n            \"name\": \"Chen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"50383766\",\\n            \"name\": \"Tong Xu\"\\n          },\\n          {\\n            \"authorId\": \"1422036168\",\\n            \"name\": \"Zikai Yin\"\\n          },\\n          {\\n            \"authorId\": \"2173129111\",\\n            \"name\": \"Enhong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Abstractive summarization is a classic task in Natural Language Generation (NLG), which aims to produce a concise summary of the original document. Recently, great efforts have been made on sequence-to-sequence neural networks to generate abstractive sum- maries with a high level of fluency. However, prior arts mainly focus on the optimization of token-level likelihood, while the rich semantic information in documents has been largely ignored. In this way, the summarization results could be vulnerable to hallucinations, i.e., the semantic-level inconsistency between a summary and corresponding original document. To deal with this challenge, in this paper, we propose a novel fact-aware abstractive summarization model, named Entity-Relation Pointer Generator Network (ERPGN). Specially, we attempt to formalize the facts in original document as a factual knowledge graph, and then generate the high-quality summary via directly modeling consistency between summary and the factual knowledge graph. To that end, we first leverage two pointer net- work structures to capture the fact in original documents. Then, to enhance the traditional token-level likelihood loss, we design two extra semantic-level losses to measure the disagreement between a summary and facts from its original document. Extensive experi- ments on public datasets demonstrate that our ERPGN framework could outperform both classic abstractive summarization models and the state-of-the-art fact-aware baseline methods, with significant improvement in terms of faithfulness.\"\\n      },\\n      {\\n        \"paperId\": \"ac652b7aee8ba769a698f55c61a25e1f1aa270d9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac652b7aee8ba769a698f55c61a25e1f1aa270d9\",\\n        \"title\": \"Answer Summarization for Technical Queries: Benchmark and New Approach\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3551349.3560421\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.10868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A high-quality benchmark to enable automatic evaluation of answer summarization for the technical queries for SQA sites is constructed and it is demonstrated that automatic evaluation has a much lower cost and can uncover findings similar to the ones found through user studies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-09-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2154172271\",\\n            \"name\": \"Chengran Yang\"\\n          },\\n          {\\n            \"authorId\": \"2203459\",\\n            \"name\": \"Bowen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2121315\",\\n            \"name\": \"Ferdian Thung\"\\n          },\\n          {\\n            \"authorId\": \"2185772392\",\\n            \"name\": \"Yucen Shi\"\\n          },\\n          {\\n            \"authorId\": \"2146322053\",\\n            \"name\": \"Ting Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2139059234\",\\n            \"name\": \"Zhou Yang\"\\n          },\\n          {\\n            \"authorId\": \"2148928671\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"151496516\",\\n            \"name\": \"Jieke Shi\"\\n          },\\n          {\\n            \"authorId\": \"2158107537\",\\n            \"name\": \"Junda He\"\\n          },\\n          {\\n            \"authorId\": \"7883212\",\\n            \"name\": \"Donggyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2150912791\",\\n            \"name\": \"David Lo\"\\n          }\\n        ],\\n        \"abstract\": \"Prior studies have demonstrated that approaches to generate an answer summary for a given technical query in Software Question and Answer (SQA) sites are desired. We find that existing approaches are assessed solely through user studies. Hence, a new user study needs to be performed every time a new approach is introduced; this is time-consuming, slows down the development of the new approach, and results from different user studies may not be comparable to each other. There is a need for a benchmark with ground truth summaries as a complement assessment through user studies. Unfortunately, such a benchmark is non-existent for answer summarization for technical queries from SQA sites. To fill the gap, we manually construct a high-quality benchmark to enable automatic evaluation of answer summarization for the technical queries for SQA sites. It contains 111 query-summary pairs extracted from 382 Stack Overflow answers with 2,014 sentence candidates. Using the benchmark, we comprehensively evaluate the performance of existing approaches and find that there is still a big room for improvements. Motivated by the results, we propose a new approach TechSumBot with three key modules:1) Usefulness Ranking module; 2) Centrality Estimation module; and 3) Redundancy Removal module. We evaluate TechSumBot in both automatic (i.e., using our benchmark) and manual (i.e., via a user study) manners. The results from both evaluations consistently demonstrate that TechSumBot outperforms the best performing baseline approaches from both SE and NLP domains by a large margin, i.e., 10.83%\\\\u201314.90%, 32.75%\\\\u201336.59%, and 12.61%\\\\u201317.54%, in terms of ROUGE-1, ROUGE-2, and ROUGE-L on automatic evaluation, and 5.79%\\\\u20139.23% and 17.03%\\\\u201317.68%, in terms of average usefulness and diversity score on human evaluation. This highlights that automatic evaluation on our benchmark can uncover findings similar to the ones found through user studies. More importantly, the automatic evaluation has a much lower cost, especially when it is used to assess a new approach. Additionally, we also conducted an ablation study, which demonstrates that each module in TechSumBot contributes to boosting the overall performance of TechSumBot. We release the benchmark as well as the replication package of our experiment at https://github.com/TechSumBot/TechSumBot.\"\\n      },\\n      {\\n        \"paperId\": \"e391740cd4b5865868b86770479860c3911f5844\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e391740cd4b5865868b86770479860c3911f5844\",\\n        \"title\": \"Static Video Summarization Using Video Coding Features with Frame-Level Temporal Subsampling and Deep Learning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2076-3417/13/10/6065/pdf?version=1684152118\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app13106065?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app13106065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper offers innovative approaches to the problem of key frame extraction for the purpose of video summarization, which includes the extraction of feature variables from the bit streams of coded videos, followed by optional stepwise regression for dimensionality reduction.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2217753873\",\\n            \"name\": \"Obada Issa\"\\n          },\\n          {\\n            \"authorId\": \"1805004\",\\n            \"name\": \"T. Shanableh\"\\n          }\\n        ],\\n        \"abstract\": \"There is an abundance of digital video content due to the cloud\\\\u2019s phenomenal growth and security footage; it is therefore essential to summarize these videos in data centers. This paper offers innovative approaches to the problem of key frame extraction for the purpose of video summarization. Our approach includes the extraction of feature variables from the bit streams of coded videos, followed by optional stepwise regression for dimensionality reduction. Once the features are extracted and their dimensionality is reduced, we apply innovative frame-level temporal subsampling techniques, followed by training and testing using deep learning architectures. The frame-level temporal subsampling techniques are based on cosine similarity and the PCA projections of feature vectors. We create three different learning architectures by utilizing LSTM networks, 1D-CNN networks, and random forests. The four most popular video summarization datasets, namely, TVSum, SumMe, OVP, and VSUMM, are used to evaluate the accuracy of the proposed solutions. This includes the precision, recall, F-score measures, and computational time. It is shown that the proposed solutions, when trained and tested on all subjective user summaries, achieved F-scores of 0.79, 0.74, 0.88, and 0.81, respectively, for the aforementioned datasets, showing clear improvements over prior studies.\"\\n      },\\n      {\\n        \"paperId\": \"7bb09bedd2d356667dc8aa286b62425c0246cd60\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7bb09bedd2d356667dc8aa286b62425c0246cd60\",\\n        \"title\": \"CAVES: A Dataset to facilitate Explainable Classification and Summarization of Concerns towards COVID Vaccines\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.13746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CAVES is the first large-scale dataset containing about 10k COVID-19 anti-vaccine tweets labelled into various specific anti- Vaccine concerns in a multi-label setting and is also the first multi- label classification dataset that provides explanations for each of the labels.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-04-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1388367454\",\\n            \"name\": \"Soham Poddar\"\\n          },\\n          {\\n            \"authorId\": \"2163948982\",\\n            \"name\": \"Azlaan Mustafa Samad\"\\n          },\\n          {\\n            \"authorId\": \"2182282\",\\n            \"name\": \"Rajdeep Mukherjee\"\\n          },\\n          {\\n            \"authorId\": \"4213990\",\\n            \"name\": \"Niloy Ganguly\"\\n          },\\n          {\\n            \"authorId\": \"143841814\",\\n            \"name\": \"Saptarshi Ghosh\"\\n          }\\n        ],\\n        \"abstract\": \"Convincing people to get vaccinated against COVID-19 is a key societal challenge in the present times. As a first step towards this goal, many prior works have relied on social media analysis to understand the specific concerns that people have towards these vaccines, such as potential side-effects, ineffectiveness, political factors, and so on. Though there are datasets that broadly classify social media posts into Anti-vax and Pro-Vax labels, there is no dataset (to our knowledge) that labels social media posts according to the specific anti-vaccine concerns mentioned in the posts. In this paper, we have curated CAVES, the first large-scale dataset containing about 10k COVID-19 anti-vaccine tweets labelled into various specific anti-vaccine concerns in a multi-label setting. This is also the first multi-label classification dataset that provides explanations for each of the labels. Additionally, the dataset also provides class-wise summaries of all the tweets. We also perform preliminary experiments on the dataset and show that this is a very challenging dataset for multi-label explainable classification and tweet summarization, as is evident by the moderate scores achieved by some state-of-the-art models.\"\\n      }\\n    ]\\n  },\\n  \"iterative prompt templating\": {\\n    \"total\": 1387,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"0f49fed824a915418b9dec3429673ed6d13c829c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f49fed824a915418b9dec3429673ed6d13c829c\",\\n        \"title\": \"Claude 2.0 large language model: Tackling a real-world classification problem with a new iterative prompt engineering approach\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1016/j.iswa.2024.200336\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.iswa.2024.200336?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.iswa.2024.200336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3053303\",\\n            \"name\": \"Loredana Caruccio\"\\n          },\\n          {\\n            \"authorId\": \"2053231377\",\\n            \"name\": \"Stefano Cirillo\"\\n          },\\n          {\\n            \"authorId\": \"1745736\",\\n            \"name\": \"G. Polese\"\\n          },\\n          {\\n            \"authorId\": \"1818311573\",\\n            \"name\": \"Giandomenico Solimando\"\\n          },\\n          {\\n            \"authorId\": \"2431234\",\\n            \"name\": \"Shanmugam Sundaramurthy\"\\n          },\\n          {\\n            \"authorId\": \"66533440\",\\n            \"name\": \"G. Tortora\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"e7476b944f8e8a88242bc3f7e3fa267c5709de97\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e7476b944f8e8a88242bc3f7e3fa267c5709de97\",\\n        \"title\": \"Iterative Prompt Learning for Unsupervised Backlit Image Enhancement\",\\n        \"citationCount\": 151,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.17569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel unsupervised backlit image enhancement method by exploring the potential of Contrastive Language-Image Pre-Training (CLIP) for pixel-level image enhancement, which outperforms state-of-the-art methods in terms of visual quality and generalization ability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1972230748\",\\n            \"name\": \"Zhexin Liang\"\\n          },\\n          {\\n            \"authorId\": \"2185949399\",\\n            \"name\": \"Chongyi Li\"\\n          },\\n          {\\n            \"authorId\": \"7523259\",\\n            \"name\": \"Shangchen Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2054599089\",\\n            \"name\": \"Ruicheng Feng\"\\n          },\\n          {\\n            \"authorId\": \"1717179\",\\n            \"name\": \"Chen Change Loy\"\\n          }\\n        ],\\n        \"abstract\": \"We propose a novel unsupervised backlit image enhancement method, abbreviated as CLIP-LIT, by exploring the potential of Contrastive Language-Image Pre-Training (CLIP) for pixel-level image enhancement. We show that the openworld CLIP prior not only aids in distinguishing between backlit and well-lit images, but also in perceiving heterogeneous regions with different luminance, facilitating the optimization of the enhancement network. Unlike high-level and image manipulation tasks, directly applying CLIP to enhancement tasks is non-trivial, owing to the difficulty in finding accurate prompts. To solve this issue, we devise a prompt learning framework that first learns an initial prompt pair by constraining the text-image similarity between the prompt (negative/positive sample) and the corresponding image (backlit image/well-lit image) in the CLIP latent space. Then, we train the enhancement network based on the textimage similarity between the enhanced result and the initial prompt pair. To further improve the accuracy of the initial prompt pair, we iteratively fine-tune the prompt learning framework to reduce the distribution gaps between the backlit images, enhanced results, and well-lit images via rank learning, boosting the enhancement performance. Our method alternates between updating the prompt learning framework and enhancement network until visually pleasing results are achieved. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in terms of visual quality and generalization ability, without requiring any paired data.\"\\n      },\\n      {\\n        \"paperId\": \"1ca54f9a79ffca3eeb187f81bad5c1a1df790c06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1ca54f9a79ffca3eeb187f81bad5c1a1df790c06\",\\n        \"title\": \"Iterative Prompt Refinement for Mining Gene Relationships from ChatGPT\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10769373\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10769373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study systematically evaluated ChatGPT\\\\u2019s capacity for predicting gene relationships using GPT-3.5-turbo and GPT-4, and introduced an innovative iterative prompt refinement technique that significantly enhances the performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-12-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265518204\",\\n            \"name\": \"Yibo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2276572987\",\\n            \"name\": \"Jeffrey Gao\"\\n          },\\n          {\\n            \"authorId\": \"2849198\",\\n            \"name\": \"Marius Petruc\"\\n          },\\n          {\\n            \"authorId\": \"2265390717\",\\n            \"name\": \"Richard D. Hammer\"\\n          },\\n          {\\n            \"authorId\": \"2072374137\",\\n            \"name\": \"Mihail Popescu\"\\n          },\\n          {\\n            \"authorId\": \"2318663840\",\\n            \"name\": \"Dong Xu\"\\n          }\\n        ],\\n        \"abstract\": \"ChatGPT has demonstrated its potential as a surrogate knowledge graph. Trained on extensive data sources, including open-access publications, peer-reviewed research articles and biomedical websites, ChatGPT extracted information on gene relationships and biological pathways. However, a major challenge is model hallucination, i.e., high false positive rates. To assess and address this challenge, we systematically evaluated ChatGPT\\\\u2019s capacity for predicting gene relationships using GPT-3.5-turbo and GPT-4. Benchmarking against the KEGG Pathway Database as the ground truth, we experimented with diverse prompting strategies, targeting gene relationships of activation, inhibition, and phosphorylation. We introduced an innovative iterative prompt refinement technique. By assessing prompt efficacy using metrics like F-1 score, precision, and recall, GPT-4 was re-engaged to suggest improved prompts. A refined prompt, which combines a specialized role with explanatory text, significantly enhances the performance. Going beyond pairwise gene relationships, we also deciphered complex gene interplays, such as gene interaction chains and pathways pertinent to diseases like non-small cell lung cancer. Direct prompts showed limited success, but \\\\u201cleast-to-most\\\\u201d prompting exhibited significant potentials for such network constructions. The methods in this study may be used for some other bioinformatics prediction problems.\"\\n      },\\n      {\\n        \"paperId\": \"1e39f3ca4aff09dccc3b951cdd355c7d8e7cbc2f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e39f3ca4aff09dccc3b951cdd355c7d8e7cbc2f\",\\n        \"title\": \"Generative AI tools in art education: Exploring prompt engineering and iterative processes for enhanced creativity\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.54517/m.v4i1.2164\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54517/m.v4i1.2164?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54517/m.v4i1.2164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI\\\\u2019s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"48965602\",\\n            \"name\": \"Peter Cotroneo\"\\n          },\\n          {\\n            \"authorId\": \"145099528\",\\n            \"name\": \"James Hutson\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid development and adoption of generative artificial intelligence (AI) tools in the art and design education landscape have introduced both opportunities and challenges. This timely study addresses the need to effectively integrate these tools into the classroom while considering ethical implications and the importance of prompt engineering. By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI\\\\u2019s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context. Students in the digital at case study were taught prompt engineering techniques and were tasked with crafting multiple prompts, focusing on refining their ideas over time. Participants demonstrated an increased understanding of the potential and limitations of generative AI tools and how to manipulate subject matter for more effective results. The iterative process encouraged students to explore and experiment with their creative ideas, leading to a deeper understanding of the possibilities offered by AI tools. Despite acknowledging the ethical concerns regarding copyright and the potential replacement of artists, students appreciated the value of generative AI tools for enhancing their sketchbooks and ideation process. Through prompt engineering and iterative processes, students developed a more detail-oriented approach to their work. The challenge of using AI-generated images as final products was conceptually intriguing, requiring further investigation and consideration of the prompts. This study highlights the potential benefits and challenges of integrating generative AI tools into art and design classrooms, emphasizing the importance of prompt engineering, iterative processes, and ethical considerations as these technologies continue to evolve.\"\\n      },\\n      {\\n        \"paperId\": \"0f4219a1a0f89c7364aabb27632cdcc833ab53b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f4219a1a0f89c7364aabb27632cdcc833ab53b7\",\\n        \"title\": \"Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.15843\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.15843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a bi-directional iterative prompt-tuning method for EAE, where the EAE task is treated as a cloze-style task to take full advantage of entity information and pre-trained language models (PLMs).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2072645045\",\\n            \"name\": \"Lu Dai\"\\n          },\\n          {\\n            \"authorId\": \"2156645305\",\\n            \"name\": \"Bang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2052727822\",\\n            \"name\": \"Wei Xiang\"\\n          },\\n          {\\n            \"authorId\": \"1683407\",\\n            \"name\": \"Yijun Mo\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, prompt-tuning has attracted growing interests in event argument extraction (EAE). However, the existing prompt-tuning methods have not achieved satisfactory performance due to the lack of consideration of entity information. In this paper, we propose a bi-directional iterative prompt-tuning method for EAE, where the EAE task is treated as a cloze-style task to take full advantage of entity information and pre-trained language models (PLMs). Furthermore, our method explores event argument interactions by introducing the argument roles of contextual entities into prompt construction. Since template and verbalizer are two crucial components in a cloze-style prompt, we propose to utilize the role label semantic knowledge to construct a semantic verbalizer and design three kind of templates for the EAE task. Experiments on the ACE 2005 English dataset with standard and low-resource settings show that the proposed method significantly outperforms the peer state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"title\": \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\\n        \"citationCount\": 88,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new approach to code generation by LLMs is proposed, which is called AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72729733\",\\n            \"name\": \"T. Ridnik\"\\n          },\\n          {\\n            \"authorId\": \"2279758170\",\\n            \"name\": \"Dedy Kredo\"\\n          },\\n          {\\n            \"authorId\": \"49668367\",\\n            \"name\": \"Itamar Friedman\"\\n          }\\n        ],\\n        \"abstract\": \"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\"\\n      },\\n      {\\n        \"paperId\": \"58f14bc6de4a005b439f38fe11db6e4f95e343bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58f14bc6de4a005b439f38fe11db6e4f95e343bf\",\\n        \"title\": \"Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers, and develops a capable Gradient-inspired LLM-based Prompt Optimizer called GPO.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2109887979\",\\n            \"name\": \"Xinyu Tang\"\\n          },\\n          {\\n            \"authorId\": \"72541556\",\\n            \"name\": \"Xiaolei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257376413\",\\n            \"name\": \"Wayne Xin Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2287879620\",\\n            \"name\": \"Siyuan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2273472188\",\\n            \"name\": \"Yaliang Li\"\\n          },\\n          {\\n            \"authorId\": \"2274218622\",\\n            \"name\": \"Ji-Rong Wen\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. By systematically analyzing a rich set of improvement strategies on the two aspects, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the optimization trajectory as the update direction. Then, it utilizes the generation-based refinement strategy to perform the update, while controlling the edit distance through a cosine-based decay strategy. Extensive experiments demonstrate the effectiveness and efficiency of GPO. In particular, GPO brings an additional improvement of up to 56.8% on Big-Bench Hard and 62.6% on MMLU compared to baseline methods.\"\\n      },\\n      {\\n        \"paperId\": \"015b1f127b6c31654e3597b75876eed8e445d866\",\\n        \"url\": \"https://www.semanticscholar.org/paper/015b1f127b6c31654e3597b75876eed8e445d866\",\\n        \"title\": \"PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.00596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"PhyT2V is presented, a new data-independent T2V technique that expands the current T2V model\\\\u2019s capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333357012\",\\n            \"name\": \"Qiyao Xue\"\\n          },\\n          {\\n            \"authorId\": \"2202556933\",\\n            \"name\": \"Xiangyu Yin\"\\n          },\\n          {\\n            \"authorId\": \"2119657961\",\\n            \"name\": \"Boyuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2274008610\",\\n            \"name\": \"Wei Gao\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation has been recently enabled by transformer-based diffusion models, but current T2V models lack capabilities in adhering to the real-world common knowledge and physical rules, due to their limited understanding of physical realism and deficiency in temporal modeling. Existing solutions are either data-driven or require extra model inputs, but cannot be generalizable to out-of-distribution domains. In this paper, we present PhyT2V, a new data-independent T2V technique that expands the current T2V model\\\\u2019s capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting. Our experiments show that PhyT2V improves existing T2V models\\\\u2019 adherence to real-world physical rules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.\"\\n      },\\n      {\\n        \"paperId\": \"2ed64d90670177bf58cdce6bda04a48a8731a18f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2ed64d90670177bf58cdce6bda04a48a8731a18f\",\\n        \"title\": \"ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing\",\\n        \"citationCount\": 160,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642016\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.09128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs, and identifies three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243191212\",\\n            \"name\": \"Ian Arawjo\"\\n          },\\n          {\\n            \"authorId\": \"40964240\",\\n            \"name\": \"Chelse Swoopes\"\\n          },\\n          {\\n            \"authorId\": \"40961780\",\\n            \"name\": \"Priyan Vaithilingam\"\\n          },\\n          {\\n            \"authorId\": \"2237803620\",\\n            \"name\": \"Martin Wattenberg\"\\n          },\\n          {\\n            \"authorId\": \"143730651\",\\n            \"name\": \"Elena L. Glassman\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating outputs of large language models (LLMs) is challenging, requiring making\\\\u2014and making sense of\\\\u2014many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.\"\\n      },\\n      {\\n        \"paperId\": \"26bdde27ff70384ef7c43f2670d8ec6559c71f45\",\\n        \"url\": \"https://www.semanticscholar.org/paper/26bdde27ff70384ef7c43f2670d8ec6559c71f45\",\\n        \"title\": \"Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.00507, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Examining and comparing prompt chaining and stepwise prompt in the context of text summarization to ascertain which method stands out as the most effective has the potential to be extrapolated to other applications.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256995981\",\\n            \"name\": \"Shichao Sun\"\\n          },\\n          {\\n            \"authorId\": \"2273661632\",\\n            \"name\": \"Ruifeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2314396\",\\n            \"name\": \"Ziqiang Cao\"\\n          },\\n          {\\n            \"authorId\": \"2237591981\",\\n            \"name\": \"Wenjie Li\"\\n          },\\n          {\\n            \"authorId\": \"2256991660\",\\n            \"name\": \"Pengfei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have demonstrated the capacity to improve summary quality by mirroring a human-like iterative process of critique and refinement starting from the initial draft. Two strategies are designed to perform this iterative process: Prompt Chaining and Stepwise Prompt. Prompt chaining orchestrates the drafting, critiquing, and refining phases through a series of three discrete prompts, while Stepwise prompt integrates these phases within a single prompt. However, the relative effectiveness of the two methods has not been extensively studied. This paper is dedicated to examining and comparing these two methods in the context of text summarization to ascertain which method stands out as the most effective. Experimental results show that the prompt chaining method can produce a more favorable outcome. This might be because stepwise prompt might produce a simulated refinement process according to our various experiments. Since refinement is adaptable to diverse tasks, our conclusions have the potential to be extrapolated to other applications, thereby offering insights that may contribute to the broader development of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"a02482f81aa2651848ba060766f850fa3261b42a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a02482f81aa2651848ba060766f850fa3261b42a\",\\n        \"title\": \"Prompt engineering on leveraging large language models in generating response to InBasket messages\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1093/jamia/ocae172\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11413421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Informed by clinician and patient feedback synergistically, tuning in LLM prompt alone can be effective in creating clinically relevant and useful draft responses to PMARs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2312683202\",\\n            \"name\": \"Sherry Yan\"\\n          },\\n          {\\n            \"authorId\": \"2312112991\",\\n            \"name\": \"Wendi Knapp\"\\n          },\\n          {\\n            \"authorId\": \"2312112445\",\\n            \"name\": \"Andrew Leong\"\\n          },\\n          {\\n            \"authorId\": \"2312112555\",\\n            \"name\": \"Sarira Kadkhodazadeh\"\\n          },\\n          {\\n            \"authorId\": \"2312177380\",\\n            \"name\": \"Souvik Das\"\\n          },\\n          {\\n            \"authorId\": \"2312113058\",\\n            \"name\": \"Veena G Jones\"\\n          },\\n          {\\n            \"authorId\": \"2312408017\",\\n            \"name\": \"Robert Clark\"\\n          },\\n          {\\n            \"authorId\": \"2312112645\",\\n            \"name\": \"David Grattendick\"\\n          },\\n          {\\n            \"authorId\": \"2312166178\",\\n            \"name\": \"Kevin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2312113594\",\\n            \"name\": \"Lisa Hladik\"\\n          },\\n          {\\n            \"authorId\": \"2312112709\",\\n            \"name\": \"Lawrence Fagan\"\\n          },\\n          {\\n            \"authorId\": \"2312111094\",\\n            \"name\": \"Albert Chan\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Objectives Large Language Models (LLMs) have been proposed as a solution to address high volumes of Patient Medical Advice Requests (PMARs). This study addresses whether LLMs can generate high quality draft responses to PMARs that satisfies both patients and clinicians with prompt engineering. Materials and Methods We designed a novel human-involved iterative processes to train and validate prompts to LLM in creating appropriate responses to PMARs. GPT-4 was used to generate response to the messages. We updated the prompts, and evaluated both clinician and patient acceptance of LLM-generated draft responses at each iteration, and tested the optimized prompt on independent validation data sets. The optimized prompt was implemented in the electronic health record production environment and tested by 69 primary care clinicians. Results After 3 iterations of prompt engineering, physician acceptance of draft suitability increased from 62% to 84% (P\\\\u2009<.001) in the validation dataset (N\\\\u2009=\\\\u2009200), and 74% of drafts in the test dataset were rated as \\\\u201chelpful.\\\\u201d Patients also noted significantly increased favorability of message tone (78%) and overall quality (80%) for the optimized prompt compared to the original prompt in the training dataset, patients were unable to differentiate human and LLM-generated draft PMAR responses for 76% of the messages, in contrast to the earlier preference for human-generated responses. Majority (72%) of clinicians believed it can reduce cognitive load in dealing with InBasket messages. Discussion and Conclusion Informed by clinician and patient feedback synergistically, tuning in LLM prompt alone can be effective in creating clinically relevant and useful draft responses to PMARs.\"\\n      },\\n      {\\n        \"paperId\": \"c8e1dc10f302fb5cd748672d5d10d95a27c3bd0b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c8e1dc10f302fb5cd748672d5d10d95a27c3bd0b\",\\n        \"title\": \"Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.08995, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Self-prompt tuning is proposed, making LLMs themselves generate role-play prompts through fine-tuning, thereby automating complex prompting strategies and illustrating the great potential of utilizing fine-tuning to enable LLMs to self-prompt, thereby automating complex prompting strategies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216561498\",\\n            \"name\": \"Aobo Kong\"\\n          },\\n          {\\n            \"authorId\": \"2275765829\",\\n            \"name\": \"Shiwan Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2242179099\",\\n            \"name\": \"Hao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2311304336\",\\n            \"name\": \"Qicheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2275736194\",\\n            \"name\": \"Yong Qin\"\\n          },\\n          {\\n            \"authorId\": \"2216621163\",\\n            \"name\": \"Ruiqi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2311736771\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2275780366\",\\n            \"name\": \"Jiaming Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2275584632\",\\n            \"name\": \"Haoqin Sun\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in LLMs have showcased their remarkable role-playing capabilities, able to accurately simulate the dialogue styles and cognitive processes of various roles based on different instructions and contexts. Studies indicate that assigning LLMs the roles of experts, a strategy known as role-play prompting, can enhance their performance in the corresponding domains. However, the prompt needs to be manually designed for the given problem, requiring certain expertise and iterative modifications. To this end, we propose self-prompt tuning, making LLMs themselves generate role-play prompts through fine-tuning. Leveraging the LIMA dataset as our foundational corpus, we employ GPT-4 to annotate role-play prompts for each data points, resulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like Llama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned LLMs can automatically generate expert role prompts for any given question. We extensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and open-ended question test. Our empirical results illustrate that self-prompt tuned LLMs outperform standard instruction tuned baselines across most datasets. This highlights the great potential of utilizing fine-tuning to enable LLMs to self-prompt, thereby automating complex prompting strategies. We release the dataset, models, and code at this \\\\\\\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.\"\\n      },\\n      {\\n        \"paperId\": \"b503050327ba9e01d27dd8d06589544f89f40cbd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b503050327ba9e01d27dd8d06589544f89f40cbd\",\\n        \"title\": \"LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.16929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Inspired by structured reusable programming languages, LangGPT, a dual-layer prompt design framework as the programming language for LLMs is proposed, which has an easy-to-learn normative structure and provides an extended structure for migration and reuse.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-02-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2249764692\",\\n            \"name\": \"Ming Wang\"\\n          },\\n          {\\n            \"authorId\": \"2288139031\",\\n            \"name\": \"Yuanzhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2287879683\",\\n            \"name\": \"Xiaoming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2287801318\",\\n            \"name\": \"Songlian Li\"\\n          },\\n          {\\n            \"authorId\": \"2287879297\",\\n            \"name\": \"Yijie Huang\"\\n          },\\n          {\\n            \"authorId\": \"2287880544\",\\n            \"name\": \"Chi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2111226672\",\\n            \"name\": \"Daling Wang\"\\n          },\\n          {\\n            \"authorId\": \"2087586948\",\\n            \"name\": \"Shi Feng\"\\n          },\\n          {\\n            \"authorId\": \"2287871665\",\\n            \"name\": \"Jigang Li\"\\n          }\\n        ],\\n        \"abstract\": \"LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. In addition, it is not conducive to the iterative updating of prompts. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the performance of LLMs. Moreover, the case study shows that LangGPT leads LLMs to generate higher-quality responses. Furthermore, we analyzed the ease of use and reusability of LangGPT through a user survey in our online community.\"\\n      },\\n      {\\n        \"paperId\": \"0b8e991c334bfaa5d8ede08bb0fdbe446622931a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b8e991c334bfaa5d8ede08bb0fdbe446622931a\",\\n        \"title\": \"Leveraging enhanced egret swarm optimization algorithm and artificial intelligence-driven prompt strategies for portfolio selection\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11535209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A structured prompt framework specifically designed for stock selection, aiming to provide direct and interpretable stock-selecting tools for investors of various levels and a novel algorithm that combines the Nonlinear-Activated Beetle Antennae Search strategy with the Egret Swarm Optimization Algorithm to address the Mean-Variance Portfolio Selection problem with Transaction Costs and Cardinality Constraints.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2329413534\",\\n            \"name\": \"Zhendai Huang\"\\n          },\\n          {\\n            \"authorId\": \"2294153042\",\\n            \"name\": \"Zhen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2181171694\",\\n            \"name\": \"Cheng Hua\"\\n          },\\n          {\\n            \"authorId\": \"2329153233\",\\n            \"name\": \"Bolin Liao\"\\n          },\\n          {\\n            \"authorId\": \"2133437125\",\\n            \"name\": \"Shuai Li\"\\n          }\\n        ],\\n        \"abstract\": \"In the financial field, constructing efficient investment portfolios is a focal point of research, encompassing asset selection and optimization of asset allocation. With the advancements in Large Language Models (LLMs), generative Artificial Intelligence (AI) tools have showcased capabilities never seen before. However, the black-box nature of these tools renders their outputs difficult to interpret directly, often necessitating iterative fine-tuning to align with users\\\\u2019 expected outcomes. This study presents a structured prompt framework specifically designed for stock selection, aiming to provide direct and interpretable stock-selecting tools for investors of various levels. By creating representative scenarios and combining them into different cases for experimentation, we can explore how the construction of prompts influences the responses generated by generative AI tools. Additionally, this paper proposes a novel algorithm that combines the Nonlinear-Activated Beetle Antennae Search strategy with the Egret Swarm Optimization Algorithm (NBESOA) to address the Mean-Variance Portfolio Selection problem with Transaction Costs and Cardinality Constraints (MVPS-TCCC), utilizing real stock market data to construct portfolios based on generative AI tools recommendations. Simulation results indicate that, compared to other algorithms, NBESOA prefers optimizing portfolio configurations to achieve the highest Sharpe Ratio with the strictest constraints, bringing the outcomes closer to the portfolio\\\\u2019s efficient frontier.\"\\n      },\\n      {\\n        \"paperId\": \"168ce6b5844f8cd4a3375f69952ccf630874689d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/168ce6b5844f8cd4a3375f69952ccf630874689d\",\\n        \"title\": \"Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.11105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method called Source Prompt Disentangled Inversion (SPDInv), which aims at reducing the impact of source prompt, thereby enhancing the text-driven image editing performance by employing diffusion models and can easily adapt customized image generation models to localized editing tasks and produce promising performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284415331\",\\n            \"name\": \"Rui Li\"\\n          },\\n          {\\n            \"authorId\": \"151482572\",\\n            \"name\": \"Ruihuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2279757730\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"2277508910\",\\n            \"name\": \"Lei Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-driven diffusion models have significantly advanced the image editing performance by using text prompts as inputs. One crucial step in text-driven image editing is to invert the original image into a latent noise code conditioned on the source prompt. While previous methods have achieved promising results by refactoring the image synthesizing process, the inverted latent noise code is tightly coupled with the source prompt, limiting the image editability by target text prompts. To address this issue, we propose a novel method called Source Prompt Disentangled Inversion (SPDInv), which aims at reducing the impact of source prompt, thereby enhancing the text-driven image editing performance by employing diffusion models. To make the inverted noise code be independent of the given source prompt as much as possible, we indicate that the iterative inversion process should satisfy a fixed-point constraint. Consequently, we transform the inversion problem into a searching problem to find the fixed-point solution, and utilize the pre-trained diffusion models to facilitate the searching process. The experimental results show that our proposed SPDInv method can effectively mitigate the conflicts between the target editing prompt and the source prompt, leading to a significant decrease in editing artifacts. In addition to text-driven image editing, with SPDInv we can easily adapt customized image generation models to localized editing tasks and produce promising performance. The source code are available at https://github.com/leeruibin/SPDInv.\"\\n      },\\n      {\\n        \"paperId\": \"f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"title\": \"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.01701?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"a0a3d3349497782861ecc436c3807bfe1a7d90a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0a3d3349497782861ecc436c3807bfe1a7d90a1\",\\n        \"title\": \"Prompting Change: Exploring Prompt Engineering in Large Language Model AI and Its Potential to Transform Education\",\\n        \"citationCount\": 127,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11528-023-00896-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11528-023-00896-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The paper underscores how the natural language capabilities of LLM AI tools can help students and educators transition from passive recipients to active co-creators of their learning experiences, and charts the evolving trajectory of LLM AI as a tool poised to reshape educational practices and assumptions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260528529\",\\n            \"name\": \"William Cain\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"27af585e0e48741a18a042a7b7590f021483e151\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27af585e0e48741a18a042a7b7590f021483e151\",\\n        \"title\": \"Semantic-guided Prompt Organization for Universal Goal Hijacking against LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2405.14189?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2405.14189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A universal goal hijacking method called POUGH that incorporates semantic-guided prompt processing strategies and employs an iterative optimization algorithm to generate the universal fixed suffix for the prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142367654\",\\n            \"name\": \"Yihao Huang\"\\n          },\\n          {\\n            \"authorId\": \"2276510804\",\\n            \"name\": \"Chong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2275762052\",\\n            \"name\": \"Xiaojun Jia\"\\n          },\\n          {\\n            \"authorId\": \"2260611391\",\\n            \"name\": \"Qing Guo\"\\n          },\\n          {\\n            \"authorId\": \"2119277493\",\\n            \"name\": \"Felix Juefei-Xu\"\\n          },\\n          {\\n            \"authorId\": \"2265725720\",\\n            \"name\": \"Jian Zhang\"\\n          },\\n          {\\n            \"authorId\": \"4166714\",\\n            \"name\": \"G. Pu\"\\n          },\\n          {\\n            \"authorId\": \"2265753294\",\\n            \"name\": \"Yang Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"d5544571fa37a6eaac972153cec57df591c38a04\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d5544571fa37a6eaac972153cec57df591c38a04\",\\n        \"title\": \"MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.13191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MCQG-SRefine is proposed, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions, and introduces an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1576489304\",\\n            \"name\": \"Zonghai Yao\"\\n          },\\n          {\\n            \"authorId\": \"2326302479\",\\n            \"name\": \"Aditya Parashar\"\\n          },\\n          {\\n            \"authorId\": \"2261556443\",\\n            \"name\": \"Huixue Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2323786086\",\\n            \"name\": \"Won Seok Jang\"\\n          },\\n          {\\n            \"authorId\": \"2263773255\",\\n            \"name\": \"Feiyun Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2322987613\",\\n            \"name\": \"Zhichao Yang\"\\n          },\\n          {\\n            \"authorId\": \"2261455807\",\\n            \"name\": \"Hong Yu\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty. To address these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions. By integrating expert-driven prompt engineering with iterative self-critique and self-correction feedback, MCQG-SRefine significantly enhances human expert satisfaction regarding both the quality and difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.\"\\n      },\\n      {\\n        \"paperId\": \"e314d182fd9d35a05870b38a56ee38eb3149b47d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e314d182fd9d35a05870b38a56ee38eb3149b47d\",\\n        \"title\": \"Attack Prompt Generation for Red Teaming and Defending Large Language Models\",\\n        \"citationCount\": 81,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.12505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning to enhance their safety against red teaming attacks, and proposes a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260342358\",\\n            \"name\": \"Boyi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2117833732\",\\n            \"name\": \"Wenjie Wang\"\\n          },\\n          {\\n            \"authorId\": \"2163400298\",\\n            \"name\": \"Fuli Feng\"\\n          },\\n          {\\n            \"authorId\": \"2367303636\",\\n            \"name\": \"Yang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2260433198\",\\n            \"name\": \"Qifan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2239071206\",\\n            \"name\": \"Xiangnan He\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facilitating the safety evaluation and enhancement of more LLMs. Our code and dataset is available on https://github.com/Aatrox103/SAP .\"\\n      },\\n      {\\n        \"paperId\": \"848909fbae167f21589bfc7a54fbf27e306b883c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/848909fbae167f21589bfc7a54fbf27e306b883c\",\\n        \"title\": \"An Iterative Optimizing Framework for Radiology Report Summarization With ChatGPT\",\\n        \"citationCount\": 123,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2304.08448\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.08448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed ImpressionGPT model achieves superior performance of AIG task on both the Medical Information Mart for Intensive Care - Chest X-ray database and Open Access Biomedical Image Search Engine datasets without requiring additional training data or fine-tuning the LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2132543537\",\\n            \"name\": \"Chong Ma\"\\n          },\\n          {\\n            \"authorId\": \"47039788\",\\n            \"name\": \"Zihao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2110238778\",\\n            \"name\": \"Jiaqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2211904452\",\\n            \"name\": \"Shaochen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2163675860\",\\n            \"name\": \"Yaonai Wei\"\\n          },\\n          {\\n            \"authorId\": \"2145977326\",\\n            \"name\": \"Zheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"143621713\",\\n            \"name\": \"Lei Guo\"\\n          },\\n          {\\n            \"authorId\": \"2174965546\",\\n            \"name\": \"Xiaoya Cai\"\\n          },\\n          {\\n            \"authorId\": \"2108086798\",\\n            \"name\": \"Shu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"49104946\",\\n            \"name\": \"Tuo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2181182\",\\n            \"name\": \"Dajiang Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2150038187\",\\n            \"name\": \"Dinggang Shen\"\\n          },\\n          {\\n            \"authorId\": \"2115345993\",\\n            \"name\": \"Tianming Liu\"\\n          },\\n          {\\n            \"authorId\": \"2144438902\",\\n            \"name\": \"Xiang Li\"\\n          }\\n        ],\\n        \"abstract\": \"The \\\\u201cImpression\\\\u201d section of a radiology report is a critical basis for communication between radiologists and other physicians. Typically written by radiologists, this part is derived from the \\\\u201cFindings\\\\u201d section, which can be laborious and error-prone. Although deep-learning-based models, such as bidirectional encoder representation from transformers (BERT), have achieved promising results in automatic impression generation (AIG), such models often require substantial amounts of medical data and have poor generalization performance. Recently, large language models (LLMs) like Chat Generative Pre-trained Transformer (ChatGPT) have shown strong generalization capabilities and performance, but their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, leveraging the contextual learning capabilities of LLMs through our dynamic prompt and iterative optimization algorithm to accomplish the AIG task. ImpressionGPT initially employs a small amount of domain-specific data to create a dynamic prompt, extracting contextual semantic information closely related to the test data. Subsequently, the iterative optimization algorithm automatically evaluates the output of LLMs and provides optimization suggestions, continuously refining the output results. The proposed ImpressionGPT model achieves superior performance of AIG task on both the Medical Information Mart for Intensive Care - Chest X-ray database (MIMIC-CXR) and Open Access Biomedical Image Search Engine (OpenI) datasets without requiring additional training data or fine-tuning the LLMs. This work presents a paradigm for localizing LLMs that can be applied in a wide range of similar application scenarios, bridging the gap between general-purpose LLMs and the specific language processing needs of various domains.\"\\n      },\\n      {\\n        \"paperId\": \"2d77b7203824e617206634277bce7eec2b71a2bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d77b7203824e617206634277bce7eec2b71a2bd\",\\n        \"title\": \"Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.20701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is revealed that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans, and a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE) is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105515515\",\\n            \"name\": \"Pengwei Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2294783971\",\\n            \"name\": \"Zhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2149324430\",\\n            \"name\": \"Qian Tan\"\\n          },\\n          {\\n            \"authorId\": \"2304366173\",\\n            \"name\": \"Jie Song\"\\n          },\\n          {\\n            \"authorId\": \"2304326919\",\\n            \"name\": \"Ru Xie\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.\"\\n      },\\n      {\\n        \"paperId\": \"6c7a1bcaa69dd29cb5fa89ac4a9ec61621c211fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6c7a1bcaa69dd29cb5fa89ac4a9ec61621c211fb\",\\n        \"title\": \"SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.06426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research highlights the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse, and introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2330190651\",\\n            \"name\": \"Bijoy Ahmed Saiem\"\\n          },\\n          {\\n            \"authorId\": \"2330191650\",\\n            \"name\": \"MD Sadik Hossain Shanto\"\\n          },\\n          {\\n            \"authorId\": \"2330191231\",\\n            \"name\": \"Rakib Ahsan\"\\n          },\\n          {\\n            \"authorId\": \"2150249269\",\\n            \"name\": \"Md. Rafi Ur Rashid\"\\n          }\\n        ],\\n        \"abstract\": \"As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.\"\\n      },\\n      {\\n        \"paperId\": \"113873a4e58e2ff15ce3523ee9fb629ff6dddfe4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/113873a4e58e2ff15ce3523ee9fb629ff6dddfe4\",\\n        \"title\": \"Does Prompt Formatting Have Any Impact on LLM Performance?\",\\n        \"citationCount\": 116,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.10541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper examines the impact of different prompt templates on LLM performance, and highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2331365723\",\\n            \"name\": \"Jia He\"\\n          },\\n          {\\n            \"authorId\": \"35315626\",\\n            \"name\": \"Mukund Rungta\"\\n          },\\n          {\\n            \"authorId\": \"2120266368\",\\n            \"name\": \"David Koleczek\"\\n          },\\n          {\\n            \"authorId\": \"5968525\",\\n            \"name\": \"Arshdeep Sekhon\"\\n          },\\n          {\\n            \"authorId\": \"2331375861\",\\n            \"name\": \"Franklin X Wang\"\\n          },\\n          {\\n            \"authorId\": \"2322137989\",\\n            \"name\": \"Sadid A. Hasan\"\\n          }\\n        ],\\n        \"abstract\": \"In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI\\'s GPT models. Experiments show that GPT-3.5-turbo\\'s performance varies by up to 40\\\\\\\\% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.\"\\n      },\\n      {\\n        \"paperId\": \"3f8d2977a7753f9a3b0b5d18e6bee6afb0028ed8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f8d2977a7753f9a3b0b5d18e6bee6afb0028ed8\",\\n        \"title\": \"PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs\",\\n        \"citationCount\": 164,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel visual prompting approach for VLMs that is called Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering, and finds that it enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3457048\",\\n            \"name\": \"Soroush Nasiriany\"\\n          },\\n          {\\n            \"authorId\": \"2267320085\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2265397442\",\\n            \"name\": \"Wenhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"9961095\",\\n            \"name\": \"Ted Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2271707306\",\\n            \"name\": \"Jacky Liang\"\\n          },\\n          {\\n            \"authorId\": \"2263548023\",\\n            \"name\": \"Ishita Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"14484808\",\\n            \"name\": \"Annie Xie\"\\n          },\\n          {\\n            \"authorId\": \"2283848260\",\\n            \"name\": \"Danny Driess\"\\n          },\\n          {\\n            \"authorId\": \"88728227\",\\n            \"name\": \"Ayzaan Wahid\"\\n          },\\n          {\\n            \"authorId\": \"2265456732\",\\n            \"name\": \"Zhuo Xu\"\\n          },\\n          {\\n            \"authorId\": \"144579461\",\\n            \"name\": \"Q. Vuong\"\\n          },\\n          {\\n            \"authorId\": \"2240715659\",\\n            \"name\": \"Tingnan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2110662889\",\\n            \"name\": \"T. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2145145412\",\\n            \"name\": \"Kuang-Huei Lee\"\\n          },\\n          {\\n            \"authorId\": \"2153917744\",\\n            \"name\": \"Peng Xu\"\\n          },\\n          {\\n            \"authorId\": \"51881277\",\\n            \"name\": \"Sean Kirmani\"\\n          },\\n          {\\n            \"authorId\": \"2253507326\",\\n            \"name\": \"Yuke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2267246149\",\\n            \"name\": \"Andy Zeng\"\\n          },\\n          {\\n            \"authorId\": \"1944801\",\\n            \"name\": \"Karol Hausman\"\\n          },\\n          {\\n            \"authorId\": \"2801204\",\\n            \"name\": \"N. Heess\"\\n          },\\n          {\\n            \"authorId\": \"2257346440\",\\n            \"name\": \"Chelsea Finn\"\\n          },\\n          {\\n            \"authorId\": \"2257062067\",\\n            \"name\": \"Sergey Levine\"\\n          },\\n          {\\n            \"authorId\": \"2704814\",\\n            \"name\": \"Brian Ichter\"\\n          }\\n        ],\\n        \"abstract\": \"Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.\"\\n      },\\n      {\\n        \"paperId\": \"b9b93b2946ed13f33f3dd75254fff0ca4fd4e651\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b9b93b2946ed13f33f3dd75254fff0ca4fd4e651\",\\n        \"title\": \"Unsupervised Human Activity Recognition Via Large Language Models and Iterative Evolution\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICASSP48485.2024.10446819?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICASSP48485.2024.10446819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292733359\",\\n            \"name\": \"Jiayuan Gao\"\\n          },\\n          {\\n            \"authorId\": \"2145025105\",\\n            \"name\": \"Yingwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2243383757\",\\n            \"name\": \"Yiqiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"19264986\",\\n            \"name\": \"Tengxiang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2292488219\",\\n            \"name\": \"Boshi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2214851453\",\\n            \"name\": \"Xiaoyu Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Human activity recognition (HAR) is crucial for health monitoring and disease diagnosis in Internet-of-Things environments. However, existing HAR approaches either suffer from poor accuracy or achieve high accuracy at the expense of costly manual annotations. To overcome the challenge above, we propose a novel method named LLMIE-UHAR that that leverages LLMs and Iterative Evolution to realize Unsupervised HAR. Specifically, with our designed prompt engineering mechanism, we employ large language models to fuse both contextual and semantic information, and annotate key samples selected by a clustering algorithm. Moreover, LLMIE-UHAR enhances the recognition accuracy with iterative evolution of clustering algorithm, large language models and the neural network based recognition model. Experiments conducted on the public ARAS datasets show the efficiency of our method, achieving an accuracy of 96.00%. This highlights the practical value of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6\",\\n        \"title\": \"Ignore Previous Prompt: Attack Techniques For Language Models\",\\n        \"citationCount\": 575,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2211.09527\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.09527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates two types of attacks -- goal hijacking and prompt leaking -- and demonstrates that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3\\'s stochastic nature, creating long-tail risks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-11-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2191070702\",\\n            \"name\": \"F\\\\u00e1bio Perez\"\\n          },\\n          {\\n            \"authorId\": \"2191076673\",\\n            \"name\": \"Ian Ribeiro\"\\n          }\\n        ],\\n        \"abstract\": \"Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3\\'s stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.\"\\n      },\\n      {\\n        \"paperId\": \"e4503969a4cc5c6a1930a2845109221c3f544e64\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e4503969a4cc5c6a1930a2845109221c3f544e64\",\\n        \"title\": \"De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks via Iterative Grounding\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, a technique that grounds the predictions of an LLM through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt through a novel combination of retrieving suitable API references and iteratively querying the model.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-01-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) trained on datasets of publicly available source code have established a new state of the art in code generation tasks. However, these models are mostly unaware of the code that exists within a specific project, preventing the models from making good use of existing APIs. Instead, LLMs often invent, or\\\\\"hallucinate\\\\\", non-existent APIs or produce variants of already existing code. This paper presents De-Hallucinator, a technique that grounds the predictions of an LLM through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt. The approach exploits the observation that predictions by LLMs often resemble the desired code, but they fail to correctly refer to already existing APIs. De-Hallucinator automatically identifies project-specific API references related to the model\\'s initial predictions and adds these references into the prompt. Unlike retrieval-augmented generation (RAG), our approach uses the initial prediction(s) by the model to iteratively retrieve increasingly suitable API references. Our evaluation applies the approach to two tasks: predicting API usages in Python and generating tests in JavaScript. We show that De-Hallucinator consistently improves the generated code across five LLMs. In particular, the approach improves the edit distance by 23.3-50.6% and the recall of correctly predicted API usages by 23.9-61.0% for code completion, and improves the number of fixed tests that initially failed because of hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage for test generation.\"\\n      },\\n      {\\n        \"paperId\": \"7eaaa5aec72ffb95cf538462ed205fa4d4faef9c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7eaaa5aec72ffb95cf538462ed205fa4d4faef9c\",\\n        \"title\": \"Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.09993\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.09993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reprompting, an iterative sampling algorithm that automatically learns the Chain-of-Thought recipes for a given task without human intervention, achieves consistently better performance than the state-of-the-art prompt optimization and decoding algorithms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47210642\",\\n            \"name\": \"Weijia Xu\"\\n          },\\n          {\\n            \"authorId\": \"2168558581\",\\n            \"name\": \"Andrzej Banburski-Fahey\"\\n          },\\n          {\\n            \"authorId\": \"1698689\",\\n            \"name\": \"N. Jojic\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce Reprompting, an iterative sampling algorithm that automatically learns the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, Reprompting infers the CoT recipes that work consistently well for a set of training samples by iteratively sampling new recipes using previously sampled recipes as parent prompts to solve other training problems. We conduct extensive experiments on 20 challenging reasoning tasks. Results show that Reprompting outperforms human-written CoT prompts substantially by +9.4 points on average. It also achieves consistently better performance than the state-of-the-art prompt optimization and decoding algorithms.\"\\n      },\\n      {\\n        \"paperId\": \"f53a4f34757d1f237446b4d887d5323f2a17ed02\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f53a4f34757d1f237446b4d887d5323f2a17ed02\",\\n        \"title\": \"PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2308.12033\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.12033, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations of prompt ensemble and builds a feedback mechanism for reflecting on the inadequacies of existing weak learners.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2313691609\",\\n            \"name\": \"Chenrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2166025432\",\\n            \"name\": \"Lina Liu\"\\n          },\\n          {\\n            \"authorId\": \"48093158\",\\n            \"name\": \"Jinpeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260827558\",\\n            \"name\": \"Chuyuan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2168803085\",\\n            \"name\": \"Xiaodi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2109800642\",\\n            \"name\": \"Hongyu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2149926941\",\\n            \"name\": \"Mingchen Cai\"\\n          }\\n        ],\\n        \"abstract\": \"As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1\",\\n        \"title\": \"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.04749\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.04749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment is explored and it is found that the assertion level alignment scores provide a useful feedback which can be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2112713578\",\\n            \"name\": \"Jaskirat Singh\"\\n          },\\n          {\\n            \"authorId\": \"144802394\",\\n            \"name\": \"Liang Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs. Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy. Project page for our paper is available at https://1jsingh.github.io/divide-evaluate-and-refine\"\\n      },\\n      {\\n        \"paperId\": \"2809a68abc9be8f1118ac2dea2af50e619b659c0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2809a68abc9be8f1118ac2dea2af50e619b659c0\",\\n        \"title\": \"DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2304.02827\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.02827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DITTO-NeRF is proposed, a novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a single image that outperforms state-of-the-art methods in terms of fidelity and diversity qualitatively and quantitatively with much faster training times than prior arts on image/text-to-3D such as DreamFusion, and NeuralLift-360.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"51245215\",\\n            \"name\": \"H. Seo\"\\n          },\\n          {\\n            \"authorId\": \"47298448\",\\n            \"name\": \"Hayeon Kim\"\\n          },\\n          {\\n            \"authorId\": \"2109334279\",\\n            \"name\": \"Gwanghyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"34971370\",\\n            \"name\": \"S. Chun\"\\n          }\\n        ],\\n        \"abstract\": \"The increasing demand for high-quality 3D content creation has motivated the development of automated methods for creating 3D object models from a single image and/or from a text prompt. However, the reconstructed 3D objects using state-of-the-art image-to-3D methods still exhibit low correspondence to the given image and low multi-view consistency. Recent state-of-the-art text-to-3D methods are also limited, yielding 3D samples with low diversity per prompt with long synthesis time. To address these challenges, we propose DITTO-NeRF, a novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a single image. Our DITTO-NeRF consists of constructing high-quality partial 3D object for limited in-boundary (IB) angles using the given or text-generated 2D image from the frontal view and then iteratively reconstructing the remaining 3D NeRF using inpainting latent diffusion model. We propose progressive 3D object reconstruction schemes in terms of scales (low to high resolution), angles (IB angles initially to outer-boundary (OB) later), and masks (object to background boundary) in our DITTO-NeRF so that high-quality information on IB can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods in terms of fidelity and diversity qualitatively and quantitatively with much faster training times than prior arts on image/text-to-3D such as DreamFusion, and NeuralLift-360.\"\\n      },\\n      {\\n        \"paperId\": \"9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2\",\\n        \"title\": \"Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates\",\\n        \"citationCount\": 80,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.18540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing\\'\\' (PTST) strategy -- fine-tune models without a safety prompt, but include it at test time.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41049476\",\\n            \"name\": \"Kaifeng Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2288581125\",\\n            \"name\": \"Haoyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2261386415\",\\n            \"name\": \"Xinran Gu\"\\n          },\\n          {\\n            \"authorId\": \"2141026731\",\\n            \"name\": \"Dingli Yu\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2261737783\",\\n            \"name\": \"Sanjeev Arora\"\\n          }\\n        ],\\n        \"abstract\": \"Public LLMs such as the Llama 2-Chat underwent alignment training and were considered safe. Recently Qi et al. [2024] reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment. Through extensive experiments on several chat models (Meta\\'s Llama 2-Chat, Mistral AI\\'s Mistral 7B Instruct v0.2, and OpenAI\\'s GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing\\'\\' (PTST) strategy -- fine-tune models without a safety prompt, but include it at test time. This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.\"\\n      },\\n      {\\n        \"paperId\": \"149ab16bed8554e71358b3642cb6979884c786b9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/149ab16bed8554e71358b3642cb6979884c786b9\",\\n        \"title\": \"Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.04031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Bi-Modal Adversarial Prompt Attack (BAP) is introduced, which executes jailbreaks by optimizing textual and visual prompts cohesively and significantly outperforms other methods by large margins.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2304955104\",\\n            \"name\": \"Zonghao Ying\"\\n          },\\n          {\\n            \"authorId\": \"2257572247\",\\n            \"name\": \"Aishan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2300719164\",\\n            \"name\": \"Tianyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305061690\",\\n            \"name\": \"Zhengmin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2114786732\",\\n            \"name\": \"Siyuan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2237942988\",\\n            \"name\": \"Xianglong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2237906923\",\\n            \"name\": \"Dacheng Tao\"\\n          }\\n        ],\\n        \"abstract\": \"In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation. To address this limitation, this paper introduces the Bi-Modal Adversarial Prompt Attack (BAP), which executes jailbreaks by optimizing textual and visual prompts cohesively. Initially, we adversarially embed universally adversarial perturbations in an image, guided by a few-shot query-agnostic corpus (e.g., affirmative prefixes and negative inhibitions). This process ensures that the adversarial image prompt LVLMs to respond positively to harmful queries. Subsequently, leveraging the image, we optimize textual prompts with specific harmful intent. In particular, we utilize a large language model to analyze jailbreak failures and employ chain-of-thought reasoning to refine textual prompts through a feedback-iteration manner. To validate the efficacy of our approach, we conducted extensive evaluations on various datasets and LVLMs, demonstrating that our BAP significantly outperforms other methods by large margins (+29.03% in attack success rate on average). Additionally, we showcase the potential of our attacks on black-box commercial LVLMs, such as GPT-4o and Gemini. Our code is available at https://anonymous.4open.science/r/BAP-Jailbreak-Vision-Language-Models-via-Bi-Modal-Adversarial-Prompt-5496\"\\n      },\\n      {\\n        \"paperId\": \"1ecd677e3d12bffec77d23db696610a7744cf33f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1ecd677e3d12bffec77d23db696610a7744cf33f\",\\n        \"title\": \"WorldSmith: Iterative and Expressive Prompting for World Building with a Generative AI\",\\n        \"citationCount\": 38,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2308.13355\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.13355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates the use of recent multi-modal image generation systems to enable users iteratively visualize and modify elements of their fictional world using a combination of text input, sketching, and region-based filling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2767073\",\\n            \"name\": \"Hai Dang\"\\n          },\\n          {\\n            \"authorId\": \"2408148\",\\n            \"name\": \"Frederik Brudy\"\\n          },\\n          {\\n            \"authorId\": \"1703735\",\\n            \"name\": \"G. Fitzmaurice\"\\n          },\\n          {\\n            \"authorId\": \"2408187\",\\n            \"name\": \"Fraser Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Crafting a rich and unique environment is crucial for fictional world-building, but can be difficult to achieve since illustrating a world from scratch requires time and significant skill. We investigate the use of recent multi-modal image generation systems to enable users iteratively visualize and modify elements of their fictional world using a combination of text input, sketching, and region-based filling. WorldSmith enables novice world builders to quickly visualize a fictional world with layered edits and hierarchical compositions. Through a formative study (4 participants) and first-use study (13 participants) we demonstrate that WorldSmith offers more expressive interactions with prompt-based models. With this work, we explore how creatives can be empowered to leverage prompt-based generative AI as a tool in their creative process, beyond current \\\\\"click-once\\\\\" prompting UI paradigms.\"\\n      },\\n      {\\n        \"paperId\": \"1d14a708622917da4b9820ada6d32af24fc1651a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d14a708622917da4b9820ada6d32af24fc1651a\",\\n        \"title\": \"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.08541\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.08541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2149231840\",\\n            \"name\": \"Zhengyuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2124948371\",\\n            \"name\": \"Jianfeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"50703697\",\\n            \"name\": \"Linjie Li\"\\n          },\\n          {\\n            \"authorId\": \"143786724\",\\n            \"name\": \"Kevin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2249709105\",\\n            \"name\": \"Chung-Ching Lin\"\\n          },\\n          {\\n            \"authorId\": \"2251726216\",\\n            \"name\": \"Zicheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"29957038\",\\n            \"name\": \"Lijuan Wang\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce ``Idea to Image,\\'\\' a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model\\'s characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative self-refinement on automatic image design and generation.\"\\n      },\\n      {\\n        \"paperId\": \"a2fae006e6c5ac346fd51bc8a009127f9abe22df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a2fae006e6c5ac346fd51bc8a009127f9abe22df\",\\n        \"title\": \"Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.13006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An open-source framework to evaluate, compare, and visualize the reliability and alignment of LLM judges, which facilitates practitioners to choose LLM judges for alignment tasks and indicates a significant impact of prompt templates on LLM judge performance, as well as a mediocre alignment level between the tested LLM judges and human evaluators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316959296\",\\n            \"name\": \"Hui Wei\"\\n          },\\n          {\\n            \"authorId\": \"2318243914\",\\n            \"name\": \"Shenghua He\"\\n          },\\n          {\\n            \"authorId\": \"2316860672\",\\n            \"name\": \"Tian Xia\"\\n          },\\n          {\\n            \"authorId\": \"2316960662\",\\n            \"name\": \"Andy Wong\"\\n          },\\n          {\\n            \"authorId\": \"2317016595\",\\n            \"name\": \"Jingyang Lin\"\\n          },\\n          {\\n            \"authorId\": \"2317047664\",\\n            \"name\": \"Mei Han\"\\n          }\\n        ],\\n        \"abstract\": \"LLM-as-a-Judge has been widely applied to evaluate and compare different LLM alignmnet approaches (e.g., RLHF and DPO). However, concerns regarding its reliability have emerged, due to LLM judges\\' biases and inconsistent decision-making. Previous research has developed evaluation frameworks to assess reliability of LLM judges and their alignment with human preferences. However, the employed evaluation metrics often lack adequate explainability and fail to address LLM internal inconsistency. Additionally, existing studies inadequately explore the impact of various prompt templates when applying LLM-as-a-Judge methods, leading to potentially inconsistent comparisons between different alignment algorithms. In this work, we systematically evaluate LLM-as-a-Judge on alignment tasks by defining more theoretically interpretable evaluation metrics and explicitly mitigating LLM internal inconsistency from reliability metrics. We develop an open-source framework to evaluate, compare, and visualize the reliability and alignment of LLM judges, which facilitates practitioners to choose LLM judges for alignment tasks. In the experiments, we examine effects of diverse prompt templates on LLM-judge reliability and also demonstrate our developed framework by comparing various LLM judges on two common alignment datasets (i.e., TL;DR Summarization and HH-RLHF-Helpfulness). Our results indicate a significant impact of prompt templates on LLM judge performance, as well as a mediocre alignment level between the tested LLM judges and human evaluators.\"\\n      },\\n      {\\n        \"paperId\": \"4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"title\": \"Improving Text-to-Image Consistency via Automatic Prompt Optimization\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models and paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1796269096\",\\n            \"name\": \"Oscar Ma\\\\u00f1as\"\\n          },\\n          {\\n            \"authorId\": \"2274101827\",\\n            \"name\": \"Pietro Astolfi\"\\n          },\\n          {\\n            \"authorId\": \"2293590162\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"2256372432\",\\n            \"name\": \"Candace Ross\"\\n          },\\n          {\\n            \"authorId\": \"39219656\",\\n            \"name\": \"Jack Urbanek\"\\n          },\\n          {\\n            \"authorId\": \"2293907712\",\\n            \"name\": \"Adina Williams\"\\n          },\\n          {\\n            \"authorId\": \"2801949\",\\n            \"name\": \"Aishwarya Agrawal\"\\n          },\\n          {\\n            \"authorId\": \"1456285042\",\\n            \"name\": \"Adriana Romero-Soriano\"\\n          },\\n          {\\n            \"authorId\": \"3325894\",\\n            \"name\": \"M. Drozdzal\"\\n          }\\n        ],\\n        \"abstract\": \"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"ac0235cbeefb7b638c4de6af0f29e9650180150d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac0235cbeefb7b638c4de6af0f29e9650180150d\",\\n        \"title\": \"PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.08985\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.08985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt and supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"92480323\",\\n            \"name\": \"Seungho Baek\"\\n          },\\n          {\\n            \"authorId\": \"151351626\",\\n            \"name\": \"Hyerin Im\"\\n          },\\n          {\\n            \"authorId\": \"2223757354\",\\n            \"name\": \"Jiseung Ryu\"\\n          },\\n          {\\n            \"authorId\": \"2223886488\",\\n            \"name\": \"Juhyeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2110755619\",\\n            \"name\": \"T. Lee\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation model is able to generate images across a diverse range of subjects and styles based on a single prompt. Recent works have proposed a variety of interaction methods that help users understand the capabilities of models and utilize them. However, how to support users to efficiently explore the model\\'s capability and to create effective prompts are still open-ended research questions. In this paper, we present PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt. Through the iterative process, users can efficiently explore the model\\'s capability, and clarify their intent. PromptCrafter also supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model. Lastly, users can revert to a desired step by reviewing the work history. In this workshop paper, we discuss the design process of PromptCrafter and our plans for follow-up studies.\"\\n      },\\n      {\\n        \"paperId\": \"a1e2557fa6d5373c8f89b8c4d426168cdf31d7d5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1e2557fa6d5373c8f89b8c4d426168cdf31d7d5\",\\n        \"title\": \"Efficient multi-prompt evaluation of LLMs\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.17202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is proved that PromptEval consistently estimates the performance distribution and its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry; for example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490941219\",\\n            \"name\": \"Felipe Maia Polo\"\\n          },\\n          {\\n            \"authorId\": \"2304142327\",\\n            \"name\": \"Ronald Xu\"\\n          },\\n          {\\n            \"authorId\": \"2286343827\",\\n            \"name\": \"Lucas Weber\"\\n          },\\n          {\\n            \"authorId\": \"2247874639\",\\n            \"name\": \"M\\'irian Silva\"\\n          },\\n          {\\n            \"authorId\": \"2303400299\",\\n            \"name\": \"Onkar Bhardwaj\"\\n          },\\n          {\\n            \"authorId\": \"2283849613\",\\n            \"name\": \"Leshem Choshen\"\\n          },\\n          {\\n            \"authorId\": \"2303415785\",\\n            \"name\": \"Allysson Flavio Melo de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"2247880508\",\\n            \"name\": \"Yuekai Sun\"\\n          },\\n          {\\n            \"authorId\": \"8202372\",\\n            \"name\": \"M. Yurochkin\"\\n          }\\n        ],\\n        \"abstract\": \"Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs\\' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry; for example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Moreover, we show how PromptEval can be useful in LLM-as-a-judge and best prompt identification applications.\"\\n      },\\n      {\\n        \"paperId\": \"bb57af33e505681b1649366e494681e5684bdec7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bb57af33e505681b1649366e494681e5684bdec7\",\\n        \"title\": \"SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.13684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a two-stage adaptation approach termed SPTNet, which iteratively optimizes model parameters and data parameters and proposes a novel spatial prompt tuning method (SPT) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292339349\",\\n            \"name\": \"Hongjun Wang\"\\n          },\\n          {\\n            \"authorId\": \"51286373\",\\n            \"name\": \"S. Vaze\"\\n          },\\n          {\\n            \"authorId\": \"2335536188\",\\n            \"name\": \"Kai Han\"\\n          }\\n        ],\\n        \"abstract\": \"Generalized Category Discovery (GCD) aims to classify unlabelled images from both `seen\\' and `unseen\\' classes by transferring knowledge from a set of labelled `seen\\' class images. A key theme in existing GCD approaches is adapting large-scale pre-trained models for the GCD task. An alternate perspective, however, is to adapt the data representation itself for better alignment with the pre-trained model. As such, in this paper, we introduce a two-stage adaptation approach termed SPTNet, which iteratively optimizes model parameters (i.e., model-finetuning) and data parameters (i.e., prompt learning). Furthermore, we propose a novel spatial prompt tuning method (SPT) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes. We thoroughly evaluate our SPTNet on standard benchmarks and demonstrate that our method outperforms existing GCD methods. Notably, we find our method achieves an average accuracy of 61.4% on the SSB, surpassing prior state-of-the-art methods by approximately 10%. The improvement is particularly remarkable as our method yields extra parameters amounting to only 0.117% of those in the backbone architecture. Project page: https://visual-ai.github.io/sptnet.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"2bb9a87bdfc8a35bc1813e5a88180f43615785a8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2bb9a87bdfc8a35bc1813e5a88180f43615785a8\",\\n        \"title\": \"Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.00727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts, and describes the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2048053804\",\\n            \"name\": \"Guangzhi Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2261406713\",\\n            \"name\": \"Qiao Jin\"\\n          },\\n          {\\n            \"authorId\": \"2314146159\",\\n            \"name\": \"Xiao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314147661\",\\n            \"name\": \"Minjia Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2237094367\",\\n            \"name\": \"Zhiyong Lu\"\\n          },\\n          {\\n            \"authorId\": \"2265729351\",\\n            \"name\": \"Aidong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"The emergent abilities of large language models (LLMs) have demonstrated great potential in solving medical questions. They can possess considerable medical knowledge, but may still hallucinate and are inflexible in the knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed to enhance the medical question-answering capabilities of LLMs with external knowledge bases, it may still fail in complex cases where multiple rounds of information-seeking are required. To address such an issue, we propose iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts. In each iteration of i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and they will be further used to guide the query generation in the next iteration. Our experiments show the improved performance of various LLMs brought by i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes in the United States Medical Licensing Examination (USMLE), as well as various knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68% on the MedQA dataset. In addition, we characterize the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration. Our case studies show that i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing an in-depth analysis of medical questions. To the best of our knowledge, this is the first-of-its-kind study on incorporating follow-up queries into medical RAG.\"\\n      },\\n      {\\n        \"paperId\": \"51b7b3ad7645a69e3c1c80cae69473b8bd472f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51b7b3ad7645a69e3c1c80cae69473b8bd472f67\",\\n        \"title\": \"ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models\",\\n        \"citationCount\": 99,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.07738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes ResearchAgent, a system that automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents, to enhance the productivity of researchers.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-04-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"90765684\",\\n            \"name\": \"Jinheon Baek\"\\n          },\\n          {\\n            \"authorId\": \"3001990\",\\n            \"name\": \"S. Jauhar\"\\n          },\\n          {\\n            \"authorId\": \"73040249\",\\n            \"name\": \"Silviu Cucerzan\"\\n          },\\n          {\\n            \"authorId\": \"2260611009\",\\n            \"name\": \"Sung Ju Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically, starting with a core scientific paper, ResearchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers. Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based ReviewingAgents that provide reviews and feedback via iterative revision processes. These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and model-based evaluation results. Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work.\"\\n      },\\n      {\\n        \"paperId\": \"192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"title\": \"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2312.01663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.01663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269468811\",\\n            \"name\": \"Runze He\"\\n          },\\n          {\\n            \"authorId\": \"2052151521\",\\n            \"name\": \"Shaofei Huang\"\\n          },\\n          {\\n            \"authorId\": \"2269461105\",\\n            \"name\": \"Xuecheng Nie\"\\n          },\\n          {\\n            \"authorId\": \"151475424\",\\n            \"name\": \"Tianrui Hui\"\\n          },\\n          {\\n            \"authorId\": \"1776665\",\\n            \"name\": \"Luoqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2108984\",\\n            \"name\": \"Jiao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2269685669\",\\n            \"name\": \"Jizhong Han\"\\n          },\\n          {\\n            \"authorId\": \"2269748083\",\\n            \"name\": \"Guanbin Li\"\\n          },\\n          {\\n            \"authorId\": \"2269687302\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\"\\n      },\\n      {\\n        \"paperId\": \"d0482bd01de9d0912acf4e5338c7799eba4b9360\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d0482bd01de9d0912acf4e5338c7799eba4b9360\",\\n        \"title\": \"NAISTeacher: A Prompt and Rerank Approach to Generating Teacher Utterances in Educational Dialogues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.bea-1.63.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://aclanthology.org/2023.bea-1.63, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Through the proposed methodology, the BEA 2023 shared task of generating teacher responses in educational dialogues achieved the top score on both automated metrics and human evaluation, surpassing the reference human teachers on the latter.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1572641769\",\\n            \"name\": \"Justin Vasselli\"\\n          },\\n          {\\n            \"authorId\": \"2221318238\",\\n            \"name\": \"Christopher Vasselli\"\\n          },\\n          {\\n            \"authorId\": \"103680841\",\\n            \"name\": \"Adam Nohejl\"\\n          },\\n          {\\n            \"authorId\": \"2110694221\",\\n            \"name\": \"Taro Watanabe\"\\n          }\\n        ],\\n        \"abstract\": \"This paper presents our approach to the BEA 2023 shared task of generating teacher responses in educational dialogues, using the Teacher-Student Chatroom Corpus. Our system prompts GPT-3.5-turbo to generate initial suggestions, which are then subjected to reranking. We explore multiple strategies for candidate generation, including prompting for multiple candidates and employing iterative few-shot prompts with negative examples. We aggregate all candidate responses and rerank them based on DialogRPT scores. To handle consecutive turns in the dialogue data, we divide the task of generating teacher utterances into two components: teacher replies to the student and teacher continuations of previously sent messages. Through our proposed methodology, our system achieved the top score on both automated metrics and human evaluation, surpassing the reference human teachers on the latter.\"\\n      },\\n      {\\n        \"paperId\": \"b56df33446b37bdf5f17fb494201da728fb62502\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b56df33446b37bdf5f17fb494201da728fb62502\",\\n        \"title\": \"Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.15205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is argued that MLLM hallucinations can reveal valuable contextual insights when leveraged correctly, as they represent pre-trained large-scale knowledge beyond individual images as they represent pre-trained large-scale knowledge beyond individual images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"123925137\",\\n            \"name\": \"Jian Hu\"\\n          },\\n          {\\n            \"authorId\": \"2273817980\",\\n            \"name\": \"Jiayi Lin\"\\n          },\\n          {\\n            \"authorId\": \"2317041351\",\\n            \"name\": \"Junchi Yan\"\\n          },\\n          {\\n            \"authorId\": \"2273646767\",\\n            \"name\": \"Shaogang Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Promptable segmentation typically requires instance-specific manual prompts to guide the segmentation of each desired object. To minimize such a need, task-generic promptable segmentation has been introduced, which employs a single task-generic prompt to segment various images of different objects in the same task. Current methods use Multimodal Large Language Models (MLLMs) to reason detailed instance-specific prompts from a task-generic prompt for improving segmentation accuracy. The effectiveness of this segmentation heavily depends on the precision of these derived prompts. However, MLLMs often suffer hallucinations during reasoning, resulting in inaccurate prompting. While existing methods focus on eliminating hallucinations to improve a model, we argue that MLLM hallucinations can reveal valuable contextual insights when leveraged correctly, as they represent pre-trained large-scale knowledge beyond individual images. In this paper, we utilize hallucinations to mine task-related information from images and verify its accuracy for enhancing precision of the generated prompts. Specifically, we introduce an iterative Prompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a mask generator.The prompt generator uses a multi-scale chain of thought prompting, initially exploring hallucinations for extracting extended contextual knowledge on a test image.These hallucinations are then reduced to formulate precise instance-specific prompts, directing the mask generator to produce masks that are consistent with task semantics by mask semantic alignment. The generated masks iteratively induce the prompt generator to focus more on task-relevant image areas and reduce irrelevant hallucinations, resulting jointly in better prompts and masks. Experiments on 5 benchmarks demonstrate the effectiveness of ProMaC. Code given in https://lwpyh.github.io/ProMaC/.\"\\n      },\\n      {\\n        \"paperId\": \"03ccf2e13cdbcc156ee48aa26f500b574da07af8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/03ccf2e13cdbcc156ee48aa26f500b574da07af8\",\\n        \"title\": \"PromptWizard: Task-Aware Prompt Optimization Framework\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18369, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving, self-adapting mechanism, and demonstrates PromptWizard\\'s efficiency, scalability, and advantages over existing prompt optimization strategies.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-05-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2303465593\",\\n            \"name\": \"Eshaan Agarwal\"\\n          },\\n          {\\n            \"authorId\": \"2308075211\",\\n            \"name\": \"Joykirat Singh\"\\n          },\\n          {\\n            \"authorId\": \"2303466479\",\\n            \"name\": \"Vivek Dani\"\\n          },\\n          {\\n            \"authorId\": \"2324053080\",\\n            \"name\": \"Raghav Magazine\"\\n          },\\n          {\\n            \"authorId\": \"1785978\",\\n            \"name\": \"Tanuja Ganu\"\\n          },\\n          {\\n            \"authorId\": \"51464520\",\\n            \"name\": \"A. Nambi\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have transformed AI across diverse domains, with prompting being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving, self-adapting mechanism. Through a feedback-driven critique and synthesis process, PromptWizard achieves an effective balance between exploration and exploitation, iteratively refining both prompt instructions and in-context examples to generate human-readable, task-specific prompts. This guided approach systematically improves prompt quality, resulting in superior performance across 45 tasks. PromptWizard excels even with limited training data, smaller LLMs, and various LLM architectures. Additionally, our cost analysis reveals a substantial reduction in API calls, token usage, and overall cost, demonstrating PromptWizard\\'s efficiency, scalability, and advantages over existing prompt optimization strategies.\"\\n      },\\n      {\\n        \"paperId\": \"69a7ae20845c47c5efe3a93c94e6888b852fe532\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69a7ae20845c47c5efe3a93c94e6888b852fe532\",\\n        \"title\": \"DivLog: Log Parsing with Prompt Enhanced In-Context Learning\",\\n        \"citationCount\": 77,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3597503.3639155?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3597503.3639155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2186630124\",\\n            \"name\": \"Junjielong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2115429277\",\\n            \"name\": \"Ruichun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2036553451\",\\n            \"name\": \"Yintong Huo\"\\n          },\\n          {\\n            \"authorId\": \"2287118778\",\\n            \"name\": \"Chengyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265706586\",\\n            \"name\": \"Pinjia He\"\\n          }\\n        ],\\n        \"abstract\": \"Log parsing, which involves log template extraction from semi-structured logs to produce structured logs, is the first and the most critical step in automated log analysis. However, current log parsers suffer from limited effectiveness for two reasons. First, traditional data-driven log parsers solely rely on heuristics or handcrafted features designed by domain experts, which may not consistently perform well on logs from diverse systems. Second, existing super-vised log parsers require model tuning, which is often limited to fixed training samples and causes sub-optimal performance across the entire log source. To address this limitation, we propose Di-vLog, an effective log parsing framework based on the in-context learning (ICL) ability of large language models (LLMs). Specifically, before log parsing, DivLog samples a small amount of offline logs as candidates by maximizing their diversity. Then, during log parsing, DivLog selects five appropriate labeled candidates as examples for each target log and constructs them into a prompt. By mining the semantics of examples in the prompt, DivLog generates a target log template in a training-free manner. In addition, we design a straightforward yet effective prompt format to extract the output and enhance the quality of the generated log templates. We conducted experiments on 16 widely-used public datasets. The results show that DivLog achieves (1) 98.1% Parsing Accuracy, (2) 92.1% Precision Template Accuracy, and (3) 92.9% Recall Template Accuracy on average, exhibiting state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"2ca32be903662ec64f0a2fd014265bc8c00d8254\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2ca32be903662ec64f0a2fd014265bc8c00d8254\",\\n        \"title\": \"Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.03099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent, and demonstrates the effectiveness of the method with respect to strong proprietary models on real-world tasks such as moderation and generation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34490455\",\\n            \"name\": \"Elad Levi\"\\n          },\\n          {\\n            \"authorId\": \"2282532582\",\\n            \"name\": \"Eli Brosh\"\\n          },\\n          {\\n            \"authorId\": \"2282531918\",\\n            \"name\": \"Matan Friedmann\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system\\'s key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\\\\\\\\href{https://github.com/Eladlev/AutoPrompt}{here}$.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='d91bd339-39e5-4007-b93a-ef5d26028fec')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ad548c68-67b0-408a-9b14-9783c57b304a'), AIMessage(content='{\\n  \"research_question\": \"How can Dynamic Prompt Adaptation improve coherence and engagement of large language models over extended interactions and creative narratives?\",\\n  \"problem_domain\": \"Natural Language Processing; dialogue systems; long-form text generation\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"prompt templating\",\\n    \"summarization of prior interactions\"\\n  ],\\n  \"key_concepts\": [\\n    \"coherence over extended interactions\",\\n    \"thematic consistency\",\\n    \"iterative dialogue\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"summarization/synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Three-phase Dynamic Prompt Adaptation framework (Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update) to maintain thematic coherence across extended interactions.\",\\n    \"Contextual Analysis extracts key themes and tonal shifts from prior outputs and prompts.\",\\n    \"Adaptive Prompt Generation creates updated prompts to introduce new elements or clarify past responses.\",\\n    \"Iterative Context Update synthesizes all prior interactions to maintain coherence.\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='ae974c28-bf76-4b15-a33d-b03c97254c5c'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation\",\\n      \"rationale\": \"Targets studies of frameworks that adapt prompts in real time to sustain coherence over long dialogues.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, coherence, long-form dialogue\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Searches for methods that generate updated prompts to guide LLMs for consistent narrative and context handling.\",\\n      \"priority_concept\": \"adaptive prompt generation, context management, coherence\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Focuses on techniques that update system prompts/context iteratively to maintain dialogue continuity.\",\\n      \"priority_concept\": \"iterative context update, coherence, dialogue\"\\n    },\\n    {\\n      \"query_string\": \"summarization prior interactions\",\\n      \"rationale\": \"Looks for work on summarizing or synthesizing past interactions to preserve thematic coherence.\",\\n      \"priority_concept\": \"summarization, prior interactions, thematic coherence\"\\n    },\\n    {\\n      \"query_string\": \"iterative prompt templating\",\\n      \"rationale\": \"Explores templating strategies that evolve across iterations to embed new elements while preserving coherence.\",\\n      \"priority_concept\": \"iterative prompt templating, prompt engineering, coherence\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='9a9405f7-5b76-4099-897c-3852c95bb26a'), AIMessage(content='{\\n  \"dynamic prompt adaptation\": {\\n    \"total\": 5410,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 128,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 197,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"title\": \"Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/28759/29459\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i8.28759?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i8.28759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy to enable prompt adaptation to the evolving distribution of the dynamic graph.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9071547\",\\n            \"name\": \"Binwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2108814780\",\\n            \"name\": \"Pengkun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292591250\",\\n            \"name\": \"Yudong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108599981\",\\n            \"name\": \"Xu Wang\"\\n          },\\n          {\\n            \"authorId\": \"6231985\",\\n            \"name\": \"Zhengyang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2187108259\",\\n            \"name\": \"Lei Bai\"\\n          },\\n          {\\n            \"authorId\": \"46396284\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"With the progress of urban transportation systems, a significant amount of high-quality traffic data is continuously collected through streaming manners, which has propelled the prosperity of the field of spatial-temporal graph prediction. In this paper, rather than solely focusing on designing powerful models for static graphs, we shift our focus to spatial-temporal graph prediction in the dynamic scenario, which involves a continuously expanding and evolving underlying graph. To address inherent challenges, a decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy. Incorporating inductive biases of time-series structures, DSTG can interpret time dependencies into latent trend and seasonal terms. To enable prompt adaptation to the evolving distribution of the dynamic graph, our decoupling training strategy is devised to iteratively update these two types of patterns. Specifically, for learning seasonal patterns, we conduct thorough training for the model using a long time series (e.g., three months of data). To enhance the learning ability of the model, we also introduce the masked auto-encoding mechanism. During this period, we frequently update trend patterns to expand new information from dynamic graphs. Considering both effectiveness and efficiency, we develop a subnet sampling strategy to select a few representative nodes for fine-tuning the weights of the model. These sampled nodes cover unseen patterns and previously learned patterns. Experiments on dynamic spatial-temporal graph datasets further demonstrate the competitive performance, superior efficiency, and strong scalability of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"title\": \"Dynamics of Smallholder Farmers\\\\u2019 Livelihood Adaptation Decision-Making in Central Ethiopia\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2071-1050/12/11/4526/pdf?version=1591863876\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/su12114526?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/su12114526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2020-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13422136\",\\n            \"name\": \"D. Etana\"\\n          },\\n          {\\n            \"authorId\": \"4266116\",\\n            \"name\": \"D. Snelder\"\\n          },\\n          {\\n            \"authorId\": \"2051704940\",\\n            \"name\": \"C. V. van Wesenbeeck\"\\n          },\\n          {\\n            \"authorId\": \"3606491\",\\n            \"name\": \"T. de Cock Buning\"\\n          }\\n        ],\\n        \"abstract\": \"In previous studies mainly focusing on determinants of adaptation, evidence of the dynamic process of adaptation decision-making is negligible. The objective of this study was to investigate the effects of socio-cultural factors, changes in household characteristics, and climate variables on the transition from non-use to use of adaptation strategies. The study integrated primary data collected from households with secondary rainfall and temperature data. The quantitative and qualitative data were analysed using a dynamic random-effects probit model and a thematic approach, respectively. The result shows strong evidence of path dependence in which use of a strategy during the previous year significantly increases its current use. Climate-related risk perception and factual knowledge may not necessarily prompt adaptation action, whereas access to financial resources and farming-related trainings were consistent positive predictors of farmers\\\\u2019 adaptation decisions. The findings entail that economic capacity and the associated intrinsic motivation help few farmers to utilise robust and contesting adaptation strategies. For most households, economic problems and the consequent fatalistic attitude and risk-avoidance behaviour induce either non-use or use of responsive and accommodating strategies aimed at ensuring survival. Path dependence in non-use of adaptation strategies and sub-optimal adaptation actions demand effective institutional supports to address the behavioural and economic barriers of these households in order to build overall community resilience.\"\\n      },\\n      {\\n        \"paperId\": \"f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"title\": \"Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.08394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model, designs an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2309491187\",\\n            \"name\": \"Zhengbo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2288264614\",\\n            \"name\": \"Li Xu\"\\n          },\\n          {\\n            \"authorId\": \"2067913944\",\\n            \"name\": \"Duo Peng\"\\n          },\\n          {\\n            \"authorId\": \"2265553215\",\\n            \"name\": \"Hossein Rahmani\"\\n          },\\n          {\\n            \"authorId\": \"2309177751\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model. Our main idea is to leverage the rich knowledge encapsulated within the pre-trained diffusion model, such as the understanding of image semantics and structural information, to address unsupervised visual tracking. To this end, we design an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target. Furthermore, to facilitate dynamic adaptation of the prompt to the target\\'s movements, we propose an online prompt updater. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method, which also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"736275f05cbff41484ff4f0c90fd1401af85090f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/736275f05cbff41484ff4f0c90fd1401af85090f\",\\n        \"title\": \"A kaleidoscope of photosynthetic antenna proteins and their emerging roles\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1093/plphys/kiac175\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9237682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Some of the latest fundamental discoveries on the molecular mechanisms and regulation of light harvesting that can potentially be exploited for the optimization of photosynthesis are underline.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1811744891\",\\n            \"name\": \"Rameez Arshad\"\\n          },\\n          {\\n            \"authorId\": \"35279014\",\\n            \"name\": \"Francesco Saccon\"\\n          },\\n          {\\n            \"authorId\": \"1743932730\",\\n            \"name\": \"P. Bag\"\\n          },\\n          {\\n            \"authorId\": \"1498519841\",\\n            \"name\": \"Avratanu Biswas\"\\n          },\\n          {\\n            \"authorId\": \"36840944\",\\n            \"name\": \"C. Calvaruso\"\\n          },\\n          {\\n            \"authorId\": \"1791956159\",\\n            \"name\": \"Ahmad Farhan Bhatti\"\\n          },\\n          {\\n            \"authorId\": \"37543271\",\\n            \"name\": \"Steffen Grebe\"\\n          },\\n          {\\n            \"authorId\": \"1490962043\",\\n            \"name\": \"Vincenzo Mascoli\"\\n          },\\n          {\\n            \"authorId\": \"145679412\",\\n            \"name\": \"M. Mahbub\"\\n          },\\n          {\\n            \"authorId\": \"25484278\",\\n            \"name\": \"F. Muzzopappa\"\\n          },\\n          {\\n            \"authorId\": \"2042646658\",\\n            \"name\": \"Alexandros Polyzois\"\\n          },\\n          {\\n            \"authorId\": \"150064364\",\\n            \"name\": \"C. Schiphorst\"\\n          },\\n          {\\n            \"authorId\": \"68976915\",\\n            \"name\": \"M. Sorrentino\"\\n          },\\n          {\\n            \"authorId\": \"40827588\",\\n            \"name\": \"Simona Streckait\\\\u0117\"\\n          },\\n          {\\n            \"authorId\": \"6651561\",\\n            \"name\": \"H. van Amerongen\"\\n          },\\n          {\\n            \"authorId\": \"90940595\",\\n            \"name\": \"E. Aro\"\\n          },\\n          {\\n            \"authorId\": \"1881036\",\\n            \"name\": \"R. Bassi\"\\n          },\\n          {\\n            \"authorId\": \"4490696\",\\n            \"name\": \"E. Boekema\"\\n          },\\n          {\\n            \"authorId\": \"145609130\",\\n            \"name\": \"R. Croce\"\\n          },\\n          {\\n            \"authorId\": \"4296150\",\\n            \"name\": \"J. Dekker\"\\n          },\\n          {\\n            \"authorId\": \"3189715\",\\n            \"name\": \"R. van Grondelle\"\\n          },\\n          {\\n            \"authorId\": \"144071537\",\\n            \"name\": \"S. Jansson\"\\n          },\\n          {\\n            \"authorId\": \"5089855\",\\n            \"name\": \"D. Kirilovsky\"\\n          },\\n          {\\n            \"authorId\": \"4282160\",\\n            \"name\": \"R. Kou\\\\u0159il\"\\n          },\\n          {\\n            \"authorId\": \"145135253\",\\n            \"name\": \"S. Michel\"\\n          },\\n          {\\n            \"authorId\": \"5289431\",\\n            \"name\": \"C. Mullineaux\"\\n          },\\n          {\\n            \"authorId\": \"7175075\",\\n            \"name\": \"Kl\\\\u00e1ra Panzarov\\\\u00e1\"\\n          },\\n          {\\n            \"authorId\": \"145448524\",\\n            \"name\": \"B. Robert\"\\n          },\\n          {\\n            \"authorId\": \"2161625\",\\n            \"name\": \"A. Ruban\"\\n          },\\n          {\\n            \"authorId\": \"15201549\",\\n            \"name\": \"I. V. van Stokkum\"\\n          },\\n          {\\n            \"authorId\": \"114063537\",\\n            \"name\": \"E. Wientjes\"\\n          },\\n          {\\n            \"authorId\": \"3857871\",\\n            \"name\": \"C. B\\\\u00fcchel\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Photosynthetic light-harvesting antennae are pigment-binding proteins that perform one of the most fundamental tasks on Earth, capturing light and transferring energy that enables life in our biosphere. Adaptation to different light environments led to the evolution of an astonishing diversity of light-harvesting systems. At the same time, several strategies have been developed to optimize the light energy input into photosynthetic membranes in response to fluctuating conditions. The basic feature of these prompt responses is the dynamic nature of antenna complexes, whose function readily adapts to the light available. High-resolution microscopy and spectroscopic studies on membrane dynamics demonstrate the crosstalk between antennae and other thylakoid membrane components. With the increased understanding of light-harvesting mechanisms and their regulation, efforts are focusing on the development of sustainable processes for effective conversion of sunlight into functional bio-products. The major challenge in this approach lies in the application of fundamental discoveries in light-harvesting systems for the improvement of plant or algal photosynthesis. Here, we underline some of the latest fundamental discoveries on the molecular mechanisms and regulation of light harvesting that can potentially be exploited for the optimization of photosynthesis.\"\\n      },\\n      {\\n        \"paperId\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04095\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces the Prompt Auto-Editing (PAE) method, which employs an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148661301\",\\n            \"name\": \"Wenyi Mo\"\\n          },\\n          {\\n            \"authorId\": \"2146332319\",\\n            \"name\": \"Tianyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2281418241\",\\n            \"name\": \"Yalong Bai\"\\n          },\\n          {\\n            \"authorId\": \"2295513824\",\\n            \"name\": \"Bing Su\"\\n          },\\n          {\\n            \"authorId\": \"2293310016\",\\n            \"name\": \"Ji-Rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2281323801\",\\n            \"name\": \"Qing Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\"\\n      },\\n      {\\n        \"paperId\": \"ef9d13b78e418b05eba22563d460900dfb6acd2e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef9d13b78e418b05eba22563d460900dfb6acd2e\",\\n        \"title\": \"MedSAM-U: Uncertainty-Guided Auto Multi-Prompt Adaptation for Reliable MedSAM\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces MedSAM-U, an uncertainty-guided framework designed to automatically refine multi-prompt inputs for more reliable and precise medical image segmentation, and employs uncertainty-guided multi-prompt to effectively estimate the uncertainties associated with the prompts and their initial segmentation results.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295375184\",\\n            \"name\": \"Nan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"9258203\",\\n            \"name\": \"K. Zou\"\\n          },\\n          {\\n            \"authorId\": \"2295886187\",\\n            \"name\": \"Kai Ren\"\\n          },\\n          {\\n            \"authorId\": \"151501329\",\\n            \"name\": \"Mengting Luo\"\\n          },\\n          {\\n            \"authorId\": \"2308862755\",\\n            \"name\": \"Linchao He\"\\n          },\\n          {\\n            \"authorId\": \"2171627854\",\\n            \"name\": \"Meng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2199172643\",\\n            \"name\": \"Yidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2304326718\",\\n            \"name\": \"Yi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2287877133\",\\n            \"name\": \"Hu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2302371411\",\\n            \"name\": \"Huazhu Fu\"\\n          }\\n        ],\\n        \"abstract\": \"The Medical Segment Anything Model (MedSAM) has shown remarkable performance in medical image segmentation, drawing significant attention in the field. However, its sensitivity to varying prompt types and locations poses challenges. This paper addresses these challenges by focusing on the development of reliable prompts that enhance MedSAM\\'s accuracy. We introduce MedSAM-U, an uncertainty-guided framework designed to automatically refine multi-prompt inputs for more reliable and precise medical image segmentation. Specifically, we first train a Multi-Prompt Adapter integrated with MedSAM, creating MPA-MedSAM, to adapt to diverse multi-prompt inputs. We then employ uncertainty-guided multi-prompt to effectively estimate the uncertainties associated with the prompts and their initial segmentation results. In particular, a novel uncertainty-guided prompts adaptation technique is then applied automatically to derive reliable prompts and their corresponding segmentation outcomes. We validate MedSAM-U using datasets from multiple modalities to train a universal image segmentation model. Compared to MedSAM, experimental results on five distinct modal datasets demonstrate that the proposed MedSAM-U achieves an average performance improvement of 1.7\\\\\\\\% to 20.5\\\\\\\\% across uncertainty-guided prompts.\"\\n      },\\n      {\\n        \"paperId\": \"f3edc5a87051be2b4b249c4896c8ab14462c659c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f3edc5a87051be2b4b249c4896c8ab14462c659c\",\\n        \"title\": \"Knowledge-based dynamic prompt learning for multi-label disease diagnosis\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.knosys.2024.111395?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.knosys.2024.111395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2259963003\",\\n            \"name\": \"Jing Xie\"\\n          },\\n          {\\n            \"authorId\": \"2280033365\",\\n            \"name\": \"Xin Li\"\\n          },\\n          {\\n            \"authorId\": \"2279980846\",\\n            \"name\": \"Ye Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2153979217\",\\n            \"name\": \"Yi Guan\"\\n          },\\n          {\\n            \"authorId\": \"2259229\",\\n            \"name\": \"Jingchi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2279958972\",\\n            \"name\": \"Xitong Guo\"\\n          },\\n          {\\n            \"authorId\": \"2280535977\",\\n            \"name\": \"Xin Peng\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"81ca1ca9b81470988d29e0d0def9712320243e06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/81ca1ca9b81470988d29e0d0def9712320243e06\",\\n        \"title\": \"HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target Binding Affinity Prediction\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.17697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic prompts within a hybrid Graph-Transformer framework, and outperforms state-of-the-art DTA prediction methods in both prediction performance and generalization ability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2349232014\",\\n            \"name\": \"Xi Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2275183852\",\\n            \"name\": \"Wentao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2303777566\",\\n            \"name\": \"Jiacheng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2308231466\",\\n            \"name\": \"Lijing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2308228559\",\\n            \"name\": \"Gaofei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2303708410\",\\n            \"name\": \"Zhengji Li\"\\n          },\\n          {\\n            \"authorId\": \"2257021623\",\\n            \"name\": \"Tianyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2308674053\",\\n            \"name\": \"Min Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Drug target binding affinity (DTA) is a key criterion for drug screening. Existing experimental methods are time-consuming and rely on limited structural and domain information. While learning-based methods can model sequence and structural information, they struggle to integrate contextual data and often lack comprehensive modeling of drug-target interactions. In this study, we propose a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic prompts within a hybrid Graph-Transformer framework. Our method generates context-specific prompts for each drug-target pair, enhancing the model\\'s ability to capture unique interactions. The introduction of prompt tuning further optimizes the prediction process by filtering out irrelevant noise and emphasizing task-relevant information, dynamically adjusting the input features of the molecular graph. The proposed hybrid Graph-Transformer architecture combines structural information from Graph Convolutional Networks (GCNs) with sequence information captured by Transformers, facilitating the interaction between global and local information. Additionally, we adopted the multi-view feature fusion method to project molecular graph views and affinity subgraph views into a common feature space, effectively combining structural and contextual information. Experiments on two widely used public datasets, Davis and KIBA, show that HGTDP-DTA outperforms state-of-the-art DTA prediction methods in both prediction performance and generalization ability.\"\\n      },\\n      {\\n        \"paperId\": \"3967189742efab8859da542ce3953d4c72957aca\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3967189742efab8859da542ce3953d4c72957aca\",\\n        \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.23904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI), designed to design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275054108\",\\n            \"name\": \"Qinqian Lei\"\\n          },\\n          {\\n            \"authorId\": \"2313081973\",\\n            \"name\": \"Bo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256998291\",\\n            \"name\": \"Robby T. Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\"\\n      },\\n      {\\n        \"paperId\": \"1d24f2dfe942e2d67381d8fcfac3423145bd557f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d24f2dfe942e2d67381d8fcfac3423145bd557f\",\\n        \"title\": \"DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.10588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention that outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1390537791\",\\n            \"name\": \"Xiangpeng Yang\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2109074276\",\\n            \"name\": \"Xiaohan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-video retrieval is a critical multi-modal task to find the most relevant video for a text query. Although pretrained models like CLIP have demonstrated impressive potential in this area, the rising cost of fully finetuning these models due to increasing model size continues to pose a problem. To address this challenge, prompt tuning has emerged as an alternative. However, existing works still face two problems when adapting pretrained image-text models to downstream video-text tasks: (1) The visual encoder could only encode frame-level features and failed to extract global-level general video information. (2) Equipping the visual and text encoder with separated prompts failed to mitigate the visual-text modality gap. To this end, we propose DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention. In contrast to previous prompt tuning methods, we employ the shared latent space to generate local-level text and frame prompts that encourage inter-modal interaction. Furthermore, we propose modeling video in a global-local attention mechanism to capture global video information from the perspective of prompt tuning. Extensive experiments reveal that when only 0.67% parameters are tuned, our cross-modal prompt tuning strategy DGL outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets. Code will be available at https://github.com/knightyxp/DGL.\"\\n      },\\n      {\\n        \"paperId\": \"92d477bb89b43a73b6e8ca54ae2e1def9f3fe2dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/92d477bb89b43a73b6e8ca54ae2e1def9f3fe2dd\",\\n        \"title\": \"UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes the first universal CSR framework, dubbed UCIP, with dynamic prompt learning, intending to jointly support the CSR distortions of any compression codecs/modes, and introduces the novel MLP-like framework backbone for the UCIP.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2253011048\",\\n            \"name\": \"Xin Li\"\\n          },\\n          {\\n            \"authorId\": \"2289786700\",\\n            \"name\": \"Bingchen Li\"\\n          },\\n          {\\n            \"authorId\": \"2288532590\",\\n            \"name\": \"Yeying Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"2282209914\",\\n            \"name\": \"Hanxin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2232648762\",\\n            \"name\": \"Yulin Ren\"\\n          },\\n          {\\n            \"authorId\": \"2287374267\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Compressed Image Super-resolution (CSR) aims to simultaneously super-resolve the compressed images and tackle the challenging hybrid distortions caused by compression. However, existing works on CSR usually focuses on a single compression codec, i.e., JPEG, ignoring the diverse traditional or learning-based codecs in the practical application, e.g., HEVC, VVC, HIFIC, etc. In this work, we propose the first universal CSR framework, dubbed UCIP, with dynamic prompt learning, intending to jointly support the CSR distortions of any compression codecs/modes. Particularly, an efficient dynamic prompt strategy is proposed to mine the content/spatial-aware task-adaptive contextual information for the universal CSR task, using only a small amount of prompts with spatial size 1x1. To simplify contextual information mining, we introduce the novel MLP-like framework backbone for our UCIP by adapting the Active Token Mixer (ATM) to CSR tasks for the first time, where the global information modeling is only taken in horizontal and vertical directions with offset prediction. We also build an all-in-one benchmark dataset for the CSR task by collecting the datasets with the popular 6 diverse traditional and learning-based codecs, including JPEG, HEVC, VVC, HIFIC, etc., resulting in 23 common degradations. Extensive experiments have shown the consistent and excellent performance of our UCIP on universal CSR tasks. The project can be found in https://lixinustc.github.io/UCIP.github.io\"\\n      },\\n      {\\n        \"paperId\": \"dbad406ab82697b153d28c6654e7808187859701\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dbad406ab82697b153d28c6654e7808187859701\",\\n        \"title\": \"Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.07221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The essence of IDPT is to develop a dynamic prompt generation module to perceive semantic prior features of each point cloud instance and generate adaptive prompt tokens to enhance the model\\'s robustness, providing a promising solution to parameter-efficient learning for pre-trained point cloud models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-04-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2214515552\",\\n            \"name\": \"Yaohua Zha\"\\n          },\\n          {\\n            \"authorId\": \"2110174485\",\\n            \"name\": \"Jinpeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"151470385\",\\n            \"name\": \"Tao Dai\"\\n          },\\n          {\\n            \"authorId\": \"50461900\",\\n            \"name\": \"Bin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2108389544\",\\n            \"name\": \"Zhi Wang\"\\n          },\\n          {\\n            \"authorId\": \"3085483\",\\n            \"name\": \"Shutao Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained point cloud models have found extensive applications in 3D understanding tasks like object classification and part segmentation. However, the prevailing strategy of full fine-tuning in downstream tasks leads to large per-task storage overhead for model parameters, which limits the efficiency when applying large-scale pre-trained models. Inspired by the recent success of visual prompt tuning (VPT), this paper attempts to explore prompt tuning on pre-trained point cloud models, to pursue an elegant balance between performance and parameter efficiency. We find while instance-agnostic static prompting, e.g. VPT, shows some efficacy in downstream transfer, it is vulnerable to the distribution diversity caused by various types of noises in real-world point cloud data. To conquer this limitation, we propose a novel Instance-aware Dynamic Prompt Tuning (IDPT) strategy for pre-trained point cloud models. The essence of IDPT is to develop a dynamic prompt generation module to perceive semantic prior features of each point cloud instance and generate adaptive prompt tokens to enhance the model\\'s robustness. Notably, extensive experiments demonstrate that IDPT outperforms full finetuning in most tasks with a mere 7% of the trainable parameters, providing a promising solution to parameter-efficient learning for pre-trained point cloud models. Code is available at https://github.com/zyh16143998882/ICCV23-IDPT.\"\\n      },\\n      {\\n        \"paperId\": \"51ac2dceecc75431eab5b2c320822e081b282b27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51ac2dceecc75431eab5b2c320822e081b282b27\",\\n        \"title\": \"SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models\",\\n        \"citationCount\": 52,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed SwapPrompt is a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation and achieves state-of-the-art test-time adaptation performance on ImageNet and nine other datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2139700088\",\\n            \"name\": \"Xiaosong Ma\"\\n          },\\n          {\\n            \"authorId\": \"2265432385\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2277415299\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"50232004\",\\n            \"name\": \"Wenchao Xu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0911ae6be53e4e649eb8da3d8ae738f65e93993d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0911ae6be53e4e649eb8da3d8ae738f65e93993d\",\\n        \"title\": \"Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2402.18447\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.18447, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts and the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158676096\",\\n            \"name\": \"Deng Li\"\\n          },\\n          {\\n            \"authorId\": \"48352212\",\\n            \"name\": \"Aming Wu\"\\n          },\\n          {\\n            \"authorId\": \"2288039856\",\\n            \"name\": \"Yaowei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261890091\",\\n            \"name\": \"Yahong Han\"\\n          }\\n        ],\\n        \"abstract\": \"Single-domain generalization aims to learn a model from single source domain data attaining generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in crossdomain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and versatility of our proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"666cced301ad62e8cfb9a5ca02bb4f5320dbc0a5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/666cced301ad62e8cfb9a5ca02bb4f5320dbc0a5\",\\n        \"title\": \"Unsupervised Domain Adaptation Enhanced by Fuzzy Prompt Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TFUZZ.2024.3389705?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TFUZZ.2024.3389705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2297072145\",\\n            \"name\": \"Kuo Shi\"\\n          },\\n          {\\n            \"authorId\": \"2272678886\",\\n            \"name\": \"Jie Lu\"\\n          },\\n          {\\n            \"authorId\": \"2243460919\",\\n            \"name\": \"Zhen Fang\"\\n          },\\n          {\\n            \"authorId\": \"46266495\",\\n            \"name\": \"Guangquan Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) addresses the challenge of distribution shift between a labeled source domain and an unlabeled target domain by utilizing knowledge from the source. Traditional UDA methods mainly focus on single-modal scenarios, either vision or language, thus, not fully exploring the advantages of multimodal representations. Visionlanguage models utilize multimodal information, applying prompt learning techniques for addressing target domain tasks. Motivated by the recent advancements in pretrained visionlanguage models, this article expands the UDA framework to incorporate multimodal approaches using fuzzy techniques. The adoption of fuzzy techniques, preferred over conventional domain adaptation methods, is based on the following two key aspects: 1) the nature of prompt learning is intrinsically linked to fuzzy logic, and 2) the superior capability of fuzzy techniques in processing soft information and effectively utilizing inherent relationships both within and across domains. To this end, we propose UDA enhanced by fuzzy prompt learning (FUZZLE), a simple and effective method for aligning the source and target domains via domain-specific prompt learning. Specifically, we introduce a novel technique to enhance prompt learning in the target domain. This method integrates fuzzy C-means clustering and a novel instance-level fuzzy vector into the prompt learning loss function, minimizing the distance between prompt cluster centers and instance prompts, thereby, enhancing the prompt learning process. In addition, we propose a Kullback\\\\u2013Leibler (KL) divergence-based loss function with a fuzzification factor. This function is designed to minimize the distribution discrepancy in the classification of similar cross-domain data, aligning domain-specific prompts during the training process. We contribute an in-depth analysis to understand the effectiveness of FUZZLE. Extensive experiments demonstrate that our method achieves superior performance on standard UDA benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"5d48b087d3d4a16904c7e5442a4a815ff901cc0d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5d48b087d3d4a16904c7e5442a4a815ff901cc0d\",\\n        \"title\": \"CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17369, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoDA is proposed which instructs models to distinguish, focus, and learn from discrepancies within all adverse scenes at scene and image levels, and achieves SOTA performances on widely-used benchmarks under all adverse scenes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261751604\",\\n            \"name\": \"Ziyang Gong\"\\n          },\\n          {\\n            \"authorId\": \"2261889755\",\\n            \"name\": \"Fuhao Li\"\\n          },\\n          {\\n            \"authorId\": \"2262092854\",\\n            \"name\": \"Yupeng Deng\"\\n          },\\n          {\\n            \"authorId\": \"2293394749\",\\n            \"name\": \"Deblina Bhattacharjee\"\\n          },\\n          {\\n            \"authorId\": \"2293450075\",\\n            \"name\": \"Xiangwei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261751710\",\\n            \"name\": \"Zhenming Ji\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised Domain Adaptation (UDA) aims to adapt models from labeled source domains to unlabeled target domains. When adapting to adverse scenes, existing UDA methods fail to perform well due to the lack of instructions, leading their models to overlook discrepancies within all adverse scenes. To tackle this, we propose CoDA which instructs models to distinguish, focus, and learn from these discrepancies at scene and image levels. Specifically, CoDA consists of a Chain-of-Domain (CoD) strategy and a Severity-Aware Visual Prompt Tuning (SAVPT) mechanism. CoD focuses on scene-level instructions to divide all adverse scenes into easy and hard scenes, guiding models to adapt from source to easy domains with easy scene images, and then to hard domains with hard scene images, thereby laying a solid foundation for whole adaptations. Building upon this foundation, we employ SAVPT to dive into more detailed image-level instructions to boost performance. SAVPT features a novel metric Severity that divides all adverse scene images into low-severity and high-severity images. Then Severity directs visual prompts and adapters, instructing models to concentrate on unified severity features instead of scene-specific features, without adding complexity to the model architecture. CoDA achieves SOTA performances on widely-used benchmarks under all adverse scenes. Notably, CoDA outperforms the existing ones by 4.6%, and 10.3% mIoU on the Foggy Driving, and Foggy Zurich benchmarks, respectively. Our code is available at https://github.com/Cuzyoung/CoDA\"\\n      },\\n      {\\n        \"paperId\": \"8d531cb8cf51eec3b8f1106d189295fa3c81c02a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8d531cb8cf51eec3b8f1106d189295fa3c81c02a\",\\n        \"title\": \"Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing\",\\n        \"citationCount\": 83,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.15664\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.15664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Dynamic Prompt Learning (DPL) is proposed to force cross-attention maps to focus on correct noun words in the text prompt by updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses to achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2237135280\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"2115413065\",\\n            \"name\": \"Fei Yang\"\\n          },\\n          {\\n            \"authorId\": \"49080524\",\\n            \"name\": \"Shiqi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2248121772\",\\n            \"name\": \"Muhammad Atif Butt\"\\n          },\\n          {\\n            \"authorId\": \"2820687\",\\n            \"name\": \"Joost van de Weijer\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale text-to-image generative models have been a ground-breaking development in generative AI, with diffusion models showing their astounding ability to synthesize convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are susceptible to unintended modifications of regions outside the targeted area, such as on the background or on distractor objects which have some semantic or visual relationship with the targeted object. According to our experimental findings, inaccurate cross-attention maps are at the root of this problem. Based on this observation, we propose Dynamic Prompt Learning (DPL) to force cross-attention maps to focus on correct noun words in the text prompt. By updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses, we achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions. Our method DPL, based on the publicly available Stable Diffusion, is extensively evaluated on a wide range of images, and consistently obtains superior results both quantitatively (CLIP score, Structure-Dist) and qualitatively (on user-evaluation). We show improved prompt editing results for Word-Swap, Prompt Refinement, and Attention Re-weighting, especially for complex multi-object scenes.\"\\n      },\\n      {\\n        \"paperId\": \"b1b688cf506f64bc4a06e037bfcdfa8b22be51aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b1b688cf506f64bc4a06e037bfcdfa8b22be51aa\",\\n        \"title\": \"Cascade Prompt Learning for Vision-Language Model Adaptation\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.17805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel Cascade Prompt Learning CasPL framework to enable prompt learning to serve both generic and specific expertise simultaneously, which achieves a significantly better balance between performance and inference speed, which is especially beneficial for deploying smaller VLM models in resource-constrained environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-09-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2212236182\",\\n            \"name\": \"Ge Wu\"\\n          },\\n          {\\n            \"authorId\": \"2268132119\",\\n            \"name\": \"Xin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2146248526\",\\n            \"name\": \"Zheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2268797049\",\\n            \"name\": \"Zhaowei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2320744562\",\\n            \"name\": \"Jiajun Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284825678\",\\n            \"name\": \"Jian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2320925245\",\\n            \"name\": \"Xiang Li\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt learning has surfaced as an effective approach to enhance the performance of Vision-Language Models (VLMs) like CLIP when applied to downstream tasks. However, current learnable prompt tokens are primarily used for the single phase of adapting to tasks (i.e., adapting prompt), easily leading to overfitting risks. In this work, we propose a novel Cascade Prompt Learning CasPL framework to enable prompt learning to serve both generic and specific expertise (i.e., boosting and adapting prompt) simultaneously. Specifically, CasPL is a new learning paradigm comprising two distinct phases of learnable prompts: the first boosting prompt is crafted to extract domain-general knowledge from a senior larger CLIP teacher model by aligning their predicted logits using extensive unlabeled domain images. The second adapting prompt is then cascaded with the frozen first set to fine-tune the downstream tasks, following the approaches employed in prior research. In this manner, CasPL can effectively capture both domain-general and task-specific representations into explicitly different gradual groups of prompts, thus potentially alleviating overfitting issues in the target domain. It\\'s worth noting that CasPL serves as a plug-and-play module that can seamlessly integrate into any existing prompt learning approach. CasPL achieves a significantly better balance between performance and inference speed, which is especially beneficial for deploying smaller VLM models in resource-constrained environments. Compared to the previous state-of-the-art method PromptSRC, CasPL shows an average improvement of 1.85% for base classes, 3.44% for novel classes, and 2.72% for the harmonic mean over 11 image classification datasets. Code is publicly available at: https://github.com/megvii-research/CasPL.\"\\n      },\\n      {\\n        \"paperId\": \"3e565c544a8639cc9c7568833e484d7610f5e5d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4\",\\n        \"title\": \"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning\",\\n        \"citationCount\": 372,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2209.14610\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.14610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel approach is proposed, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example, which verifies its effectiveness in selecting in- context examples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-09-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2887562\",\\n            \"name\": \"Pan Lu\"\\n          },\\n          {\\n            \"authorId\": \"47659905\",\\n            \"name\": \"Liang Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2782886\",\\n            \"name\": \"Kai-Wei Chang\"\\n          },\\n          {\\n            \"authorId\": \"39092098\",\\n            \"name\": \"Y. Wu\"\\n          },\\n          {\\n            \"authorId\": \"145380991\",\\n            \"name\": \"Song-Chun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2590556\",\\n            \"name\": \"Tanmay Rajpurohit\"\\n          },\\n          {\\n            \"authorId\": \"48323507\",\\n            \"name\": \"Peter Clark\"\\n          },\\n          {\\n            \"authorId\": \"51043791\",\\n            \"name\": \"A. Kalyan\"\\n          }\\n        ],\\n        \"abstract\": \"Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.\"\\n      },\\n      {\\n        \"paperId\": \"fc31c17e27c23cc89f91c110acd68b9b66ee0d40\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fc31c17e27c23cc89f91c110acd68b9b66ee0d40\",\\n        \"title\": \"Dynamic prompt-based virtual assistant framework for BIM information search\",\\n        \"citationCount\": 82,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.autcon.2023.105067?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.autcon.2023.105067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2202987937\",\\n            \"name\": \"Junwen Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2113447170\",\\n            \"name\": \"M. Fischer\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"4f9766356478ddf606457af735f70682839194b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4f9766356478ddf606457af735f70682839194b1\",\\n        \"title\": \"VPA: Fully Test-Time Visual Prompt Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.15251\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.15251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents Visual Prompt Adaptation (VPA), the first framework that generalizes visual prompting with test-time adaptation, and evaluates VPA on multiple tasks, including out-of-distribution generalization, corruption robustness, and domain adaptation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-09-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135326251\",\\n            \"name\": \"Jiachen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2249088902\",\\n            \"name\": \"Mark Ibrahim\"\\n          },\\n          {\\n            \"authorId\": \"2248025897\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"22229139\",\\n            \"name\": \"I. Evtimov\"\\n          },\\n          {\\n            \"authorId\": \"3895596\",\\n            \"name\": \"Z. Morley Mao\"\\n          },\\n          {\\n            \"authorId\": \"66286536\",\\n            \"name\": \"Cris-tian Cant\\\\u00f3n Ferrer\"\\n          },\\n          {\\n            \"authorId\": \"3322806\",\\n            \"name\": \"C. Hazirbas\"\\n          }\\n        ],\\n        \"abstract\": \"Textual prompt tuning has demonstrated significant performance improvements in adapting natural language processing models to a variety of downstream tasks by treating hand-engineered prompts as trainable parameters. Inspired by the success of textual prompting, several studies have investigated the efficacy of visual prompt tuning. In this work, we present Visual Prompt Adaptation (VPA), the first framework that generalizes visual prompting with test-time adaptation. VPA introduces a small number of learnable tokens, enabling fully test-time and storage-efficient adaptation without necessitating source-domain information. We examine our VPA design under diverse adaptation settings, encompassing single-image, batched-image, and pseudo-label adaptation. We evaluate VPA on multiple tasks, including out-of-distribution (OOD) generalization, corruption robustness, and domain adaptation. Experimental results reveal that VPA effectively enhances OOD generalization by 3.3% across various models, surpassing previous test-time approaches. Furthermore, we show that VPA improves corruption robustness by 6.5% compared to strong baselines. Finally, we demonstrate that VPA also boosts domain adaptation performance by relatively 5.2%. Our VPA also exhibits marked effectiveness in improving the robustness of zero-shot recognition for vision-language models.\"\\n      },\\n      {\\n        \"paperId\": \"546caf796b0ae3578852a5de8fe34f19f2de0cba\",\\n        \"url\": \"https://www.semanticscholar.org/paper/546caf796b0ae3578852a5de8fe34f19f2de0cba\",\\n        \"title\": \"Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.11484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enabling efficient policy learning and adaptation upon a wide range of environmental and physical changes encountered by embodied agents.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287939397\",\\n            \"name\": \"Wonje Choi\"\\n          },\\n          {\\n            \"authorId\": \"2174282424\",\\n            \"name\": \"Woo Kyung Kim\"\\n          },\\n          {\\n            \"authorId\": \"2288614462\",\\n            \"name\": \"Seunghyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"35756158\",\\n            \"name\": \"Hong-Suh Woo\"\\n          }\\n        ],\\n        \"abstract\": \"For embodied reinforcement learning (RL) agents interacting with the environment, it is desirable to have rapid policy adaptation to unseen visual observations, but achieving zero-shot adaptation capability is considered as a challenging problem in the RL context. To address the problem, we present a novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enabling efficient policy learning and adaptation upon a wide range of environmental and physical changes encountered by embodied agents. Specifically, we devise a guided-attention-based ensemble approach with multiple visual prompts on the vision-language model to construct robust state representations. Each prompt is contrastively learned in terms of an individual domain factor that significantly affects the agent\\'s egocentric perception and observation. For a given task, the attention-based ensemble and policy are jointly learned so that the resulting state representations not only generalize to various domains but are also optimized for learning the task. Through experiments, we show that ConPE outperforms other state-of-the-art algorithms for several embodied agent tasks including navigation in AI2THOR, manipulation in egocentric-Metaworld, and autonomous driving in CARLA, while also improving the sample efficiency of policy learning and adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n        \"title\": \"Prompt-and-Transfer: Dynamic Class-Aware Enhancement for Few-Shot Segmentation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.10389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel and powerful prompt-driven scheme, called \\\\u201cPrompt and Transfer\\\\u201d (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216452880\",\\n            \"name\": \"Hanbo Bi\"\\n          },\\n          {\\n            \"authorId\": \"6962129\",\\n            \"name\": \"Yingchao Feng\"\\n          },\\n          {\\n            \"authorId\": \"2600667\",\\n            \"name\": \"Wenhui Diao\"\\n          },\\n          {\\n            \"authorId\": \"152702629\",\\n            \"name\": \"Peijin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2161856190\",\\n            \"name\": \"Yongqiang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2303405697\",\\n            \"name\": \"Kun Fu\"\\n          },\\n          {\\n            \"authorId\": \"2242774676\",\\n            \"name\": \"Hongqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2177485483\",\\n            \"name\": \"Xian Sun\"\\n          }\\n        ],\\n        \"abstract\": \"For more efficient generalization to unseen domains (classes), most Few-shot Segmentation (FSS) would directly exploit pre-trained encoders and only fine-tune the decoder, especially in the current era of large models. However, such fixed feature encoders tend to be class-agnostic, inevitably activating objects that are irrelevant to the target class. In contrast, humans can effortlessly focus on specific objects in the line of sight. This paper mimics the visual perception pattern of human beings and proposes a novel and powerful prompt-driven scheme, called \\\\u201cPrompt and Transfer\\\\u201d (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task. Three key points are elaborated to enhance the prompting: 1) Cross-modal linguistic information is introduced to initialize prompts for each task. 2) Semantic Prompt Transfer (SPT) that precisely transfers the class-specific semantics within the images to prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT to adaptively generate different but complementary part prompts for different individuals. Surprisingly, PAT achieves competitive performance on 4 different tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new state-of-the-arts on 11 benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yu-Feng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"bd46cb09425c1eb8e2d2e7dd612d839cdf4d0f39\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bd46cb09425c1eb8e2d2e7dd612d839cdf4d0f39\",\\n        \"title\": \"Prompt-based Distribution Alignment for Unsupervised Domain Adaptation\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.09553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper experimentally demonstrates that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA and proposes a Prompt-based Distribution Alignment method to incorporate the domain knowledge into prompt learning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2275570714\",\\n            \"name\": \"Min Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"122132048\",\\n            \"name\": \"Siteng Huang\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275032226\",\\n            \"name\": \"Donglin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, despite the unprecedented success of large pre-trained visual-language models (VLMs) on a wide range of downstream tasks, the real-world unsupervised domain adaptation (UDA) problem is still not well explored. Therefore, in this paper, we first experimentally demonstrate that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA. However, a major challenge for directly deploying such models on downstream UDA tasks is prompt engineering, which requires aligning the domain knowledge of source and target domains, since the performance of UDA is severely influenced by a good domain-invariant representation. We further propose a Prompt-based Distribution Alignment (PDA) method to incorporate the domain knowledge into prompt learning. Specifically, PDA employs a two-branch prompt-tuning paradigm, namely base branch and alignment branch. The base branch focuses on integrating class-related representation into prompts, ensuring discrimination among different classes. To further minimize domain discrepancy, for the alignment branch, we construct feature banks for both the source and target domains and propose image-guided feature tuning (IFT) to make the input attend to feature banks, which effectively integrates self-enhanced and cross-domain features into the model. In this way, these two branches can be mutually promoted to enhance the adaptation of VLMs for UDA. We conduct extensive experiments on three benchmarks to demonstrate that our proposed PDA achieves state-of-the-art performance. The code is available at https://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.\"\\n      },\\n      {\\n        \"paperId\": \"fa9d49f32440aff7417ce46419d1073239b58b5b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa9d49f32440aff7417ce46419d1073239b58b5b\",\\n        \"title\": \"API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.03987\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.03987, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERE), an API entity-relation joint extraction model based on the large pre-trained language model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2181288908\",\\n            \"name\": \"Qing Huang\"\\n          },\\n          {\\n            \"authorId\": \"2199853522\",\\n            \"name\": \"Yanbang Sun\"\\n          },\\n          {\\n            \"authorId\": \"3138980\",\\n            \"name\": \"Zhenchang Xing\"\\n          },\\n          {\\n            \"authorId\": \"2152596785\",\\n            \"name\": \"Mingming Yu\"\\n          },\\n          {\\n            \"authorId\": \"3087664\",\\n            \"name\": \"Xiwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2151674574\",\\n            \"name\": \"Qinghua Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.\"\\n      },\\n      {\\n        \"paperId\": \"aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n        \"url\": \"https://www.semanticscholar.org/paper/aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n        \"title\": \"Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2304.03119\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.03119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Qualitative and quantitative evaluations on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-04-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148900054\",\\n            \"name\": \"Jiayi Guo\"\\n          },\\n          {\\n            \"authorId\": \"2135747990\",\\n            \"name\": \"Chaofei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2115860363\",\\n            \"name\": \"You Wu\"\\n          },\\n          {\\n            \"authorId\": \"2176400796\",\\n            \"name\": \"Eric Zhang\"\\n          },\\n          {\\n            \"authorId\": \"37833805\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"101246501\",\\n            \"name\": \"Xingqian Xu\"\\n          },\\n          {\\n            \"authorId\": \"30619669\",\\n            \"name\": \"S. Song\"\\n          },\\n          {\\n            \"authorId\": \"48667025\",\\n            \"name\": \"Humphrey Shi\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, CLIP-guided image synthesis has shown appealing performance on adapting a pre-trained source-domain generator to an unseen target domain. It does not require any target-domain samples but only the textual domain labels. The training is highly efficient, e.g., a few minutes. However, existing methods still have some limitations in the quality of generated images and may suffer from the mode collapse issue. A key reason is that a fixed adaptation direction is applied for all cross-domain image pairs, which leads to identical supervision signals. To address this issue, we propose an Image-specific Prompt Learning (IPL) method, which learns specific prompt vectors for each source-domain image. This produces a more precise adaptation direction for every cross-domain image pair, endowing the target-domain generator with greatly enhanced flexibility. Qualitative and quantitative evaluations on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse. Moreover, IPL is independent of the structure of the generative model, such as generative adversarial networks or diffusion models. Code is available at https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"5a8541bb468afb45d68e16d2f83b327768f97c67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5a8541bb468afb45d68e16d2f83b327768f97c67\",\\n        \"title\": \"Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.07648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to apply an auxiliary video captioning objective to train the frame representations, which facilitates the learning of detailed video semantics by providing fine-grained guidance in the semantic space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41036094\",\\n            \"name\": \"Chaorui Deng\"\\n          },\\n          {\\n            \"authorId\": \"2115814744\",\\n            \"name\": \"Qi Chen\"\\n          },\\n          {\\n            \"authorId\": \"1937786\",\\n            \"name\": \"Pengda Qin\"\\n          },\\n          {\\n            \"authorId\": \"73286206\",\\n            \"name\": \"Dave Zhenyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2143599197\",\\n            \"name\": \"Qi Wu\"\\n          }\\n        ],\\n        \"abstract\": \"In text-video retrieval, recent works have benefited from the powerful learning capabilities of pre-trained text-image foundation models (e.g., CLIP) by adapting them to the video domain. A critical problem for them is how to effectively capture the rich semantics inside the video using the image encoder of CLIP. To tackle this, state-of-the-art methods adopt complex cross-modal modeling techniques to fuse the text information into video frame representations, which, however, incurs severe efficiency issues in large-scale retrieval systems as the video representations must be recomputed online for every text query. In this paper, we discard this problematic cross-modal fusion process and aim to learn semantically-enhanced representations purely from the video, so that the video representations can be computed offline and reused for different texts. Concretely, we first introduce a spatial-temporal \\\\\"Prompt Cube\\\\\" into the CLIP image encoder and iteratively switch it within the encoder layers to efficiently incorporate the global video semantics into frame representations. We then propose to apply an auxiliary video captioning objective to train the frame representations, which facilitates the learning of detailed video semantics by providing fine-grained guidance in the semantic space. With a naive temporal fusion strategy (i.e., mean-pooling) on the enhanced frame representations, we obtain state-of-the-art performances on three benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.\"\\n      },\\n      {\\n        \"paperId\": \"a4a26d7c6bc022f604a246c6ce6cc1cfbe618441\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a4a26d7c6bc022f604a246c6ce6cc1cfbe618441\",\\n        \"title\": \"APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.emnlp-main.567.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.emnlp-main.567?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.emnlp-main.567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Attention Prompt tuning method, namely AP ROMPT, which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2266712798\",\\n            \"name\": \"Qifan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2272672481\",\\n            \"name\": \"Yuning Mao\"\\n          },\\n          {\\n            \"authorId\": \"2109593338\",\\n            \"name\": \"Jingang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273361006\",\\n            \"name\": \"Hanchao Yu\"\\n          },\\n          {\\n            \"authorId\": \"35557488\",\\n            \"name\": \"Shaoliang Nie\"\\n          },\\n          {\\n            \"authorId\": \"2237101143\",\\n            \"name\": \"Sinong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2271382818\",\\n            \"name\": \"Fuli Feng\"\\n          },\\n          {\\n            \"authorId\": \"2273197501\",\\n            \"name\": \"Lifu Huang\"\\n          },\\n          {\\n            \"authorId\": \"2266062308\",\\n            \"name\": \"Xiaojun Quan\"\\n          },\\n          {\\n            \"authorId\": \"2273481670\",\\n            \"name\": \"Zenglin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2264378887\",\\n            \"name\": \"Dongfang Liu\"\\n          }\\n        ],\\n        \"abstract\": \"With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large pre-trained language models. However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement. In this work, we propose a novel Attention Prompt tuning method, namely AP ROMPT , for efficient adaptation of pre-trained language models. We first demonstrate that existing prompt tuning can be considered as a special case of attention prompt tuning. We then formally introduce AP ROMPT , which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning. Experimental results on the SuperGLUE benchmark consistently demonstrate that our proposed approach outperforms state-of-the-art baselines and full fine-tuning method with pre-trained models at different scales. In addition, a comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f849b873e94f28e1f2a3e1dc4d7bef17eb64adab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f849b873e94f28e1f2a3e1dc4d7bef17eb64adab\",\\n        \"title\": \"Towards Stable Test-Time Adaptation in Dynamic Wild World\",\\n        \"citationCount\": 372,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2302.12400\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.12400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1411039233\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"2239515454\",\\n            \"name\": \"Jiaxiang Wu\"\\n          },\\n          {\\n            \"authorId\": \"2108463891\",\\n            \"name\": \"Yifan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143976851\",\\n            \"name\": \"Z. Wen\"\\n          },\\n          {\\n            \"authorId\": \"1816749481\",\\n            \"name\": \"Yaofo Chen\"\\n          },\\n          {\\n            \"authorId\": \"144259957\",\\n            \"name\": \"P. Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2823637\",\\n            \"name\": \"Mingkui Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, \\\\\\\\ie, group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, \\\\\\\\ie, assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5c76e929496414ff9b31f100eaf72ff46ae4d8d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5c76e929496414ff9b31f100eaf72ff46ae4d8d7\",\\n        \"title\": \"Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.11808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyT is proposed, a novel approach to improve both parameter and inference efficiency for ViT adaptation by proposing a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292217857\",\\n            \"name\": \"Wangbo Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2284481794\",\\n            \"name\": \"Jiasheng Tang\"\\n          },\\n          {\\n            \"authorId\": \"40961502\",\\n            \"name\": \"Yizeng Han\"\\n          },\\n          {\\n            \"authorId\": \"2292187802\",\\n            \"name\": \"Yibing Song\"\\n          },\\n          {\\n            \"authorId\": \"2292214744\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"2249906537\",\\n            \"name\": \"Gao Huang\"\\n          },\\n          {\\n            \"authorId\": \"1716453\",\\n            \"name\": \"F. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2283134324\",\\n            \"name\": \"Yang You\"\\n          }\\n        ],\\n        \"abstract\": \"Existing parameter-efficient fine-tuning (PEFT) methods have achieved significant success on vision transformers (ViTs) adaptation by improving parameter efficiency. However, the exploration of enhancing inference efficiency during adaptation remains underexplored. This limits the broader application of pre-trained ViT models, especially when the model is computationally extensive. In this paper, we propose Dynamic Tuning (DyT), a novel approach to improve both parameter and inference efficiency for ViT adaptation. Specifically, besides using the lightweight adapter modules, we propose a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference. Additionally, we explore multiple design variants to find the best practice of DyT. Finally, inspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced adapter to further boost the adaptation performance. We validate DyT across various tasks, including image/video recognition and semantic segmentation. For instance, DyT achieves superior performance compared to existing PEFT methods while evoking only 71% of their FLOPs on the VTAB-1K benchmark.\"\\n      },\\n      {\\n        \"paperId\": \"64ffcf31b03aaae2905e06f9e24be58f51e2f776\",\\n        \"url\": \"https://www.semanticscholar.org/paper/64ffcf31b03aaae2905e06f9e24be58f51e2f776\",\\n        \"title\": \"Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2311.18363\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.18363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the pretrained model and proposes the Visual Prompt-based Test-Time Adaptation (VPTTA) method, which is lightweight with only a few parameters and can be effectively trained in a single iteration, and presents the low-frequency prompt, which is lightweight with only a few parameters and can be effectively trained in a single iteration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268795762\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2170220899\",\\n            \"name\": \"Yiwen Ye\"\\n          },\\n          {\\n            \"authorId\": \"143746093\",\\n            \"name\": \"Mengkang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2268798668\",\\n            \"name\": \"Yongsheng Pan\"\\n          },\\n          {\\n            \"authorId\": \"2256029665\",\\n            \"name\": \"Yong Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Distribution shift widely exists in medical images acquired from different medical centres and poses a significant obstacle to deploying the pretrained semantic segmentation model in real-world applications. Test-time adaptation has proven its effectiveness in tackling the cross-domain distribution shift during inference. However, most existing methods achieve adaptation by updating the pretrained models, rendering them susceptible to error accumulation and catastrophic forgetting when encountering a series of distribution shifts (i.e., under the continual test-time adaptation setup). To overcome these challenges caused by updating the models, in this paper, we freeze the pretrained model and propose the Visual Prompt-based Test-Time Adaptation (VPTTA) method to train a specific prompt for each test image to align the statistics in the batch normalization layers. Specifically, we present the low-frequency prompt, which is lightweight with only a few parameters and can be effectively trained in a single iteration. To enhance prompt initialization, we equip VPTTA with a memory bank to benefit the current prompt from previous ones. Additionally, we design a warm-up mechanism, which mixes source and target statistics to construct warm-up statistics, thereby facilitating the training process. Extensive experiments demonstrate the superiority of our VPTTA over other state-of-the-art methods on two medical image segmentation benchmark tasks. The code and weights of pretrained source models are available at https://github.com/Chen-Ziyang/VPTTA.\"\\n      },\\n      {\\n        \"paperId\": \"8125cc111e55b9d3e14bf3df9aba5d10c08417f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8125cc111e55b9d3e14bf3df9aba5d10c08417f1\",\\n        \"title\": \"A dynamic collaborative adversarial domain adaptation network for unsupervised rotating machinery fault diagnosis\",\\n        \"citationCount\": 76,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.ress.2024.110662?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.ress.2024.110662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299759595\",\\n            \"name\": \"Xin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256951279\",\\n            \"name\": \"Hongkai Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2266603243\",\\n            \"name\": \"Mingzhe Mu\"\\n          },\\n          {\\n            \"authorId\": \"2152989254\",\\n            \"name\": \"Yutong Dong\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"e6357834e6145b607c350b92238b5bc4d662e920\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e6357834e6145b607c350b92238b5bc4d662e920\",\\n        \"title\": \"Robust Test-Time Adaptation in Dynamic Scenarios\",\\n        \"citationCount\": 171,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.13899\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.13899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA, and presents a robust batch normalization scheme to estimate the normalization statistics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145235797\",\\n            \"name\": \"Longhui Yuan\"\\n          },\\n          {\\n            \"authorId\": \"1896719712\",\\n            \"name\": \"Binhui Xie\"\\n          },\\n          {\\n            \"authorId\": \"2165375715\",\\n            \"name\": \"Shuangliang Li\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time adaptation (TTA) intends to adapt the pretrained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distributions. However, these attempts may fail in dynamic scenarios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we explore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Extensive experiments prove that RoTTA enables continual test-time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA\"\\n      },\\n      {\\n        \"paperId\": \"b53604e693978aaa929136bd33d432fe69214d88\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b53604e693978aaa929136bd33d432fe69214d88\",\\n        \"title\": \"Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.01120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RAGPT is proposed, a novel Retrieval-AuGmented dynamic Prompt Tuning framework that consistently outperforms all competitive baselines in handling incomplete modality problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310823837\",\\n            \"name\": \"Jian Lang\"\\n          },\\n          {\\n            \"authorId\": \"2174885298\",\\n            \"name\": \"Zhangtao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"46456474\",\\n            \"name\": \"Ting Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2273625855\",\\n            \"name\": \"Fan Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Multimodal learning with incomplete modality is practical and challenging. Recently, researchers have focused on enhancing the robustness of pre-trained MultiModal Transformers (MMTs) under missing modality conditions by applying learnable prompts. However, these prompt-based methods face several limitations: (1) incomplete modalities provide restricted modal cues for task-specific inference, (2) dummy imputation for missing content causes information loss and introduces noise, and (3) static prompts are instance-agnostic, offering limited knowledge for instances with various missing conditions. To address these issues, we propose RAGPT, a novel Retrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three modules: (I) the multi-channel retriever, which identifies similar instances through a within-modality retrieval strategy, (II) the missing modality generator, which recovers missing information using retrieved contexts, and (III) the context-aware prompter, which captures contextual knowledge from relevant instances and generates dynamic prompts to largely enhance the MMT\\\\u2019s robustness. Extensive experiments conducted on three real-world datasets show that RAGPT consistently outperforms all competitive baselines in handling incomplete modality problems.\"\\n      },\\n      {\\n        \"paperId\": \"6f75404b0d01f9a09afe428f9efd5cbcd7825469\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6f75404b0d01f9a09afe428f9efd5cbcd7825469\",\\n        \"title\": \"Dynamic Prompting: A Unified Framework for Prompt Tuning\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.02909\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.02909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A theoretical analysis is provided, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-03-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145170944\",\\n            \"name\": \"Xianjun Yang\"\\n          },\\n          {\\n            \"authorId\": \"145859270\",\\n            \"name\": \"Wei Cheng\"\\n          },\\n          {\\n            \"authorId\": \"50879401\",\\n            \"name\": \"Xujiang Zhao\"\\n          },\\n          {\\n            \"authorId\": \"21038849\",\\n            \"name\": \"Linda Petzold\"\\n          },\\n          {\\n            \"authorId\": \"2145225543\",\\n            \"name\": \"Haifeng Chen\"\\n          }\\n        ],\\n        \"abstract\": \"It has been demonstrated that the art of prompt tuning is highly effective in efficiently extracting knowledge from pretrained foundation models, encompassing pretrained language models (PLMs), vision pretrained models, and vision-language (V-L) models. However, the efficacy of employing fixed soft prompts with a predetermined position for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. In this context, we provide a theoretical analysis, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Building upon our analysis, we present a unified dynamic prompt (DP) tuning strategy that dynamically determines different factors of prompts based on specific tasks and instances. To accomplish this, we employ a lightweight learning network with Gumble-Softmax, allowing us to learn instance-dependent guidance. Experimental results underscore the significant performance improvement achieved by dynamic prompt tuning across a wide range of tasks, including NLP tasks, vision recognition tasks, and vision-language tasks. Furthermore, we establish the universal applicability of our approach under full-data, few-shot, and multitask scenarios. Codes are available at https://github.com/Xianjun-Yang/DPT.\"\\n      },\\n      {\\n        \"paperId\": \"ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"title\": \"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.10462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation, is proposed and experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2241468593\",\\n            \"name\": \"Hossein Rajabzadeh\"\\n          },\\n          {\\n            \"authorId\": \"9200111\",\\n            \"name\": \"Mojtaba Valipour\"\\n          },\\n          {\\n            \"authorId\": \"2284643707\",\\n            \"name\": \"Tianshu Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1996315\",\\n            \"name\": \"Marzieh S. Tahaei\"\\n          },\\n          {\\n            \"authorId\": \"2241480742\",\\n            \"name\": \"Hyock Ju Kwon\"\\n          },\\n          {\\n            \"authorId\": \"2237425782\",\\n            \"name\": \"Ali Ghodsi\"\\n          },\\n          {\\n            \"authorId\": \"2237517964\",\\n            \"name\": \"Boxing Chen\"\\n          },\\n          {\\n            \"authorId\": \"2066076226\",\\n            \"name\": \"Mehdi Rezagholizadeh\"\\n          }\\n        ],\\n        \"abstract\": \"Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n      },\\n      {\\n        \"paperId\": \"74a68f4798a34c7254297babd1f3222408198799\",\\n        \"url\": \"https://www.semanticscholar.org/paper/74a68f4798a34c7254297babd1f3222408198799\",\\n        \"title\": \"Adversarial Domain Adaptation Network With Calibrated Prototype and Dynamic Instance Convolution for Hyperspectral Image Classification\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TGRS.2024.3387990?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TGRS.2024.3387990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2125738235\",\\n            \"name\": \"Yi Huang\"\\n          },\\n          {\\n            \"authorId\": \"2444202\",\\n            \"name\": \"Jiangtao Peng\"\\n          },\\n          {\\n            \"authorId\": \"2296669816\",\\n            \"name\": \"Genwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2282635307\",\\n            \"name\": \"Weiwei Sun\"\\n          },\\n          {\\n            \"authorId\": \"2118767432\",\\n            \"name\": \"Na Chen\"\\n          },\\n          {\\n            \"authorId\": \"2072520954\",\\n            \"name\": \"Q. Du\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, the adversarial domain adaptation (ADA) methods have been widely investigated and applied in cross-domain hyperspectral image (HSI) classification. However, most ADA algorithms aim to align the cross-domain distribution without focusing on the class separability of the aligned target features and the information of samples within the domain. To address these issues, a new ADA framework based on calibrated prototype and dynamic instance convolution (CPDIC) is proposed in this article for cross-domain HSI classification. The CPDIC is composed of a generator, a calibrated discriminator (CD), and a classifier. The generator includes a static 3-D convolutional network (SCN) and a dynamic instance convolutional network (DICN), where the SCN is used to extract coarse-grained features of HSI and the DICN can extract sample-specific fine-grained features using instance convolutions generated from dynamic instance convolution kernel generation (DCKG) module. As for the generator, the static and dynamic interactive feature extraction network extracts robust domain-invariant features with discriminability. The CD aligns the marginal distribution between domains and calibrate the predicted pseudo-labels of target domain. For classification, a calibrated prototype loss (CPL) is introduced to align the class distribution across domains. The results of three cross-domain HSI classification tasks show that the proposed CPDIC outperforms existing unsupervised domain adaptation (UDA) algorithms.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 208,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"fb2331ec0bb1efdfd023d69fdb1c2a15b30f1f87\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fb2331ec0bb1efdfd023d69fdb1c2a15b30f1f87\",\\n        \"title\": \"MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.09010\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.09010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders for DFER, and identifies main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72707603\",\\n            \"name\": \"K. Chumachenko\"\\n          },\\n          {\\n            \"authorId\": \"2248136895\",\\n            \"name\": \"Alexandros Iosifidis\"\\n          },\\n          {\\n            \"authorId\": \"1715200\",\\n            \"name\": \"M. Gabbouj\"\\n          }\\n        ],\\n        \"abstract\": \"Dynamic Facial Expression Recognition (DFER) has received significant interest in the recent years dictated by its pivotal role in enabling empathic and human-compatible technologies. Achieving robustness towards in-the-wild data in DFER is particularly important for real-world applications. One of the directions aimed at improving such models is multimodal emotion recognition based on audio and video data. Multimodal learning in DFER increases the model capabilities by leveraging richer, complementary data representations. Within the field of multimodal DFER, recent methods have focused on exploiting advances of self-supervised learning (SSL) for pre-training of strong multi-modal encoders [40]. Another line of research has focused on adapting pre-trained static models for DFER [8]. In this work, we propose a different perspective on the problem and investigate the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders. We identify main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation, and propose solutions to each of them. As a result, we demonstrate improvement over current state-of-the-art on two popular DFER benchmarks, namely DFEW [19] and MFAW [29].\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"1b4012f38daa8f09299e16771973c91ce9464ee2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b4012f38daa8f09299e16771973c91ce9464ee2\",\\n        \"title\": \"DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.09787\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.09787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a novel method called Dynamic Visual Prompt Tuning (DVPT), which efficiently adapts pre-trained models to the medical domain but also enhances data efficiency with limited labeled data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153245407\",\\n            \"name\": \"Along He\"\\n          },\\n          {\\n            \"authorId\": \"37833805\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"2216099246\",\\n            \"name\": \"Zhihong Wang\"\\n          },\\n          {\\n            \"authorId\": \"1840553660\",\\n            \"name\": \"Tao Li\"\\n          },\\n          {\\n            \"authorId\": \"1929093\",\\n            \"name\": \"H. Fu\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-training and fine-tuning have become popular due to the rich representations embedded in large pre-trained models, which can be leveraged for downstream medical tasks. However, existing methods typically either fine-tune all parameters or only task-specific layers of pre-trained models, overlooking the variability in input medical images. As a result, these approaches may lack efficiency or effectiveness. In this study, our goal is to explore parameter-efficient fine-tuning (PEFT) for medical image analysis. To address this challenge, we introduce a novel method called Dynamic Visual Prompt Tuning (DVPT). It can extract knowledge beneficial to downstream tasks from large models with only a few trainable parameters. First, the frozen features are transformed by a lightweight bottleneck layer to learn the domain-specific distribution of downstream medical tasks. Then, a few learnable visual prompts are employed as dynamic queries to conduct cross-attention with the transformed features, aiming to acquire sample-specific features. This DVPT module can be shared across different Transformer layers, further reducing the number of trainable parameters. We conduct extensive experiments with various pre-trained models on medical classification and segmentation tasks. We find that this PEFT method not only efficiently adapts pre-trained models to the medical domain but also enhances data efficiency with limited labeled data. For example, with only 0.5% additional trainable parameters, our method not only outperforms state-of-the-art PEFT methods but also surpasses full fine-tuning by more than 2.20% in Kappa score on the medical classification task. It can save up to 60% of labeled data and 99% of storage cost of ViT-B/16.\"\\n      },\\n      {\\n        \"paperId\": \"c28b7dfe341f1e13a5a98efbce7946ef795cf9b8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c28b7dfe341f1e13a5a98efbce7946ef795cf9b8\",\\n        \"title\": \"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer\",\\n        \"citationCount\": 309,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2022.acl-long.346.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2110.07904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that SPoT significantly boosts the performance of Prompt Tuning across many tasks, and an efficient retrieval approach is proposed that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144244743\",\\n            \"name\": \"Tu Vu\"\\n          },\\n          {\\n            \"authorId\": \"144104130\",\\n            \"name\": \"Brian Lester\"\\n          },\\n          {\\n            \"authorId\": \"40832517\",\\n            \"name\": \"Noah Constant\"\\n          },\\n          {\\n            \"authorId\": \"1388360943\",\\n            \"name\": \"Rami Al-Rfou\"\\n          },\\n          {\\n            \"authorId\": \"46724030\",\\n            \"name\": \"Daniel Matthew Cer\"\\n          }\\n        ],\\n        \"abstract\": \"There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks. Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks. More remarkably, across all model sizes, SPoT matches or outperforms standard Model Tuning (which fine-tunes all model parameters) on the SuperGLUE benchmark, while using up to 27,000\\\\u00d7 fewer task-specific parameters. To understand where SPoT is most effective, we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations, and demonstrate that many tasks can benefit each other via prompt transfer. Finally, we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.\"\\n      },\\n      {\\n        \"paperId\": \"81babe3cb42e620b537ecb5e6af540e3be2c3ca7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/81babe3cb42e620b537ecb5e6af540e3be2c3ca7\",\\n        \"title\": \"Visual Prompt Tuning for Test-time Domain Adaptation\",\\n        \"citationCount\": 105,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.04831\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.04831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work tackles the practical yet challenging test-time adaptation (TTA) problem, where a model adapts to the target domain without accessing the source data, with a simple recipe called Data-efficient Prompt Tuning (DePT).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282248267\",\\n            \"name\": \"Yunhe Gao\"\\n          },\\n          {\\n            \"authorId\": \"2110332219\",\\n            \"name\": \"Xingjian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2128816752\",\\n            \"name\": \"Yi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2359832\",\\n            \"name\": \"Hongya Wang\"\\n          },\\n          {\\n            \"authorId\": \"2986505\",\\n            \"name\": \"Zhiqiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"51453885\",\\n            \"name\": \"Xiong Zhou\"\\n          },\\n          {\\n            \"authorId\": \"1701799\",\\n            \"name\": \"Mu Li\"\\n          },\\n          {\\n            \"authorId\": \"1711560\",\\n            \"name\": \"Dimitris N. Metaxas\"\\n          }\\n        ],\\n        \"abstract\": \"Models should be able to adapt to unseen data during test-time to avoid performance drops caused by inevitable distribution shifts in real-world deployment scenarios. In this work, we tackle the practical yet challenging test-time adaptation (TTA) problem, where a model adapts to the target domain without accessing the source data. We propose a simple recipe called \\\\\\\\textit{Data-efficient Prompt Tuning} (DePT) with two key ingredients. First, DePT plugs visual prompts into the vision Transformer and only tunes these source-initialized prompts during adaptation. We find such parameter-efficient finetuning can efficiently adapt the model representation to the target domain without overfitting to the noise in the learning objective. Second, DePT bootstraps the source representation to the target domain by memory bank-based online pseudo-labeling. A hierarchical self-supervised regularization specially designed for prompts is jointly optimized to alleviate error accumulation during self-training. With much fewer tunable parameters, DePT demonstrates not only state-of-the-art performance on major adaptation benchmarks VisDA-C, ImageNet-C, and DomainNet-126, but also superior data efficiency, i.e., adaptation with only 1\\\\\\\\% or 10\\\\\\\\% data without much performance degradation compared to 100\\\\\\\\% data. In addition, DePT is also versatile to be extended to online or multi-source TTA settings.\"\\n      },\\n      {\\n        \"paperId\": \"0392d58335ce674a70f5e58ac8c438de296a0e6a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0392d58335ce674a70f5e58ac8c438de296a0e6a\",\\n        \"title\": \"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models\",\\n        \"citationCount\": 172,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2208.07852\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2208.07852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task, and then allows easy deployment of the newly created ad-hoc models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-08-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2879705\",\\n            \"name\": \"Hendrik Strobelt\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2285868436\",\\n            \"name\": \"Victor Sanh\"\\n          },\\n          {\\n            \"authorId\": \"2061407009\",\\n            \"name\": \"Benjamin Hoover\"\\n          },\\n          {\\n            \"authorId\": \"47273741\",\\n            \"name\": \"Johanna Beyer\"\\n          },\\n          {\\n            \"authorId\": \"143758236\",\\n            \"name\": \"H. Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2531268\",\\n            \"name\": \"Alexander M. Rush\"\\n          }\\n        ],\\n        \"abstract\": \"State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.\"\\n      }\\n    ]\\n  },\\n  \"adaptive prompt generation\": {\\n    \"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\",\\n    \"code\": \"429\"\\n  },\\n  \"iterative context update\": {\\n    \"total\": 8986,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 276,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 201,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\', \\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 182,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 359,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 322,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 68,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 47,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 40,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2801,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"title\": \"Spontaneous Reward Hacking in Iterative Self-Refinement\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Using an essay editing task, it is shown that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311112294\",\\n            \"name\": \"Jane Pan\"\\n          },\\n          {\\n            \"authorId\": \"2321875898\",\\n            \"name\": \"He He\"\\n          },\\n          {\\n            \"authorId\": \"2297768298\",\\n            \"name\": \"Samuel R. Bowman\"\\n          },\\n          {\\n            \"authorId\": \"2297816489\",\\n            \"name\": \"Shi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of human users, a second language model can be used as an evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator\\'s ratings improve while the generation quality remains stagnant or even decreases as judged by actual user preference. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive them to exploit shared vulnerabilities. Using an essay editing task, we show that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement. In addition, we study conditions under which reward hacking occurs and observe two factors that affect reward hacking severity: model size and context sharing between the generator and the evaluator.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      },\\n      {\\n        \"paperId\": \"f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"title\": \"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.01701?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      }\\n    ]\\n  },\\n  \"summarization prior interactions\": {\\n    \"total\": 16594,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a\",\\n        \"title\": \"TidyBot: Personalized Robot Assistance with Large Language Models\",\\n        \"citationCount\": 370,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2305.05658\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.05658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155142153\",\\n            \"name\": \"Jimmy Wu\"\\n          },\\n          {\\n            \"authorId\": \"39534622\",\\n            \"name\": \"Rika Antonova\"\\n          },\\n          {\\n            \"authorId\": \"2216606824\",\\n            \"name\": \"Adam Kan\"\\n          },\\n          {\\n            \"authorId\": \"10710717\",\\n            \"name\": \"Marion Lepert\"\\n          },\\n          {\\n            \"authorId\": \"38591293\",\\n            \"name\": \"Andy Zeng\"\\n          },\\n          {\\n            \"authorId\": \"3340170\",\\n            \"name\": \"Shuran Song\"\\n          },\\n          {\\n            \"authorId\": \"1775407\",\\n            \"name\": \"Jeannette Bohg\"\\n          },\\n          {\\n            \"authorId\": \"7723706\",\\n            \"name\": \"S. Rusinkiewicz\"\\n          },\\n          {\\n            \"authorId\": \"1807080\",\\n            \"name\": \"T. Funkhouser\"\\n          }\\n        ],\\n        \"abstract\": \"For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people\\'s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"0725debf3183589626823dbb64107bba8ed22448\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0725debf3183589626823dbb64107bba8ed22448\",\\n        \"title\": \"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.01596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids, is introduced and the value of integrating visual information from images to improve the creation of medically detailed summaries is demonstrated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275156796\",\\n            \"name\": \"Akash Ghosh\"\\n          },\\n          {\\n            \"authorId\": \"2273560136\",\\n            \"name\": \"Arkadeep Acharya\"\\n          },\\n          {\\n            \"authorId\": \"2165947844\",\\n            \"name\": \"Prince Jha\"\\n          },\\n          {\\n            \"authorId\": \"2277599094\",\\n            \"name\": \"Aniket Gaudgaul\"\\n          },\\n          {\\n            \"authorId\": \"2277600473\",\\n            \"name\": \"Rajdeep Majumdar\"\\n          },\\n          {\\n            \"authorId\": \"2243463123\",\\n            \"name\": \"Sriparna Saha\"\\n          },\\n          {\\n            \"authorId\": \"2275226689\",\\n            \"name\": \"Aman Chadha\"\\n          },\\n          {\\n            \"authorId\": \"2088137695\",\\n            \"name\": \"Raghav Jain\"\\n          },\\n          {\\n            \"authorId\": \"2275161937\",\\n            \"name\": \"Setu Sinha\"\\n          },\\n          {\\n            \"authorId\": \"2277651208\",\\n            \"name\": \"Shivani Agarwal\"\\n          }\\n        ],\\n        \"abstract\": \"In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient\\'s medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries. This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care. Our dataset, code, and pre-trained models will be made publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c1799bf28d1ae93e1631be5b59196ee1e568f538\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c1799bf28d1ae93e1631be5b59196ee1e568f538\",\\n        \"title\": \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\",\\n        \"citationCount\": 820,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GraphRAG is proposed, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text and leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2298275009\",\\n            \"name\": \"Darren Edge\"\\n          },\\n          {\\n            \"authorId\": \"2213073417\",\\n            \"name\": \"Ha Trinh\"\\n          },\\n          {\\n            \"authorId\": \"2298273973\",\\n            \"name\": \"Newman Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2298275805\",\\n            \"name\": \"Joshua Bradley\"\\n          },\\n          {\\n            \"authorId\": \"2298274563\",\\n            \"name\": \"Alex Chao\"\\n          },\\n          {\\n            \"authorId\": \"2210994342\",\\n            \"name\": \"Apurva N. Mody\"\\n          },\\n          {\\n            \"authorId\": \"2298273810\",\\n            \"name\": \"Steven Truitt\"\\n          },\\n          {\\n            \"authorId\": \"2298278846\",\\n            \"name\": \"Jonathan Larson\"\\n          }\\n        ],\\n        \"abstract\": \"The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as\\\\\"What are the main themes in the dataset?\\\\\", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.\"\\n      },\\n      {\\n        \"paperId\": \"4bf0c0f4aaf67f5138d5036c044a5d0de8938181\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4bf0c0f4aaf67f5138d5036c044a5d0de8938181\",\\n        \"title\": \"Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.13190\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.13190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel role interaction enhanced method that adopts cross attention and decoder self-attention interactions to interactively acquire other roles\\\\u2019 critical information for role-oriented dialogue summarization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152781993\",\\n            \"name\": \"Haitao Lin\"\\n          },\\n          {\\n            \"authorId\": \"24925751\",\\n            \"name\": \"Junnan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"145475526\",\\n            \"name\": \"Lu Xiang\"\\n          },\\n          {\\n            \"authorId\": \"2110631853\",\\n            \"name\": \"Yu Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2124819243\",\\n            \"name\": \"Jiajun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2064100826\",\\n            \"name\": \"Chengqing Zong\"\\n          }\\n        ],\\n        \"abstract\": \"Role-oriented dialogue summarization is to generate summaries for different roles in the dialogue, e.g., merchants and consumers. Existing methods handle this task by summarizing each role\\\\u2019s content separately and thus are prone to ignore the information from other roles. However, we believe that other roles\\\\u2019 content could benefit the quality of summaries, such as the omitted information mentioned by other roles. Therefore, we propose a novel role interaction enhanced method for role-oriented dialogue summarization. It adopts cross attention and decoder self-attention interactions to interactively acquire other roles\\\\u2019 critical information. The cross attention interaction aims to select other roles\\\\u2019 critical dialogue utterances, while the decoder self-attention interaction aims to obtain key information from other roles\\\\u2019 summaries. Experimental results have shown that our proposed method significantly outperforms strong baselines on two public role-oriented dialogue summarization datasets. Extensive analyses have demonstrated that other roles\\\\u2019 content could help generate summaries with more complete semantics and correct topic structures.\"\\n      },\\n      {\\n        \"paperId\": \"65c5e3e3be6e881c7d2feef4c0c3d07323f3aeff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/65c5e3e3be6e881c7d2feef4c0c3d07323f3aeff\",\\n        \"title\": \"Make The Most of Prior Data: A Solution for Interactive Text Summarization with Preference Feedback\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2204.05512\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.05512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new framework to train summarization models with preference feedback interactively with properly leveraging offline data and a novel reward model is introduced to improve the performance regarding ROUGE scores and sample-efficiency.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2153960542\",\\n            \"name\": \"Duy-Hung Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"38714383\",\\n            \"name\": \"Nguyen-Viet-Dung Nghiem\"\\n          },\\n          {\\n            \"authorId\": \"2140448395\",\\n            \"name\": \"Bao-Sinh Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2142143947\",\\n            \"name\": \"Dung Tien Le\"\\n          },\\n          {\\n            \"authorId\": \"2162186181\",\\n            \"name\": \"Shahab Sabahi\"\\n          },\\n          {\\n            \"authorId\": \"1789308\",\\n            \"name\": \"Minh Le Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2064728738\",\\n            \"name\": \"Hung Le\"\\n          }\\n        ],\\n        \"abstract\": \"For summarization, human preference is critical to tame outputs of the summarizer in favor of human interests, as ground-truth summaries are scarce and ambiguous. Practical settings require dynamic exchanges between human and AI agent wherein feedback is provided in an online manner, a few at a time. In this paper, we introduce a new framework to train summarization models with preference feedback interactively. By properly leveraging offline data and a novel reward model, we improve the performance regarding ROUGE scores and sample-efficiency. Our experiments on three various datasets confirm the benefit of the proposed framework in active, few-shot and online settings of preference learning.\"\\n      },\\n      {\\n        \"paperId\": \"9b86fc9596f5bc027a0a507414b2c4ecf60bcbbf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9b86fc9596f5bc027a0a507414b2c4ecf60bcbbf\",\\n        \"title\": \"Towards Clinical Encounter Summarization: Learning to Compose Discharge Summaries from Prior Notes\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2104.13498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Two new measures, faithfulness and hallucination rate, are introduced for evaluation in this task, which complement existing measures for fluency and informativeness.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-04-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46183205\",\\n            \"name\": \"Han-Chin Shing\"\\n          },\\n          {\\n            \"authorId\": \"1866532\",\\n            \"name\": \"Chaitanya P. Shivade\"\\n          },\\n          {\\n            \"authorId\": \"3158273\",\\n            \"name\": \"Nima Pourdamghani\"\\n          },\\n          {\\n            \"authorId\": \"144647318\",\\n            \"name\": \"Feng Nan\"\\n          },\\n          {\\n            \"authorId\": \"1680292\",\\n            \"name\": \"P. Resnik\"\\n          },\\n          {\\n            \"authorId\": \"1737250\",\\n            \"name\": \"Douglas W. Oard\"\\n          },\\n          {\\n            \"authorId\": \"50339091\",\\n            \"name\": \"Parminder Bhatia\"\\n          }\\n        ],\\n        \"abstract\": \"The records of a clinical encounter can be extensive and complex, thus placing a premium on tools that can extract and summarize relevant information. This paper introduces the task of generating discharge summaries for a clinical encounter. Summaries in this setting need to be faithful, traceable, and scale to multiple long documents, motivating the use of extract-then-abstract summarization cascades. We introduce two new measures, faithfulness and hallucination rate for evaluation in this task, which complement existing measures for fluency and informativeness. Results across seven medical sections and five models show that a summarization architecture that supports traceability yields promising results, and that a sentence-rewriting approach performs consistently on the measure used for faithfulness (faithfulness-adjusted $F_3$) over a diverse range of generated sections.\"\\n      },\\n      {\\n        \"paperId\": \"ba4548c01a8b29a406f269db80f7105b88ca9751\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ba4548c01a8b29a406f269db80f7105b88ca9751\",\\n        \"title\": \"FABLES: Evaluating faithfulness and content selection in book-length summarization\",\\n        \"citationCount\": 64,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper conducts the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books, and develops a typology of omission errors related to crucial narrative elements and identifies a systematic over-emphasis on events occurring towards the end of the book.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2294805305\",\\n            \"name\": \"Yekyung Kim\"\\n          },\\n          {\\n            \"authorId\": \"144455052\",\\n            \"name\": \"Yapei Chang\"\\n          },\\n          {\\n            \"authorId\": \"37796923\",\\n            \"name\": \"Marzena Karpinska\"\\n          },\\n          {\\n            \"authorId\": \"31099365\",\\n            \"name\": \"Aparna Garimella\"\\n          },\\n          {\\n            \"authorId\": \"2380898326\",\\n            \"name\": \"Varun Manjunatha\"\\n          },\\n          {\\n            \"authorId\": \"46258841\",\\n            \"name\": \"Kyle Lo\"\\n          },\\n          {\\n            \"authorId\": \"2253400779\",\\n            \"name\": \"Tanya Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2136562\",\\n            \"name\": \"Mohit Iyyer\"\\n          }\\n        ],\\n        \"abstract\": \"While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.\"\\n      },\\n      {\\n        \"paperId\": \"58b0923c25a721061f6a43574ed50846d637d0a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58b0923c25a721061f6a43574ed50846d637d0a9\",\\n        \"title\": \"Web-based online resources about adverse interactions or side effects associated with complementary and alternative medicine: a systematic review, summarization and quality assessment\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://bmcmedinformdecismak.biomedcentral.com/track/pdf/10.1186/s12911-020-01298-5\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7653751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study provides a summarized list of high quality, evidence-based, online resources about CAM and CAM-related adverse effects, and can thereby serve as a useful reference for clinicians, researchers, and patients.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-11-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8563902\",\\n            \"name\": \"J. Y. Ng\"\\n          },\\n          {\\n            \"authorId\": \"1840486037\",\\n            \"name\": \"Vanessa Munford\"\\n          },\\n          {\\n            \"authorId\": \"1572362560\",\\n            \"name\": \"Harmy Thakar\"\\n          }\\n        ],\\n        \"abstract\": \"Background Given an increased global prevalence of complementary and alternative medicine (CAM) use, healthcare providers commonly seek CAM-related health information online. Numerous online resources containing CAM-specific information exist, many of which are readily available/accessible, containing information shareable with their patients. To the authors\\\\u2019 knowledge, no study has summarized nor assessed the quality of content contained within these online resources for at least a decade, specifically pertaining to information about adverse effects or interactions. Methods This study provides summaries of web-based online resources that provide safety information on potential interactions or adverse effects of CAM. Specifically, clinicians are the intended users of these online resources containing patient information which they can then disseminate to their patients. All online resources were assessed for content quality using the validated rating tool, DISCERN. Results Of 21 articles identified in our previously published scoping review, 23 online resources were eligible. DISCERN assessments suggests that online resources containing CAM-specific information vary in quality. Summed DISCERN scores had a mean of 56.13 (SD\\\\u2009=\\\\u200910.25) out of 75. Online resources with the highest total DISCERN scores across all questions included Micromedex (68.50), Merck Manual (67.50) and Drugs.com (66.50). Online resources with the lowest total scores included Drug Information (33.00), Caremark Drug Interactions (42.50) and HIV Drug Interactions (43.00). The DISCERN questions that received the highest mean score across all online resources referred to whether the risks were described for each treatment (4.66), whether the aims were clear (4.58), whether the source achieved those aims (4.58), and whether the website referred to areas of uncertainty (4.58). The DISCERN questions that received the lowest mean score across all online resources assessed whether there was discussion about no treatment being used (1.29) and how treatment choices would affect quality of life (2.00). Conclusion This study provides a comprehensive list of online resources containing CAM-specific information. Informed by the appraisal of these resources, this study provides a summarized list of high quality, evidence-based, online resources about CAM and CAM-related adverse effects. This list of recommended resources can thereby serve as a useful reference for clinicians, researchers, and patients.\"\\n      },\\n      {\\n        \"paperId\": \"0b66b4398a0a768ca32ea23ae827a4b1fe9746ab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b66b4398a0a768ca32ea23ae827a4b1fe9746ab\",\\n        \"title\": \"Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies, and proposes a refined taxonomy of errors, coining the category of \\\\\"Circumstantial Inference\\\\\".\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"98806251\",\\n            \"name\": \"S. Ramprasad\"\\n          },\\n          {\\n            \"authorId\": \"2249355743\",\\n            \"name\": \"Elisa Ferracane\"\\n          },\\n          {\\n            \"authorId\": \"32219137\",\\n            \"name\": \"Zachary Chase Lipton\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems. However, they continue to face concerns about hallucinations. While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness. Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies. Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models. We propose a refined taxonomy of errors, coining the category of\\\\\"Circumstantial Inference\\\\\"to bucket these LLM behaviors and release the dataset. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors. To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying\\\\\"Circumstantial Inference.\\\\\"\"\\n      },\\n      {\\n        \"paperId\": \"fa253d232a933595ff8aa892730a49bcf63be4db\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa253d232a933595ff8aa892730a49bcf63be4db\",\\n        \"title\": \"Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.15496\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.15496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes an instruction fine-tuning model: Baichuan2-Sum, for role-oriented dialogue summarization, which shows an improvement in Rouge scores on both datasets compared to the previously best-performing model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282252656\",\\n            \"name\": \"Jianfei Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2281789572\",\\n            \"name\": \"Yancan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2148234209\",\\n            \"name\": \"Yimin Ou\"\\n          },\\n          {\\n            \"authorId\": \"2313881077\",\\n            \"name\": \"Hanyi Yu\"\\n          },\\n          {\\n            \"authorId\": \"2281897968\",\\n            \"name\": \"Yiyong Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) like LLaMA, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods foucus on small models (e.g BART and BERT). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented dialogue summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the desired summaries. Furthermore, we applied NEFTune technique to add suitable noise during training, improving the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. The Baichuan2-Sum model shows an improvement in Rouge scores on both datasets compared to the previously best-performing model. Notably, for the SAMSUM dataset, there is a 21% increase in the ROUGE-1 score, a 32% increase in the ROUGE-2 score, and a 9% increase in the ROUGE-L score. We have released our model and related codes to facilitate future studies in the dialogue summarization task.\"\\n      },\\n      {\\n        \"paperId\": \"2338d7c9ab07e6d0f4160335dce0e6e6a87c4749\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2338d7c9ab07e6d0f4160335dce0e6e6a87c4749\",\\n        \"title\": \"Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13412\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2143482843\",\\n            \"name\": \"Yiming Wang\"\\n          },\\n          {\\n            \"authorId\": \"3322871\",\\n            \"name\": \"Zhuosheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2151038501\",\\n            \"name\": \"Rui Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the \\\\u201cLasswell Communication Model\\\\u201d proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs\\\\u2019 zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.\"\\n      },\\n      {\\n        \"paperId\": \"d9046db67af64e78791099d646b5d22792fabc63\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d9046db67af64e78791099d646b5d22792fabc63\",\\n        \"title\": \"Overview of the MEDIQA-Chat 2023 Shared Tasks on the Summarization & Generation of Doctor-Patient Conversations\",\\n        \"citationCount\": 56,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.clinicalnlp-1.52.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://aclanthology.org/2023.clinicalnlp-1.52, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes the three MEDIQA-Chat 2023 tasks, the datasets, and the participants\\\\u2019 results and methods and hopes that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2127117221\",\\n            \"name\": \"Asma Ben Abacha\"\\n          },\\n          {\\n            \"authorId\": \"38845771\",\\n            \"name\": \"Wen-wai Yim\"\\n          },\\n          {\\n            \"authorId\": \"49496422\",\\n            \"name\": \"Griffin Adams\"\\n          },\\n          {\\n            \"authorId\": \"2370273\",\\n            \"name\": \"N. Snider\"\\n          },\\n          {\\n            \"authorId\": \"1398215463\",\\n            \"name\": \"Meliha Yetisgen-Yildiz\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic generation of clinical notes from doctor-patient conversations can play a key role in reducing daily doctors\\\\u2019 workload and improving their interactions with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective solutions through shared tasks on the automatic summarization of doctor-patient conversations and on the generation of synthetic dialogues from clinical notes for data augmentation. Seventeen teams participated in the challenge and experimented with a broad range of approaches and models. In this paper, we describe the three MEDIQA-Chat 2023 tasks, the datasets, and the participants\\\\u2019 results and methods. We hope that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes.\"\\n      },\\n      {\\n        \"paperId\": \"de546efe75ef22c5471e9e92c21cc093975ca9d6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/de546efe75ef22c5471e9e92c21cc093975ca9d6\",\\n        \"title\": \"Prompting Large Language Models with Audio for General-Purpose Speech Summarization\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2406.05968\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.05968, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An end-to-end system that combines an instruction-tuned LLM with an audio encoder that converts speech into token representations that the LLM can interpret that allows the LLM to process speech inputs in the same way as text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2113751566\",\\n            \"name\": \"Wonjune Kang\"\\n          },\\n          {\\n            \"authorId\": \"2305604637\",\\n            \"name\": \"Deb Roy\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, we introduce a framework for speech summarization that leverages the processing and reasoning capabilities of large language models (LLMs). We propose an end-to-end system that combines an instruction-tuned LLM with an audio encoder that converts speech into token representations that the LLM can interpret. Using a dataset with paired speech-text data, the overall system is trained to generate consistent responses to prompts with the same semantic information regardless of the input modality. The resulting framework allows the LLM to process speech inputs in the same way as text, enabling speech summarization by simply prompting the LLM. Unlike prior approaches, our method is able to summarize spoken content from any arbitrary domain, and it can produce summaries in different styles by varying the LLM prompting strategy. Experiments demonstrate that our approach outperforms a cascade baseline of speech recognition followed by LLM text processing.\"\\n      },\\n      {\\n        \"paperId\": \"5272b2e11769a0e4bcfb9ff7cdf4c58e648afa18\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5272b2e11769a0e4bcfb9ff7cdf4c58e648afa18\",\\n        \"title\": \"EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3640457.3688185\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.11441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EmbSum is introduced, a novel framework that enables offline pre-computations of users and candidate items while capturing the interactions within the user engagement history, surpassing state-of-the-art (SoTA) methods with higher accuracy and fewer parameters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-05-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50445559\",\\n            \"name\": \"Chiyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2284621464\",\\n            \"name\": \"Yifei Sun\"\\n          },\\n          {\\n            \"authorId\": \"2145209409\",\\n            \"name\": \"Minghao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2284643679\",\\n            \"name\": \"Jun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284594046\",\\n            \"name\": \"Jie Lei\"\\n          },\\n          {\\n            \"authorId\": \"2065312024\",\\n            \"name\": \"M. Abdul-Mageed\"\\n          },\\n          {\\n            \"authorId\": \"2284592470\",\\n            \"name\": \"Rong Jin\"\\n          },\\n          {\\n            \"authorId\": \"2302524479\",\\n            \"name\": \"Angli Liu\"\\n          },\\n          {\\n            \"authorId\": \"2302370397\",\\n            \"name\": \"Ji Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2284639887\",\\n            \"name\": \"Sem Park\"\\n          },\\n          {\\n            \"authorId\": \"2284591939\",\\n            \"name\": \"Ning Yao\"\\n          },\\n          {\\n            \"authorId\": \"2284595625\",\\n            \"name\": \"Bo Long\"\\n          }\\n        ],\\n        \"abstract\": \"Content-based recommendation systems play a crucial role in delivering personalized content to users in the digital world. In this work, we introduce EmbSum, a novel framework that enables offline pre-computations of users and candidate items while capturing the interactions within the user engagement history. By utilizing the pretrained encoder-decoder model and poly-attention layers, EmbSum derives User Poly-Embedding (UPE) and Content Poly-Embedding (CPE) to calculate relevance scores between users and candidate items. EmbSum actively learns the long user engagement histories by generating user-interest summary with supervision from large language model (LLM). The effectiveness of EmbSum is validated on two datasets from different domains, surpassing state-of-the-art (SoTA) methods with higher accuracy and fewer parameters. Additionally, the model\\\\u2019s ability to generate summaries of user interests serves as a valuable by-product, enhancing its usefulness for personalized content recommendations.\"\\n      },\\n      {\\n        \"paperId\": \"60e7545ec5280ad944d2d6e49be52419301aa063\",\\n        \"url\": \"https://www.semanticscholar.org/paper/60e7545ec5280ad944d2d6e49be52419301aa063\",\\n        \"title\": \"VSS-Net: Visual Semantic Self-Mining Network for Video Summarization\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3312325?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3312325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108326383\",\\n            \"name\": \"Yunzuo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2190800936\",\\n            \"name\": \"Yameng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2194582294\",\\n            \"name\": \"Weili Kang\"\\n          },\\n          {\\n            \"authorId\": \"2070824819\",\\n            \"name\": \"Ran Tao\"\\n          }\\n        ],\\n        \"abstract\": \"Video summarization, with the target to detect valuable segments given untrimmed videos, is a meaningful yet understudied topic. Previous methods primarily consider inter-frame and inter-shot temporal dependencies, which might be insufficient to pinpoint important content due to limited valuable information that can be learned. To address this limitation, we elaborate on a Visual Semantic Self-mining Network (VSS-Net), a novel summarization framework motivated by the widespread success of cross-modality learning tasks. VSS-Net initially adopts a two-stream structure consisting of a Context Representation Graph (CRG) and a Video Semantics Encoder (VSE). They are jointly exploited to establish the groundwork for further boosting the capability of content awareness. Specifically, CRG is constructed using an edge-set strategy tailored to the hierarchical structure of videos, enriching visual features with local and non-local temporal cues from temporal order and visual relationship perspectives. Meanwhile, by learning visual similarity across features, VSE adaptively acquires an instructive video-level semantic representation of the input video from coarse to fine. Subsequently, the two streams converge in a Context-Semantics Interaction Layer (CSIL) to achieve sophisticated information exchange across frame-level temporal cues and video-level semantic representation, guaranteeing informative representations and boosting the sensitivity to important segments. Eventually, importance scores are predicted utilizing a prediction head, followed by key shot selection. We evaluate the proposed framework and demonstrate its effectiveness and superiority against state-of-the-art methods on the widely used benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"a36658b26ea4ccb58f85d8a578f6ec6767446095\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a36658b26ea4ccb58f85d8a578f6ec6767446095\",\\n        \"title\": \"Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The authors\\' model with 770M parameters consistently outperforms strong baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT itself, including models distilled from ChatGPT, and sometimes, even ChatGPT itself.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2122355046\",\\n            \"name\": \"Jaehun Jung\"\\n          },\\n          {\\n            \"authorId\": \"119659229\",\\n            \"name\": \"Peter West\"\\n          },\\n          {\\n            \"authorId\": \"2112504145\",\\n            \"name\": \"Liwei Jiang\"\\n          },\\n          {\\n            \"authorId\": \"9252833\",\\n            \"name\": \"Faeze Brahman\"\\n          },\\n          {\\n            \"authorId\": \"50085131\",\\n            \"name\": \"Ximing Lu\"\\n          },\\n          {\\n            \"authorId\": \"33772445\",\\n            \"name\": \"Jillian R. Fisher\"\\n          },\\n          {\\n            \"authorId\": \"122436831\",\\n            \"name\": \"Taylor Sorensen\"\\n          },\\n          {\\n            \"authorId\": \"1699545\",\\n            \"name\": \"Yejin Choi\"\\n          }\\n        ],\\n        \"abstract\": \"We present Impossible Distillation, a novel framework for paraphrasing and sentence summarization, that distills a high-quality dataset and model from a low-quality teacher that itself cannot perform these tasks. Unlike prior works that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific architecture, we hypothesize and verify the paraphrastic proximity intrinsic to pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in the LM distribution. By identifying and distilling generations from these subspaces, Impossible Distillation produces a high-quality dataset and model even from GPT2-scale LMs. We evaluate our method on multiple benchmarks spanning unconstrained / syntax-controlled paraphrase generation and sentence summarization. Our model with 770M parameters consistently outperforms strong baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher diversity and fidelity than up to 13 times larger datasets.\"\\n      },\\n      {\\n        \"paperId\": \"c747c6ce74bf80f1c0af97fb3aeb37875415d238\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c747c6ce74bf80f1c0af97fb3aeb37875415d238\",\\n        \"title\": \"Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13693\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that not only do automated metrics fail to capture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31860505\",\\n            \"name\": \"Lucy Lu Wang\"\\n          },\\n          {\\n            \"authorId\": \"1881453937\",\\n            \"name\": \"Yulia Otmakhova\"\\n          },\\n          {\\n            \"authorId\": \"48727916\",\\n            \"name\": \"Jay DeYoung\"\\n          },\\n          {\\n            \"authorId\": \"153574160\",\\n            \"name\": \"Thinh Hung Truong\"\\n          },\\n          {\\n            \"authorId\": \"2003338023\",\\n            \"name\": \"Bailey Kuehl\"\\n          },\\n          {\\n            \"authorId\": \"2203427167\",\\n            \"name\": \"Erin Bransom\"\\n          },\\n          {\\n            \"authorId\": \"2111879324\",\\n            \"name\": \"Byron Wallace\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better automated evaluation methods for literature review MDS. We take advantage of community submissions to the Multi-document Summarization for Literature Review (MSLR) shared task to compile a diverse and representative sample of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical features of generated summaries, to other automated metrics including several we propose in this work, and to aspects of human-assessed summary quality. We find that not only do automated metrics fail to capture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators.\"\\n      },\\n      {\\n        \"paperId\": \"be658d4482299d7a77be324bdb9946bfcbcbad25\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be658d4482299d7a77be324bdb9946bfcbcbad25\",\\n        \"title\": \"Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16784\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16784, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2167123387\",\\n            \"name\": \"Dongqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2293409512\",\\n            \"name\": \"Yifan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293393732\",\\n            \"name\": \"Vera Demberg\"\\n          }\\n        ],\\n        \"abstract\": \"For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the \\\\u2018RSTformer\\\\u2019, a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.\"\\n      },\\n      {\\n        \"paperId\": \"5f6c67f3111ca652e6709f5ae3b6821c13838a80\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5f6c67f3111ca652e6709f5ae3b6821c13838a80\",\\n        \"title\": \"Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.12324\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.12324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs into figure captions, and shows that it can be more effectively tackled as a text summarization task in scientific documents.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1414028847\",\\n            \"name\": \"Huang Chieh-Yang\"\\n          },\\n          {\\n            \"authorId\": \"4500245\",\\n            \"name\": \"Ting-Yao Hsu\"\\n          },\\n          {\\n            \"authorId\": \"2066337266\",\\n            \"name\": \"Ryan A. Rossi\"\\n          },\\n          {\\n            \"authorId\": \"3115414\",\\n            \"name\": \"A. Nenkova\"\\n          },\\n          {\\n            \"authorId\": \"2109571021\",\\n            \"name\": \"Sungchul Kim\"\\n          },\\n          {\\n            \"authorId\": \"51192588\",\\n            \"name\": \"G. Chan\"\\n          },\\n          {\\n            \"authorId\": \"35910905\",\\n            \"name\": \"Eunyee Koh\"\\n          },\\n          {\\n            \"authorId\": \"145157784\",\\n            \"name\": \"C. Lee Giles\"\\n          },\\n          {\\n            \"authorId\": \"2109026884\",\\n            \"name\": \"Ting Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., \\\\u201cFigure 3 shows...\\\\u201d) into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and data are available at: https://github.com/Crowd-AI-Lab/Generating-Figure-Captions-as-a-Text-Summarization-Task.\"\\n      },\\n      {\\n        \"paperId\": \"6b9b1409c013ed3fca1a2bc697ad442c337de462\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b9b1409c013ed3fca1a2bc697ad442c337de462\",\\n        \"title\": \"Abstractive Financial News Summarization via Transformer-BiLSTM Encoder and Graph Attention-Based Decoder\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASLP.2023.3304473?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASLP.2023.3304473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1509369579\",\\n            \"name\": \"Haozhou Li\"\\n          },\\n          {\\n            \"authorId\": \"144135237\",\\n            \"name\": \"Qinke Peng\"\\n          },\\n          {\\n            \"authorId\": \"2005209281\",\\n            \"name\": \"Xu Mou\"\\n          },\\n          {\\n            \"authorId\": \"2155516032\",\\n            \"name\": \"Ying Wang\"\\n          },\\n          {\\n            \"authorId\": \"2150468905\",\\n            \"name\": \"Zeyuan Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2131541300\",\\n            \"name\": \"Muhammad Fiaz Bashir\"\\n          }\\n        ],\\n        \"abstract\": \"Financial news summarization (FNS) has been an attractive research problem in recent years, which aims to generate a shorter highlight of the news article while preserving key factual aspects, emotions, and opinions, providing significant assistance in stock trading and investment decision-making. However, FNS faces two challenges compared to the common domain. Firstly, financial news involves professional qualitative and quantitative information and salient content always scatters across long-range interactions. Secondly, financial news contains latent causal relationships, where historical information in the early generated sequence can significantly affect the subsequent decoding process. To address these difficulties, we propose an enhanced Seq2Seq model named TLGA, where the hierarchical Transformer-BiLSTM encoder can capture long-range interactions and sequential semantics while the Graph Attention-based decoder can fully utilize the historical information of decoded tokens and capture key causal relations. Moreover, we propose history-enhanced attention to concentrate on salient input content based on history semantics, guiding our decoder to generate the summary around the corresponding contents. It is also the first attempt to reuse history information of previously generated summary sequences in FNS using the idea of the Graph Attention Mechanism. Additionally, we construct the LCFNS dataset with 430,820 news-summary pairs for the lack of large-scale high-quality datasets in FNS. Experimental results on two financial datasets and two benchmark datasets indicate that our model outperforms other baselines.\"\\n      },\\n      {\\n        \"paperId\": \"2b5e411e664a521bcc223329562e14ece650b6bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2b5e411e664a521bcc223329562e14ece650b6bf\",\\n        \"title\": \"Mr. HiSum: A Large-scale Dataset for Video Highlight Detection and Summarization\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents Mr. HiSum, a large-scale dataset for video highlight detection and summarization, containing 31,892 videos and reliable labels aggregated over 50,000+ users per video, and empirically prove reliability of the labels as frame importance by cross-dataset transfer and user study.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2231980663\",\\n            \"name\": \"Jinhwan Sul\"\\n          },\\n          {\\n            \"authorId\": \"2288035283\",\\n            \"name\": \"Jihoon Han\"\\n          },\\n          {\\n            \"authorId\": \"2288343910\",\\n            \"name\": \"Joonseok Lee\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"17330c07a59b9dfb6cb01612c4b8be8841735396\",\\n        \"url\": \"https://www.semanticscholar.org/paper/17330c07a59b9dfb6cb01612c4b8be8841735396\",\\n        \"title\": \"Leveraging Summary Guidance on Medical Report Summarization\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.04001\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.04001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5878432\",\\n            \"name\": \"Yunqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1860612\",\\n            \"name\": \"Xuebing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2204743673\",\\n            \"name\": \"Yuanyuan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2108167911\",\\n            \"name\": \"Wensheng Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"This study presents three deidentified large medical text datasets, named DISCHARGE, ECHO and RADIOLOGY, which contain 50 K, 16 K and 378 K pairs of report and summary that are derived from MIMIC-III, respectively. We implement convincing baselines of automated abstractive summarization on the created datasets with pre-trained encoder-decoder language models, including BERT2BERT, BERTShare, RoBERTaShare, Pegasus, ProphetNet, T5-large, BART and GSUM. Further, based on the BART model, we leverage the sampled summaries from the training set as prior knowledge guidance, for encoding additional contextual representations of the guidance with the encoder and enhancing the decoding representations in the decoder. The experimental results confirm the improvement of ROUGE scores and BERTScore made by the proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"0ff8c04c8bdbf93b39b49582c9195cf3fc894d03\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0ff8c04c8bdbf93b39b49582c9195cf3fc894d03\",\\n        \"title\": \"Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)\",\\n        \"citationCount\": 154,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3597503.3639183\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.06815, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that adding semantic facts to the code in the prompt actually does help and this approach improves performance in several different settings suggested by prior work, including for three different Large Language Models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-04-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2214523312\",\\n            \"name\": \"K. Pai\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"1757975\",\\n            \"name\": \"Earl T. Barr\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLM) are a new class of computation engines, \\\\u201cprogrammed\\\\u201d via prompt engineering. Researchers are still learning how to best \\\\u201cprogram\\\\u201d these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of \\\\u201ccode analysis\\\\u201d and extracting such information, while processing code: but are they, really? If they aren\\'t, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM\\'s prompt with semantic facts explicitly. actually helps. Prior work shows that LLM performance on code summarization benefits from embedding a few code & summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU 11Scores of 30\\\\u201340 BLEU are considered \\\\u201cGood\\\\u201d to \\\\u201cUnderstandable\\\\u201d for natural language translation; see https://cloud.google.com/translate/automl/docs/evaluate.. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs\\' line completion performance.\"\\n      },\\n      {\\n        \"paperId\": \"c91b2c65da5e50df129b877da57ec07bb7f9c363\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c91b2c65da5e50df129b877da57ec07bb7f9c363\",\\n        \"title\": \"DragAPart: Learning a Part-Level Motion Prior for Articulated Objects\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.15382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces DragAPart, a method that, given an image and a set of drags as input, generates a new image of the same object that responds to the action of the drags, and demonstrates much better part-level motion understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2213346491\",\\n            \"name\": \"Ruining Li\"\\n          },\\n          {\\n            \"authorId\": \"2272947462\",\\n            \"name\": \"Chuanxia Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275252600\",\\n            \"name\": \"Christian Rupprecht\"\\n          },\\n          {\\n            \"authorId\": \"2258587368\",\\n            \"name\": \"Andrea Vedaldi\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce DragAPart, a method that, given an image and a set of drags as input, generates a new image of the same object that responds to the action of the drags. Differently from prior works that focused on repositioning objects, DragAPart predicts part-level interactions, such as opening and closing a drawer. We study this problem as a proxy for learning a generalist motion model, not restricted to a specific kinematic structure or object category. We start from a pre-trained image generator and fine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce. Combined with a new encoding for the drags and dataset randomization, the model generalizes well to real images and different categories. Compared to prior motion-controlled generators, we demonstrate much better part-level motion understanding.\"\\n      },\\n      {\\n        \"paperId\": \"eaa0078838310afb0e9a92e929e85a274d696dbb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eaa0078838310afb0e9a92e929e85a274d696dbb\",\\n        \"title\": \"G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.12383\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.12383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category, represents a first approach that allows jointly generating both hand and object.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9653518\",\\n            \"name\": \"Yufei Ye\"\\n          },\\n          {\\n            \"authorId\": \"2240431852\",\\n            \"name\": \"Abhinav Gupta\"\\n          },\\n          {\\n            \"authorId\": \"2297185992\",\\n            \"name\": \"Kris Kitani\"\\n          },\\n          {\\n            \"authorId\": \"2757335\",\\n            \"name\": \"Shubham Tulsiani\"\\n          }\\n        ],\\n        \"abstract\": \"We propose G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category. To learn a 3D spatial diffusion model that can capture this joint distribution, we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object. We show that this hand-object prior can then serve as generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis. We believe that our model, trained by aggregating seven diverse real-world interaction datasets spanning across 155 cate-gories, represents a first approach that allows jointly generating both hand and object. Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis, outperforming current task-specific baselines.\"\\n      },\\n      {\\n        \"paperId\": \"68571a8c3598888035c5c4f0a99793434ad61ef4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/68571a8c3598888035c5c4f0a99793434ad61ef4\",\\n        \"title\": \"LoRaLay: A Multilingual and Multimodal Dataset for Long Range and Layout-Aware Summarization\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.11312\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.11312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LoRaLay is presented, a collection of datasets for long-range summarization with accompanying visual/layout information and four novel datasets \\\\u2013 consistently built from scholar resources \\\\u2013 covering French, Spanish, Portuguese, and Korean languages are proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-01-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2150610138\",\\n            \"name\": \"Laura Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"90745780\",\\n            \"name\": \"Thomas Scialom\"\\n          },\\n          {\\n            \"authorId\": \"1703777\",\\n            \"name\": \"Benjamin Piwowarski\"\\n          },\\n          {\\n            \"authorId\": \"1767493\",\\n            \"name\": \"Jacopo Staiano\"\\n          }\\n        ],\\n        \"abstract\": \"Text Summarization is a popular task and an active area of research for the Natural Language Processing community. By definition, it requires to account for long input texts, a characteristic which poses computational challenges for neural models. Moreover, real-world documents come in a variety of complex, visually-rich, layouts. This information is of great relevance, whether to highlight salient content or to encode long-range interactions between textual passages. Yet, all publicly available summarization datasets only provide plain text content. To facilitate research on how to exploit visual/layout information to better capture long-range dependencies in summarization models, we present LoRaLay, a collection of datasets for long-range summarization with accompanying visual/layout information. We extend existing and popular English datasets (arXiv and PubMed) with layout information and propose four novel datasets \\\\u2013 consistently built from scholar resources \\\\u2013 covering French, Spanish, Portuguese, and Korean languages. Further, we propose new baselines merging layout-aware and long-range models \\\\u2013 two orthogonal approaches \\\\u2013 and obtain state-of-the-art results, showing the importance of combining both lines of research.\"\\n      },\\n      {\\n        \"paperId\": \"7ed835ecf9f3f6a222ec830e7d8ee40fd809dae5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7ed835ecf9f3f6a222ec830e7d8ee40fd809dae5\",\\n        \"title\": \"Context-aware Decoding Reduces Hallucination in Query-focused Summarization\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.14335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A large-scale reproducibility study on one recently proposed decoding method -- Context-aware Decoding (CAD), which improves QFS quality by reducing factuality errors/hallucinations while also at a cost of increased inference-time FLOPs and reduced decoding speed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-12-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284763197\",\\n            \"name\": \"Zhichao Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Query-focused summarization (QFS) aims to provide a summary of a single document/multi documents that can satisfy the information needs of a given query. It is useful for various real-world applications, such as abstractive snippet generation or more recent retrieval augmented generation (RAG). A prototypical QFS pipeline consists of a retriever (sparse or dense retrieval) and a generator (usually a large language model). However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs. There has been growing interest in developing new decoding methods to improve generation quality and reduce hallucination. In this work, we conduct a large-scale reproducibility study on one recently proposed decoding method -- Context-aware Decoding (CAD). In addition to replicating CAD\\'s experiments on news summarization datasets, we include experiments on QFS datasets, and conduct more rigorous analysis on computational complexity and hyperparameter sensitivity. Experiments with eight different language models show that performance-wise, CAD improves QFS quality by (1) reducing factuality errors/hallucinations while (2) mostly retaining the match of lexical patterns, measured by ROUGE scores, while also at a cost of increased inference-time FLOPs and reduced decoding speed. The code implementation based on Huggingface Library is made available https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs\"\\n      },\\n      {\\n        \"paperId\": \"4e444759dc61f26dee627fbb909a15a5ad32b184\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4e444759dc61f26dee627fbb909a15a5ad32b184\",\\n        \"title\": \"AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.09521\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.09521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AMRFact is proposed, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs) and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261278256\",\\n            \"name\": \"Haoyi Qiu\"\\n          },\\n          {\\n            \"authorId\": \"1956008001\",\\n            \"name\": \"Kung-Hsiang Huang\"\\n          },\\n          {\\n            \"authorId\": \"2266841207\",\\n            \"name\": \"Jingnong Qu\"\\n          },\\n          {\\n            \"authorId\": \"2261277442\",\\n            \"name\": \"Nanyun Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate our approach significantly outperforms previous systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating factuality of abstractive summarization.\"\\n      },\\n      {\\n        \"paperId\": \"143b5ef2b4b8f80923e0b46a09dc6b3fac7e3575\",\\n        \"url\": \"https://www.semanticscholar.org/paper/143b5ef2b4b8f80923e0b46a09dc6b3fac7e3575\",\\n        \"title\": \"A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.08503\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.08503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work enforces hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task, and outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152867918\",\\n            \"name\": \"Chenhui Shen\"\\n          },\\n          {\\n            \"authorId\": \"123962152\",\\n            \"name\": \"Liying Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2147330214\",\\n            \"name\": \"Yang You\"\\n          },\\n          {\\n            \"authorId\": \"1996394\",\\n            \"name\": \"Lidong Bing\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained language models (PLMs) have achieved outstanding achievements in abstractive single-document summarization (SDS). However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex. Previous works either design new MDS architectures or apply PLMs bluntly with concatenated source documents as a reformulated SDS task. While the former does not utilize previous pre-training efforts and may not generalize well across different domains, the latter may not sufficiently attend to the intricate cross-document relationships unique to MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task. Across 10 MDS benchmarks from various domains, our method outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters. It outperforms its corresponding PLM backbone by up to 3 Rouge-L and is favored by humans.\"\\n      },\\n      {\\n        \"paperId\": \"12f881588e6b461b86b34209c30a2bdfe67f904d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/12f881588e6b461b86b34209c30a2bdfe67f904d\",\\n        \"title\": \"Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.14981\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.14981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that using a contrastive learning framework with the refined candidate summaries leads to significant gains on both factuality and similarity-based metrics, and a ranking strategy in which it effectively combine two metrics, thereby preventing any conflict during training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2126503480\",\\n            \"name\": \"Tanay Dixit\"\\n          },\\n          {\\n            \"authorId\": \"47939052\",\\n            \"name\": \"Fei Wang\"\\n          },\\n          {\\n            \"authorId\": \"1998918\",\\n            \"name\": \"Muhao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose {pasted macro \\\\u2018MODEL\\\\u2019}name (i.e. Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.\"\\n      },\\n      {\\n        \"paperId\": \"c014d501abdeb23c6c1c5075e66989a72d3f6f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c014d501abdeb23c6c1c5075e66989a72d3f6f69\",\\n        \"title\": \"Generic protein\\\\u2013ligand interaction scoring by integrating physical prior knowledge and data augmentation modelling\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s42256-024-00849-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s42256-024-00849-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EquiScore, which utilizes a heterogeneous graph neural network to integrate physical prior knowledge and characterize protein\\\\u2013ligand interactions in equivariant geometric space, is introduced and is shown to have robust capabilities when applied to unseen protein targets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2220518044\",\\n            \"name\": \"Duanhua Cao\"\\n          },\\n          {\\n            \"authorId\": \"2175243828\",\\n            \"name\": \"Geng Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305221348\",\\n            \"name\": \"Jiaxin Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2155427582\",\\n            \"name\": \"Jie Yu\"\\n          },\\n          {\\n            \"authorId\": \"2219042185\",\\n            \"name\": \"Runze Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2276034718\",\\n            \"name\": \"Mingan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2256596397\",\\n            \"name\": \"Wei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2109298404\",\\n            \"name\": \"Lifan Chen\"\\n          },\\n          {\\n            \"authorId\": \"51135036\",\\n            \"name\": \"Feisheng Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2220532589\",\\n            \"name\": \"Yingying Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2220589745\",\\n            \"name\": \"Chenghao Lu\"\\n          },\\n          {\\n            \"authorId\": \"47056625\",\\n            \"name\": \"Xutong Li\"\\n          },\\n          {\\n            \"authorId\": \"2261477527\",\\n            \"name\": \"Xiaomin Luo\"\\n          },\\n          {\\n            \"authorId\": \"2145313647\",\\n            \"name\": \"Sulin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2114135568\",\\n            \"name\": \"Mingyue Zheng\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"ee1ef7b70dc34adcc90c42cc28168165ea56501f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ee1ef7b70dc34adcc90c42cc28168165ea56501f\",\\n        \"title\": \"SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization\",\\n        \"citationCount\": 435,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00453/1987014/tacl_a_00453.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2111.09525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work revisits the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-11-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46180754\",\\n            \"name\": \"Philippe Laban\"\\n          },\\n          {\\n            \"authorId\": \"48839531\",\\n            \"name\": \"Tobias Schnabel\"\\n          },\\n          {\\n            \"authorId\": \"144609235\",\\n            \"name\": \"Paul N. Bennett\"\\n          },\\n          {\\n            \"authorId\": \"1716902\",\\n            \"name\": \"Marti A. Hearst\"\\n          }\\n        ],\\n        \"abstract\": \"In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work.\"\\n      },\\n      {\\n        \"paperId\": \"1e2b6cd7247d9066c97a5f7befa5c4f890075f7b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e2b6cd7247d9066c97a5f7befa5c4f890075f7b\",\\n        \"title\": \"Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2407.12192\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.12192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A feature-oriented workflow for systematic prompt evaluation of summarization prompts is introduced and found that it helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and has the potential to generalize to other NLG and image-generation tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-07-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2289789049\",\\n            \"name\": \"Sam Yu-Te Lee\"\\n          },\\n          {\\n            \"authorId\": \"2311699054\",\\n            \"name\": \"Aryaman Bahukhandi\"\\n          },\\n          {\\n            \"authorId\": \"2312133175\",\\n            \"name\": \"Dongyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2239167039\",\\n            \"name\": \"Kwan-Liu Ma\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.\"\\n      },\\n      {\\n        \"paperId\": \"1b95053af03b5a06809a4967c6cf5ca137bbcde4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b95053af03b5a06809a4967c6cf5ca137bbcde4\",\\n        \"title\": \"Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.09177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a first multi-turn approach that leverages benign preliminary questions to interact with the LLM, and focuses on a new attack form, called Contextual Interaction Attack, which is black-box and can also transfer across LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282098821\",\\n            \"name\": \"Yixin Cheng\"\\n          },\\n          {\\n            \"authorId\": \"34291068\",\\n            \"name\": \"Markos Georgopoulos\"\\n          },\\n          {\\n            \"authorId\": \"1678641\",\\n            \"name\": \"V. Cevher\"\\n          },\\n          {\\n            \"authorId\": \"2140285545\",\\n            \"name\": \"Grigorios G. Chrysos\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired from Chomsky\\'s transformational-generative grammar theory and human practices of indirect context to elicit harmful information, we focus on a new attack form, called Contextual Interaction Attack. We contend that the prior context\\\\\\\\u2014the information preceding the attack query\\\\\\\\u2014plays a pivotal role in enabling strong Jailbreaking attacks. Specifically, we propose a first multi-turn approach that leverages benign preliminary questions to interact with the LLM. Due to the autoregressive nature of LLMs, which use previous conversation rounds as context during generation, we guide the model\\'s question-response pair to construct a context that is semantically aligned with the attack query to execute the attack. We conduct experiments on seven different LLMs and demonstrate the efficacy of this attack, which is black-box and can also transfer across LLMs. We believe this can lead to further developments and understanding of security in LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"ab36a9ced3ade6ca6e48055b4579c4ee8bed52f5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ab36a9ced3ade6ca6e48055b4579c4ee8bed52f5\",\\n        \"title\": \"InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion\",\\n        \"citationCount\": 162,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2308.16905\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.16905, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes InterDiff, a framework comprising two key steps: interaction diffusion, where a diffusion model is leverage to encode the distribution of future human-object interactions; and interaction correction, where a physics-informed predictor is introduced to correct denoised HOIs in a diffusion step.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8775961\",\\n            \"name\": \"Sirui Xu\"\\n          },\\n          {\\n            \"authorId\": \"2221242078\",\\n            \"name\": \"Zhengyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2236683644\",\\n            \"name\": \"Yu-Xiong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2587808\",\\n            \"name\": \"Liangyan Gui\"\\n          }\\n        ],\\n        \"abstract\": \"This paper addresses a novel task of anticipating 3D human-object interactions (HOIs). Most existing research on HOI synthesis lacks comprehensive whole-body interactions with dynamic objects, e.g., often limited to manipulating small or static objects. Our task is significantly more challenging, as it requires modeling dynamic objects with various shapes, capturing whole-body motion, and ensuring physically valid interactions. To this end, we propose InterDiff, a framework comprising two key steps: (i) interaction diffusion, where we leverage a diffusion model to encode the distribution of future human-object interactions; (ii) interaction correction, where we introduce a physics-informed predictor to correct denoised HOIs in a diffusion step. Our key insight is to inject prior knowledge that the interactions under reference with respect to contact points follow a simple pattern and are easily predictable. Experiments on multiple human-object interaction datasets demonstrate the effectiveness of our method for this task, capable of producing realistic, vivid, and remarkably longterm 3D HOI predictions.\"\\n      },\\n      {\\n        \"paperId\": \"59982844ae41696577f3fe4c396a8569f7d0fbd3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/59982844ae41696577f3fe4c396a8569f7d0fbd3\",\\n        \"title\": \"Calibration of Large Language Models on Code Summarization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151266129\",\\n            \"name\": \"Yuvraj Virk\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          }\\n        ],\\n        \"abstract\": \"A brief, fluent, and relevant summary can be helpful during program comprehension; however, such a summary does require significant human effort to produce. Often, good summaries are unavailable in software projects, which makes maintenance more difficult. There has been a considerable body of research into automated AI-based methods, using Large Language models (LLMs), to generate summaries of code; there also has been quite a bit of work on ways to measure the performance of such summarization methods, with special attention paid to how closely these AI-generated summaries resemble a summary a human might have produced. Measures such as BERTScore and BLEU have been suggested and evaluated with human-subject studies. However, LLM-generated summaries can be inaccurate, incomplete, etc: generally, too dissimilar to one that a good developer might write. Given an LLM-generated code summary, how can a user rationally judge if a summary is sufficiently good and reliable? Given just some input source code, and an LLM-generated summary, existing approaches can help judge brevity, fluency and relevance of the summary; however, it\\\\u2019s difficult to gauge whether an LLM-generated summary sufficiently resembles what a human might produce, without a \\\\u201cgolden\\\\u201d human-produced summary to compare against. Prior research indicates that human-produced summaries are generally preferred by human-raters, so we explore this issue in this paper. We study this resemblance question as a calibration problem: given just the code & the summary from an LLM, can we compute a confidence measure, that provides a reliable indication of whether the summary sufficiently resembles what a human would have produced in this situation? We examine this question using several LLMs, for several languages, and in several different settings. Our investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code.\"\\n      },\\n      {\\n        \"paperId\": \"c4960e3cd29f0af3c9120a4173bf681ed2131814\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c4960e3cd29f0af3c9120a4173bf681ed2131814\",\\n        \"title\": \"FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations\",\\n        \"citationCount\": 63,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2204.06508\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.06508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"FactGraph is proposed, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation and improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-04-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"10430740\",\\n            \"name\": \"Leonardo F. R. Ribeiro\"\\n          }\\n        ],\\n        \"abstract\": \"Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.\"\\n      },\\n      {\\n        \"paperId\": \"3e9b4218f6b943de8e57fe70f903911db11d24b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e9b4218f6b943de8e57fe70f903911db11d24b6\",\\n        \"title\": \"Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling\",\\n        \"citationCount\": 53,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.12378\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.12378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes to generate hard, representative synthetic examples of non-factual summaries through infilling language models to improve factual consistency inAbstractive summarization models and shows that this approach vastly outperforms prior methods in correcting erroneous summaries.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"143820870\",\\n            \"name\": \"Vidhisha Balachandran\"\\n          },\\n          {\\n            \"authorId\": \"2548384\",\\n            \"name\": \"Hannaneh Hajishirzi\"\\n          },\\n          {\\n            \"authorId\": \"2058480371\",\\n            \"name\": \"W. Cohen\"\\n          },\\n          {\\n            \"authorId\": \"2073587169\",\\n            \"name\": \"Yulia Tsvetkov\"\\n          }\\n        ],\\n        \"abstract\": \"Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets\\\\u2014 CNN/DM and XSum\\\\u2014we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model\\\\u2014FactEdit\\\\u2014improves factuality scores by over ~11 points on CNN/DM and over ~31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality.\"\\n      },\\n      {\\n        \"paperId\": \"4746b30f271a4acc472930dde079aeffa2e47367\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4746b30f271a4acc472930dde079aeffa2e47367\",\\n        \"title\": \"Evolution of coordination in pairwise and multi-player interactions via prior commitments\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://journals.sagepub.com/doi/pdf/10.1177/1059712321993166\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2009.11727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Whether prior commitment would be a viable evolutionary mechanism for enhancing coordination and the overall population social welfare strongly depends on the collective benefit and severity of competition, and more importantly, how asymmetric benefits are resolved in a commitment deal.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2164337059\",\\n            \"name\": \"Ndidi Bianca Ogbo\"\\n          },\\n          {\\n            \"authorId\": \"12078870\",\\n            \"name\": \"A. Elragig\"\\n          },\\n          {\\n            \"authorId\": \"2244073417\",\\n            \"name\": \"The Anh Han\"\\n          }\\n        ],\\n        \"abstract\": \"Upon starting a collective endeavour, it is important to understand your partners\\\\u2019 preferences and how strongly they commit to a common goal. Establishing a prior commitment or agreement in terms of posterior benefits and consequences from those engaging in it provides an important mechanism for securing cooperation. Resorting to methods from Evolutionary Game Theory (EGT), here we analyse how prior commitments can also be adopted as a tool for enhancing coordination when its outcomes exhibit an asymmetric payoff structure, in both pairwise and multi-party interactions. Arguably, coordination is more complex to achieve than cooperation since there might be several desirable collective outcomes in a coordination problem (compared to mutual cooperation, the only desirable collective outcome in cooperation dilemmas). Our analysis, both analytically and via numerical simulations, shows that whether prior commitment would be a viable evolutionary mechanism for enhancing coordination and the overall population social welfare strongly depends on the collective benefit and severity of competition, and more importantly, how asymmetric benefits are resolved in a commitment deal. Moreover, in multi-party interactions, prior commitments prove to be crucial when a high level of group diversity is required for optimal coordination. The results are robust for different selection intensities. Overall, our analysis provides new insights into the complexity and beauty of behavioural evolution driven by humans\\\\u2019 capacity for commitment, as well as for the design of self-organised and distributed multi-agent systems for ensuring coordination among autonomous agents.\"\\n      },\\n      {\\n        \"paperId\": \"903ce14de7226276a66e1bd3e3d3d97934d2dfd2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/903ce14de7226276a66e1bd3e3d3d97934d2dfd2\",\\n        \"title\": \"Mapping the Design Space of Human-AI Interaction in Text Summarization\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2206.14863\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2206.14863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work conducts a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions, and designed text summarization prototypes for each interaction.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2022-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"117397243\",\\n            \"name\": \"Ruijia Cheng\"\\n          },\\n          {\\n            \"authorId\": \"1405364873\",\\n            \"name\": \"Alison Smith-Renner\"\\n          },\\n          {\\n            \"authorId\": \"152645158\",\\n            \"name\": \"Kecheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1739099\",\\n            \"name\": \"Joel R. Tetreault\"\\n          },\\n          {\\n            \"authorId\": \"144633617\",\\n            \"name\": \"A. Jaimes\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans\\\\u2019 roles, experience, and needs when interacting with or being assisted by AI. From a human-centered perspective, we map the design opportunities and considerations for human-AI interaction in text summarization and broader text generation tasks. We first conducted a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions. We designed text summarization prototypes for each interaction. We then interviewed 16 users, aided by the prototypes, to understand their expectations, experience, and needs regarding efficiency, control, and trust with AI in text summarization and propose design considerations accordingly.\"\\n      },\\n      {\\n        \"paperId\": \"64c1f37f75d08902b460ff2c670293d903a74067\",\\n        \"url\": \"https://www.semanticscholar.org/paper/64c1f37f75d08902b460ff2c670293d903a74067\",\\n        \"title\": \"Expanding the coverage of regulons from high-confidence prior knowledge for accurate estimation of transcription factor activities\",\\n        \"citationCount\": 146,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://academic.oup.com/nar/advance-article-pdf/doi/10.1093/nar/gkad841/52174356/gkad841.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10639077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that the signed CollecTRI-derived regulons outperform other public collections of regulatory interactions in accurately inferring changes in TF activities in perturbation experiments and enable the accurate and comprehensive estimation of TF activities and thereby help to interpret transcriptomics data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2139664698\",\\n            \"name\": \"S. M\\\\u00fcller-Dott\"\\n          },\\n          {\\n            \"authorId\": \"1397165866\",\\n            \"name\": \"Eirini Tsirvouli\"\\n          },\\n          {\\n            \"authorId\": \"145019636\",\\n            \"name\": \"M. V\\\\u00e1zquez\"\\n          },\\n          {\\n            \"authorId\": \"1716214602\",\\n            \"name\": \"R. O. Ramirez Flores\"\\n          },\\n          {\\n            \"authorId\": \"1820842454\",\\n            \"name\": \"Pau Badia-i-Mompel\"\\n          },\\n          {\\n            \"authorId\": \"2213560713\",\\n            \"name\": \"Robin Fallegger\"\\n          },\\n          {\\n            \"authorId\": \"3124875\",\\n            \"name\": \"A. L\\\\u00e6greid\"\\n          },\\n          {\\n            \"authorId\": \"1400885451\",\\n            \"name\": \"J. Saez-Rodriguez\"\\n          }\\n        ],\\n        \"abstract\": \"Gene regulation plays a critical role in the cellular processes that underlie human health and disease. The regulatory relationship between transcription factors (TFs), key regulators of gene expression, and their target genes, the so called TF regulons, can be coupled with computational algorithms to estimate the activity of TFs. However, to interpret these findings accurately, regulons of high reliability and coverage are needed. In this study, we present and evaluate a collection of regulons created using the CollecTRI meta-resource containing signed TF-gene interactions for 1,183 TFs. In this context, we introduce a workflow to integrate information from multiple resources and assign the sign of regulation to TF-gene interactions that could be applied to other comprehensive knowledge bases. We find that the signed CollecTRI-derived regulons outperform other public collections of regulatory interactions in accurately inferring changes in TF activities in perturbation experiments. Furthermore, we showcase the value of the regulons by investigating hallmarks of TF activity profiles inferred from the transcriptomes of three different cancer types. Overall, the CollecTRI-derived TF regulons enable the accurate and comprehensive estimation of TF activities and thereby help to interpret transcriptomics data. GRAPHICAL ABSTRACT\"\\n      },\\n      {\\n        \"paperId\": \"ae1e48a74cb2f313e8e99c82f0aa4487b0805002\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ae1e48a74cb2f313e8e99c82f0aa4487b0805002\",\\n        \"title\": \"Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2204.13761\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.13761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that by utilizing an external knowledge base, one can improve the faithfulness of summaries without simply making them more extractive, and additionally, that external knowledge bases linked from the source can benefit the factuality of generated summaries.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-04-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49265991\",\\n            \"name\": \"Yue Dong\"\\n          },\\n          {\\n            \"authorId\": \"1771118\",\\n            \"name\": \"J. Wieting\"\\n          },\\n          {\\n            \"authorId\": \"2986975\",\\n            \"name\": \"Pat Verga\"\\n          }\\n        ],\\n        \"abstract\": \"Despite recent advances in abstractive summarization, current summarization systems still suffer from content hallucinations where models generate text that is either irrelevant or contradictory to the source document. However, prior work has been predicated on the assumption that any generated facts not appearing explicitly in the source are undesired hallucinations. Methods have been proposed to address this scenario by ultimately improving `faithfulness\\' to the source document, but in reality, there is a large portion of entities in the gold reference targets that are not directly in the source. In this work, we show that these entities are not aberrations, but they instead require utilizing external world knowledge to infer reasoning paths from entities in the source. We show that by utilizing an external knowledge base, we can improve the faithfulness of summaries without simply making them more extractive, and additionally, we show that external knowledge bases linked from the source can benefit the factuality of generated summaries.\"\\n      },\\n      {\\n        \"paperId\": \"81c83519b924bffca465f5a40183c62e711c0a69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/81c83519b924bffca465f5a40183c62e711c0a69\",\\n        \"title\": \"Toward Expanding the Scope of Radiology Report Summarization to Multiple Anatomies and Modalities\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.acl-short.41.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.08584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A dataset involving three new modalities and seven new anatomies based on the MIMIC-III and MIMic-CXR datasets is proposed and the performance of models both within and across modality-anatomy pairs in MIM IC-RRS is evaluated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-11-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35935570\",\\n            \"name\": \"Jean-Benoit Delbrouck\"\\n          },\\n          {\\n            \"authorId\": \"145859954\",\\n            \"name\": \"M. Varma\"\\n          },\\n          {\\n            \"authorId\": \"1584627064\",\\n            \"name\": \"C. Langlotz\"\\n          }\\n        ],\\n        \"abstract\": \"Radiology report summarization (RRS) is a growing area of research. Given the Findings section of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. However, RRS currently faces essential limitations.First, many prior studies conduct experiments on private datasets, preventing reproduction of results and fair comparisons across different systems and solutions. Second, most prior approaches are evaluated solely on chest X-rays. To address these limitations, we propose a dataset (MIMIC-RRS) involving three new modalities and seven new anatomies based on the MIMIC-III and MIMIC-CXR datasets. We then conduct extensive experiments to evaluate the performance of models both within and across modality-anatomy pairs in MIMIC-RRS. In addition, we evaluate their clinical efficacy via RadGraph, a factual correctness metric.\"\\n      },\\n      {\\n        \"paperId\": \"cbc05849bd04058e4aee09738dd73d9ad6621964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cbc05849bd04058e4aee09738dd73d9ad6621964\",\\n        \"title\": \"Dual-Prior Augmented Decoding Network for Long Tail Distribution in HOI Detection\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/27949/27918\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i3.27949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i3.27949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a dual-prior augmented decoding network by decomposing the HOI task into two sub-tasks: human-object pair detection and interaction recognition and leverages external knowledge to enhance the model\\'s ability at a finer granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293390597\",\\n            \"name\": \"Jiayi Gao\"\\n          },\\n          {\\n            \"authorId\": \"2582309\",\\n            \"name\": \"Kongming Liang\"\\n          },\\n          {\\n            \"authorId\": \"2240049431\",\\n            \"name\": \"Tao Wei\"\\n          },\\n          {\\n            \"authorId\": \"2256715874\",\\n            \"name\": \"Wei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268678016\",\\n            \"name\": \"Zhanyu Ma\"\\n          },\\n          {\\n            \"authorId\": \"2273915241\",\\n            \"name\": \"Jun Guo\"\\n          }\\n        ],\\n        \"abstract\": \"Human object interaction detection aims at localizing human-object pairs and recognizing their interactions. Trapped by the long-tailed distribution of the data, existing HOI detection methods often have difficulty recognizing the tail categories. Many approaches try to improve the recognition of HOI tasks by utilizing external knowledge (e.g. pre-trained visual-language models). However, these approaches mainly utilize external knowledge at the HOI combination level and achieve limited improvement in the tail categories. In this paper, we propose a dual-prior augmented decoding network by decomposing the HOI task into two sub-tasks: human-object pair detection and interaction recognition. For each subtask, we leverage external knowledge to enhance the model\\'s ability at a finer granularity. Specifically, we acquire the prior candidates from an external classifier and embed them to assist the subsequent decoding process. Thus, the long-tail problem is mitigated from a coarse-to-fine level with the corresponding external knowledge. Our approach outperforms existing state-of-the-art models in various settings and significantly boosts the performance on the tail HOI categories. The source code is available at https://github.com/PRIS-CV/DP-ADN.\"\\n      },\\n      {\\n        \"paperId\": \"18f86a1f9896795069d8bf7d5098e3debf2fce81\",\\n        \"url\": \"https://www.semanticscholar.org/paper/18f86a1f9896795069d8bf7d5098e3debf2fce81\",\\n        \"title\": \"Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2203.08257\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.08257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work designs a two-step approach: extractive summarization followed by abstractive summarizations of radiology report summarization, and additionally breaks down the extractive part into two independent tasks: extraction of salient sentences and keywords.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-03-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34112357\",\\n            \"name\": \"Sanjeev Kumar Karn\"\\n          },\\n          {\\n            \"authorId\": \"47695637\",\\n            \"name\": \"Ning Liu\"\\n          },\\n          {\\n            \"authorId\": \"144418438\",\\n            \"name\": \"Hinrich Sch\\\\u00fctze\"\\n          },\\n          {\\n            \"authorId\": \"2211973\",\\n            \"name\": \"Oladimeji Farri\"\\n          }\\n        ],\\n        \"abstract\": \"The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist\\\\u2019s reasoning and conclusions, and it also aids the referring physician in confirming or excluding certain diagnoses. A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report. These tasks include acquisition of salient content from the report and generation of a concise, easily consumable IMPRESSIONS section. Prior research on radiology report summarization has focused on single-step end-to-end models \\\\u2013 which subsume the task of salient content acquisition. To fully explore the cascade structure and explainability of radiology report summarization, we introduce two innovations. First, we design a two-step approach: extractive summarization followed by abstractive summarization. Second, we additionally break down the extractive part into two independent tasks: extraction of salient (1) sentences and (2) keywords. Experiments on a publicly available radiology report dataset show our novel approach leads to a more precise summary compared to single-step and to two-step-with-single-extractive-process baselines with an overall improvement in F1 score of 3-4%.\"\\n      },\\n      {\\n        \"paperId\": \"a9dc25d11fadca961b762c173f5b27a445f645d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a9dc25d11fadca961b762c173f5b27a445f645d7\",\\n        \"title\": \"SARS-CoV-2 biology and host interactions\",\\n        \"citationCount\": 117,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s41579-023-01003-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s41579-023-01003-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An updated summary of the SARS-CoV-2 life cycle, gene function and virus\\\\u2013host interactions, including recent landmark findings on general aspects of coronavirus biology and newly discovered host factors necessary for virus replication are provided.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40397706\",\\n            \"name\": \"Silvio Steiner\"\\n          },\\n          {\\n            \"authorId\": \"1580760257\",\\n            \"name\": \"A. Kratzel\"\\n          },\\n          {\\n            \"authorId\": \"2322491040\",\\n            \"name\": \"G. Barut\"\\n          },\\n          {\\n            \"authorId\": \"2054345681\",\\n            \"name\": \"Reto M. Lang\"\\n          },\\n          {\\n            \"authorId\": \"1398526410\",\\n            \"name\": \"Etori Aguiar Moreira\"\\n          },\\n          {\\n            \"authorId\": \"2007902383\",\\n            \"name\": \"Lisa Thomann\"\\n          },\\n          {\\n            \"authorId\": \"152841375\",\\n            \"name\": \"Jenna N. Kelly\"\\n          },\\n          {\\n            \"authorId\": \"2262194481\",\\n            \"name\": \"Volker Thiel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"e9bc2a727d54ad514cfcacae01f65a6a4f0d0159\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e9bc2a727d54ad514cfcacae01f65a6a4f0d0159\",\\n        \"title\": \"Faithful Abstractive Summarization via Fact-aware Consistency-constrained Transformer\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3511808.3557319?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3511808.3557319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel fact-aware abstractive summarization model, named Entity-Relation Pointer Generator Network (ERPGN), which attempts to formalize the facts in original document as a factual knowledge graph, and then generates the high-quality summary via directly modeling consistency between summary and the factualknowledge graph.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2187857206\",\\n            \"name\": \"Yuanjie Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2118043140\",\\n            \"name\": \"Chen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"50383766\",\\n            \"name\": \"Tong Xu\"\\n          },\\n          {\\n            \"authorId\": \"1422036168\",\\n            \"name\": \"Zikai Yin\"\\n          },\\n          {\\n            \"authorId\": \"2173129111\",\\n            \"name\": \"Enhong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Abstractive summarization is a classic task in Natural Language Generation (NLG), which aims to produce a concise summary of the original document. Recently, great efforts have been made on sequence-to-sequence neural networks to generate abstractive sum- maries with a high level of fluency. However, prior arts mainly focus on the optimization of token-level likelihood, while the rich semantic information in documents has been largely ignored. In this way, the summarization results could be vulnerable to hallucinations, i.e., the semantic-level inconsistency between a summary and corresponding original document. To deal with this challenge, in this paper, we propose a novel fact-aware abstractive summarization model, named Entity-Relation Pointer Generator Network (ERPGN). Specially, we attempt to formalize the facts in original document as a factual knowledge graph, and then generate the high-quality summary via directly modeling consistency between summary and the factual knowledge graph. To that end, we first leverage two pointer net- work structures to capture the fact in original documents. Then, to enhance the traditional token-level likelihood loss, we design two extra semantic-level losses to measure the disagreement between a summary and facts from its original document. Extensive experi- ments on public datasets demonstrate that our ERPGN framework could outperform both classic abstractive summarization models and the state-of-the-art fact-aware baseline methods, with significant improvement in terms of faithfulness.\"\\n      },\\n      {\\n        \"paperId\": \"ac652b7aee8ba769a698f55c61a25e1f1aa270d9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac652b7aee8ba769a698f55c61a25e1f1aa270d9\",\\n        \"title\": \"Answer Summarization for Technical Queries: Benchmark and New Approach\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3551349.3560421\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.10868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A high-quality benchmark to enable automatic evaluation of answer summarization for the technical queries for SQA sites is constructed and it is demonstrated that automatic evaluation has a much lower cost and can uncover findings similar to the ones found through user studies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-09-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2154172271\",\\n            \"name\": \"Chengran Yang\"\\n          },\\n          {\\n            \"authorId\": \"2203459\",\\n            \"name\": \"Bowen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2121315\",\\n            \"name\": \"Ferdian Thung\"\\n          },\\n          {\\n            \"authorId\": \"2185772392\",\\n            \"name\": \"Yucen Shi\"\\n          },\\n          {\\n            \"authorId\": \"2146322053\",\\n            \"name\": \"Ting Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2139059234\",\\n            \"name\": \"Zhou Yang\"\\n          },\\n          {\\n            \"authorId\": \"2148928671\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"151496516\",\\n            \"name\": \"Jieke Shi\"\\n          },\\n          {\\n            \"authorId\": \"2158107537\",\\n            \"name\": \"Junda He\"\\n          },\\n          {\\n            \"authorId\": \"7883212\",\\n            \"name\": \"Donggyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2150912791\",\\n            \"name\": \"David Lo\"\\n          }\\n        ],\\n        \"abstract\": \"Prior studies have demonstrated that approaches to generate an answer summary for a given technical query in Software Question and Answer (SQA) sites are desired. We find that existing approaches are assessed solely through user studies. Hence, a new user study needs to be performed every time a new approach is introduced; this is time-consuming, slows down the development of the new approach, and results from different user studies may not be comparable to each other. There is a need for a benchmark with ground truth summaries as a complement assessment through user studies. Unfortunately, such a benchmark is non-existent for answer summarization for technical queries from SQA sites. To fill the gap, we manually construct a high-quality benchmark to enable automatic evaluation of answer summarization for the technical queries for SQA sites. It contains 111 query-summary pairs extracted from 382 Stack Overflow answers with 2,014 sentence candidates. Using the benchmark, we comprehensively evaluate the performance of existing approaches and find that there is still a big room for improvements. Motivated by the results, we propose a new approach TechSumBot with three key modules:1) Usefulness Ranking module; 2) Centrality Estimation module; and 3) Redundancy Removal module. We evaluate TechSumBot in both automatic (i.e., using our benchmark) and manual (i.e., via a user study) manners. The results from both evaluations consistently demonstrate that TechSumBot outperforms the best performing baseline approaches from both SE and NLP domains by a large margin, i.e., 10.83%\\\\u201314.90%, 32.75%\\\\u201336.59%, and 12.61%\\\\u201317.54%, in terms of ROUGE-1, ROUGE-2, and ROUGE-L on automatic evaluation, and 5.79%\\\\u20139.23% and 17.03%\\\\u201317.68%, in terms of average usefulness and diversity score on human evaluation. This highlights that automatic evaluation on our benchmark can uncover findings similar to the ones found through user studies. More importantly, the automatic evaluation has a much lower cost, especially when it is used to assess a new approach. Additionally, we also conducted an ablation study, which demonstrates that each module in TechSumBot contributes to boosting the overall performance of TechSumBot. We release the benchmark as well as the replication package of our experiment at https://github.com/TechSumBot/TechSumBot.\"\\n      },\\n      {\\n        \"paperId\": \"e391740cd4b5865868b86770479860c3911f5844\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e391740cd4b5865868b86770479860c3911f5844\",\\n        \"title\": \"Static Video Summarization Using Video Coding Features with Frame-Level Temporal Subsampling and Deep Learning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2076-3417/13/10/6065/pdf?version=1684152118\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app13106065?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app13106065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper offers innovative approaches to the problem of key frame extraction for the purpose of video summarization, which includes the extraction of feature variables from the bit streams of coded videos, followed by optional stepwise regression for dimensionality reduction.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2217753873\",\\n            \"name\": \"Obada Issa\"\\n          },\\n          {\\n            \"authorId\": \"1805004\",\\n            \"name\": \"T. Shanableh\"\\n          }\\n        ],\\n        \"abstract\": \"There is an abundance of digital video content due to the cloud\\\\u2019s phenomenal growth and security footage; it is therefore essential to summarize these videos in data centers. This paper offers innovative approaches to the problem of key frame extraction for the purpose of video summarization. Our approach includes the extraction of feature variables from the bit streams of coded videos, followed by optional stepwise regression for dimensionality reduction. Once the features are extracted and their dimensionality is reduced, we apply innovative frame-level temporal subsampling techniques, followed by training and testing using deep learning architectures. The frame-level temporal subsampling techniques are based on cosine similarity and the PCA projections of feature vectors. We create three different learning architectures by utilizing LSTM networks, 1D-CNN networks, and random forests. The four most popular video summarization datasets, namely, TVSum, SumMe, OVP, and VSUMM, are used to evaluate the accuracy of the proposed solutions. This includes the precision, recall, F-score measures, and computational time. It is shown that the proposed solutions, when trained and tested on all subjective user summaries, achieved F-scores of 0.79, 0.74, 0.88, and 0.81, respectively, for the aforementioned datasets, showing clear improvements over prior studies.\"\\n      },\\n      {\\n        \"paperId\": \"7bb09bedd2d356667dc8aa286b62425c0246cd60\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7bb09bedd2d356667dc8aa286b62425c0246cd60\",\\n        \"title\": \"CAVES: A Dataset to facilitate Explainable Classification and Summarization of Concerns towards COVID Vaccines\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.13746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CAVES is the first large-scale dataset containing about 10k COVID-19 anti-vaccine tweets labelled into various specific anti- Vaccine concerns in a multi-label setting and is also the first multi- label classification dataset that provides explanations for each of the labels.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-04-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1388367454\",\\n            \"name\": \"Soham Poddar\"\\n          },\\n          {\\n            \"authorId\": \"2163948982\",\\n            \"name\": \"Azlaan Mustafa Samad\"\\n          },\\n          {\\n            \"authorId\": \"2182282\",\\n            \"name\": \"Rajdeep Mukherjee\"\\n          },\\n          {\\n            \"authorId\": \"4213990\",\\n            \"name\": \"Niloy Ganguly\"\\n          },\\n          {\\n            \"authorId\": \"143841814\",\\n            \"name\": \"Saptarshi Ghosh\"\\n          }\\n        ],\\n        \"abstract\": \"Convincing people to get vaccinated against COVID-19 is a key societal challenge in the present times. As a first step towards this goal, many prior works have relied on social media analysis to understand the specific concerns that people have towards these vaccines, such as potential side-effects, ineffectiveness, political factors, and so on. Though there are datasets that broadly classify social media posts into Anti-vax and Pro-Vax labels, there is no dataset (to our knowledge) that labels social media posts according to the specific anti-vaccine concerns mentioned in the posts. In this paper, we have curated CAVES, the first large-scale dataset containing about 10k COVID-19 anti-vaccine tweets labelled into various specific anti-vaccine concerns in a multi-label setting. This is also the first multi-label classification dataset that provides explanations for each of the labels. Additionally, the dataset also provides class-wise summaries of all the tweets. We also perform preliminary experiments on the dataset and show that this is a very challenging dataset for multi-label explainable classification and tweet summarization, as is evident by the moderate scores achieved by some state-of-the-art models.\"\\n      }\\n    ]\\n  },\\n  \"iterative prompt templating\": {\\n    \"total\": 1387,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"0f49fed824a915418b9dec3429673ed6d13c829c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f49fed824a915418b9dec3429673ed6d13c829c\",\\n        \"title\": \"Claude 2.0 large language model: Tackling a real-world classification problem with a new iterative prompt engineering approach\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1016/j.iswa.2024.200336\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.iswa.2024.200336?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.iswa.2024.200336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3053303\",\\n            \"name\": \"Loredana Caruccio\"\\n          },\\n          {\\n            \"authorId\": \"2053231377\",\\n            \"name\": \"Stefano Cirillo\"\\n          },\\n          {\\n            \"authorId\": \"1745736\",\\n            \"name\": \"G. Polese\"\\n          },\\n          {\\n            \"authorId\": \"1818311573\",\\n            \"name\": \"Giandomenico Solimando\"\\n          },\\n          {\\n            \"authorId\": \"2431234\",\\n            \"name\": \"Shanmugam Sundaramurthy\"\\n          },\\n          {\\n            \"authorId\": \"66533440\",\\n            \"name\": \"G. Tortora\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"e7476b944f8e8a88242bc3f7e3fa267c5709de97\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e7476b944f8e8a88242bc3f7e3fa267c5709de97\",\\n        \"title\": \"Iterative Prompt Learning for Unsupervised Backlit Image Enhancement\",\\n        \"citationCount\": 151,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.17569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel unsupervised backlit image enhancement method by exploring the potential of Contrastive Language-Image Pre-Training (CLIP) for pixel-level image enhancement, which outperforms state-of-the-art methods in terms of visual quality and generalization ability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1972230748\",\\n            \"name\": \"Zhexin Liang\"\\n          },\\n          {\\n            \"authorId\": \"2185949399\",\\n            \"name\": \"Chongyi Li\"\\n          },\\n          {\\n            \"authorId\": \"7523259\",\\n            \"name\": \"Shangchen Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2054599089\",\\n            \"name\": \"Ruicheng Feng\"\\n          },\\n          {\\n            \"authorId\": \"1717179\",\\n            \"name\": \"Chen Change Loy\"\\n          }\\n        ],\\n        \"abstract\": \"We propose a novel unsupervised backlit image enhancement method, abbreviated as CLIP-LIT, by exploring the potential of Contrastive Language-Image Pre-Training (CLIP) for pixel-level image enhancement. We show that the openworld CLIP prior not only aids in distinguishing between backlit and well-lit images, but also in perceiving heterogeneous regions with different luminance, facilitating the optimization of the enhancement network. Unlike high-level and image manipulation tasks, directly applying CLIP to enhancement tasks is non-trivial, owing to the difficulty in finding accurate prompts. To solve this issue, we devise a prompt learning framework that first learns an initial prompt pair by constraining the text-image similarity between the prompt (negative/positive sample) and the corresponding image (backlit image/well-lit image) in the CLIP latent space. Then, we train the enhancement network based on the textimage similarity between the enhanced result and the initial prompt pair. To further improve the accuracy of the initial prompt pair, we iteratively fine-tune the prompt learning framework to reduce the distribution gaps between the backlit images, enhanced results, and well-lit images via rank learning, boosting the enhancement performance. Our method alternates between updating the prompt learning framework and enhancement network until visually pleasing results are achieved. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in terms of visual quality and generalization ability, without requiring any paired data.\"\\n      },\\n      {\\n        \"paperId\": \"1ca54f9a79ffca3eeb187f81bad5c1a1df790c06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1ca54f9a79ffca3eeb187f81bad5c1a1df790c06\",\\n        \"title\": \"Iterative Prompt Refinement for Mining Gene Relationships from ChatGPT\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10769373\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10769373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study systematically evaluated ChatGPT\\\\u2019s capacity for predicting gene relationships using GPT-3.5-turbo and GPT-4, and introduced an innovative iterative prompt refinement technique that significantly enhances the performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-12-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265518204\",\\n            \"name\": \"Yibo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2276572987\",\\n            \"name\": \"Jeffrey Gao\"\\n          },\\n          {\\n            \"authorId\": \"2849198\",\\n            \"name\": \"Marius Petruc\"\\n          },\\n          {\\n            \"authorId\": \"2265390717\",\\n            \"name\": \"Richard D. Hammer\"\\n          },\\n          {\\n            \"authorId\": \"2072374137\",\\n            \"name\": \"Mihail Popescu\"\\n          },\\n          {\\n            \"authorId\": \"2318663840\",\\n            \"name\": \"Dong Xu\"\\n          }\\n        ],\\n        \"abstract\": \"ChatGPT has demonstrated its potential as a surrogate knowledge graph. Trained on extensive data sources, including open-access publications, peer-reviewed research articles and biomedical websites, ChatGPT extracted information on gene relationships and biological pathways. However, a major challenge is model hallucination, i.e., high false positive rates. To assess and address this challenge, we systematically evaluated ChatGPT\\\\u2019s capacity for predicting gene relationships using GPT-3.5-turbo and GPT-4. Benchmarking against the KEGG Pathway Database as the ground truth, we experimented with diverse prompting strategies, targeting gene relationships of activation, inhibition, and phosphorylation. We introduced an innovative iterative prompt refinement technique. By assessing prompt efficacy using metrics like F-1 score, precision, and recall, GPT-4 was re-engaged to suggest improved prompts. A refined prompt, which combines a specialized role with explanatory text, significantly enhances the performance. Going beyond pairwise gene relationships, we also deciphered complex gene interplays, such as gene interaction chains and pathways pertinent to diseases like non-small cell lung cancer. Direct prompts showed limited success, but \\\\u201cleast-to-most\\\\u201d prompting exhibited significant potentials for such network constructions. The methods in this study may be used for some other bioinformatics prediction problems.\"\\n      },\\n      {\\n        \"paperId\": \"1e39f3ca4aff09dccc3b951cdd355c7d8e7cbc2f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e39f3ca4aff09dccc3b951cdd355c7d8e7cbc2f\",\\n        \"title\": \"Generative AI tools in art education: Exploring prompt engineering and iterative processes for enhanced creativity\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.54517/m.v4i1.2164\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54517/m.v4i1.2164?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54517/m.v4i1.2164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI\\\\u2019s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"48965602\",\\n            \"name\": \"Peter Cotroneo\"\\n          },\\n          {\\n            \"authorId\": \"145099528\",\\n            \"name\": \"James Hutson\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid development and adoption of generative artificial intelligence (AI) tools in the art and design education landscape have introduced both opportunities and challenges. This timely study addresses the need to effectively integrate these tools into the classroom while considering ethical implications and the importance of prompt engineering. By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI\\\\u2019s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context. Students in the digital at case study were taught prompt engineering techniques and were tasked with crafting multiple prompts, focusing on refining their ideas over time. Participants demonstrated an increased understanding of the potential and limitations of generative AI tools and how to manipulate subject matter for more effective results. The iterative process encouraged students to explore and experiment with their creative ideas, leading to a deeper understanding of the possibilities offered by AI tools. Despite acknowledging the ethical concerns regarding copyright and the potential replacement of artists, students appreciated the value of generative AI tools for enhancing their sketchbooks and ideation process. Through prompt engineering and iterative processes, students developed a more detail-oriented approach to their work. The challenge of using AI-generated images as final products was conceptually intriguing, requiring further investigation and consideration of the prompts. This study highlights the potential benefits and challenges of integrating generative AI tools into art and design classrooms, emphasizing the importance of prompt engineering, iterative processes, and ethical considerations as these technologies continue to evolve.\"\\n      },\\n      {\\n        \"paperId\": \"0f4219a1a0f89c7364aabb27632cdcc833ab53b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f4219a1a0f89c7364aabb27632cdcc833ab53b7\",\\n        \"title\": \"Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.15843\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.15843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a bi-directional iterative prompt-tuning method for EAE, where the EAE task is treated as a cloze-style task to take full advantage of entity information and pre-trained language models (PLMs).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2072645045\",\\n            \"name\": \"Lu Dai\"\\n          },\\n          {\\n            \"authorId\": \"2156645305\",\\n            \"name\": \"Bang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2052727822\",\\n            \"name\": \"Wei Xiang\"\\n          },\\n          {\\n            \"authorId\": \"1683407\",\\n            \"name\": \"Yijun Mo\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, prompt-tuning has attracted growing interests in event argument extraction (EAE). However, the existing prompt-tuning methods have not achieved satisfactory performance due to the lack of consideration of entity information. In this paper, we propose a bi-directional iterative prompt-tuning method for EAE, where the EAE task is treated as a cloze-style task to take full advantage of entity information and pre-trained language models (PLMs). Furthermore, our method explores event argument interactions by introducing the argument roles of contextual entities into prompt construction. Since template and verbalizer are two crucial components in a cloze-style prompt, we propose to utilize the role label semantic knowledge to construct a semantic verbalizer and design three kind of templates for the EAE task. Experiments on the ACE 2005 English dataset with standard and low-resource settings show that the proposed method significantly outperforms the peer state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"title\": \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\\n        \"citationCount\": 88,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new approach to code generation by LLMs is proposed, which is called AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72729733\",\\n            \"name\": \"T. Ridnik\"\\n          },\\n          {\\n            \"authorId\": \"2279758170\",\\n            \"name\": \"Dedy Kredo\"\\n          },\\n          {\\n            \"authorId\": \"49668367\",\\n            \"name\": \"Itamar Friedman\"\\n          }\\n        ],\\n        \"abstract\": \"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\"\\n      },\\n      {\\n        \"paperId\": \"58f14bc6de4a005b439f38fe11db6e4f95e343bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58f14bc6de4a005b439f38fe11db6e4f95e343bf\",\\n        \"title\": \"Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers, and develops a capable Gradient-inspired LLM-based Prompt Optimizer called GPO.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2109887979\",\\n            \"name\": \"Xinyu Tang\"\\n          },\\n          {\\n            \"authorId\": \"72541556\",\\n            \"name\": \"Xiaolei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257376413\",\\n            \"name\": \"Wayne Xin Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2287879620\",\\n            \"name\": \"Siyuan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2273472188\",\\n            \"name\": \"Yaliang Li\"\\n          },\\n          {\\n            \"authorId\": \"2274218622\",\\n            \"name\": \"Ji-Rong Wen\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. By systematically analyzing a rich set of improvement strategies on the two aspects, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the optimization trajectory as the update direction. Then, it utilizes the generation-based refinement strategy to perform the update, while controlling the edit distance through a cosine-based decay strategy. Extensive experiments demonstrate the effectiveness and efficiency of GPO. In particular, GPO brings an additional improvement of up to 56.8% on Big-Bench Hard and 62.6% on MMLU compared to baseline methods.\"\\n      },\\n      {\\n        \"paperId\": \"015b1f127b6c31654e3597b75876eed8e445d866\",\\n        \"url\": \"https://www.semanticscholar.org/paper/015b1f127b6c31654e3597b75876eed8e445d866\",\\n        \"title\": \"PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.00596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"PhyT2V is presented, a new data-independent T2V technique that expands the current T2V model\\\\u2019s capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333357012\",\\n            \"name\": \"Qiyao Xue\"\\n          },\\n          {\\n            \"authorId\": \"2202556933\",\\n            \"name\": \"Xiangyu Yin\"\\n          },\\n          {\\n            \"authorId\": \"2119657961\",\\n            \"name\": \"Boyuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2274008610\",\\n            \"name\": \"Wei Gao\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation has been recently enabled by transformer-based diffusion models, but current T2V models lack capabilities in adhering to the real-world common knowledge and physical rules, due to their limited understanding of physical realism and deficiency in temporal modeling. Existing solutions are either data-driven or require extra model inputs, but cannot be generalizable to out-of-distribution domains. In this paper, we present PhyT2V, a new data-independent T2V technique that expands the current T2V model\\\\u2019s capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting. Our experiments show that PhyT2V improves existing T2V models\\\\u2019 adherence to real-world physical rules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.\"\\n      },\\n      {\\n        \"paperId\": \"2ed64d90670177bf58cdce6bda04a48a8731a18f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2ed64d90670177bf58cdce6bda04a48a8731a18f\",\\n        \"title\": \"ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing\",\\n        \"citationCount\": 160,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642016\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.09128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs, and identifies three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243191212\",\\n            \"name\": \"Ian Arawjo\"\\n          },\\n          {\\n            \"authorId\": \"40964240\",\\n            \"name\": \"Chelse Swoopes\"\\n          },\\n          {\\n            \"authorId\": \"40961780\",\\n            \"name\": \"Priyan Vaithilingam\"\\n          },\\n          {\\n            \"authorId\": \"2237803620\",\\n            \"name\": \"Martin Wattenberg\"\\n          },\\n          {\\n            \"authorId\": \"143730651\",\\n            \"name\": \"Elena L. Glassman\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating outputs of large language models (LLMs) is challenging, requiring making\\\\u2014and making sense of\\\\u2014many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.\"\\n      },\\n      {\\n        \"paperId\": \"26bdde27ff70384ef7c43f2670d8ec6559c71f45\",\\n        \"url\": \"https://www.semanticscholar.org/paper/26bdde27ff70384ef7c43f2670d8ec6559c71f45\",\\n        \"title\": \"Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.00507, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Examining and comparing prompt chaining and stepwise prompt in the context of text summarization to ascertain which method stands out as the most effective has the potential to be extrapolated to other applications.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256995981\",\\n            \"name\": \"Shichao Sun\"\\n          },\\n          {\\n            \"authorId\": \"2273661632\",\\n            \"name\": \"Ruifeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2314396\",\\n            \"name\": \"Ziqiang Cao\"\\n          },\\n          {\\n            \"authorId\": \"2237591981\",\\n            \"name\": \"Wenjie Li\"\\n          },\\n          {\\n            \"authorId\": \"2256991660\",\\n            \"name\": \"Pengfei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have demonstrated the capacity to improve summary quality by mirroring a human-like iterative process of critique and refinement starting from the initial draft. Two strategies are designed to perform this iterative process: Prompt Chaining and Stepwise Prompt. Prompt chaining orchestrates the drafting, critiquing, and refining phases through a series of three discrete prompts, while Stepwise prompt integrates these phases within a single prompt. However, the relative effectiveness of the two methods has not been extensively studied. This paper is dedicated to examining and comparing these two methods in the context of text summarization to ascertain which method stands out as the most effective. Experimental results show that the prompt chaining method can produce a more favorable outcome. This might be because stepwise prompt might produce a simulated refinement process according to our various experiments. Since refinement is adaptable to diverse tasks, our conclusions have the potential to be extrapolated to other applications, thereby offering insights that may contribute to the broader development of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"a02482f81aa2651848ba060766f850fa3261b42a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a02482f81aa2651848ba060766f850fa3261b42a\",\\n        \"title\": \"Prompt engineering on leveraging large language models in generating response to InBasket messages\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1093/jamia/ocae172\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11413421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Informed by clinician and patient feedback synergistically, tuning in LLM prompt alone can be effective in creating clinically relevant and useful draft responses to PMARs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2312683202\",\\n            \"name\": \"Sherry Yan\"\\n          },\\n          {\\n            \"authorId\": \"2312112991\",\\n            \"name\": \"Wendi Knapp\"\\n          },\\n          {\\n            \"authorId\": \"2312112445\",\\n            \"name\": \"Andrew Leong\"\\n          },\\n          {\\n            \"authorId\": \"2312112555\",\\n            \"name\": \"Sarira Kadkhodazadeh\"\\n          },\\n          {\\n            \"authorId\": \"2312177380\",\\n            \"name\": \"Souvik Das\"\\n          },\\n          {\\n            \"authorId\": \"2312113058\",\\n            \"name\": \"Veena G Jones\"\\n          },\\n          {\\n            \"authorId\": \"2312408017\",\\n            \"name\": \"Robert Clark\"\\n          },\\n          {\\n            \"authorId\": \"2312112645\",\\n            \"name\": \"David Grattendick\"\\n          },\\n          {\\n            \"authorId\": \"2312166178\",\\n            \"name\": \"Kevin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2312113594\",\\n            \"name\": \"Lisa Hladik\"\\n          },\\n          {\\n            \"authorId\": \"2312112709\",\\n            \"name\": \"Lawrence Fagan\"\\n          },\\n          {\\n            \"authorId\": \"2312111094\",\\n            \"name\": \"Albert Chan\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Objectives Large Language Models (LLMs) have been proposed as a solution to address high volumes of Patient Medical Advice Requests (PMARs). This study addresses whether LLMs can generate high quality draft responses to PMARs that satisfies both patients and clinicians with prompt engineering. Materials and Methods We designed a novel human-involved iterative processes to train and validate prompts to LLM in creating appropriate responses to PMARs. GPT-4 was used to generate response to the messages. We updated the prompts, and evaluated both clinician and patient acceptance of LLM-generated draft responses at each iteration, and tested the optimized prompt on independent validation data sets. The optimized prompt was implemented in the electronic health record production environment and tested by 69 primary care clinicians. Results After 3 iterations of prompt engineering, physician acceptance of draft suitability increased from 62% to 84% (P\\\\u2009<.001) in the validation dataset (N\\\\u2009=\\\\u2009200), and 74% of drafts in the test dataset were rated as \\\\u201chelpful.\\\\u201d Patients also noted significantly increased favorability of message tone (78%) and overall quality (80%) for the optimized prompt compared to the original prompt in the training dataset, patients were unable to differentiate human and LLM-generated draft PMAR responses for 76% of the messages, in contrast to the earlier preference for human-generated responses. Majority (72%) of clinicians believed it can reduce cognitive load in dealing with InBasket messages. Discussion and Conclusion Informed by clinician and patient feedback synergistically, tuning in LLM prompt alone can be effective in creating clinically relevant and useful draft responses to PMARs.\"\\n      },\\n      {\\n        \"paperId\": \"c8e1dc10f302fb5cd748672d5d10d95a27c3bd0b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c8e1dc10f302fb5cd748672d5d10d95a27c3bd0b\",\\n        \"title\": \"Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.08995, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Self-prompt tuning is proposed, making LLMs themselves generate role-play prompts through fine-tuning, thereby automating complex prompting strategies and illustrating the great potential of utilizing fine-tuning to enable LLMs to self-prompt, thereby automating complex prompting strategies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216561498\",\\n            \"name\": \"Aobo Kong\"\\n          },\\n          {\\n            \"authorId\": \"2275765829\",\\n            \"name\": \"Shiwan Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2242179099\",\\n            \"name\": \"Hao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2311304336\",\\n            \"name\": \"Qicheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2275736194\",\\n            \"name\": \"Yong Qin\"\\n          },\\n          {\\n            \"authorId\": \"2216621163\",\\n            \"name\": \"Ruiqi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2311736771\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2275780366\",\\n            \"name\": \"Jiaming Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2275584632\",\\n            \"name\": \"Haoqin Sun\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in LLMs have showcased their remarkable role-playing capabilities, able to accurately simulate the dialogue styles and cognitive processes of various roles based on different instructions and contexts. Studies indicate that assigning LLMs the roles of experts, a strategy known as role-play prompting, can enhance their performance in the corresponding domains. However, the prompt needs to be manually designed for the given problem, requiring certain expertise and iterative modifications. To this end, we propose self-prompt tuning, making LLMs themselves generate role-play prompts through fine-tuning. Leveraging the LIMA dataset as our foundational corpus, we employ GPT-4 to annotate role-play prompts for each data points, resulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like Llama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned LLMs can automatically generate expert role prompts for any given question. We extensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and open-ended question test. Our empirical results illustrate that self-prompt tuned LLMs outperform standard instruction tuned baselines across most datasets. This highlights the great potential of utilizing fine-tuning to enable LLMs to self-prompt, thereby automating complex prompting strategies. We release the dataset, models, and code at this \\\\\\\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.\"\\n      },\\n      {\\n        \"paperId\": \"b503050327ba9e01d27dd8d06589544f89f40cbd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b503050327ba9e01d27dd8d06589544f89f40cbd\",\\n        \"title\": \"LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.16929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Inspired by structured reusable programming languages, LangGPT, a dual-layer prompt design framework as the programming language for LLMs is proposed, which has an easy-to-learn normative structure and provides an extended structure for migration and reuse.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-02-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2249764692\",\\n            \"name\": \"Ming Wang\"\\n          },\\n          {\\n            \"authorId\": \"2288139031\",\\n            \"name\": \"Yuanzhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2287879683\",\\n            \"name\": \"Xiaoming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2287801318\",\\n            \"name\": \"Songlian Li\"\\n          },\\n          {\\n            \"authorId\": \"2287879297\",\\n            \"name\": \"Yijie Huang\"\\n          },\\n          {\\n            \"authorId\": \"2287880544\",\\n            \"name\": \"Chi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2111226672\",\\n            \"name\": \"Daling Wang\"\\n          },\\n          {\\n            \"authorId\": \"2087586948\",\\n            \"name\": \"Shi Feng\"\\n          },\\n          {\\n            \"authorId\": \"2287871665\",\\n            \"name\": \"Jigang Li\"\\n          }\\n        ],\\n        \"abstract\": \"LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. In addition, it is not conducive to the iterative updating of prompts. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the performance of LLMs. Moreover, the case study shows that LangGPT leads LLMs to generate higher-quality responses. Furthermore, we analyzed the ease of use and reusability of LangGPT through a user survey in our online community.\"\\n      },\\n      {\\n        \"paperId\": \"0b8e991c334bfaa5d8ede08bb0fdbe446622931a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b8e991c334bfaa5d8ede08bb0fdbe446622931a\",\\n        \"title\": \"Leveraging enhanced egret swarm optimization algorithm and artificial intelligence-driven prompt strategies for portfolio selection\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11535209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A structured prompt framework specifically designed for stock selection, aiming to provide direct and interpretable stock-selecting tools for investors of various levels and a novel algorithm that combines the Nonlinear-Activated Beetle Antennae Search strategy with the Egret Swarm Optimization Algorithm to address the Mean-Variance Portfolio Selection problem with Transaction Costs and Cardinality Constraints.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2329413534\",\\n            \"name\": \"Zhendai Huang\"\\n          },\\n          {\\n            \"authorId\": \"2294153042\",\\n            \"name\": \"Zhen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2181171694\",\\n            \"name\": \"Cheng Hua\"\\n          },\\n          {\\n            \"authorId\": \"2329153233\",\\n            \"name\": \"Bolin Liao\"\\n          },\\n          {\\n            \"authorId\": \"2133437125\",\\n            \"name\": \"Shuai Li\"\\n          }\\n        ],\\n        \"abstract\": \"In the financial field, constructing efficient investment portfolios is a focal point of research, encompassing asset selection and optimization of asset allocation. With the advancements in Large Language Models (LLMs), generative Artificial Intelligence (AI) tools have showcased capabilities never seen before. However, the black-box nature of these tools renders their outputs difficult to interpret directly, often necessitating iterative fine-tuning to align with users\\\\u2019 expected outcomes. This study presents a structured prompt framework specifically designed for stock selection, aiming to provide direct and interpretable stock-selecting tools for investors of various levels. By creating representative scenarios and combining them into different cases for experimentation, we can explore how the construction of prompts influences the responses generated by generative AI tools. Additionally, this paper proposes a novel algorithm that combines the Nonlinear-Activated Beetle Antennae Search strategy with the Egret Swarm Optimization Algorithm (NBESOA) to address the Mean-Variance Portfolio Selection problem with Transaction Costs and Cardinality Constraints (MVPS-TCCC), utilizing real stock market data to construct portfolios based on generative AI tools recommendations. Simulation results indicate that, compared to other algorithms, NBESOA prefers optimizing portfolio configurations to achieve the highest Sharpe Ratio with the strictest constraints, bringing the outcomes closer to the portfolio\\\\u2019s efficient frontier.\"\\n      },\\n      {\\n        \"paperId\": \"168ce6b5844f8cd4a3375f69952ccf630874689d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/168ce6b5844f8cd4a3375f69952ccf630874689d\",\\n        \"title\": \"Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.11105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method called Source Prompt Disentangled Inversion (SPDInv), which aims at reducing the impact of source prompt, thereby enhancing the text-driven image editing performance by employing diffusion models and can easily adapt customized image generation models to localized editing tasks and produce promising performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284415331\",\\n            \"name\": \"Rui Li\"\\n          },\\n          {\\n            \"authorId\": \"151482572\",\\n            \"name\": \"Ruihuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2279757730\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"2277508910\",\\n            \"name\": \"Lei Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-driven diffusion models have significantly advanced the image editing performance by using text prompts as inputs. One crucial step in text-driven image editing is to invert the original image into a latent noise code conditioned on the source prompt. While previous methods have achieved promising results by refactoring the image synthesizing process, the inverted latent noise code is tightly coupled with the source prompt, limiting the image editability by target text prompts. To address this issue, we propose a novel method called Source Prompt Disentangled Inversion (SPDInv), which aims at reducing the impact of source prompt, thereby enhancing the text-driven image editing performance by employing diffusion models. To make the inverted noise code be independent of the given source prompt as much as possible, we indicate that the iterative inversion process should satisfy a fixed-point constraint. Consequently, we transform the inversion problem into a searching problem to find the fixed-point solution, and utilize the pre-trained diffusion models to facilitate the searching process. The experimental results show that our proposed SPDInv method can effectively mitigate the conflicts between the target editing prompt and the source prompt, leading to a significant decrease in editing artifacts. In addition to text-driven image editing, with SPDInv we can easily adapt customized image generation models to localized editing tasks and produce promising performance. The source code are available at https://github.com/leeruibin/SPDInv.\"\\n      },\\n      {\\n        \"paperId\": \"f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"title\": \"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.01701?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"a0a3d3349497782861ecc436c3807bfe1a7d90a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0a3d3349497782861ecc436c3807bfe1a7d90a1\",\\n        \"title\": \"Prompting Change: Exploring Prompt Engineering in Large Language Model AI and Its Potential to Transform Education\",\\n        \"citationCount\": 127,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11528-023-00896-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11528-023-00896-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The paper underscores how the natural language capabilities of LLM AI tools can help students and educators transition from passive recipients to active co-creators of their learning experiences, and charts the evolving trajectory of LLM AI as a tool poised to reshape educational practices and assumptions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260528529\",\\n            \"name\": \"William Cain\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"27af585e0e48741a18a042a7b7590f021483e151\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27af585e0e48741a18a042a7b7590f021483e151\",\\n        \"title\": \"Semantic-guided Prompt Organization for Universal Goal Hijacking against LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2405.14189?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2405.14189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A universal goal hijacking method called POUGH that incorporates semantic-guided prompt processing strategies and employs an iterative optimization algorithm to generate the universal fixed suffix for the prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142367654\",\\n            \"name\": \"Yihao Huang\"\\n          },\\n          {\\n            \"authorId\": \"2276510804\",\\n            \"name\": \"Chong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2275762052\",\\n            \"name\": \"Xiaojun Jia\"\\n          },\\n          {\\n            \"authorId\": \"2260611391\",\\n            \"name\": \"Qing Guo\"\\n          },\\n          {\\n            \"authorId\": \"2119277493\",\\n            \"name\": \"Felix Juefei-Xu\"\\n          },\\n          {\\n            \"authorId\": \"2265725720\",\\n            \"name\": \"Jian Zhang\"\\n          },\\n          {\\n            \"authorId\": \"4166714\",\\n            \"name\": \"G. Pu\"\\n          },\\n          {\\n            \"authorId\": \"2265753294\",\\n            \"name\": \"Yang Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"d5544571fa37a6eaac972153cec57df591c38a04\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d5544571fa37a6eaac972153cec57df591c38a04\",\\n        \"title\": \"MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.13191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MCQG-SRefine is proposed, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions, and introduces an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1576489304\",\\n            \"name\": \"Zonghai Yao\"\\n          },\\n          {\\n            \"authorId\": \"2326302479\",\\n            \"name\": \"Aditya Parashar\"\\n          },\\n          {\\n            \"authorId\": \"2261556443\",\\n            \"name\": \"Huixue Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2323786086\",\\n            \"name\": \"Won Seok Jang\"\\n          },\\n          {\\n            \"authorId\": \"2263773255\",\\n            \"name\": \"Feiyun Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2322987613\",\\n            \"name\": \"Zhichao Yang\"\\n          },\\n          {\\n            \"authorId\": \"2261455807\",\\n            \"name\": \"Hong Yu\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty. To address these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions. By integrating expert-driven prompt engineering with iterative self-critique and self-correction feedback, MCQG-SRefine significantly enhances human expert satisfaction regarding both the quality and difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.\"\\n      },\\n      {\\n        \"paperId\": \"e314d182fd9d35a05870b38a56ee38eb3149b47d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e314d182fd9d35a05870b38a56ee38eb3149b47d\",\\n        \"title\": \"Attack Prompt Generation for Red Teaming and Defending Large Language Models\",\\n        \"citationCount\": 81,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.12505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning to enhance their safety against red teaming attacks, and proposes a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260342358\",\\n            \"name\": \"Boyi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2117833732\",\\n            \"name\": \"Wenjie Wang\"\\n          },\\n          {\\n            \"authorId\": \"2163400298\",\\n            \"name\": \"Fuli Feng\"\\n          },\\n          {\\n            \"authorId\": \"2367303636\",\\n            \"name\": \"Yang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2260433198\",\\n            \"name\": \"Qifan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2239071206\",\\n            \"name\": \"Xiangnan He\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facilitating the safety evaluation and enhancement of more LLMs. Our code and dataset is available on https://github.com/Aatrox103/SAP .\"\\n      },\\n      {\\n        \"paperId\": \"848909fbae167f21589bfc7a54fbf27e306b883c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/848909fbae167f21589bfc7a54fbf27e306b883c\",\\n        \"title\": \"An Iterative Optimizing Framework for Radiology Report Summarization With ChatGPT\",\\n        \"citationCount\": 123,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2304.08448\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.08448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed ImpressionGPT model achieves superior performance of AIG task on both the Medical Information Mart for Intensive Care - Chest X-ray database and Open Access Biomedical Image Search Engine datasets without requiring additional training data or fine-tuning the LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2132543537\",\\n            \"name\": \"Chong Ma\"\\n          },\\n          {\\n            \"authorId\": \"47039788\",\\n            \"name\": \"Zihao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2110238778\",\\n            \"name\": \"Jiaqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2211904452\",\\n            \"name\": \"Shaochen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2163675860\",\\n            \"name\": \"Yaonai Wei\"\\n          },\\n          {\\n            \"authorId\": \"2145977326\",\\n            \"name\": \"Zheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"143621713\",\\n            \"name\": \"Lei Guo\"\\n          },\\n          {\\n            \"authorId\": \"2174965546\",\\n            \"name\": \"Xiaoya Cai\"\\n          },\\n          {\\n            \"authorId\": \"2108086798\",\\n            \"name\": \"Shu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"49104946\",\\n            \"name\": \"Tuo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2181182\",\\n            \"name\": \"Dajiang Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2150038187\",\\n            \"name\": \"Dinggang Shen\"\\n          },\\n          {\\n            \"authorId\": \"2115345993\",\\n            \"name\": \"Tianming Liu\"\\n          },\\n          {\\n            \"authorId\": \"2144438902\",\\n            \"name\": \"Xiang Li\"\\n          }\\n        ],\\n        \"abstract\": \"The \\\\u201cImpression\\\\u201d section of a radiology report is a critical basis for communication between radiologists and other physicians. Typically written by radiologists, this part is derived from the \\\\u201cFindings\\\\u201d section, which can be laborious and error-prone. Although deep-learning-based models, such as bidirectional encoder representation from transformers (BERT), have achieved promising results in automatic impression generation (AIG), such models often require substantial amounts of medical data and have poor generalization performance. Recently, large language models (LLMs) like Chat Generative Pre-trained Transformer (ChatGPT) have shown strong generalization capabilities and performance, but their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, leveraging the contextual learning capabilities of LLMs through our dynamic prompt and iterative optimization algorithm to accomplish the AIG task. ImpressionGPT initially employs a small amount of domain-specific data to create a dynamic prompt, extracting contextual semantic information closely related to the test data. Subsequently, the iterative optimization algorithm automatically evaluates the output of LLMs and provides optimization suggestions, continuously refining the output results. The proposed ImpressionGPT model achieves superior performance of AIG task on both the Medical Information Mart for Intensive Care - Chest X-ray database (MIMIC-CXR) and Open Access Biomedical Image Search Engine (OpenI) datasets without requiring additional training data or fine-tuning the LLMs. This work presents a paradigm for localizing LLMs that can be applied in a wide range of similar application scenarios, bridging the gap between general-purpose LLMs and the specific language processing needs of various domains.\"\\n      },\\n      {\\n        \"paperId\": \"2d77b7203824e617206634277bce7eec2b71a2bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d77b7203824e617206634277bce7eec2b71a2bd\",\\n        \"title\": \"Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.20701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is revealed that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans, and a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE) is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105515515\",\\n            \"name\": \"Pengwei Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2294783971\",\\n            \"name\": \"Zhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2149324430\",\\n            \"name\": \"Qian Tan\"\\n          },\\n          {\\n            \"authorId\": \"2304366173\",\\n            \"name\": \"Jie Song\"\\n          },\\n          {\\n            \"authorId\": \"2304326919\",\\n            \"name\": \"Ru Xie\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.\"\\n      },\\n      {\\n        \"paperId\": \"6c7a1bcaa69dd29cb5fa89ac4a9ec61621c211fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6c7a1bcaa69dd29cb5fa89ac4a9ec61621c211fb\",\\n        \"title\": \"SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.06426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research highlights the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse, and introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2330190651\",\\n            \"name\": \"Bijoy Ahmed Saiem\"\\n          },\\n          {\\n            \"authorId\": \"2330191650\",\\n            \"name\": \"MD Sadik Hossain Shanto\"\\n          },\\n          {\\n            \"authorId\": \"2330191231\",\\n            \"name\": \"Rakib Ahsan\"\\n          },\\n          {\\n            \"authorId\": \"2150249269\",\\n            \"name\": \"Md. Rafi Ur Rashid\"\\n          }\\n        ],\\n        \"abstract\": \"As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.\"\\n      },\\n      {\\n        \"paperId\": \"113873a4e58e2ff15ce3523ee9fb629ff6dddfe4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/113873a4e58e2ff15ce3523ee9fb629ff6dddfe4\",\\n        \"title\": \"Does Prompt Formatting Have Any Impact on LLM Performance?\",\\n        \"citationCount\": 116,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.10541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper examines the impact of different prompt templates on LLM performance, and highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2331365723\",\\n            \"name\": \"Jia He\"\\n          },\\n          {\\n            \"authorId\": \"35315626\",\\n            \"name\": \"Mukund Rungta\"\\n          },\\n          {\\n            \"authorId\": \"2120266368\",\\n            \"name\": \"David Koleczek\"\\n          },\\n          {\\n            \"authorId\": \"5968525\",\\n            \"name\": \"Arshdeep Sekhon\"\\n          },\\n          {\\n            \"authorId\": \"2331375861\",\\n            \"name\": \"Franklin X Wang\"\\n          },\\n          {\\n            \"authorId\": \"2322137989\",\\n            \"name\": \"Sadid A. Hasan\"\\n          }\\n        ],\\n        \"abstract\": \"In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI\\'s GPT models. Experiments show that GPT-3.5-turbo\\'s performance varies by up to 40\\\\\\\\% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.\"\\n      },\\n      {\\n        \"paperId\": \"3f8d2977a7753f9a3b0b5d18e6bee6afb0028ed8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f8d2977a7753f9a3b0b5d18e6bee6afb0028ed8\",\\n        \"title\": \"PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs\",\\n        \"citationCount\": 164,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel visual prompting approach for VLMs that is called Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering, and finds that it enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3457048\",\\n            \"name\": \"Soroush Nasiriany\"\\n          },\\n          {\\n            \"authorId\": \"2267320085\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2265397442\",\\n            \"name\": \"Wenhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"9961095\",\\n            \"name\": \"Ted Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2271707306\",\\n            \"name\": \"Jacky Liang\"\\n          },\\n          {\\n            \"authorId\": \"2263548023\",\\n            \"name\": \"Ishita Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"14484808\",\\n            \"name\": \"Annie Xie\"\\n          },\\n          {\\n            \"authorId\": \"2283848260\",\\n            \"name\": \"Danny Driess\"\\n          },\\n          {\\n            \"authorId\": \"88728227\",\\n            \"name\": \"Ayzaan Wahid\"\\n          },\\n          {\\n            \"authorId\": \"2265456732\",\\n            \"name\": \"Zhuo Xu\"\\n          },\\n          {\\n            \"authorId\": \"144579461\",\\n            \"name\": \"Q. Vuong\"\\n          },\\n          {\\n            \"authorId\": \"2240715659\",\\n            \"name\": \"Tingnan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2110662889\",\\n            \"name\": \"T. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2145145412\",\\n            \"name\": \"Kuang-Huei Lee\"\\n          },\\n          {\\n            \"authorId\": \"2153917744\",\\n            \"name\": \"Peng Xu\"\\n          },\\n          {\\n            \"authorId\": \"51881277\",\\n            \"name\": \"Sean Kirmani\"\\n          },\\n          {\\n            \"authorId\": \"2253507326\",\\n            \"name\": \"Yuke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2267246149\",\\n            \"name\": \"Andy Zeng\"\\n          },\\n          {\\n            \"authorId\": \"1944801\",\\n            \"name\": \"Karol Hausman\"\\n          },\\n          {\\n            \"authorId\": \"2801204\",\\n            \"name\": \"N. Heess\"\\n          },\\n          {\\n            \"authorId\": \"2257346440\",\\n            \"name\": \"Chelsea Finn\"\\n          },\\n          {\\n            \"authorId\": \"2257062067\",\\n            \"name\": \"Sergey Levine\"\\n          },\\n          {\\n            \"authorId\": \"2704814\",\\n            \"name\": \"Brian Ichter\"\\n          }\\n        ],\\n        \"abstract\": \"Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.\"\\n      },\\n      {\\n        \"paperId\": \"b9b93b2946ed13f33f3dd75254fff0ca4fd4e651\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b9b93b2946ed13f33f3dd75254fff0ca4fd4e651\",\\n        \"title\": \"Unsupervised Human Activity Recognition Via Large Language Models and Iterative Evolution\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICASSP48485.2024.10446819?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICASSP48485.2024.10446819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292733359\",\\n            \"name\": \"Jiayuan Gao\"\\n          },\\n          {\\n            \"authorId\": \"2145025105\",\\n            \"name\": \"Yingwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2243383757\",\\n            \"name\": \"Yiqiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"19264986\",\\n            \"name\": \"Tengxiang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2292488219\",\\n            \"name\": \"Boshi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2214851453\",\\n            \"name\": \"Xiaoyu Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Human activity recognition (HAR) is crucial for health monitoring and disease diagnosis in Internet-of-Things environments. However, existing HAR approaches either suffer from poor accuracy or achieve high accuracy at the expense of costly manual annotations. To overcome the challenge above, we propose a novel method named LLMIE-UHAR that that leverages LLMs and Iterative Evolution to realize Unsupervised HAR. Specifically, with our designed prompt engineering mechanism, we employ large language models to fuse both contextual and semantic information, and annotate key samples selected by a clustering algorithm. Moreover, LLMIE-UHAR enhances the recognition accuracy with iterative evolution of clustering algorithm, large language models and the neural network based recognition model. Experiments conducted on the public ARAS datasets show the efficiency of our method, achieving an accuracy of 96.00%. This highlights the practical value of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6\",\\n        \"title\": \"Ignore Previous Prompt: Attack Techniques For Language Models\",\\n        \"citationCount\": 575,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2211.09527\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.09527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates two types of attacks -- goal hijacking and prompt leaking -- and demonstrates that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3\\'s stochastic nature, creating long-tail risks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-11-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2191070702\",\\n            \"name\": \"F\\\\u00e1bio Perez\"\\n          },\\n          {\\n            \"authorId\": \"2191076673\",\\n            \"name\": \"Ian Ribeiro\"\\n          }\\n        ],\\n        \"abstract\": \"Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3\\'s stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.\"\\n      },\\n      {\\n        \"paperId\": \"e4503969a4cc5c6a1930a2845109221c3f544e64\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e4503969a4cc5c6a1930a2845109221c3f544e64\",\\n        \"title\": \"De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks via Iterative Grounding\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, a technique that grounds the predictions of an LLM through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt through a novel combination of retrieving suitable API references and iteratively querying the model.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-01-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) trained on datasets of publicly available source code have established a new state of the art in code generation tasks. However, these models are mostly unaware of the code that exists within a specific project, preventing the models from making good use of existing APIs. Instead, LLMs often invent, or\\\\\"hallucinate\\\\\", non-existent APIs or produce variants of already existing code. This paper presents De-Hallucinator, a technique that grounds the predictions of an LLM through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt. The approach exploits the observation that predictions by LLMs often resemble the desired code, but they fail to correctly refer to already existing APIs. De-Hallucinator automatically identifies project-specific API references related to the model\\'s initial predictions and adds these references into the prompt. Unlike retrieval-augmented generation (RAG), our approach uses the initial prediction(s) by the model to iteratively retrieve increasingly suitable API references. Our evaluation applies the approach to two tasks: predicting API usages in Python and generating tests in JavaScript. We show that De-Hallucinator consistently improves the generated code across five LLMs. In particular, the approach improves the edit distance by 23.3-50.6% and the recall of correctly predicted API usages by 23.9-61.0% for code completion, and improves the number of fixed tests that initially failed because of hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage for test generation.\"\\n      },\\n      {\\n        \"paperId\": \"7eaaa5aec72ffb95cf538462ed205fa4d4faef9c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7eaaa5aec72ffb95cf538462ed205fa4d4faef9c\",\\n        \"title\": \"Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.09993\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.09993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reprompting, an iterative sampling algorithm that automatically learns the Chain-of-Thought recipes for a given task without human intervention, achieves consistently better performance than the state-of-the-art prompt optimization and decoding algorithms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47210642\",\\n            \"name\": \"Weijia Xu\"\\n          },\\n          {\\n            \"authorId\": \"2168558581\",\\n            \"name\": \"Andrzej Banburski-Fahey\"\\n          },\\n          {\\n            \"authorId\": \"1698689\",\\n            \"name\": \"N. Jojic\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce Reprompting, an iterative sampling algorithm that automatically learns the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, Reprompting infers the CoT recipes that work consistently well for a set of training samples by iteratively sampling new recipes using previously sampled recipes as parent prompts to solve other training problems. We conduct extensive experiments on 20 challenging reasoning tasks. Results show that Reprompting outperforms human-written CoT prompts substantially by +9.4 points on average. It also achieves consistently better performance than the state-of-the-art prompt optimization and decoding algorithms.\"\\n      },\\n      {\\n        \"paperId\": \"f53a4f34757d1f237446b4d887d5323f2a17ed02\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f53a4f34757d1f237446b4d887d5323f2a17ed02\",\\n        \"title\": \"PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2308.12033\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.12033, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations of prompt ensemble and builds a feedback mechanism for reflecting on the inadequacies of existing weak learners.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2313691609\",\\n            \"name\": \"Chenrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2166025432\",\\n            \"name\": \"Lina Liu\"\\n          },\\n          {\\n            \"authorId\": \"48093158\",\\n            \"name\": \"Jinpeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260827558\",\\n            \"name\": \"Chuyuan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2168803085\",\\n            \"name\": \"Xiaodi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2109800642\",\\n            \"name\": \"Hongyu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2149926941\",\\n            \"name\": \"Mingchen Cai\"\\n          }\\n        ],\\n        \"abstract\": \"As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1\",\\n        \"title\": \"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.04749\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.04749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment is explored and it is found that the assertion level alignment scores provide a useful feedback which can be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2112713578\",\\n            \"name\": \"Jaskirat Singh\"\\n          },\\n          {\\n            \"authorId\": \"144802394\",\\n            \"name\": \"Liang Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs. Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy. Project page for our paper is available at https://1jsingh.github.io/divide-evaluate-and-refine\"\\n      },\\n      {\\n        \"paperId\": \"2809a68abc9be8f1118ac2dea2af50e619b659c0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2809a68abc9be8f1118ac2dea2af50e619b659c0\",\\n        \"title\": \"DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2304.02827\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.02827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DITTO-NeRF is proposed, a novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a single image that outperforms state-of-the-art methods in terms of fidelity and diversity qualitatively and quantitatively with much faster training times than prior arts on image/text-to-3D such as DreamFusion, and NeuralLift-360.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"51245215\",\\n            \"name\": \"H. Seo\"\\n          },\\n          {\\n            \"authorId\": \"47298448\",\\n            \"name\": \"Hayeon Kim\"\\n          },\\n          {\\n            \"authorId\": \"2109334279\",\\n            \"name\": \"Gwanghyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"34971370\",\\n            \"name\": \"S. Chun\"\\n          }\\n        ],\\n        \"abstract\": \"The increasing demand for high-quality 3D content creation has motivated the development of automated methods for creating 3D object models from a single image and/or from a text prompt. However, the reconstructed 3D objects using state-of-the-art image-to-3D methods still exhibit low correspondence to the given image and low multi-view consistency. Recent state-of-the-art text-to-3D methods are also limited, yielding 3D samples with low diversity per prompt with long synthesis time. To address these challenges, we propose DITTO-NeRF, a novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a single image. Our DITTO-NeRF consists of constructing high-quality partial 3D object for limited in-boundary (IB) angles using the given or text-generated 2D image from the frontal view and then iteratively reconstructing the remaining 3D NeRF using inpainting latent diffusion model. We propose progressive 3D object reconstruction schemes in terms of scales (low to high resolution), angles (IB angles initially to outer-boundary (OB) later), and masks (object to background boundary) in our DITTO-NeRF so that high-quality information on IB can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods in terms of fidelity and diversity qualitatively and quantitatively with much faster training times than prior arts on image/text-to-3D such as DreamFusion, and NeuralLift-360.\"\\n      },\\n      {\\n        \"paperId\": \"9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2\",\\n        \"title\": \"Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates\",\\n        \"citationCount\": 80,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.18540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing\\'\\' (PTST) strategy -- fine-tune models without a safety prompt, but include it at test time.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41049476\",\\n            \"name\": \"Kaifeng Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2288581125\",\\n            \"name\": \"Haoyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2261386415\",\\n            \"name\": \"Xinran Gu\"\\n          },\\n          {\\n            \"authorId\": \"2141026731\",\\n            \"name\": \"Dingli Yu\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2261737783\",\\n            \"name\": \"Sanjeev Arora\"\\n          }\\n        ],\\n        \"abstract\": \"Public LLMs such as the Llama 2-Chat underwent alignment training and were considered safe. Recently Qi et al. [2024] reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment. Through extensive experiments on several chat models (Meta\\'s Llama 2-Chat, Mistral AI\\'s Mistral 7B Instruct v0.2, and OpenAI\\'s GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing\\'\\' (PTST) strategy -- fine-tune models without a safety prompt, but include it at test time. This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.\"\\n      },\\n      {\\n        \"paperId\": \"149ab16bed8554e71358b3642cb6979884c786b9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/149ab16bed8554e71358b3642cb6979884c786b9\",\\n        \"title\": \"Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.04031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Bi-Modal Adversarial Prompt Attack (BAP) is introduced, which executes jailbreaks by optimizing textual and visual prompts cohesively and significantly outperforms other methods by large margins.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2304955104\",\\n            \"name\": \"Zonghao Ying\"\\n          },\\n          {\\n            \"authorId\": \"2257572247\",\\n            \"name\": \"Aishan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2300719164\",\\n            \"name\": \"Tianyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305061690\",\\n            \"name\": \"Zhengmin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2114786732\",\\n            \"name\": \"Siyuan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2237942988\",\\n            \"name\": \"Xianglong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2237906923\",\\n            \"name\": \"Dacheng Tao\"\\n          }\\n        ],\\n        \"abstract\": \"In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation. To address this limitation, this paper introduces the Bi-Modal Adversarial Prompt Attack (BAP), which executes jailbreaks by optimizing textual and visual prompts cohesively. Initially, we adversarially embed universally adversarial perturbations in an image, guided by a few-shot query-agnostic corpus (e.g., affirmative prefixes and negative inhibitions). This process ensures that the adversarial image prompt LVLMs to respond positively to harmful queries. Subsequently, leveraging the image, we optimize textual prompts with specific harmful intent. In particular, we utilize a large language model to analyze jailbreak failures and employ chain-of-thought reasoning to refine textual prompts through a feedback-iteration manner. To validate the efficacy of our approach, we conducted extensive evaluations on various datasets and LVLMs, demonstrating that our BAP significantly outperforms other methods by large margins (+29.03% in attack success rate on average). Additionally, we showcase the potential of our attacks on black-box commercial LVLMs, such as GPT-4o and Gemini. Our code is available at https://anonymous.4open.science/r/BAP-Jailbreak-Vision-Language-Models-via-Bi-Modal-Adversarial-Prompt-5496\"\\n      },\\n      {\\n        \"paperId\": \"1ecd677e3d12bffec77d23db696610a7744cf33f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1ecd677e3d12bffec77d23db696610a7744cf33f\",\\n        \"title\": \"WorldSmith: Iterative and Expressive Prompting for World Building with a Generative AI\",\\n        \"citationCount\": 38,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2308.13355\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.13355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates the use of recent multi-modal image generation systems to enable users iteratively visualize and modify elements of their fictional world using a combination of text input, sketching, and region-based filling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2767073\",\\n            \"name\": \"Hai Dang\"\\n          },\\n          {\\n            \"authorId\": \"2408148\",\\n            \"name\": \"Frederik Brudy\"\\n          },\\n          {\\n            \"authorId\": \"1703735\",\\n            \"name\": \"G. Fitzmaurice\"\\n          },\\n          {\\n            \"authorId\": \"2408187\",\\n            \"name\": \"Fraser Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Crafting a rich and unique environment is crucial for fictional world-building, but can be difficult to achieve since illustrating a world from scratch requires time and significant skill. We investigate the use of recent multi-modal image generation systems to enable users iteratively visualize and modify elements of their fictional world using a combination of text input, sketching, and region-based filling. WorldSmith enables novice world builders to quickly visualize a fictional world with layered edits and hierarchical compositions. Through a formative study (4 participants) and first-use study (13 participants) we demonstrate that WorldSmith offers more expressive interactions with prompt-based models. With this work, we explore how creatives can be empowered to leverage prompt-based generative AI as a tool in their creative process, beyond current \\\\\"click-once\\\\\" prompting UI paradigms.\"\\n      },\\n      {\\n        \"paperId\": \"1d14a708622917da4b9820ada6d32af24fc1651a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d14a708622917da4b9820ada6d32af24fc1651a\",\\n        \"title\": \"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.08541\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.08541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2149231840\",\\n            \"name\": \"Zhengyuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2124948371\",\\n            \"name\": \"Jianfeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"50703697\",\\n            \"name\": \"Linjie Li\"\\n          },\\n          {\\n            \"authorId\": \"143786724\",\\n            \"name\": \"Kevin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2249709105\",\\n            \"name\": \"Chung-Ching Lin\"\\n          },\\n          {\\n            \"authorId\": \"2251726216\",\\n            \"name\": \"Zicheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"29957038\",\\n            \"name\": \"Lijuan Wang\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce ``Idea to Image,\\'\\' a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model\\'s characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative self-refinement on automatic image design and generation.\"\\n      },\\n      {\\n        \"paperId\": \"a2fae006e6c5ac346fd51bc8a009127f9abe22df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a2fae006e6c5ac346fd51bc8a009127f9abe22df\",\\n        \"title\": \"Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.13006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An open-source framework to evaluate, compare, and visualize the reliability and alignment of LLM judges, which facilitates practitioners to choose LLM judges for alignment tasks and indicates a significant impact of prompt templates on LLM judge performance, as well as a mediocre alignment level between the tested LLM judges and human evaluators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316959296\",\\n            \"name\": \"Hui Wei\"\\n          },\\n          {\\n            \"authorId\": \"2318243914\",\\n            \"name\": \"Shenghua He\"\\n          },\\n          {\\n            \"authorId\": \"2316860672\",\\n            \"name\": \"Tian Xia\"\\n          },\\n          {\\n            \"authorId\": \"2316960662\",\\n            \"name\": \"Andy Wong\"\\n          },\\n          {\\n            \"authorId\": \"2317016595\",\\n            \"name\": \"Jingyang Lin\"\\n          },\\n          {\\n            \"authorId\": \"2317047664\",\\n            \"name\": \"Mei Han\"\\n          }\\n        ],\\n        \"abstract\": \"LLM-as-a-Judge has been widely applied to evaluate and compare different LLM alignmnet approaches (e.g., RLHF and DPO). However, concerns regarding its reliability have emerged, due to LLM judges\\' biases and inconsistent decision-making. Previous research has developed evaluation frameworks to assess reliability of LLM judges and their alignment with human preferences. However, the employed evaluation metrics often lack adequate explainability and fail to address LLM internal inconsistency. Additionally, existing studies inadequately explore the impact of various prompt templates when applying LLM-as-a-Judge methods, leading to potentially inconsistent comparisons between different alignment algorithms. In this work, we systematically evaluate LLM-as-a-Judge on alignment tasks by defining more theoretically interpretable evaluation metrics and explicitly mitigating LLM internal inconsistency from reliability metrics. We develop an open-source framework to evaluate, compare, and visualize the reliability and alignment of LLM judges, which facilitates practitioners to choose LLM judges for alignment tasks. In the experiments, we examine effects of diverse prompt templates on LLM-judge reliability and also demonstrate our developed framework by comparing various LLM judges on two common alignment datasets (i.e., TL;DR Summarization and HH-RLHF-Helpfulness). Our results indicate a significant impact of prompt templates on LLM judge performance, as well as a mediocre alignment level between the tested LLM judges and human evaluators.\"\\n      },\\n      {\\n        \"paperId\": \"4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"title\": \"Improving Text-to-Image Consistency via Automatic Prompt Optimization\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models and paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1796269096\",\\n            \"name\": \"Oscar Ma\\\\u00f1as\"\\n          },\\n          {\\n            \"authorId\": \"2274101827\",\\n            \"name\": \"Pietro Astolfi\"\\n          },\\n          {\\n            \"authorId\": \"2293590162\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"2256372432\",\\n            \"name\": \"Candace Ross\"\\n          },\\n          {\\n            \"authorId\": \"39219656\",\\n            \"name\": \"Jack Urbanek\"\\n          },\\n          {\\n            \"authorId\": \"2293907712\",\\n            \"name\": \"Adina Williams\"\\n          },\\n          {\\n            \"authorId\": \"2801949\",\\n            \"name\": \"Aishwarya Agrawal\"\\n          },\\n          {\\n            \"authorId\": \"1456285042\",\\n            \"name\": \"Adriana Romero-Soriano\"\\n          },\\n          {\\n            \"authorId\": \"3325894\",\\n            \"name\": \"M. Drozdzal\"\\n          }\\n        ],\\n        \"abstract\": \"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"ac0235cbeefb7b638c4de6af0f29e9650180150d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac0235cbeefb7b638c4de6af0f29e9650180150d\",\\n        \"title\": \"PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.08985\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.08985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt and supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"92480323\",\\n            \"name\": \"Seungho Baek\"\\n          },\\n          {\\n            \"authorId\": \"151351626\",\\n            \"name\": \"Hyerin Im\"\\n          },\\n          {\\n            \"authorId\": \"2223757354\",\\n            \"name\": \"Jiseung Ryu\"\\n          },\\n          {\\n            \"authorId\": \"2223886488\",\\n            \"name\": \"Juhyeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2110755619\",\\n            \"name\": \"T. Lee\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation model is able to generate images across a diverse range of subjects and styles based on a single prompt. Recent works have proposed a variety of interaction methods that help users understand the capabilities of models and utilize them. However, how to support users to efficiently explore the model\\'s capability and to create effective prompts are still open-ended research questions. In this paper, we present PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt. Through the iterative process, users can efficiently explore the model\\'s capability, and clarify their intent. PromptCrafter also supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model. Lastly, users can revert to a desired step by reviewing the work history. In this workshop paper, we discuss the design process of PromptCrafter and our plans for follow-up studies.\"\\n      },\\n      {\\n        \"paperId\": \"a1e2557fa6d5373c8f89b8c4d426168cdf31d7d5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1e2557fa6d5373c8f89b8c4d426168cdf31d7d5\",\\n        \"title\": \"Efficient multi-prompt evaluation of LLMs\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.17202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is proved that PromptEval consistently estimates the performance distribution and its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry; for example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490941219\",\\n            \"name\": \"Felipe Maia Polo\"\\n          },\\n          {\\n            \"authorId\": \"2304142327\",\\n            \"name\": \"Ronald Xu\"\\n          },\\n          {\\n            \"authorId\": \"2286343827\",\\n            \"name\": \"Lucas Weber\"\\n          },\\n          {\\n            \"authorId\": \"2247874639\",\\n            \"name\": \"M\\'irian Silva\"\\n          },\\n          {\\n            \"authorId\": \"2303400299\",\\n            \"name\": \"Onkar Bhardwaj\"\\n          },\\n          {\\n            \"authorId\": \"2283849613\",\\n            \"name\": \"Leshem Choshen\"\\n          },\\n          {\\n            \"authorId\": \"2303415785\",\\n            \"name\": \"Allysson Flavio Melo de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"2247880508\",\\n            \"name\": \"Yuekai Sun\"\\n          },\\n          {\\n            \"authorId\": \"8202372\",\\n            \"name\": \"M. Yurochkin\"\\n          }\\n        ],\\n        \"abstract\": \"Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs\\' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry; for example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Moreover, we show how PromptEval can be useful in LLM-as-a-judge and best prompt identification applications.\"\\n      },\\n      {\\n        \"paperId\": \"bb57af33e505681b1649366e494681e5684bdec7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bb57af33e505681b1649366e494681e5684bdec7\",\\n        \"title\": \"SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.13684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a two-stage adaptation approach termed SPTNet, which iteratively optimizes model parameters and data parameters and proposes a novel spatial prompt tuning method (SPT) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292339349\",\\n            \"name\": \"Hongjun Wang\"\\n          },\\n          {\\n            \"authorId\": \"51286373\",\\n            \"name\": \"S. Vaze\"\\n          },\\n          {\\n            \"authorId\": \"2335536188\",\\n            \"name\": \"Kai Han\"\\n          }\\n        ],\\n        \"abstract\": \"Generalized Category Discovery (GCD) aims to classify unlabelled images from both `seen\\' and `unseen\\' classes by transferring knowledge from a set of labelled `seen\\' class images. A key theme in existing GCD approaches is adapting large-scale pre-trained models for the GCD task. An alternate perspective, however, is to adapt the data representation itself for better alignment with the pre-trained model. As such, in this paper, we introduce a two-stage adaptation approach termed SPTNet, which iteratively optimizes model parameters (i.e., model-finetuning) and data parameters (i.e., prompt learning). Furthermore, we propose a novel spatial prompt tuning method (SPT) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes. We thoroughly evaluate our SPTNet on standard benchmarks and demonstrate that our method outperforms existing GCD methods. Notably, we find our method achieves an average accuracy of 61.4% on the SSB, surpassing prior state-of-the-art methods by approximately 10%. The improvement is particularly remarkable as our method yields extra parameters amounting to only 0.117% of those in the backbone architecture. Project page: https://visual-ai.github.io/sptnet.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"2bb9a87bdfc8a35bc1813e5a88180f43615785a8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2bb9a87bdfc8a35bc1813e5a88180f43615785a8\",\\n        \"title\": \"Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.00727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts, and describes the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2048053804\",\\n            \"name\": \"Guangzhi Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2261406713\",\\n            \"name\": \"Qiao Jin\"\\n          },\\n          {\\n            \"authorId\": \"2314146159\",\\n            \"name\": \"Xiao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314147661\",\\n            \"name\": \"Minjia Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2237094367\",\\n            \"name\": \"Zhiyong Lu\"\\n          },\\n          {\\n            \"authorId\": \"2265729351\",\\n            \"name\": \"Aidong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"The emergent abilities of large language models (LLMs) have demonstrated great potential in solving medical questions. They can possess considerable medical knowledge, but may still hallucinate and are inflexible in the knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed to enhance the medical question-answering capabilities of LLMs with external knowledge bases, it may still fail in complex cases where multiple rounds of information-seeking are required. To address such an issue, we propose iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts. In each iteration of i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and they will be further used to guide the query generation in the next iteration. Our experiments show the improved performance of various LLMs brought by i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes in the United States Medical Licensing Examination (USMLE), as well as various knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68% on the MedQA dataset. In addition, we characterize the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration. Our case studies show that i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing an in-depth analysis of medical questions. To the best of our knowledge, this is the first-of-its-kind study on incorporating follow-up queries into medical RAG.\"\\n      },\\n      {\\n        \"paperId\": \"51b7b3ad7645a69e3c1c80cae69473b8bd472f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51b7b3ad7645a69e3c1c80cae69473b8bd472f67\",\\n        \"title\": \"ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models\",\\n        \"citationCount\": 99,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.07738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes ResearchAgent, a system that automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents, to enhance the productivity of researchers.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-04-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"90765684\",\\n            \"name\": \"Jinheon Baek\"\\n          },\\n          {\\n            \"authorId\": \"3001990\",\\n            \"name\": \"S. Jauhar\"\\n          },\\n          {\\n            \"authorId\": \"73040249\",\\n            \"name\": \"Silviu Cucerzan\"\\n          },\\n          {\\n            \"authorId\": \"2260611009\",\\n            \"name\": \"Sung Ju Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically, starting with a core scientific paper, ResearchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers. Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based ReviewingAgents that provide reviews and feedback via iterative revision processes. These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and model-based evaluation results. Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work.\"\\n      },\\n      {\\n        \"paperId\": \"192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"title\": \"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2312.01663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.01663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269468811\",\\n            \"name\": \"Runze He\"\\n          },\\n          {\\n            \"authorId\": \"2052151521\",\\n            \"name\": \"Shaofei Huang\"\\n          },\\n          {\\n            \"authorId\": \"2269461105\",\\n            \"name\": \"Xuecheng Nie\"\\n          },\\n          {\\n            \"authorId\": \"151475424\",\\n            \"name\": \"Tianrui Hui\"\\n          },\\n          {\\n            \"authorId\": \"1776665\",\\n            \"name\": \"Luoqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2108984\",\\n            \"name\": \"Jiao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2269685669\",\\n            \"name\": \"Jizhong Han\"\\n          },\\n          {\\n            \"authorId\": \"2269748083\",\\n            \"name\": \"Guanbin Li\"\\n          },\\n          {\\n            \"authorId\": \"2269687302\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\"\\n      },\\n      {\\n        \"paperId\": \"d0482bd01de9d0912acf4e5338c7799eba4b9360\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d0482bd01de9d0912acf4e5338c7799eba4b9360\",\\n        \"title\": \"NAISTeacher: A Prompt and Rerank Approach to Generating Teacher Utterances in Educational Dialogues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.bea-1.63.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://aclanthology.org/2023.bea-1.63, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Through the proposed methodology, the BEA 2023 shared task of generating teacher responses in educational dialogues achieved the top score on both automated metrics and human evaluation, surpassing the reference human teachers on the latter.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1572641769\",\\n            \"name\": \"Justin Vasselli\"\\n          },\\n          {\\n            \"authorId\": \"2221318238\",\\n            \"name\": \"Christopher Vasselli\"\\n          },\\n          {\\n            \"authorId\": \"103680841\",\\n            \"name\": \"Adam Nohejl\"\\n          },\\n          {\\n            \"authorId\": \"2110694221\",\\n            \"name\": \"Taro Watanabe\"\\n          }\\n        ],\\n        \"abstract\": \"This paper presents our approach to the BEA 2023 shared task of generating teacher responses in educational dialogues, using the Teacher-Student Chatroom Corpus. Our system prompts GPT-3.5-turbo to generate initial suggestions, which are then subjected to reranking. We explore multiple strategies for candidate generation, including prompting for multiple candidates and employing iterative few-shot prompts with negative examples. We aggregate all candidate responses and rerank them based on DialogRPT scores. To handle consecutive turns in the dialogue data, we divide the task of generating teacher utterances into two components: teacher replies to the student and teacher continuations of previously sent messages. Through our proposed methodology, our system achieved the top score on both automated metrics and human evaluation, surpassing the reference human teachers on the latter.\"\\n      },\\n      {\\n        \"paperId\": \"b56df33446b37bdf5f17fb494201da728fb62502\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b56df33446b37bdf5f17fb494201da728fb62502\",\\n        \"title\": \"Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.15205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is argued that MLLM hallucinations can reveal valuable contextual insights when leveraged correctly, as they represent pre-trained large-scale knowledge beyond individual images as they represent pre-trained large-scale knowledge beyond individual images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"123925137\",\\n            \"name\": \"Jian Hu\"\\n          },\\n          {\\n            \"authorId\": \"2273817980\",\\n            \"name\": \"Jiayi Lin\"\\n          },\\n          {\\n            \"authorId\": \"2317041351\",\\n            \"name\": \"Junchi Yan\"\\n          },\\n          {\\n            \"authorId\": \"2273646767\",\\n            \"name\": \"Shaogang Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Promptable segmentation typically requires instance-specific manual prompts to guide the segmentation of each desired object. To minimize such a need, task-generic promptable segmentation has been introduced, which employs a single task-generic prompt to segment various images of different objects in the same task. Current methods use Multimodal Large Language Models (MLLMs) to reason detailed instance-specific prompts from a task-generic prompt for improving segmentation accuracy. The effectiveness of this segmentation heavily depends on the precision of these derived prompts. However, MLLMs often suffer hallucinations during reasoning, resulting in inaccurate prompting. While existing methods focus on eliminating hallucinations to improve a model, we argue that MLLM hallucinations can reveal valuable contextual insights when leveraged correctly, as they represent pre-trained large-scale knowledge beyond individual images. In this paper, we utilize hallucinations to mine task-related information from images and verify its accuracy for enhancing precision of the generated prompts. Specifically, we introduce an iterative Prompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a mask generator.The prompt generator uses a multi-scale chain of thought prompting, initially exploring hallucinations for extracting extended contextual knowledge on a test image.These hallucinations are then reduced to formulate precise instance-specific prompts, directing the mask generator to produce masks that are consistent with task semantics by mask semantic alignment. The generated masks iteratively induce the prompt generator to focus more on task-relevant image areas and reduce irrelevant hallucinations, resulting jointly in better prompts and masks. Experiments on 5 benchmarks demonstrate the effectiveness of ProMaC. Code given in https://lwpyh.github.io/ProMaC/.\"\\n      },\\n      {\\n        \"paperId\": \"03ccf2e13cdbcc156ee48aa26f500b574da07af8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/03ccf2e13cdbcc156ee48aa26f500b574da07af8\",\\n        \"title\": \"PromptWizard: Task-Aware Prompt Optimization Framework\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18369, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving, self-adapting mechanism, and demonstrates PromptWizard\\'s efficiency, scalability, and advantages over existing prompt optimization strategies.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-05-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2303465593\",\\n            \"name\": \"Eshaan Agarwal\"\\n          },\\n          {\\n            \"authorId\": \"2308075211\",\\n            \"name\": \"Joykirat Singh\"\\n          },\\n          {\\n            \"authorId\": \"2303466479\",\\n            \"name\": \"Vivek Dani\"\\n          },\\n          {\\n            \"authorId\": \"2324053080\",\\n            \"name\": \"Raghav Magazine\"\\n          },\\n          {\\n            \"authorId\": \"1785978\",\\n            \"name\": \"Tanuja Ganu\"\\n          },\\n          {\\n            \"authorId\": \"51464520\",\\n            \"name\": \"A. Nambi\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have transformed AI across diverse domains, with prompting being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving, self-adapting mechanism. Through a feedback-driven critique and synthesis process, PromptWizard achieves an effective balance between exploration and exploitation, iteratively refining both prompt instructions and in-context examples to generate human-readable, task-specific prompts. This guided approach systematically improves prompt quality, resulting in superior performance across 45 tasks. PromptWizard excels even with limited training data, smaller LLMs, and various LLM architectures. Additionally, our cost analysis reveals a substantial reduction in API calls, token usage, and overall cost, demonstrating PromptWizard\\'s efficiency, scalability, and advantages over existing prompt optimization strategies.\"\\n      },\\n      {\\n        \"paperId\": \"69a7ae20845c47c5efe3a93c94e6888b852fe532\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69a7ae20845c47c5efe3a93c94e6888b852fe532\",\\n        \"title\": \"DivLog: Log Parsing with Prompt Enhanced In-Context Learning\",\\n        \"citationCount\": 77,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3597503.3639155?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3597503.3639155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2186630124\",\\n            \"name\": \"Junjielong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2115429277\",\\n            \"name\": \"Ruichun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2036553451\",\\n            \"name\": \"Yintong Huo\"\\n          },\\n          {\\n            \"authorId\": \"2287118778\",\\n            \"name\": \"Chengyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265706586\",\\n            \"name\": \"Pinjia He\"\\n          }\\n        ],\\n        \"abstract\": \"Log parsing, which involves log template extraction from semi-structured logs to produce structured logs, is the first and the most critical step in automated log analysis. However, current log parsers suffer from limited effectiveness for two reasons. First, traditional data-driven log parsers solely rely on heuristics or handcrafted features designed by domain experts, which may not consistently perform well on logs from diverse systems. Second, existing super-vised log parsers require model tuning, which is often limited to fixed training samples and causes sub-optimal performance across the entire log source. To address this limitation, we propose Di-vLog, an effective log parsing framework based on the in-context learning (ICL) ability of large language models (LLMs). Specifically, before log parsing, DivLog samples a small amount of offline logs as candidates by maximizing their diversity. Then, during log parsing, DivLog selects five appropriate labeled candidates as examples for each target log and constructs them into a prompt. By mining the semantics of examples in the prompt, DivLog generates a target log template in a training-free manner. In addition, we design a straightforward yet effective prompt format to extract the output and enhance the quality of the generated log templates. We conducted experiments on 16 widely-used public datasets. The results show that DivLog achieves (1) 98.1% Parsing Accuracy, (2) 92.1% Precision Template Accuracy, and (3) 92.9% Recall Template Accuracy on average, exhibiting state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"2ca32be903662ec64f0a2fd014265bc8c00d8254\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2ca32be903662ec64f0a2fd014265bc8c00d8254\",\\n        \"title\": \"Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.03099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent, and demonstrates the effectiveness of the method with respect to strong proprietary models on real-world tasks such as moderation and generation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34490455\",\\n            \"name\": \"Elad Levi\"\\n          },\\n          {\\n            \"authorId\": \"2282532582\",\\n            \"name\": \"Eli Brosh\"\\n          },\\n          {\\n            \"authorId\": \"2282531918\",\\n            \"name\": \"Matan Friedmann\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system\\'s key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\\\\\\\\href{https://github.com/Eladlev/AutoPrompt}{here}$.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='d91bd339-39e5-4007-b93a-ef5d26028fec')]}\n",
      "\u001b[1m[updates]\u001b[0m {'prior_work_analysis': {'messages': [AIMessage(content='{\\n  \"papers\": [\\n    {\\n      \"paper_id\": \"6f75404b0d01f9a09afe428f9efd5cbcd7825469\",\\n      \"title\": \"Dynamic Prompting: A Unified Framework for Prompt Tuning\",\\n      \"overlap_score\": 0.88,\\n      \"methodology_overlap\": 0.85,\\n      \"problem_overlap\": 0.85,\\n      \"domain_overlap\": 0.7,\\n      \"key_overlaps\": [\\n        \"Dynamic prompt tuning\",\\n        \"Cross-domain applicability\",\\n        \"Parameter-efficient prompt tuning\"\\n      ],\\n      \"key_differences\": [\\n        \"More general theoretical framework across NLP/vision modalities; not restricted to single-task fine-tuning or test-time adaptation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"51ac2dceecc75431eab5b2c320822e081b282b27\",\\n      \"title\": \"SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models\",\\n      \"overlap_score\": 0.86,\\n      \"methodology_overlap\": 0.84,\\n      \"problem_overlap\": 0.82,\\n      \"domain_overlap\": 0.78,\\n      \"key_overlaps\": [\\n        \"Test-time prompt adaptation\",\\n        \"Vision-Language Models\",\\n        \"Prompt optimization across domains\"\\n      ],\\n      \"key_differences\": [\\n        \"Focuses on test-time prompt adaptation, not end-to-end fine-tuning or multi-turn reasoning prompts.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"1d24f2dfe942e2d67381d8fcfac3423145bd557f\",\\n      \"title\": \"DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval\",\\n      \"overlap_score\": 0.85,\\n      \"methodology_overlap\": 0.83,\\n      \"problem_overlap\": 0.82,\\n      \"domain_overlap\": 0.8,\\n      \"key_overlaps\": [\\n        \"Cross-modal prompt tuning\",\\n        \"Global-local video attention\",\\n        \"Prompt-based retrieval alignment\"\\n      ],\\n      \"key_differences\": [\\n        \"Specifically targets video retrieval with global-local prompts; emphasis on cross-modal prompt design rather than dialog/story coherence.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n      \"title\": \"Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning\",\\n      \"overlap_score\": 0.72,\\n      \"methodology_overlap\": 0.7,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.75,\\n      \"key_overlaps\": [\\n        \"Image-specific prompts\",\\n        \"Zero-shot domain adaptation\",\\n        \"Prompt-based adaptation for generative models\"\\n      ],\\n      \"key_differences\": [\\n        \"Centers on image-domain prompt specialization for generative models rather than iterative, multi-turn adaptation for narratives or dialogues.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"5a8541bb468afb45d68e16d2f83b327768f97c67\",\\n      \"title\": \"Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval\",\\n      \"overlap_score\": 0.8,\\n      \"methodology_overlap\": 0.78,\\n      \"problem_overlap\": 0.75,\\n      \"domain_overlap\": 0.76,\\n      \"key_overlaps\": [\\n        \"Prompt-based CLIP adaptation\",\\n        \"Video retrieval\",\\n        \"Semantics-enhancing prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Uses prompt switching to adapt CLIP for video tasks, rather than dynamic, narrative coherence prompts.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"81ca1ca9b81470988d29e0d0def9712320243e06a\",\\n      \"title\": \"HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target Binding Affinity Prediction\",\\n      \"overlap_score\": 0.7,\\n      \"methodology_overlap\": 0.72,\\n      \"problem_overlap\": 0.68,\\n      \"domain_overlap\": 0.65,\\n      \"key_overlaps\": [\\n        \"Dynamic prompts for domain-specific interactions\",\\n        \"Hybrid Graph-Transformer models\",\\n        \"Context-aware feature prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Applied to bioinformatics DTA prediction; uses dynamic prompts in a graph-transformer setting rather than natural language generation tasks.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"3967189742efab8859da542ce3953d4c72957aca\",\\n      \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\\n      \"overlap_score\": 0.82,\\n      \"methodology_overlap\": 0.8,\\n      \"problem_overlap\": 0.8,\\n      \"domain_overlap\": 0.8,\\n      \"key_overlaps\": [\\n        \"Prompt learning for VLM adaptation\",\\n        \"Zero-shot HOI detection\",\\n        \"LLM/VLM guidance for prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Focus on visual-human-object interaction detection rather than narrative generation or text-only prompts.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"8d531cb8cf51eec3b8f1106d189295fa3c81c02a\",\\n      \"title\": \"Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing\",\\n      \"overlap_score\": 0.74,\\n      \"methodology_overlap\": 0.72,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.7,\\n      \"key_overlaps\": [\\n        \"Dynamic prompt learning\",\\n        \"Image editing with cross-attention control\",\\n        \"Noun-word prompt targeting\"\\n      ],\\n      \"key_differences\": [\\n        \"Targets fine-grained image editing; not about long-form narrative coherence or dialogue.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n      \"title\": \"Prompt-and-Transfer: Dynamic Class-Aware Enhancement for Few-Shot Segmentation\",\\n      \"overlap_score\": 0.68,\\n      \"methodology_overlap\": 0.66,\\n      \"problem_overlap\": 0.66,\\n      \"domain_overlap\": 0.7,\\n      \"key_overlaps\": [\\n        \"Dynamic class-aware prompts\",\\n        \"Few-shot segmentation\",\\n        \"Prompt-driven encoder adaptation\"\\n      ],\\n      \"key_differences\": [\\n        \"Applied to segmentation tasks with few-shot data; different domain and evaluation than narrative prompt adaptation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"64ffcf31b03aaae2905e06f9e24be58f51e2f776\",\\n      \"title\": \"Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation\",\\n      \"overlap_score\": 0.7,\\n      \"methodology_overlap\": 0.68,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.72,\\n      \"key_overlaps\": [\\n        \"Test-time visual prompts\",\\n        \"Continual test-time adaptation\",\\n        \"Memory-informed prompt initialization\"\\n      ],\\n      \"key_differences\": [\\n        \"Medical imaging application; continual test-time adaptation with prompts rather than narrative generation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"4f9766356478ddf606457af735f70682839194b1\",\\n      \"title\": \"VPA: Fully Test-Time Visual Prompt Adaptation\",\\n      \"overlap_score\": 0.83,\\n      \"methodology_overlap\": 0.81,\\n      \"problem_overlap\": 0.82,\\n      \"domain_overlap\": 0.79,\\n      \"key_overlaps\": [\\n        \"Visual prompt tuning\",\\n        \"Test-time adaptation\",\\n        \"Domain robustness via prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Visual prompts for OOD/generalization; not narrative multi-turn prompt adaptation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"5d48b087d3d4a16904c7e5442a4a815ff901cc0d\",\\n      \"title\": \"CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning\",\\n      \"overlap_score\": 0.75,\\n      \"methodology_overlap\": 0.73,\\n      \"problem_overlap\": 0.74,\\n      \"domain_overlap\": 0.72,\\n      \"key_overlaps\": [\\n        \"Chain-of-domain adaptation\",\\n        \"Severity-aware visual prompts\",\\n        \"Prompt-based domain adaptation\"\\n      ],\\n      \"key_differences\": [\\n        \"Domain leverage within computer vision domain adaptation; separate from dialogue/storytelling coherence prompts.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n      \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\\n      \"overlap_score\": 0.79,\\n      \"methodology_overlap\": 0.77,\\n      \"problem_overlap\": 0.76,\\n      \"domain_overlap\": 0.75,\\n      \"key_overlaps\": [\\n        \"Prompt auto-editing\",\\n        \"Weight and step optimization for prompts\",\\n        \"Online reinforcement learning for prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Image generation prompts optimization; tasks differ from dialogue coherence and long-context prompt adaptation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"b1b688cf506f64bc4a06e037bfcdfa8b22be51aa\",\\n      \"title\": \"Cascade Prompt Learning for Vision-Language Model Adaptation\",\\n      \"overlap_score\": 0.82,\\n      \"methodology_overlap\": 0.8,\\n      \"problem_overlap\": 0.78,\\n      \"domain_overlap\": 0.79,\\n      \"key_overlaps\": [\\n        \"Cascade prompting\",\\n        \"Domain-general and task-specific prompts\",\\n        \"Teacher-student prompt learning\"\\n      ],\\n      \"key_differences\": [\\n        \"Two-stage prompting to separate domain-general and task-specific prompts; still about prompting rather than long narrative coherence.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"03ccf2e13cdbcc156ee48aa26f500b574da07af8\",\\n      \"title\": \"PromptWizard: Task-Aware Prompt Optimization Framework\",\\n      \"overlap_score\": 0.74,\\n      \"methodology_overlap\": 0.72,\\n      \"problem_overlap\": 0.74,\\n      \"domain_overlap\": 0.72,\\n      \"key_overlaps\": [\\n        \"Automated discrete prompt optimization\",\\n        \"Self-evolving critique and synthesis\",\\n        \"Task-aware prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Focuses on automating discrete prompt optimization with self-evolving mechanisms; while related, less focus on continual narrative coherence.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n      \"title\": \"Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning\",\\n      \"overlap_score\": 0.72,\\n      \"methodology_overlap\": 0.7,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.75,\\n      \"key_overlaps\": [\\n        \"Image-specific prompts\",\\n        \"Domain adaptation for generative models\",\\n        \"Prompt-based adaptation\"\\n      ],\\n      \"key_differences\": [\\n        \"Generative modeling domain; not about long-form dialogue coherence or iterative narrative prompting.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"0f4219a1a0f89c7364aabb27632cdcc833ab53b7\",\\n      \"title\": \"Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction\",\\n      \"overlap_score\": 0.66,\\n      \"methodology_overlap\": 0.64,\\n      \"problem_overlap\": 0.66,\\n      \"domain_overlap\": 0.68,\\n      \"key_overlaps\": [\\n        \"Iterative prompt-tuning\",\\n        \"Event argument extraction\",\\n        \"Bidirectional prompting\"\\n      ],\\n      \"key_differences\": [\\n        \"NLP event extraction-focused; not narrative coherence or long-form dialogue generation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n      \"title\": \"Prompt-and-Transfer: Dynamic Class-Aware Enhancement for Few-Shot Segmentation\",\\n      \"overlap_score\": 0.68,\\n      \"methodology_overlap\": 0.66,\\n      \"problem_overlap\": 0.66,\\n      \"domain_overlap\": 0.7,\\n      \"key_overlaps\": [\\n        \"Dynamic class-aware prompting\",\\n        \"Few-shot segmentation\",\\n        \"Encoder adaptation via prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Segmentation domain; uses prompts for feature adaptation rather than narrative continuation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"e39? (placeholder)\",\\n      \"title\": \"Generative AI tools in art education: prompt engineering and iterative processes for enhanced creativity\",\\n      \"overlap_score\": 0.6,\\n      \"methodology_overlap\": 0.58,\\n      \"problem_overlap\": 0.62,\\n      \"domain_overlap\": 0.6,\\n      \"key_overlaps\": [\\n        \"Prompt engineering\",\\n        \"Iterative design processes\",\\n        \"Creative generation education\"\\n      ],\\n      \"key_differences\": [\\n        \"Educational context; not primarily about LLM prompt adaptation in long interactions.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"ac0235cbeefb7b638c4de6af0f29e9650180150d\",\\n      \"title\": \"PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM\",\\n      \"overlap_score\": 0.7,\\n      \"methodology_overlap\": 0.68,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.72,\\n      \"key_overlaps\": [\\n        \"Mixed-initiative prompt crafting\",\\n        \"Dialogue-guided prompt design\",\\n        \"LLM-based prompt optimization\"\\n      ],\\n      \"key_differences\": [\\n        \"Prompts for T2I in visual tasks; not specifically about long-running interactive coherence.\"\\n      ]\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='646e4d96-0582-4af8-8717-fdc99de2f427')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='ad548c68-67b0-408a-9b14-9783c57b304a'), AIMessage(content='{\\n  \"research_question\": \"How can Dynamic Prompt Adaptation improve coherence and engagement of large language models over extended interactions and creative narratives?\",\\n  \"problem_domain\": \"Natural Language Processing; dialogue systems; long-form text generation\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"prompt templating\",\\n    \"summarization of prior interactions\"\\n  ],\\n  \"key_concepts\": [\\n    \"coherence over extended interactions\",\\n    \"thematic consistency\",\\n    \"iterative dialogue\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"summarization/synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Three-phase Dynamic Prompt Adaptation framework (Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update) to maintain thematic coherence across extended interactions.\",\\n    \"Contextual Analysis extracts key themes and tonal shifts from prior outputs and prompts.\",\\n    \"Adaptive Prompt Generation creates updated prompts to introduce new elements or clarify past responses.\",\\n    \"Iterative Context Update synthesizes all prior interactions to maintain coherence.\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='ae974c28-bf76-4b15-a33d-b03c97254c5c'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation\",\\n      \"rationale\": \"Targets studies of frameworks that adapt prompts in real time to sustain coherence over long dialogues.\",\\n      \"priority_concept\": \"dynamic prompt adaptation, coherence, long-form dialogue\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation\",\\n      \"rationale\": \"Searches for methods that generate updated prompts to guide LLMs for consistent narrative and context handling.\",\\n      \"priority_concept\": \"adaptive prompt generation, context management, coherence\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Focuses on techniques that update system prompts/context iteratively to maintain dialogue continuity.\",\\n      \"priority_concept\": \"iterative context update, coherence, dialogue\"\\n    },\\n    {\\n      \"query_string\": \"summarization prior interactions\",\\n      \"rationale\": \"Looks for work on summarizing or synthesizing past interactions to preserve thematic coherence.\",\\n      \"priority_concept\": \"summarization, prior interactions, thematic coherence\"\\n    },\\n    {\\n      \"query_string\": \"iterative prompt templating\",\\n      \"rationale\": \"Explores templating strategies that evolve across iterations to embed new elements while preserving coherence.\",\\n      \"priority_concept\": \"iterative prompt templating, prompt engineering, coherence\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='9a9405f7-5b76-4099-897c-3852c95bb26a'), AIMessage(content='{\\n  \"dynamic prompt adaptation\": {\\n    \"total\": 5410,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 128,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 197,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"title\": \"Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/28759/29459\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i8.28759?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i8.28759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy to enable prompt adaptation to the evolving distribution of the dynamic graph.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9071547\",\\n            \"name\": \"Binwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2108814780\",\\n            \"name\": \"Pengkun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292591250\",\\n            \"name\": \"Yudong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108599981\",\\n            \"name\": \"Xu Wang\"\\n          },\\n          {\\n            \"authorId\": \"6231985\",\\n            \"name\": \"Zhengyang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2187108259\",\\n            \"name\": \"Lei Bai\"\\n          },\\n          {\\n            \"authorId\": \"46396284\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"With the progress of urban transportation systems, a significant amount of high-quality traffic data is continuously collected through streaming manners, which has propelled the prosperity of the field of spatial-temporal graph prediction. In this paper, rather than solely focusing on designing powerful models for static graphs, we shift our focus to spatial-temporal graph prediction in the dynamic scenario, which involves a continuously expanding and evolving underlying graph. To address inherent challenges, a decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy. Incorporating inductive biases of time-series structures, DSTG can interpret time dependencies into latent trend and seasonal terms. To enable prompt adaptation to the evolving distribution of the dynamic graph, our decoupling training strategy is devised to iteratively update these two types of patterns. Specifically, for learning seasonal patterns, we conduct thorough training for the model using a long time series (e.g., three months of data). To enhance the learning ability of the model, we also introduce the masked auto-encoding mechanism. During this period, we frequently update trend patterns to expand new information from dynamic graphs. Considering both effectiveness and efficiency, we develop a subnet sampling strategy to select a few representative nodes for fine-tuning the weights of the model. These sampled nodes cover unseen patterns and previously learned patterns. Experiments on dynamic spatial-temporal graph datasets further demonstrate the competitive performance, superior efficiency, and strong scalability of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"title\": \"Dynamics of Smallholder Farmers\\\\u2019 Livelihood Adaptation Decision-Making in Central Ethiopia\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2071-1050/12/11/4526/pdf?version=1591863876\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/su12114526?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/su12114526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2020-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13422136\",\\n            \"name\": \"D. Etana\"\\n          },\\n          {\\n            \"authorId\": \"4266116\",\\n            \"name\": \"D. Snelder\"\\n          },\\n          {\\n            \"authorId\": \"2051704940\",\\n            \"name\": \"C. V. van Wesenbeeck\"\\n          },\\n          {\\n            \"authorId\": \"3606491\",\\n            \"name\": \"T. de Cock Buning\"\\n          }\\n        ],\\n        \"abstract\": \"In previous studies mainly focusing on determinants of adaptation, evidence of the dynamic process of adaptation decision-making is negligible. The objective of this study was to investigate the effects of socio-cultural factors, changes in household characteristics, and climate variables on the transition from non-use to use of adaptation strategies. The study integrated primary data collected from households with secondary rainfall and temperature data. The quantitative and qualitative data were analysed using a dynamic random-effects probit model and a thematic approach, respectively. The result shows strong evidence of path dependence in which use of a strategy during the previous year significantly increases its current use. Climate-related risk perception and factual knowledge may not necessarily prompt adaptation action, whereas access to financial resources and farming-related trainings were consistent positive predictors of farmers\\\\u2019 adaptation decisions. The findings entail that economic capacity and the associated intrinsic motivation help few farmers to utilise robust and contesting adaptation strategies. For most households, economic problems and the consequent fatalistic attitude and risk-avoidance behaviour induce either non-use or use of responsive and accommodating strategies aimed at ensuring survival. Path dependence in non-use of adaptation strategies and sub-optimal adaptation actions demand effective institutional supports to address the behavioural and economic barriers of these households in order to build overall community resilience.\"\\n      },\\n      {\\n        \"paperId\": \"f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"title\": \"Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.08394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model, designs an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2309491187\",\\n            \"name\": \"Zhengbo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2288264614\",\\n            \"name\": \"Li Xu\"\\n          },\\n          {\\n            \"authorId\": \"2067913944\",\\n            \"name\": \"Duo Peng\"\\n          },\\n          {\\n            \"authorId\": \"2265553215\",\\n            \"name\": \"Hossein Rahmani\"\\n          },\\n          {\\n            \"authorId\": \"2309177751\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model. Our main idea is to leverage the rich knowledge encapsulated within the pre-trained diffusion model, such as the understanding of image semantics and structural information, to address unsupervised visual tracking. To this end, we design an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target. Furthermore, to facilitate dynamic adaptation of the prompt to the target\\'s movements, we propose an online prompt updater. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method, which also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"736275f05cbff41484ff4f0c90fd1401af85090f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/736275f05cbff41484ff4f0c90fd1401af85090f\",\\n        \"title\": \"A kaleidoscope of photosynthetic antenna proteins and their emerging roles\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1093/plphys/kiac175\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9237682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Some of the latest fundamental discoveries on the molecular mechanisms and regulation of light harvesting that can potentially be exploited for the optimization of photosynthesis are underline.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1811744891\",\\n            \"name\": \"Rameez Arshad\"\\n          },\\n          {\\n            \"authorId\": \"35279014\",\\n            \"name\": \"Francesco Saccon\"\\n          },\\n          {\\n            \"authorId\": \"1743932730\",\\n            \"name\": \"P. Bag\"\\n          },\\n          {\\n            \"authorId\": \"1498519841\",\\n            \"name\": \"Avratanu Biswas\"\\n          },\\n          {\\n            \"authorId\": \"36840944\",\\n            \"name\": \"C. Calvaruso\"\\n          },\\n          {\\n            \"authorId\": \"1791956159\",\\n            \"name\": \"Ahmad Farhan Bhatti\"\\n          },\\n          {\\n            \"authorId\": \"37543271\",\\n            \"name\": \"Steffen Grebe\"\\n          },\\n          {\\n            \"authorId\": \"1490962043\",\\n            \"name\": \"Vincenzo Mascoli\"\\n          },\\n          {\\n            \"authorId\": \"145679412\",\\n            \"name\": \"M. Mahbub\"\\n          },\\n          {\\n            \"authorId\": \"25484278\",\\n            \"name\": \"F. Muzzopappa\"\\n          },\\n          {\\n            \"authorId\": \"2042646658\",\\n            \"name\": \"Alexandros Polyzois\"\\n          },\\n          {\\n            \"authorId\": \"150064364\",\\n            \"name\": \"C. Schiphorst\"\\n          },\\n          {\\n            \"authorId\": \"68976915\",\\n            \"name\": \"M. Sorrentino\"\\n          },\\n          {\\n            \"authorId\": \"40827588\",\\n            \"name\": \"Simona Streckait\\\\u0117\"\\n          },\\n          {\\n            \"authorId\": \"6651561\",\\n            \"name\": \"H. van Amerongen\"\\n          },\\n          {\\n            \"authorId\": \"90940595\",\\n            \"name\": \"E. Aro\"\\n          },\\n          {\\n            \"authorId\": \"1881036\",\\n            \"name\": \"R. Bassi\"\\n          },\\n          {\\n            \"authorId\": \"4490696\",\\n            \"name\": \"E. Boekema\"\\n          },\\n          {\\n            \"authorId\": \"145609130\",\\n            \"name\": \"R. Croce\"\\n          },\\n          {\\n            \"authorId\": \"4296150\",\\n            \"name\": \"J. Dekker\"\\n          },\\n          {\\n            \"authorId\": \"3189715\",\\n            \"name\": \"R. van Grondelle\"\\n          },\\n          {\\n            \"authorId\": \"144071537\",\\n            \"name\": \"S. Jansson\"\\n          },\\n          {\\n            \"authorId\": \"5089855\",\\n            \"name\": \"D. Kirilovsky\"\\n          },\\n          {\\n            \"authorId\": \"4282160\",\\n            \"name\": \"R. Kou\\\\u0159il\"\\n          },\\n          {\\n            \"authorId\": \"145135253\",\\n            \"name\": \"S. Michel\"\\n          },\\n          {\\n            \"authorId\": \"5289431\",\\n            \"name\": \"C. Mullineaux\"\\n          },\\n          {\\n            \"authorId\": \"7175075\",\\n            \"name\": \"Kl\\\\u00e1ra Panzarov\\\\u00e1\"\\n          },\\n          {\\n            \"authorId\": \"145448524\",\\n            \"name\": \"B. Robert\"\\n          },\\n          {\\n            \"authorId\": \"2161625\",\\n            \"name\": \"A. Ruban\"\\n          },\\n          {\\n            \"authorId\": \"15201549\",\\n            \"name\": \"I. V. van Stokkum\"\\n          },\\n          {\\n            \"authorId\": \"114063537\",\\n            \"name\": \"E. Wientjes\"\\n          },\\n          {\\n            \"authorId\": \"3857871\",\\n            \"name\": \"C. B\\\\u00fcchel\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Photosynthetic light-harvesting antennae are pigment-binding proteins that perform one of the most fundamental tasks on Earth, capturing light and transferring energy that enables life in our biosphere. Adaptation to different light environments led to the evolution of an astonishing diversity of light-harvesting systems. At the same time, several strategies have been developed to optimize the light energy input into photosynthetic membranes in response to fluctuating conditions. The basic feature of these prompt responses is the dynamic nature of antenna complexes, whose function readily adapts to the light available. High-resolution microscopy and spectroscopic studies on membrane dynamics demonstrate the crosstalk between antennae and other thylakoid membrane components. With the increased understanding of light-harvesting mechanisms and their regulation, efforts are focusing on the development of sustainable processes for effective conversion of sunlight into functional bio-products. The major challenge in this approach lies in the application of fundamental discoveries in light-harvesting systems for the improvement of plant or algal photosynthesis. Here, we underline some of the latest fundamental discoveries on the molecular mechanisms and regulation of light harvesting that can potentially be exploited for the optimization of photosynthesis.\"\\n      },\\n      {\\n        \"paperId\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n        \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04095\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces the Prompt Auto-Editing (PAE) method, which employs an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148661301\",\\n            \"name\": \"Wenyi Mo\"\\n          },\\n          {\\n            \"authorId\": \"2146332319\",\\n            \"name\": \"Tianyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2281418241\",\\n            \"name\": \"Yalong Bai\"\\n          },\\n          {\\n            \"authorId\": \"2295513824\",\\n            \"name\": \"Bing Su\"\\n          },\\n          {\\n            \"authorId\": \"2293310016\",\\n            \"name\": \"Ji-Rong Wen\"\\n          },\\n          {\\n            \"authorId\": \"2281323801\",\\n            \"name\": \"Qing Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generative models, specifically those based on diffusion models like Imagen and Stable Diffusion, have made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text prompts. Users assign weights or alter the injection time steps of certain words in the text prompts to improve the quality of generated images. However, the success of fine-control prompts depends on the accuracy of the text prompts and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the Prompt Auto-Editing (PAE) method. Besides refining the original prompts for image generation, we further employ an online reinforcement learning strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control prompts. The re-wardfunction during training encourages the model to consider aesthetic score, semantic consistency, and user prefer-ences. Experimental results demonstrate that our proposed method effectively improves the original prompts, generating visually more appealing images while maintaining semantic alignment. Code is available at this https URL.\"\\n      },\\n      {\\n        \"paperId\": \"ef9d13b78e418b05eba22563d460900dfb6acd2e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef9d13b78e418b05eba22563d460900dfb6acd2e\",\\n        \"title\": \"MedSAM-U: Uncertainty-Guided Auto Multi-Prompt Adaptation for Reliable MedSAM\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces MedSAM-U, an uncertainty-guided framework designed to automatically refine multi-prompt inputs for more reliable and precise medical image segmentation, and employs uncertainty-guided multi-prompt to effectively estimate the uncertainties associated with the prompts and their initial segmentation results.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295375184\",\\n            \"name\": \"Nan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"9258203\",\\n            \"name\": \"K. Zou\"\\n          },\\n          {\\n            \"authorId\": \"2295886187\",\\n            \"name\": \"Kai Ren\"\\n          },\\n          {\\n            \"authorId\": \"151501329\",\\n            \"name\": \"Mengting Luo\"\\n          },\\n          {\\n            \"authorId\": \"2308862755\",\\n            \"name\": \"Linchao He\"\\n          },\\n          {\\n            \"authorId\": \"2171627854\",\\n            \"name\": \"Meng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2199172643\",\\n            \"name\": \"Yidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2304326718\",\\n            \"name\": \"Yi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2287877133\",\\n            \"name\": \"Hu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2302371411\",\\n            \"name\": \"Huazhu Fu\"\\n          }\\n        ],\\n        \"abstract\": \"The Medical Segment Anything Model (MedSAM) has shown remarkable performance in medical image segmentation, drawing significant attention in the field. However, its sensitivity to varying prompt types and locations poses challenges. This paper addresses these challenges by focusing on the development of reliable prompts that enhance MedSAM\\'s accuracy. We introduce MedSAM-U, an uncertainty-guided framework designed to automatically refine multi-prompt inputs for more reliable and precise medical image segmentation. Specifically, we first train a Multi-Prompt Adapter integrated with MedSAM, creating MPA-MedSAM, to adapt to diverse multi-prompt inputs. We then employ uncertainty-guided multi-prompt to effectively estimate the uncertainties associated with the prompts and their initial segmentation results. In particular, a novel uncertainty-guided prompts adaptation technique is then applied automatically to derive reliable prompts and their corresponding segmentation outcomes. We validate MedSAM-U using datasets from multiple modalities to train a universal image segmentation model. Compared to MedSAM, experimental results on five distinct modal datasets demonstrate that the proposed MedSAM-U achieves an average performance improvement of 1.7\\\\\\\\% to 20.5\\\\\\\\% across uncertainty-guided prompts.\"\\n      },\\n      {\\n        \"paperId\": \"f3edc5a87051be2b4b249c4896c8ab14462c659c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f3edc5a87051be2b4b249c4896c8ab14462c659c\",\\n        \"title\": \"Knowledge-based dynamic prompt learning for multi-label disease diagnosis\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.knosys.2024.111395?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.knosys.2024.111395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2259963003\",\\n            \"name\": \"Jing Xie\"\\n          },\\n          {\\n            \"authorId\": \"2280033365\",\\n            \"name\": \"Xin Li\"\\n          },\\n          {\\n            \"authorId\": \"2279980846\",\\n            \"name\": \"Ye Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2153979217\",\\n            \"name\": \"Yi Guan\"\\n          },\\n          {\\n            \"authorId\": \"2259229\",\\n            \"name\": \"Jingchi Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2279958972\",\\n            \"name\": \"Xitong Guo\"\\n          },\\n          {\\n            \"authorId\": \"2280535977\",\\n            \"name\": \"Xin Peng\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"81ca1ca9b81470988d29e0d0def9712320243e06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/81ca1ca9b81470988d29e0d0def9712320243e06\",\\n        \"title\": \"HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target Binding Affinity Prediction\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.17697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic prompts within a hybrid Graph-Transformer framework, and outperforms state-of-the-art DTA prediction methods in both prediction performance and generalization ability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2349232014\",\\n            \"name\": \"Xi Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2275183852\",\\n            \"name\": \"Wentao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2303777566\",\\n            \"name\": \"Jiacheng Xie\"\\n          },\\n          {\\n            \"authorId\": \"2308231466\",\\n            \"name\": \"Lijing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2308228559\",\\n            \"name\": \"Gaofei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2303708410\",\\n            \"name\": \"Zhengji Li\"\\n          },\\n          {\\n            \"authorId\": \"2257021623\",\\n            \"name\": \"Tianyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2308674053\",\\n            \"name\": \"Min Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Drug target binding affinity (DTA) is a key criterion for drug screening. Existing experimental methods are time-consuming and rely on limited structural and domain information. While learning-based methods can model sequence and structural information, they struggle to integrate contextual data and often lack comprehensive modeling of drug-target interactions. In this study, we propose a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic prompts within a hybrid Graph-Transformer framework. Our method generates context-specific prompts for each drug-target pair, enhancing the model\\'s ability to capture unique interactions. The introduction of prompt tuning further optimizes the prediction process by filtering out irrelevant noise and emphasizing task-relevant information, dynamically adjusting the input features of the molecular graph. The proposed hybrid Graph-Transformer architecture combines structural information from Graph Convolutional Networks (GCNs) with sequence information captured by Transformers, facilitating the interaction between global and local information. Additionally, we adopted the multi-view feature fusion method to project molecular graph views and affinity subgraph views into a common feature space, effectively combining structural and contextual information. Experiments on two widely used public datasets, Davis and KIBA, show that HGTDP-DTA outperforms state-of-the-art DTA prediction methods in both prediction performance and generalization ability.\"\\n      },\\n      {\\n        \"paperId\": \"3967189742efab8859da542ce3953d4c72957aca\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3967189742efab8859da542ce3953d4c72957aca\",\\n        \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.23904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI), designed to design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275054108\",\\n            \"name\": \"Qinqian Lei\"\\n          },\\n          {\\n            \"authorId\": \"2313081973\",\\n            \"name\": \"Bo Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256998291\",\\n            \"name\": \"Robby T. Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.\"\\n      },\\n      {\\n        \"paperId\": \"1d24f2dfe942e2d67381d8fcfac3423145bd557f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d24f2dfe942e2d67381d8fcfac3423145bd557f\",\\n        \"title\": \"DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.10588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention that outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1390537791\",\\n            \"name\": \"Xiangpeng Yang\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2109074276\",\\n            \"name\": \"Xiaohan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-video retrieval is a critical multi-modal task to find the most relevant video for a text query. Although pretrained models like CLIP have demonstrated impressive potential in this area, the rising cost of fully finetuning these models due to increasing model size continues to pose a problem. To address this challenge, prompt tuning has emerged as an alternative. However, existing works still face two problems when adapting pretrained image-text models to downstream video-text tasks: (1) The visual encoder could only encode frame-level features and failed to extract global-level general video information. (2) Equipping the visual and text encoder with separated prompts failed to mitigate the visual-text modality gap. To this end, we propose DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention. In contrast to previous prompt tuning methods, we employ the shared latent space to generate local-level text and frame prompts that encourage inter-modal interaction. Furthermore, we propose modeling video in a global-local attention mechanism to capture global video information from the perspective of prompt tuning. Extensive experiments reveal that when only 0.67% parameters are tuned, our cross-modal prompt tuning strategy DGL outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets. Code will be available at https://github.com/knightyxp/DGL.\"\\n      },\\n      {\\n        \"paperId\": \"92d477bb89b43a73b6e8ca54ae2e1def9f3fe2dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/92d477bb89b43a73b6e8ca54ae2e1def9f3fe2dd\",\\n        \"title\": \"UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes the first universal CSR framework, dubbed UCIP, with dynamic prompt learning, intending to jointly support the CSR distortions of any compression codecs/modes, and introduces the novel MLP-like framework backbone for the UCIP.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2253011048\",\\n            \"name\": \"Xin Li\"\\n          },\\n          {\\n            \"authorId\": \"2289786700\",\\n            \"name\": \"Bingchen Li\"\\n          },\\n          {\\n            \"authorId\": \"2288532590\",\\n            \"name\": \"Yeying Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"2282209914\",\\n            \"name\": \"Hanxin Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2232648762\",\\n            \"name\": \"Yulin Ren\"\\n          },\\n          {\\n            \"authorId\": \"2287374267\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Compressed Image Super-resolution (CSR) aims to simultaneously super-resolve the compressed images and tackle the challenging hybrid distortions caused by compression. However, existing works on CSR usually focuses on a single compression codec, i.e., JPEG, ignoring the diverse traditional or learning-based codecs in the practical application, e.g., HEVC, VVC, HIFIC, etc. In this work, we propose the first universal CSR framework, dubbed UCIP, with dynamic prompt learning, intending to jointly support the CSR distortions of any compression codecs/modes. Particularly, an efficient dynamic prompt strategy is proposed to mine the content/spatial-aware task-adaptive contextual information for the universal CSR task, using only a small amount of prompts with spatial size 1x1. To simplify contextual information mining, we introduce the novel MLP-like framework backbone for our UCIP by adapting the Active Token Mixer (ATM) to CSR tasks for the first time, where the global information modeling is only taken in horizontal and vertical directions with offset prediction. We also build an all-in-one benchmark dataset for the CSR task by collecting the datasets with the popular 6 diverse traditional and learning-based codecs, including JPEG, HEVC, VVC, HIFIC, etc., resulting in 23 common degradations. Extensive experiments have shown the consistent and excellent performance of our UCIP on universal CSR tasks. The project can be found in https://lixinustc.github.io/UCIP.github.io\"\\n      },\\n      {\\n        \"paperId\": \"dbad406ab82697b153d28c6654e7808187859701\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dbad406ab82697b153d28c6654e7808187859701\",\\n        \"title\": \"Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.07221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The essence of IDPT is to develop a dynamic prompt generation module to perceive semantic prior features of each point cloud instance and generate adaptive prompt tokens to enhance the model\\'s robustness, providing a promising solution to parameter-efficient learning for pre-trained point cloud models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-04-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2214515552\",\\n            \"name\": \"Yaohua Zha\"\\n          },\\n          {\\n            \"authorId\": \"2110174485\",\\n            \"name\": \"Jinpeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"151470385\",\\n            \"name\": \"Tao Dai\"\\n          },\\n          {\\n            \"authorId\": \"50461900\",\\n            \"name\": \"Bin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2108389544\",\\n            \"name\": \"Zhi Wang\"\\n          },\\n          {\\n            \"authorId\": \"3085483\",\\n            \"name\": \"Shutao Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained point cloud models have found extensive applications in 3D understanding tasks like object classification and part segmentation. However, the prevailing strategy of full fine-tuning in downstream tasks leads to large per-task storage overhead for model parameters, which limits the efficiency when applying large-scale pre-trained models. Inspired by the recent success of visual prompt tuning (VPT), this paper attempts to explore prompt tuning on pre-trained point cloud models, to pursue an elegant balance between performance and parameter efficiency. We find while instance-agnostic static prompting, e.g. VPT, shows some efficacy in downstream transfer, it is vulnerable to the distribution diversity caused by various types of noises in real-world point cloud data. To conquer this limitation, we propose a novel Instance-aware Dynamic Prompt Tuning (IDPT) strategy for pre-trained point cloud models. The essence of IDPT is to develop a dynamic prompt generation module to perceive semantic prior features of each point cloud instance and generate adaptive prompt tokens to enhance the model\\'s robustness. Notably, extensive experiments demonstrate that IDPT outperforms full finetuning in most tasks with a mere 7% of the trainable parameters, providing a promising solution to parameter-efficient learning for pre-trained point cloud models. Code is available at https://github.com/zyh16143998882/ICCV23-IDPT.\"\\n      },\\n      {\\n        \"paperId\": \"51ac2dceecc75431eab5b2c320822e081b282b27\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51ac2dceecc75431eab5b2c320822e081b282b27\",\\n        \"title\": \"SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models\",\\n        \"citationCount\": 52,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed SwapPrompt is a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation and achieves state-of-the-art test-time adaptation performance on ImageNet and nine other datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2139700088\",\\n            \"name\": \"Xiaosong Ma\"\\n          },\\n          {\\n            \"authorId\": \"2265432385\",\\n            \"name\": \"Jie Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2277415299\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"50232004\",\\n            \"name\": \"Wenchao Xu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"0911ae6be53e4e649eb8da3d8ae738f65e93993d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0911ae6be53e4e649eb8da3d8ae738f65e93993d\",\\n        \"title\": \"Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2402.18447\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.18447, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts and the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158676096\",\\n            \"name\": \"Deng Li\"\\n          },\\n          {\\n            \"authorId\": \"48352212\",\\n            \"name\": \"Aming Wu\"\\n          },\\n          {\\n            \"authorId\": \"2288039856\",\\n            \"name\": \"Yaowei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261890091\",\\n            \"name\": \"Yahong Han\"\\n          }\\n        ],\\n        \"abstract\": \"Single-domain generalization aims to learn a model from single source domain data attaining generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in crossdomain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and versatility of our proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"666cced301ad62e8cfb9a5ca02bb4f5320dbc0a5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/666cced301ad62e8cfb9a5ca02bb4f5320dbc0a5\",\\n        \"title\": \"Unsupervised Domain Adaptation Enhanced by Fuzzy Prompt Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TFUZZ.2024.3389705?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TFUZZ.2024.3389705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2297072145\",\\n            \"name\": \"Kuo Shi\"\\n          },\\n          {\\n            \"authorId\": \"2272678886\",\\n            \"name\": \"Jie Lu\"\\n          },\\n          {\\n            \"authorId\": \"2243460919\",\\n            \"name\": \"Zhen Fang\"\\n          },\\n          {\\n            \"authorId\": \"46266495\",\\n            \"name\": \"Guangquan Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) addresses the challenge of distribution shift between a labeled source domain and an unlabeled target domain by utilizing knowledge from the source. Traditional UDA methods mainly focus on single-modal scenarios, either vision or language, thus, not fully exploring the advantages of multimodal representations. Visionlanguage models utilize multimodal information, applying prompt learning techniques for addressing target domain tasks. Motivated by the recent advancements in pretrained visionlanguage models, this article expands the UDA framework to incorporate multimodal approaches using fuzzy techniques. The adoption of fuzzy techniques, preferred over conventional domain adaptation methods, is based on the following two key aspects: 1) the nature of prompt learning is intrinsically linked to fuzzy logic, and 2) the superior capability of fuzzy techniques in processing soft information and effectively utilizing inherent relationships both within and across domains. To this end, we propose UDA enhanced by fuzzy prompt learning (FUZZLE), a simple and effective method for aligning the source and target domains via domain-specific prompt learning. Specifically, we introduce a novel technique to enhance prompt learning in the target domain. This method integrates fuzzy C-means clustering and a novel instance-level fuzzy vector into the prompt learning loss function, minimizing the distance between prompt cluster centers and instance prompts, thereby, enhancing the prompt learning process. In addition, we propose a Kullback\\\\u2013Leibler (KL) divergence-based loss function with a fuzzification factor. This function is designed to minimize the distribution discrepancy in the classification of similar cross-domain data, aligning domain-specific prompts during the training process. We contribute an in-depth analysis to understand the effectiveness of FUZZLE. Extensive experiments demonstrate that our method achieves superior performance on standard UDA benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"5d48b087d3d4a16904c7e5442a4a815ff901cc0d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5d48b087d3d4a16904c7e5442a4a815ff901cc0d\",\\n        \"title\": \"CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17369, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoDA is proposed which instructs models to distinguish, focus, and learn from discrepancies within all adverse scenes at scene and image levels, and achieves SOTA performances on widely-used benchmarks under all adverse scenes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261751604\",\\n            \"name\": \"Ziyang Gong\"\\n          },\\n          {\\n            \"authorId\": \"2261889755\",\\n            \"name\": \"Fuhao Li\"\\n          },\\n          {\\n            \"authorId\": \"2262092854\",\\n            \"name\": \"Yupeng Deng\"\\n          },\\n          {\\n            \"authorId\": \"2293394749\",\\n            \"name\": \"Deblina Bhattacharjee\"\\n          },\\n          {\\n            \"authorId\": \"2293450075\",\\n            \"name\": \"Xiangwei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261751710\",\\n            \"name\": \"Zhenming Ji\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised Domain Adaptation (UDA) aims to adapt models from labeled source domains to unlabeled target domains. When adapting to adverse scenes, existing UDA methods fail to perform well due to the lack of instructions, leading their models to overlook discrepancies within all adverse scenes. To tackle this, we propose CoDA which instructs models to distinguish, focus, and learn from these discrepancies at scene and image levels. Specifically, CoDA consists of a Chain-of-Domain (CoD) strategy and a Severity-Aware Visual Prompt Tuning (SAVPT) mechanism. CoD focuses on scene-level instructions to divide all adverse scenes into easy and hard scenes, guiding models to adapt from source to easy domains with easy scene images, and then to hard domains with hard scene images, thereby laying a solid foundation for whole adaptations. Building upon this foundation, we employ SAVPT to dive into more detailed image-level instructions to boost performance. SAVPT features a novel metric Severity that divides all adverse scene images into low-severity and high-severity images. Then Severity directs visual prompts and adapters, instructing models to concentrate on unified severity features instead of scene-specific features, without adding complexity to the model architecture. CoDA achieves SOTA performances on widely-used benchmarks under all adverse scenes. Notably, CoDA outperforms the existing ones by 4.6%, and 10.3% mIoU on the Foggy Driving, and Foggy Zurich benchmarks, respectively. Our code is available at https://github.com/Cuzyoung/CoDA\"\\n      },\\n      {\\n        \"paperId\": \"8d531cb8cf51eec3b8f1106d189295fa3c81c02a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8d531cb8cf51eec3b8f1106d189295fa3c81c02a\",\\n        \"title\": \"Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing\",\\n        \"citationCount\": 83,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.15664\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.15664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Dynamic Prompt Learning (DPL) is proposed to force cross-attention maps to focus on correct noun words in the text prompt by updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses to achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2237135280\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"2115413065\",\\n            \"name\": \"Fei Yang\"\\n          },\\n          {\\n            \"authorId\": \"49080524\",\\n            \"name\": \"Shiqi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2248121772\",\\n            \"name\": \"Muhammad Atif Butt\"\\n          },\\n          {\\n            \"authorId\": \"2820687\",\\n            \"name\": \"Joost van de Weijer\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale text-to-image generative models have been a ground-breaking development in generative AI, with diffusion models showing their astounding ability to synthesize convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are susceptible to unintended modifications of regions outside the targeted area, such as on the background or on distractor objects which have some semantic or visual relationship with the targeted object. According to our experimental findings, inaccurate cross-attention maps are at the root of this problem. Based on this observation, we propose Dynamic Prompt Learning (DPL) to force cross-attention maps to focus on correct noun words in the text prompt. By updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses, we achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions. Our method DPL, based on the publicly available Stable Diffusion, is extensively evaluated on a wide range of images, and consistently obtains superior results both quantitatively (CLIP score, Structure-Dist) and qualitatively (on user-evaluation). We show improved prompt editing results for Word-Swap, Prompt Refinement, and Attention Re-weighting, especially for complex multi-object scenes.\"\\n      },\\n      {\\n        \"paperId\": \"b1b688cf506f64bc4a06e037bfcdfa8b22be51aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b1b688cf506f64bc4a06e037bfcdfa8b22be51aa\",\\n        \"title\": \"Cascade Prompt Learning for Vision-Language Model Adaptation\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.17805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel Cascade Prompt Learning CasPL framework to enable prompt learning to serve both generic and specific expertise simultaneously, which achieves a significantly better balance between performance and inference speed, which is especially beneficial for deploying smaller VLM models in resource-constrained environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-09-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2212236182\",\\n            \"name\": \"Ge Wu\"\\n          },\\n          {\\n            \"authorId\": \"2268132119\",\\n            \"name\": \"Xin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2146248526\",\\n            \"name\": \"Zheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2268797049\",\\n            \"name\": \"Zhaowei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2320744562\",\\n            \"name\": \"Jiajun Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284825678\",\\n            \"name\": \"Jian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2320925245\",\\n            \"name\": \"Xiang Li\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt learning has surfaced as an effective approach to enhance the performance of Vision-Language Models (VLMs) like CLIP when applied to downstream tasks. However, current learnable prompt tokens are primarily used for the single phase of adapting to tasks (i.e., adapting prompt), easily leading to overfitting risks. In this work, we propose a novel Cascade Prompt Learning CasPL framework to enable prompt learning to serve both generic and specific expertise (i.e., boosting and adapting prompt) simultaneously. Specifically, CasPL is a new learning paradigm comprising two distinct phases of learnable prompts: the first boosting prompt is crafted to extract domain-general knowledge from a senior larger CLIP teacher model by aligning their predicted logits using extensive unlabeled domain images. The second adapting prompt is then cascaded with the frozen first set to fine-tune the downstream tasks, following the approaches employed in prior research. In this manner, CasPL can effectively capture both domain-general and task-specific representations into explicitly different gradual groups of prompts, thus potentially alleviating overfitting issues in the target domain. It\\'s worth noting that CasPL serves as a plug-and-play module that can seamlessly integrate into any existing prompt learning approach. CasPL achieves a significantly better balance between performance and inference speed, which is especially beneficial for deploying smaller VLM models in resource-constrained environments. Compared to the previous state-of-the-art method PromptSRC, CasPL shows an average improvement of 1.85% for base classes, 3.44% for novel classes, and 2.72% for the harmonic mean over 11 image classification datasets. Code is publicly available at: https://github.com/megvii-research/CasPL.\"\\n      },\\n      {\\n        \"paperId\": \"3e565c544a8639cc9c7568833e484d7610f5e5d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4\",\\n        \"title\": \"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning\",\\n        \"citationCount\": 372,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2209.14610\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.14610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel approach is proposed, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example, which verifies its effectiveness in selecting in- context examples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-09-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2887562\",\\n            \"name\": \"Pan Lu\"\\n          },\\n          {\\n            \"authorId\": \"47659905\",\\n            \"name\": \"Liang Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2782886\",\\n            \"name\": \"Kai-Wei Chang\"\\n          },\\n          {\\n            \"authorId\": \"39092098\",\\n            \"name\": \"Y. Wu\"\\n          },\\n          {\\n            \"authorId\": \"145380991\",\\n            \"name\": \"Song-Chun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2590556\",\\n            \"name\": \"Tanmay Rajpurohit\"\\n          },\\n          {\\n            \"authorId\": \"48323507\",\\n            \"name\": \"Peter Clark\"\\n          },\\n          {\\n            \"authorId\": \"51043791\",\\n            \"name\": \"A. Kalyan\"\\n          }\\n        ],\\n        \"abstract\": \"Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.\"\\n      },\\n      {\\n        \"paperId\": \"fc31c17e27c23cc89f91c110acd68b9b66ee0d40\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fc31c17e27c23cc89f91c110acd68b9b66ee0d40\",\\n        \"title\": \"Dynamic prompt-based virtual assistant framework for BIM information search\",\\n        \"citationCount\": 82,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.autcon.2023.105067?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.autcon.2023.105067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2202987937\",\\n            \"name\": \"Junwen Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2113447170\",\\n            \"name\": \"M. Fischer\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"4f9766356478ddf606457af735f70682839194b1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4f9766356478ddf606457af735f70682839194b1\",\\n        \"title\": \"VPA: Fully Test-Time Visual Prompt Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.15251\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.15251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents Visual Prompt Adaptation (VPA), the first framework that generalizes visual prompting with test-time adaptation, and evaluates VPA on multiple tasks, including out-of-distribution generalization, corruption robustness, and domain adaptation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-09-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135326251\",\\n            \"name\": \"Jiachen Sun\"\\n          },\\n          {\\n            \"authorId\": \"2249088902\",\\n            \"name\": \"Mark Ibrahim\"\\n          },\\n          {\\n            \"authorId\": \"2248025897\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"22229139\",\\n            \"name\": \"I. Evtimov\"\\n          },\\n          {\\n            \"authorId\": \"3895596\",\\n            \"name\": \"Z. Morley Mao\"\\n          },\\n          {\\n            \"authorId\": \"66286536\",\\n            \"name\": \"Cris-tian Cant\\\\u00f3n Ferrer\"\\n          },\\n          {\\n            \"authorId\": \"3322806\",\\n            \"name\": \"C. Hazirbas\"\\n          }\\n        ],\\n        \"abstract\": \"Textual prompt tuning has demonstrated significant performance improvements in adapting natural language processing models to a variety of downstream tasks by treating hand-engineered prompts as trainable parameters. Inspired by the success of textual prompting, several studies have investigated the efficacy of visual prompt tuning. In this work, we present Visual Prompt Adaptation (VPA), the first framework that generalizes visual prompting with test-time adaptation. VPA introduces a small number of learnable tokens, enabling fully test-time and storage-efficient adaptation without necessitating source-domain information. We examine our VPA design under diverse adaptation settings, encompassing single-image, batched-image, and pseudo-label adaptation. We evaluate VPA on multiple tasks, including out-of-distribution (OOD) generalization, corruption robustness, and domain adaptation. Experimental results reveal that VPA effectively enhances OOD generalization by 3.3% across various models, surpassing previous test-time approaches. Furthermore, we show that VPA improves corruption robustness by 6.5% compared to strong baselines. Finally, we demonstrate that VPA also boosts domain adaptation performance by relatively 5.2%. Our VPA also exhibits marked effectiveness in improving the robustness of zero-shot recognition for vision-language models.\"\\n      },\\n      {\\n        \"paperId\": \"546caf796b0ae3578852a5de8fe34f19f2de0cba\",\\n        \"url\": \"https://www.semanticscholar.org/paper/546caf796b0ae3578852a5de8fe34f19f2de0cba\",\\n        \"title\": \"Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.11484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enabling efficient policy learning and adaptation upon a wide range of environmental and physical changes encountered by embodied agents.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-12-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287939397\",\\n            \"name\": \"Wonje Choi\"\\n          },\\n          {\\n            \"authorId\": \"2174282424\",\\n            \"name\": \"Woo Kyung Kim\"\\n          },\\n          {\\n            \"authorId\": \"2288614462\",\\n            \"name\": \"Seunghyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"35756158\",\\n            \"name\": \"Hong-Suh Woo\"\\n          }\\n        ],\\n        \"abstract\": \"For embodied reinforcement learning (RL) agents interacting with the environment, it is desirable to have rapid policy adaptation to unseen visual observations, but achieving zero-shot adaptation capability is considered as a challenging problem in the RL context. To address the problem, we present a novel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained vision-language model and a set of visual prompts, thus enabling efficient policy learning and adaptation upon a wide range of environmental and physical changes encountered by embodied agents. Specifically, we devise a guided-attention-based ensemble approach with multiple visual prompts on the vision-language model to construct robust state representations. Each prompt is contrastively learned in terms of an individual domain factor that significantly affects the agent\\'s egocentric perception and observation. For a given task, the attention-based ensemble and policy are jointly learned so that the resulting state representations not only generalize to various domains but are also optimized for learning the task. Through experiments, we show that ConPE outperforms other state-of-the-art algorithms for several embodied agent tasks including navigation in AI2THOR, manipulation in egocentric-Metaworld, and autonomous driving in CARLA, while also improving the sample efficiency of policy learning and adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n        \"title\": \"Prompt-and-Transfer: Dynamic Class-Aware Enhancement for Few-Shot Segmentation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.10389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel and powerful prompt-driven scheme, called \\\\u201cPrompt and Transfer\\\\u201d (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216452880\",\\n            \"name\": \"Hanbo Bi\"\\n          },\\n          {\\n            \"authorId\": \"6962129\",\\n            \"name\": \"Yingchao Feng\"\\n          },\\n          {\\n            \"authorId\": \"2600667\",\\n            \"name\": \"Wenhui Diao\"\\n          },\\n          {\\n            \"authorId\": \"152702629\",\\n            \"name\": \"Peijin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2161856190\",\\n            \"name\": \"Yongqiang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2303405697\",\\n            \"name\": \"Kun Fu\"\\n          },\\n          {\\n            \"authorId\": \"2242774676\",\\n            \"name\": \"Hongqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2177485483\",\\n            \"name\": \"Xian Sun\"\\n          }\\n        ],\\n        \"abstract\": \"For more efficient generalization to unseen domains (classes), most Few-shot Segmentation (FSS) would directly exploit pre-trained encoders and only fine-tune the decoder, especially in the current era of large models. However, such fixed feature encoders tend to be class-agnostic, inevitably activating objects that are irrelevant to the target class. In contrast, humans can effortlessly focus on specific objects in the line of sight. This paper mimics the visual perception pattern of human beings and proposes a novel and powerful prompt-driven scheme, called \\\\u201cPrompt and Transfer\\\\u201d (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task. Three key points are elaborated to enhance the prompting: 1) Cross-modal linguistic information is introduced to initialize prompts for each task. 2) Semantic Prompt Transfer (SPT) that precisely transfers the class-specific semantics within the images to prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT to adaptively generate different but complementary part prompts for different individuals. Surprisingly, PAT achieves competitive performance on 4 different tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new state-of-the-arts on 11 benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yu-Feng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"bd46cb09425c1eb8e2d2e7dd612d839cdf4d0f39\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bd46cb09425c1eb8e2d2e7dd612d839cdf4d0f39\",\\n        \"title\": \"Prompt-based Distribution Alignment for Unsupervised Domain Adaptation\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.09553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper experimentally demonstrates that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA and proposes a Prompt-based Distribution Alignment method to incorporate the domain knowledge into prompt learning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2275570714\",\\n            \"name\": \"Min Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"122132048\",\\n            \"name\": \"Siteng Huang\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275032226\",\\n            \"name\": \"Donglin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, despite the unprecedented success of large pre-trained visual-language models (VLMs) on a wide range of downstream tasks, the real-world unsupervised domain adaptation (UDA) problem is still not well explored. Therefore, in this paper, we first experimentally demonstrate that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA. However, a major challenge for directly deploying such models on downstream UDA tasks is prompt engineering, which requires aligning the domain knowledge of source and target domains, since the performance of UDA is severely influenced by a good domain-invariant representation. We further propose a Prompt-based Distribution Alignment (PDA) method to incorporate the domain knowledge into prompt learning. Specifically, PDA employs a two-branch prompt-tuning paradigm, namely base branch and alignment branch. The base branch focuses on integrating class-related representation into prompts, ensuring discrimination among different classes. To further minimize domain discrepancy, for the alignment branch, we construct feature banks for both the source and target domains and propose image-guided feature tuning (IFT) to make the input attend to feature banks, which effectively integrates self-enhanced and cross-domain features into the model. In this way, these two branches can be mutually promoted to enhance the adaptation of VLMs for UDA. We conduct extensive experiments on three benchmarks to demonstrate that our proposed PDA achieves state-of-the-art performance. The code is available at https://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.\"\\n      },\\n      {\\n        \"paperId\": \"fa9d49f32440aff7417ce46419d1073239b58b5b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa9d49f32440aff7417ce46419d1073239b58b5b\",\\n        \"title\": \"API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.03987\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.03987, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERE), an API entity-relation joint extraction model based on the large pre-trained language model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2181288908\",\\n            \"name\": \"Qing Huang\"\\n          },\\n          {\\n            \"authorId\": \"2199853522\",\\n            \"name\": \"Yanbang Sun\"\\n          },\\n          {\\n            \"authorId\": \"3138980\",\\n            \"name\": \"Zhenchang Xing\"\\n          },\\n          {\\n            \"authorId\": \"2152596785\",\\n            \"name\": \"Mingming Yu\"\\n          },\\n          {\\n            \"authorId\": \"3087664\",\\n            \"name\": \"Xiwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2151674574\",\\n            \"name\": \"Qinghua Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.\"\\n      },\\n      {\\n        \"paperId\": \"aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n        \"url\": \"https://www.semanticscholar.org/paper/aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n        \"title\": \"Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2304.03119\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.03119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Qualitative and quantitative evaluations on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-04-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2148900054\",\\n            \"name\": \"Jiayi Guo\"\\n          },\\n          {\\n            \"authorId\": \"2135747990\",\\n            \"name\": \"Chaofei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2115860363\",\\n            \"name\": \"You Wu\"\\n          },\\n          {\\n            \"authorId\": \"2176400796\",\\n            \"name\": \"Eric Zhang\"\\n          },\\n          {\\n            \"authorId\": \"37833805\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"101246501\",\\n            \"name\": \"Xingqian Xu\"\\n          },\\n          {\\n            \"authorId\": \"30619669\",\\n            \"name\": \"S. Song\"\\n          },\\n          {\\n            \"authorId\": \"48667025\",\\n            \"name\": \"Humphrey Shi\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, CLIP-guided image synthesis has shown appealing performance on adapting a pre-trained source-domain generator to an unseen target domain. It does not require any target-domain samples but only the textual domain labels. The training is highly efficient, e.g., a few minutes. However, existing methods still have some limitations in the quality of generated images and may suffer from the mode collapse issue. A key reason is that a fixed adaptation direction is applied for all cross-domain image pairs, which leads to identical supervision signals. To address this issue, we propose an Image-specific Prompt Learning (IPL) method, which learns specific prompt vectors for each source-domain image. This produces a more precise adaptation direction for every cross-domain image pair, endowing the target-domain generator with greatly enhanced flexibility. Qualitative and quantitative evaluations on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse. Moreover, IPL is independent of the structure of the generative model, such as generative adversarial networks or diffusion models. Code is available at https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"5a8541bb468afb45d68e16d2f83b327768f97c67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5a8541bb468afb45d68e16d2f83b327768f97c67\",\\n        \"title\": \"Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.07648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to apply an auxiliary video captioning objective to train the frame representations, which facilitates the learning of detailed video semantics by providing fine-grained guidance in the semantic space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41036094\",\\n            \"name\": \"Chaorui Deng\"\\n          },\\n          {\\n            \"authorId\": \"2115814744\",\\n            \"name\": \"Qi Chen\"\\n          },\\n          {\\n            \"authorId\": \"1937786\",\\n            \"name\": \"Pengda Qin\"\\n          },\\n          {\\n            \"authorId\": \"73286206\",\\n            \"name\": \"Dave Zhenyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2143599197\",\\n            \"name\": \"Qi Wu\"\\n          }\\n        ],\\n        \"abstract\": \"In text-video retrieval, recent works have benefited from the powerful learning capabilities of pre-trained text-image foundation models (e.g., CLIP) by adapting them to the video domain. A critical problem for them is how to effectively capture the rich semantics inside the video using the image encoder of CLIP. To tackle this, state-of-the-art methods adopt complex cross-modal modeling techniques to fuse the text information into video frame representations, which, however, incurs severe efficiency issues in large-scale retrieval systems as the video representations must be recomputed online for every text query. In this paper, we discard this problematic cross-modal fusion process and aim to learn semantically-enhanced representations purely from the video, so that the video representations can be computed offline and reused for different texts. Concretely, we first introduce a spatial-temporal \\\\\"Prompt Cube\\\\\" into the CLIP image encoder and iteratively switch it within the encoder layers to efficiently incorporate the global video semantics into frame representations. We then propose to apply an auxiliary video captioning objective to train the frame representations, which facilitates the learning of detailed video semantics by providing fine-grained guidance in the semantic space. With a naive temporal fusion strategy (i.e., mean-pooling) on the enhanced frame representations, we obtain state-of-the-art performances on three benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.\"\\n      },\\n      {\\n        \"paperId\": \"a4a26d7c6bc022f604a246c6ce6cc1cfbe618441\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a4a26d7c6bc022f604a246c6ce6cc1cfbe618441\",\\n        \"title\": \"APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.emnlp-main.567.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.emnlp-main.567?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.emnlp-main.567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Attention Prompt tuning method, namely AP ROMPT, which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2266712798\",\\n            \"name\": \"Qifan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2272672481\",\\n            \"name\": \"Yuning Mao\"\\n          },\\n          {\\n            \"authorId\": \"2109593338\",\\n            \"name\": \"Jingang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273361006\",\\n            \"name\": \"Hanchao Yu\"\\n          },\\n          {\\n            \"authorId\": \"35557488\",\\n            \"name\": \"Shaoliang Nie\"\\n          },\\n          {\\n            \"authorId\": \"2237101143\",\\n            \"name\": \"Sinong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2271382818\",\\n            \"name\": \"Fuli Feng\"\\n          },\\n          {\\n            \"authorId\": \"2273197501\",\\n            \"name\": \"Lifu Huang\"\\n          },\\n          {\\n            \"authorId\": \"2266062308\",\\n            \"name\": \"Xiaojun Quan\"\\n          },\\n          {\\n            \"authorId\": \"2273481670\",\\n            \"name\": \"Zenglin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2264378887\",\\n            \"name\": \"Dongfang Liu\"\\n          }\\n        ],\\n        \"abstract\": \"With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large pre-trained language models. However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement. In this work, we propose a novel Attention Prompt tuning method, namely AP ROMPT , for efficient adaptation of pre-trained language models. We first demonstrate that existing prompt tuning can be considered as a special case of attention prompt tuning. We then formally introduce AP ROMPT , which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning. Experimental results on the SuperGLUE benchmark consistently demonstrate that our proposed approach outperforms state-of-the-art baselines and full fine-tuning method with pre-trained models at different scales. In addition, a comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f849b873e94f28e1f2a3e1dc4d7bef17eb64adab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f849b873e94f28e1f2a3e1dc4d7bef17eb64adab\",\\n        \"title\": \"Towards Stable Test-Time Adaptation in Dynamic Wild World\",\\n        \"citationCount\": 372,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2302.12400\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.12400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1411039233\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"2239515454\",\\n            \"name\": \"Jiaxiang Wu\"\\n          },\\n          {\\n            \"authorId\": \"2108463891\",\\n            \"name\": \"Yifan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143976851\",\\n            \"name\": \"Z. Wen\"\\n          },\\n          {\\n            \"authorId\": \"1816749481\",\\n            \"name\": \"Yaofo Chen\"\\n          },\\n          {\\n            \"authorId\": \"144259957\",\\n            \"name\": \"P. Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2823637\",\\n            \"name\": \"Mingkui Tan\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, \\\\\\\\ie, group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, \\\\\\\\ie, assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5c76e929496414ff9b31f100eaf72ff46ae4d8d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5c76e929496414ff9b31f100eaf72ff46ae4d8d7\",\\n        \"title\": \"Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.11808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyT is proposed, a novel approach to improve both parameter and inference efficiency for ViT adaptation by proposing a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292217857\",\\n            \"name\": \"Wangbo Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2284481794\",\\n            \"name\": \"Jiasheng Tang\"\\n          },\\n          {\\n            \"authorId\": \"40961502\",\\n            \"name\": \"Yizeng Han\"\\n          },\\n          {\\n            \"authorId\": \"2292187802\",\\n            \"name\": \"Yibing Song\"\\n          },\\n          {\\n            \"authorId\": \"2292214744\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"2249906537\",\\n            \"name\": \"Gao Huang\"\\n          },\\n          {\\n            \"authorId\": \"1716453\",\\n            \"name\": \"F. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2283134324\",\\n            \"name\": \"Yang You\"\\n          }\\n        ],\\n        \"abstract\": \"Existing parameter-efficient fine-tuning (PEFT) methods have achieved significant success on vision transformers (ViTs) adaptation by improving parameter efficiency. However, the exploration of enhancing inference efficiency during adaptation remains underexplored. This limits the broader application of pre-trained ViT models, especially when the model is computationally extensive. In this paper, we propose Dynamic Tuning (DyT), a novel approach to improve both parameter and inference efficiency for ViT adaptation. Specifically, besides using the lightweight adapter modules, we propose a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference. Additionally, we explore multiple design variants to find the best practice of DyT. Finally, inspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced adapter to further boost the adaptation performance. We validate DyT across various tasks, including image/video recognition and semantic segmentation. For instance, DyT achieves superior performance compared to existing PEFT methods while evoking only 71% of their FLOPs on the VTAB-1K benchmark.\"\\n      },\\n      {\\n        \"paperId\": \"64ffcf31b03aaae2905e06f9e24be58f51e2f776\",\\n        \"url\": \"https://www.semanticscholar.org/paper/64ffcf31b03aaae2905e06f9e24be58f51e2f776\",\\n        \"title\": \"Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2311.18363\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.18363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the pretrained model and proposes the Visual Prompt-based Test-Time Adaptation (VPTTA) method, which is lightweight with only a few parameters and can be effectively trained in a single iteration, and presents the low-frequency prompt, which is lightweight with only a few parameters and can be effectively trained in a single iteration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268795762\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2170220899\",\\n            \"name\": \"Yiwen Ye\"\\n          },\\n          {\\n            \"authorId\": \"143746093\",\\n            \"name\": \"Mengkang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2268798668\",\\n            \"name\": \"Yongsheng Pan\"\\n          },\\n          {\\n            \"authorId\": \"2256029665\",\\n            \"name\": \"Yong Xia\"\\n          }\\n        ],\\n        \"abstract\": \"Distribution shift widely exists in medical images acquired from different medical centres and poses a significant obstacle to deploying the pretrained semantic segmentation model in real-world applications. Test-time adaptation has proven its effectiveness in tackling the cross-domain distribution shift during inference. However, most existing methods achieve adaptation by updating the pretrained models, rendering them susceptible to error accumulation and catastrophic forgetting when encountering a series of distribution shifts (i.e., under the continual test-time adaptation setup). To overcome these challenges caused by updating the models, in this paper, we freeze the pretrained model and propose the Visual Prompt-based Test-Time Adaptation (VPTTA) method to train a specific prompt for each test image to align the statistics in the batch normalization layers. Specifically, we present the low-frequency prompt, which is lightweight with only a few parameters and can be effectively trained in a single iteration. To enhance prompt initialization, we equip VPTTA with a memory bank to benefit the current prompt from previous ones. Additionally, we design a warm-up mechanism, which mixes source and target statistics to construct warm-up statistics, thereby facilitating the training process. Extensive experiments demonstrate the superiority of our VPTTA over other state-of-the-art methods on two medical image segmentation benchmark tasks. The code and weights of pretrained source models are available at https://github.com/Chen-Ziyang/VPTTA.\"\\n      },\\n      {\\n        \"paperId\": \"8125cc111e55b9d3e14bf3df9aba5d10c08417f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8125cc111e55b9d3e14bf3df9aba5d10c08417f1\",\\n        \"title\": \"A dynamic collaborative adversarial domain adaptation network for unsupervised rotating machinery fault diagnosis\",\\n        \"citationCount\": 76,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.ress.2024.110662?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.ress.2024.110662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299759595\",\\n            \"name\": \"Xin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2256951279\",\\n            \"name\": \"Hongkai Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2266603243\",\\n            \"name\": \"Mingzhe Mu\"\\n          },\\n          {\\n            \"authorId\": \"2152989254\",\\n            \"name\": \"Yutong Dong\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"e6357834e6145b607c350b92238b5bc4d662e920\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e6357834e6145b607c350b92238b5bc4d662e920\",\\n        \"title\": \"Robust Test-Time Adaptation in Dynamic Scenarios\",\\n        \"citationCount\": 171,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.13899\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.13899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA, and presents a robust batch normalization scheme to estimate the normalization statistics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145235797\",\\n            \"name\": \"Longhui Yuan\"\\n          },\\n          {\\n            \"authorId\": \"1896719712\",\\n            \"name\": \"Binhui Xie\"\\n          },\\n          {\\n            \"authorId\": \"2165375715\",\\n            \"name\": \"Shuangliang Li\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time adaptation (TTA) intends to adapt the pretrained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distributions. However, these attempts may fail in dynamic scenarios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we explore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Extensive experiments prove that RoTTA enables continual test-time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA\"\\n      },\\n      {\\n        \"paperId\": \"b53604e693978aaa929136bd33d432fe69214d88\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b53604e693978aaa929136bd33d432fe69214d88\",\\n        \"title\": \"Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.01120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RAGPT is proposed, a novel Retrieval-AuGmented dynamic Prompt Tuning framework that consistently outperforms all competitive baselines in handling incomplete modality problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-01-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310823837\",\\n            \"name\": \"Jian Lang\"\\n          },\\n          {\\n            \"authorId\": \"2174885298\",\\n            \"name\": \"Zhangtao Cheng\"\\n          },\\n          {\\n            \"authorId\": \"46456474\",\\n            \"name\": \"Ting Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2273625855\",\\n            \"name\": \"Fan Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Multimodal learning with incomplete modality is practical and challenging. Recently, researchers have focused on enhancing the robustness of pre-trained MultiModal Transformers (MMTs) under missing modality conditions by applying learnable prompts. However, these prompt-based methods face several limitations: (1) incomplete modalities provide restricted modal cues for task-specific inference, (2) dummy imputation for missing content causes information loss and introduces noise, and (3) static prompts are instance-agnostic, offering limited knowledge for instances with various missing conditions. To address these issues, we propose RAGPT, a novel Retrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three modules: (I) the multi-channel retriever, which identifies similar instances through a within-modality retrieval strategy, (II) the missing modality generator, which recovers missing information using retrieved contexts, and (III) the context-aware prompter, which captures contextual knowledge from relevant instances and generates dynamic prompts to largely enhance the MMT\\\\u2019s robustness. Extensive experiments conducted on three real-world datasets show that RAGPT consistently outperforms all competitive baselines in handling incomplete modality problems.\"\\n      },\\n      {\\n        \"paperId\": \"6f75404b0d01f9a09afe428f9efd5cbcd7825469\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6f75404b0d01f9a09afe428f9efd5cbcd7825469\",\\n        \"title\": \"Dynamic Prompting: A Unified Framework for Prompt Tuning\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.02909\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.02909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A theoretical analysis is provided, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-03-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145170944\",\\n            \"name\": \"Xianjun Yang\"\\n          },\\n          {\\n            \"authorId\": \"145859270\",\\n            \"name\": \"Wei Cheng\"\\n          },\\n          {\\n            \"authorId\": \"50879401\",\\n            \"name\": \"Xujiang Zhao\"\\n          },\\n          {\\n            \"authorId\": \"21038849\",\\n            \"name\": \"Linda Petzold\"\\n          },\\n          {\\n            \"authorId\": \"2145225543\",\\n            \"name\": \"Haifeng Chen\"\\n          }\\n        ],\\n        \"abstract\": \"It has been demonstrated that the art of prompt tuning is highly effective in efficiently extracting knowledge from pretrained foundation models, encompassing pretrained language models (PLMs), vision pretrained models, and vision-language (V-L) models. However, the efficacy of employing fixed soft prompts with a predetermined position for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. In this context, we provide a theoretical analysis, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Building upon our analysis, we present a unified dynamic prompt (DP) tuning strategy that dynamically determines different factors of prompts based on specific tasks and instances. To accomplish this, we employ a lightweight learning network with Gumble-Softmax, allowing us to learn instance-dependent guidance. Experimental results underscore the significant performance improvement achieved by dynamic prompt tuning across a wide range of tasks, including NLP tasks, vision recognition tasks, and vision-language tasks. Furthermore, we establish the universal applicability of our approach under full-data, few-shot, and multitask scenarios. Codes are available at https://github.com/Xianjun-Yang/DPT.\"\\n      },\\n      {\\n        \"paperId\": \"ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca4f0d2c85cfe46b97ec42b38decda107780769d\",\\n        \"title\": \"QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.10462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation, is proposed and experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2241468593\",\\n            \"name\": \"Hossein Rajabzadeh\"\\n          },\\n          {\\n            \"authorId\": \"9200111\",\\n            \"name\": \"Mojtaba Valipour\"\\n          },\\n          {\\n            \"authorId\": \"2284643707\",\\n            \"name\": \"Tianshu Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1996315\",\\n            \"name\": \"Marzieh S. Tahaei\"\\n          },\\n          {\\n            \"authorId\": \"2241480742\",\\n            \"name\": \"Hyock Ju Kwon\"\\n          },\\n          {\\n            \"authorId\": \"2237425782\",\\n            \"name\": \"Ali Ghodsi\"\\n          },\\n          {\\n            \"authorId\": \"2237517964\",\\n            \"name\": \"Boxing Chen\"\\n          },\\n          {\\n            \"authorId\": \"2066076226\",\\n            \"name\": \"Mehdi Rezagholizadeh\"\\n          }\\n        ],\\n        \"abstract\": \"Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.\"\\n      },\\n      {\\n        \"paperId\": \"74a68f4798a34c7254297babd1f3222408198799\",\\n        \"url\": \"https://www.semanticscholar.org/paper/74a68f4798a34c7254297babd1f3222408198799\",\\n        \"title\": \"Adversarial Domain Adaptation Network With Calibrated Prototype and Dynamic Instance Convolution for Hyperspectral Image Classification\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TGRS.2024.3387990?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TGRS.2024.3387990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2125738235\",\\n            \"name\": \"Yi Huang\"\\n          },\\n          {\\n            \"authorId\": \"2444202\",\\n            \"name\": \"Jiangtao Peng\"\\n          },\\n          {\\n            \"authorId\": \"2296669816\",\\n            \"name\": \"Genwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2282635307\",\\n            \"name\": \"Weiwei Sun\"\\n          },\\n          {\\n            \"authorId\": \"2118767432\",\\n            \"name\": \"Na Chen\"\\n          },\\n          {\\n            \"authorId\": \"2072520954\",\\n            \"name\": \"Q. Du\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, the adversarial domain adaptation (ADA) methods have been widely investigated and applied in cross-domain hyperspectral image (HSI) classification. However, most ADA algorithms aim to align the cross-domain distribution without focusing on the class separability of the aligned target features and the information of samples within the domain. To address these issues, a new ADA framework based on calibrated prototype and dynamic instance convolution (CPDIC) is proposed in this article for cross-domain HSI classification. The CPDIC is composed of a generator, a calibrated discriminator (CD), and a classifier. The generator includes a static 3-D convolutional network (SCN) and a dynamic instance convolutional network (DICN), where the SCN is used to extract coarse-grained features of HSI and the DICN can extract sample-specific fine-grained features using instance convolutions generated from dynamic instance convolution kernel generation (DCKG) module. As for the generator, the static and dynamic interactive feature extraction network extracts robust domain-invariant features with discriminability. The CD aligns the marginal distribution between domains and calibrate the predicted pseudo-labels of target domain. For classification, a calibrated prototype loss (CPL) is introduced to align the class distribution across domains. The results of three cross-domain HSI classification tasks show that the proposed CPDIC outperforms existing unsupervised domain adaptation (UDA) algorithms.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 208,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"fb2331ec0bb1efdfd023d69fdb1c2a15b30f1f87\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fb2331ec0bb1efdfd023d69fdb1c2a15b30f1f87\",\\n        \"title\": \"MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.09010\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.09010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders for DFER, and identifies main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72707603\",\\n            \"name\": \"K. Chumachenko\"\\n          },\\n          {\\n            \"authorId\": \"2248136895\",\\n            \"name\": \"Alexandros Iosifidis\"\\n          },\\n          {\\n            \"authorId\": \"1715200\",\\n            \"name\": \"M. Gabbouj\"\\n          }\\n        ],\\n        \"abstract\": \"Dynamic Facial Expression Recognition (DFER) has received significant interest in the recent years dictated by its pivotal role in enabling empathic and human-compatible technologies. Achieving robustness towards in-the-wild data in DFER is particularly important for real-world applications. One of the directions aimed at improving such models is multimodal emotion recognition based on audio and video data. Multimodal learning in DFER increases the model capabilities by leveraging richer, complementary data representations. Within the field of multimodal DFER, recent methods have focused on exploiting advances of self-supervised learning (SSL) for pre-training of strong multi-modal encoders [40]. Another line of research has focused on adapting pre-trained static models for DFER [8]. In this work, we propose a different perspective on the problem and investigate the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders. We identify main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation, and propose solutions to each of them. As a result, we demonstrate improvement over current state-of-the-art on two popular DFER benchmarks, namely DFEW [19] and MFAW [29].\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"1b4012f38daa8f09299e16771973c91ce9464ee2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b4012f38daa8f09299e16771973c91ce9464ee2\",\\n        \"title\": \"DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.09787\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.09787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a novel method called Dynamic Visual Prompt Tuning (DVPT), which efficiently adapts pre-trained models to the medical domain but also enhances data efficiency with limited labeled data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153245407\",\\n            \"name\": \"Along He\"\\n          },\\n          {\\n            \"authorId\": \"37833805\",\\n            \"name\": \"Kai Wang\"\\n          },\\n          {\\n            \"authorId\": \"2216099246\",\\n            \"name\": \"Zhihong Wang\"\\n          },\\n          {\\n            \"authorId\": \"1840553660\",\\n            \"name\": \"Tao Li\"\\n          },\\n          {\\n            \"authorId\": \"1929093\",\\n            \"name\": \"H. Fu\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-training and fine-tuning have become popular due to the rich representations embedded in large pre-trained models, which can be leveraged for downstream medical tasks. However, existing methods typically either fine-tune all parameters or only task-specific layers of pre-trained models, overlooking the variability in input medical images. As a result, these approaches may lack efficiency or effectiveness. In this study, our goal is to explore parameter-efficient fine-tuning (PEFT) for medical image analysis. To address this challenge, we introduce a novel method called Dynamic Visual Prompt Tuning (DVPT). It can extract knowledge beneficial to downstream tasks from large models with only a few trainable parameters. First, the frozen features are transformed by a lightweight bottleneck layer to learn the domain-specific distribution of downstream medical tasks. Then, a few learnable visual prompts are employed as dynamic queries to conduct cross-attention with the transformed features, aiming to acquire sample-specific features. This DVPT module can be shared across different Transformer layers, further reducing the number of trainable parameters. We conduct extensive experiments with various pre-trained models on medical classification and segmentation tasks. We find that this PEFT method not only efficiently adapts pre-trained models to the medical domain but also enhances data efficiency with limited labeled data. For example, with only 0.5% additional trainable parameters, our method not only outperforms state-of-the-art PEFT methods but also surpasses full fine-tuning by more than 2.20% in Kappa score on the medical classification task. It can save up to 60% of labeled data and 99% of storage cost of ViT-B/16.\"\\n      },\\n      {\\n        \"paperId\": \"c28b7dfe341f1e13a5a98efbce7946ef795cf9b8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c28b7dfe341f1e13a5a98efbce7946ef795cf9b8\",\\n        \"title\": \"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer\",\\n        \"citationCount\": 309,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2022.acl-long.346.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2110.07904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that SPoT significantly boosts the performance of Prompt Tuning across many tasks, and an efficient retrieval approach is proposed that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144244743\",\\n            \"name\": \"Tu Vu\"\\n          },\\n          {\\n            \"authorId\": \"144104130\",\\n            \"name\": \"Brian Lester\"\\n          },\\n          {\\n            \"authorId\": \"40832517\",\\n            \"name\": \"Noah Constant\"\\n          },\\n          {\\n            \"authorId\": \"1388360943\",\\n            \"name\": \"Rami Al-Rfou\"\\n          },\\n          {\\n            \"authorId\": \"46724030\",\\n            \"name\": \"Daniel Matthew Cer\"\\n          }\\n        ],\\n        \"abstract\": \"There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks. Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks. More remarkably, across all model sizes, SPoT matches or outperforms standard Model Tuning (which fine-tunes all model parameters) on the SuperGLUE benchmark, while using up to 27,000\\\\u00d7 fewer task-specific parameters. To understand where SPoT is most effective, we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations, and demonstrate that many tasks can benefit each other via prompt transfer. Finally, we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.\"\\n      },\\n      {\\n        \"paperId\": \"81babe3cb42e620b537ecb5e6af540e3be2c3ca7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/81babe3cb42e620b537ecb5e6af540e3be2c3ca7\",\\n        \"title\": \"Visual Prompt Tuning for Test-time Domain Adaptation\",\\n        \"citationCount\": 105,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.04831\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.04831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work tackles the practical yet challenging test-time adaptation (TTA) problem, where a model adapts to the target domain without accessing the source data, with a simple recipe called Data-efficient Prompt Tuning (DePT).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282248267\",\\n            \"name\": \"Yunhe Gao\"\\n          },\\n          {\\n            \"authorId\": \"2110332219\",\\n            \"name\": \"Xingjian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2128816752\",\\n            \"name\": \"Yi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2359832\",\\n            \"name\": \"Hongya Wang\"\\n          },\\n          {\\n            \"authorId\": \"2986505\",\\n            \"name\": \"Zhiqiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"51453885\",\\n            \"name\": \"Xiong Zhou\"\\n          },\\n          {\\n            \"authorId\": \"1701799\",\\n            \"name\": \"Mu Li\"\\n          },\\n          {\\n            \"authorId\": \"1711560\",\\n            \"name\": \"Dimitris N. Metaxas\"\\n          }\\n        ],\\n        \"abstract\": \"Models should be able to adapt to unseen data during test-time to avoid performance drops caused by inevitable distribution shifts in real-world deployment scenarios. In this work, we tackle the practical yet challenging test-time adaptation (TTA) problem, where a model adapts to the target domain without accessing the source data. We propose a simple recipe called \\\\\\\\textit{Data-efficient Prompt Tuning} (DePT) with two key ingredients. First, DePT plugs visual prompts into the vision Transformer and only tunes these source-initialized prompts during adaptation. We find such parameter-efficient finetuning can efficiently adapt the model representation to the target domain without overfitting to the noise in the learning objective. Second, DePT bootstraps the source representation to the target domain by memory bank-based online pseudo-labeling. A hierarchical self-supervised regularization specially designed for prompts is jointly optimized to alleviate error accumulation during self-training. With much fewer tunable parameters, DePT demonstrates not only state-of-the-art performance on major adaptation benchmarks VisDA-C, ImageNet-C, and DomainNet-126, but also superior data efficiency, i.e., adaptation with only 1\\\\\\\\% or 10\\\\\\\\% data without much performance degradation compared to 100\\\\\\\\% data. In addition, DePT is also versatile to be extended to online or multi-source TTA settings.\"\\n      },\\n      {\\n        \"paperId\": \"0392d58335ce674a70f5e58ac8c438de296a0e6a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0392d58335ce674a70f5e58ac8c438de296a0e6a\",\\n        \"title\": \"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models\",\\n        \"citationCount\": 172,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2208.07852\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2208.07852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task, and then allows easy deployment of the newly created ad-hoc models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-08-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2879705\",\\n            \"name\": \"Hendrik Strobelt\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2285868436\",\\n            \"name\": \"Victor Sanh\"\\n          },\\n          {\\n            \"authorId\": \"2061407009\",\\n            \"name\": \"Benjamin Hoover\"\\n          },\\n          {\\n            \"authorId\": \"47273741\",\\n            \"name\": \"Johanna Beyer\"\\n          },\\n          {\\n            \"authorId\": \"143758236\",\\n            \"name\": \"H. Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2531268\",\\n            \"name\": \"Alexander M. Rush\"\\n          }\\n        ],\\n        \"abstract\": \"State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.\"\\n      }\\n    ]\\n  },\\n  \"adaptive prompt generation\": {\\n    \"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\",\\n    \"code\": \"429\"\\n  },\\n  \"iterative context update\": {\\n    \"total\": 8986,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 276,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 201,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\', \\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 182,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 359,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 322,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 68,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 47,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 40,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2801,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"title\": \"Spontaneous Reward Hacking in Iterative Self-Refinement\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Using an essay editing task, it is shown that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311112294\",\\n            \"name\": \"Jane Pan\"\\n          },\\n          {\\n            \"authorId\": \"2321875898\",\\n            \"name\": \"He He\"\\n          },\\n          {\\n            \"authorId\": \"2297768298\",\\n            \"name\": \"Samuel R. Bowman\"\\n          },\\n          {\\n            \"authorId\": \"2297816489\",\\n            \"name\": \"Shi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of human users, a second language model can be used as an evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator\\'s ratings improve while the generation quality remains stagnant or even decreases as judged by actual user preference. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive them to exploit shared vulnerabilities. Using an essay editing task, we show that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement. In addition, we study conditions under which reward hacking occurs and observe two factors that affect reward hacking severity: model size and context sharing between the generator and the evaluator.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      },\\n      {\\n        \"paperId\": \"f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"title\": \"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.01701?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      }\\n    ]\\n  },\\n  \"summarization prior interactions\": {\\n    \"total\": 16594,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a\",\\n        \"title\": \"TidyBot: Personalized Robot Assistance with Large Language Models\",\\n        \"citationCount\": 370,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2305.05658\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.05658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155142153\",\\n            \"name\": \"Jimmy Wu\"\\n          },\\n          {\\n            \"authorId\": \"39534622\",\\n            \"name\": \"Rika Antonova\"\\n          },\\n          {\\n            \"authorId\": \"2216606824\",\\n            \"name\": \"Adam Kan\"\\n          },\\n          {\\n            \"authorId\": \"10710717\",\\n            \"name\": \"Marion Lepert\"\\n          },\\n          {\\n            \"authorId\": \"38591293\",\\n            \"name\": \"Andy Zeng\"\\n          },\\n          {\\n            \"authorId\": \"3340170\",\\n            \"name\": \"Shuran Song\"\\n          },\\n          {\\n            \"authorId\": \"1775407\",\\n            \"name\": \"Jeannette Bohg\"\\n          },\\n          {\\n            \"authorId\": \"7723706\",\\n            \"name\": \"S. Rusinkiewicz\"\\n          },\\n          {\\n            \"authorId\": \"1807080\",\\n            \"name\": \"T. Funkhouser\"\\n          }\\n        ],\\n        \"abstract\": \"For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people\\'s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"0725debf3183589626823dbb64107bba8ed22448\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0725debf3183589626823dbb64107bba8ed22448\",\\n        \"title\": \"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.01596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids, is introduced and the value of integrating visual information from images to improve the creation of medically detailed summaries is demonstrated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275156796\",\\n            \"name\": \"Akash Ghosh\"\\n          },\\n          {\\n            \"authorId\": \"2273560136\",\\n            \"name\": \"Arkadeep Acharya\"\\n          },\\n          {\\n            \"authorId\": \"2165947844\",\\n            \"name\": \"Prince Jha\"\\n          },\\n          {\\n            \"authorId\": \"2277599094\",\\n            \"name\": \"Aniket Gaudgaul\"\\n          },\\n          {\\n            \"authorId\": \"2277600473\",\\n            \"name\": \"Rajdeep Majumdar\"\\n          },\\n          {\\n            \"authorId\": \"2243463123\",\\n            \"name\": \"Sriparna Saha\"\\n          },\\n          {\\n            \"authorId\": \"2275226689\",\\n            \"name\": \"Aman Chadha\"\\n          },\\n          {\\n            \"authorId\": \"2088137695\",\\n            \"name\": \"Raghav Jain\"\\n          },\\n          {\\n            \"authorId\": \"2275161937\",\\n            \"name\": \"Setu Sinha\"\\n          },\\n          {\\n            \"authorId\": \"2277651208\",\\n            \"name\": \"Shivani Agarwal\"\\n          }\\n        ],\\n        \"abstract\": \"In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient\\'s medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries. This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care. Our dataset, code, and pre-trained models will be made publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c1799bf28d1ae93e1631be5b59196ee1e568f538\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c1799bf28d1ae93e1631be5b59196ee1e568f538\",\\n        \"title\": \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\",\\n        \"citationCount\": 820,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GraphRAG is proposed, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text and leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2298275009\",\\n            \"name\": \"Darren Edge\"\\n          },\\n          {\\n            \"authorId\": \"2213073417\",\\n            \"name\": \"Ha Trinh\"\\n          },\\n          {\\n            \"authorId\": \"2298273973\",\\n            \"name\": \"Newman Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2298275805\",\\n            \"name\": \"Joshua Bradley\"\\n          },\\n          {\\n            \"authorId\": \"2298274563\",\\n            \"name\": \"Alex Chao\"\\n          },\\n          {\\n            \"authorId\": \"2210994342\",\\n            \"name\": \"Apurva N. Mody\"\\n          },\\n          {\\n            \"authorId\": \"2298273810\",\\n            \"name\": \"Steven Truitt\"\\n          },\\n          {\\n            \"authorId\": \"2298278846\",\\n            \"name\": \"Jonathan Larson\"\\n          }\\n        ],\\n        \"abstract\": \"The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as\\\\\"What are the main themes in the dataset?\\\\\", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.\"\\n      },\\n      {\\n        \"paperId\": \"4bf0c0f4aaf67f5138d5036c044a5d0de8938181\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4bf0c0f4aaf67f5138d5036c044a5d0de8938181\",\\n        \"title\": \"Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.13190\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.13190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel role interaction enhanced method that adopts cross attention and decoder self-attention interactions to interactively acquire other roles\\\\u2019 critical information for role-oriented dialogue summarization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152781993\",\\n            \"name\": \"Haitao Lin\"\\n          },\\n          {\\n            \"authorId\": \"24925751\",\\n            \"name\": \"Junnan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"145475526\",\\n            \"name\": \"Lu Xiang\"\\n          },\\n          {\\n            \"authorId\": \"2110631853\",\\n            \"name\": \"Yu Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2124819243\",\\n            \"name\": \"Jiajun Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2064100826\",\\n            \"name\": \"Chengqing Zong\"\\n          }\\n        ],\\n        \"abstract\": \"Role-oriented dialogue summarization is to generate summaries for different roles in the dialogue, e.g., merchants and consumers. Existing methods handle this task by summarizing each role\\\\u2019s content separately and thus are prone to ignore the information from other roles. However, we believe that other roles\\\\u2019 content could benefit the quality of summaries, such as the omitted information mentioned by other roles. Therefore, we propose a novel role interaction enhanced method for role-oriented dialogue summarization. It adopts cross attention and decoder self-attention interactions to interactively acquire other roles\\\\u2019 critical information. The cross attention interaction aims to select other roles\\\\u2019 critical dialogue utterances, while the decoder self-attention interaction aims to obtain key information from other roles\\\\u2019 summaries. Experimental results have shown that our proposed method significantly outperforms strong baselines on two public role-oriented dialogue summarization datasets. Extensive analyses have demonstrated that other roles\\\\u2019 content could help generate summaries with more complete semantics and correct topic structures.\"\\n      },\\n      {\\n        \"paperId\": \"65c5e3e3be6e881c7d2feef4c0c3d07323f3aeff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/65c5e3e3be6e881c7d2feef4c0c3d07323f3aeff\",\\n        \"title\": \"Make The Most of Prior Data: A Solution for Interactive Text Summarization with Preference Feedback\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2204.05512\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.05512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new framework to train summarization models with preference feedback interactively with properly leveraging offline data and a novel reward model is introduced to improve the performance regarding ROUGE scores and sample-efficiency.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2153960542\",\\n            \"name\": \"Duy-Hung Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"38714383\",\\n            \"name\": \"Nguyen-Viet-Dung Nghiem\"\\n          },\\n          {\\n            \"authorId\": \"2140448395\",\\n            \"name\": \"Bao-Sinh Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2142143947\",\\n            \"name\": \"Dung Tien Le\"\\n          },\\n          {\\n            \"authorId\": \"2162186181\",\\n            \"name\": \"Shahab Sabahi\"\\n          },\\n          {\\n            \"authorId\": \"1789308\",\\n            \"name\": \"Minh Le Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2064728738\",\\n            \"name\": \"Hung Le\"\\n          }\\n        ],\\n        \"abstract\": \"For summarization, human preference is critical to tame outputs of the summarizer in favor of human interests, as ground-truth summaries are scarce and ambiguous. Practical settings require dynamic exchanges between human and AI agent wherein feedback is provided in an online manner, a few at a time. In this paper, we introduce a new framework to train summarization models with preference feedback interactively. By properly leveraging offline data and a novel reward model, we improve the performance regarding ROUGE scores and sample-efficiency. Our experiments on three various datasets confirm the benefit of the proposed framework in active, few-shot and online settings of preference learning.\"\\n      },\\n      {\\n        \"paperId\": \"9b86fc9596f5bc027a0a507414b2c4ecf60bcbbf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9b86fc9596f5bc027a0a507414b2c4ecf60bcbbf\",\\n        \"title\": \"Towards Clinical Encounter Summarization: Learning to Compose Discharge Summaries from Prior Notes\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2104.13498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Two new measures, faithfulness and hallucination rate, are introduced for evaluation in this task, which complement existing measures for fluency and informativeness.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-04-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46183205\",\\n            \"name\": \"Han-Chin Shing\"\\n          },\\n          {\\n            \"authorId\": \"1866532\",\\n            \"name\": \"Chaitanya P. Shivade\"\\n          },\\n          {\\n            \"authorId\": \"3158273\",\\n            \"name\": \"Nima Pourdamghani\"\\n          },\\n          {\\n            \"authorId\": \"144647318\",\\n            \"name\": \"Feng Nan\"\\n          },\\n          {\\n            \"authorId\": \"1680292\",\\n            \"name\": \"P. Resnik\"\\n          },\\n          {\\n            \"authorId\": \"1737250\",\\n            \"name\": \"Douglas W. Oard\"\\n          },\\n          {\\n            \"authorId\": \"50339091\",\\n            \"name\": \"Parminder Bhatia\"\\n          }\\n        ],\\n        \"abstract\": \"The records of a clinical encounter can be extensive and complex, thus placing a premium on tools that can extract and summarize relevant information. This paper introduces the task of generating discharge summaries for a clinical encounter. Summaries in this setting need to be faithful, traceable, and scale to multiple long documents, motivating the use of extract-then-abstract summarization cascades. We introduce two new measures, faithfulness and hallucination rate for evaluation in this task, which complement existing measures for fluency and informativeness. Results across seven medical sections and five models show that a summarization architecture that supports traceability yields promising results, and that a sentence-rewriting approach performs consistently on the measure used for faithfulness (faithfulness-adjusted $F_3$) over a diverse range of generated sections.\"\\n      },\\n      {\\n        \"paperId\": \"ba4548c01a8b29a406f269db80f7105b88ca9751\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ba4548c01a8b29a406f269db80f7105b88ca9751\",\\n        \"title\": \"FABLES: Evaluating faithfulness and content selection in book-length summarization\",\\n        \"citationCount\": 64,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper conducts the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books, and develops a typology of omission errors related to crucial narrative elements and identifies a systematic over-emphasis on events occurring towards the end of the book.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2294805305\",\\n            \"name\": \"Yekyung Kim\"\\n          },\\n          {\\n            \"authorId\": \"144455052\",\\n            \"name\": \"Yapei Chang\"\\n          },\\n          {\\n            \"authorId\": \"37796923\",\\n            \"name\": \"Marzena Karpinska\"\\n          },\\n          {\\n            \"authorId\": \"31099365\",\\n            \"name\": \"Aparna Garimella\"\\n          },\\n          {\\n            \"authorId\": \"2380898326\",\\n            \"name\": \"Varun Manjunatha\"\\n          },\\n          {\\n            \"authorId\": \"46258841\",\\n            \"name\": \"Kyle Lo\"\\n          },\\n          {\\n            \"authorId\": \"2253400779\",\\n            \"name\": \"Tanya Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2136562\",\\n            \"name\": \"Mohit Iyyer\"\\n          }\\n        ],\\n        \"abstract\": \"While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.\"\\n      },\\n      {\\n        \"paperId\": \"58b0923c25a721061f6a43574ed50846d637d0a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58b0923c25a721061f6a43574ed50846d637d0a9\",\\n        \"title\": \"Web-based online resources about adverse interactions or side effects associated with complementary and alternative medicine: a systematic review, summarization and quality assessment\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://bmcmedinformdecismak.biomedcentral.com/track/pdf/10.1186/s12911-020-01298-5\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7653751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study provides a summarized list of high quality, evidence-based, online resources about CAM and CAM-related adverse effects, and can thereby serve as a useful reference for clinicians, researchers, and patients.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-11-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8563902\",\\n            \"name\": \"J. Y. Ng\"\\n          },\\n          {\\n            \"authorId\": \"1840486037\",\\n            \"name\": \"Vanessa Munford\"\\n          },\\n          {\\n            \"authorId\": \"1572362560\",\\n            \"name\": \"Harmy Thakar\"\\n          }\\n        ],\\n        \"abstract\": \"Background Given an increased global prevalence of complementary and alternative medicine (CAM) use, healthcare providers commonly seek CAM-related health information online. Numerous online resources containing CAM-specific information exist, many of which are readily available/accessible, containing information shareable with their patients. To the authors\\\\u2019 knowledge, no study has summarized nor assessed the quality of content contained within these online resources for at least a decade, specifically pertaining to information about adverse effects or interactions. Methods This study provides summaries of web-based online resources that provide safety information on potential interactions or adverse effects of CAM. Specifically, clinicians are the intended users of these online resources containing patient information which they can then disseminate to their patients. All online resources were assessed for content quality using the validated rating tool, DISCERN. Results Of 21 articles identified in our previously published scoping review, 23 online resources were eligible. DISCERN assessments suggests that online resources containing CAM-specific information vary in quality. Summed DISCERN scores had a mean of 56.13 (SD\\\\u2009=\\\\u200910.25) out of 75. Online resources with the highest total DISCERN scores across all questions included Micromedex (68.50), Merck Manual (67.50) and Drugs.com (66.50). Online resources with the lowest total scores included Drug Information (33.00), Caremark Drug Interactions (42.50) and HIV Drug Interactions (43.00). The DISCERN questions that received the highest mean score across all online resources referred to whether the risks were described for each treatment (4.66), whether the aims were clear (4.58), whether the source achieved those aims (4.58), and whether the website referred to areas of uncertainty (4.58). The DISCERN questions that received the lowest mean score across all online resources assessed whether there was discussion about no treatment being used (1.29) and how treatment choices would affect quality of life (2.00). Conclusion This study provides a comprehensive list of online resources containing CAM-specific information. Informed by the appraisal of these resources, this study provides a summarized list of high quality, evidence-based, online resources about CAM and CAM-related adverse effects. This list of recommended resources can thereby serve as a useful reference for clinicians, researchers, and patients.\"\\n      },\\n      {\\n        \"paperId\": \"0b66b4398a0a768ca32ea23ae827a4b1fe9746ab\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b66b4398a0a768ca32ea23ae827a4b1fe9746ab\",\\n        \"title\": \"Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies, and proposes a refined taxonomy of errors, coining the category of \\\\\"Circumstantial Inference\\\\\".\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"98806251\",\\n            \"name\": \"S. Ramprasad\"\\n          },\\n          {\\n            \"authorId\": \"2249355743\",\\n            \"name\": \"Elisa Ferracane\"\\n          },\\n          {\\n            \"authorId\": \"32219137\",\\n            \"name\": \"Zachary Chase Lipton\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems. However, they continue to face concerns about hallucinations. While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness. Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies. Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models. We propose a refined taxonomy of errors, coining the category of\\\\\"Circumstantial Inference\\\\\"to bucket these LLM behaviors and release the dataset. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors. To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying\\\\\"Circumstantial Inference.\\\\\"\"\\n      },\\n      {\\n        \"paperId\": \"fa253d232a933595ff8aa892730a49bcf63be4db\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa253d232a933595ff8aa892730a49bcf63be4db\",\\n        \"title\": \"Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.15496\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.15496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes an instruction fine-tuning model: Baichuan2-Sum, for role-oriented dialogue summarization, which shows an improvement in Rouge scores on both datasets compared to the previously best-performing model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282252656\",\\n            \"name\": \"Jianfei Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2281789572\",\\n            \"name\": \"Yancan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2148234209\",\\n            \"name\": \"Yimin Ou\"\\n          },\\n          {\\n            \"authorId\": \"2313881077\",\\n            \"name\": \"Hanyi Yu\"\\n          },\\n          {\\n            \"authorId\": \"2281897968\",\\n            \"name\": \"Yiyong Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) like LLaMA, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods foucus on small models (e.g BART and BERT). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented dialogue summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the desired summaries. Furthermore, we applied NEFTune technique to add suitable noise during training, improving the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. The Baichuan2-Sum model shows an improvement in Rouge scores on both datasets compared to the previously best-performing model. Notably, for the SAMSUM dataset, there is a 21% increase in the ROUGE-1 score, a 32% increase in the ROUGE-2 score, and a 9% increase in the ROUGE-L score. We have released our model and related codes to facilitate future studies in the dialogue summarization task.\"\\n      },\\n      {\\n        \"paperId\": \"2338d7c9ab07e6d0f4160335dce0e6e6a87c4749\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2338d7c9ab07e6d0f4160335dce0e6e6a87c4749\",\\n        \"title\": \"Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method\",\\n        \"citationCount\": 106,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13412\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2143482843\",\\n            \"name\": \"Yiming Wang\"\\n          },\\n          {\\n            \"authorId\": \"3322871\",\\n            \"name\": \"Zhuosheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2151038501\",\\n            \"name\": \"Rui Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the \\\\u201cLasswell Communication Model\\\\u201d proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs\\\\u2019 zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.\"\\n      },\\n      {\\n        \"paperId\": \"d9046db67af64e78791099d646b5d22792fabc63\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d9046db67af64e78791099d646b5d22792fabc63\",\\n        \"title\": \"Overview of the MEDIQA-Chat 2023 Shared Tasks on the Summarization & Generation of Doctor-Patient Conversations\",\\n        \"citationCount\": 56,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.clinicalnlp-1.52.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://aclanthology.org/2023.clinicalnlp-1.52, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes the three MEDIQA-Chat 2023 tasks, the datasets, and the participants\\\\u2019 results and methods and hopes that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2127117221\",\\n            \"name\": \"Asma Ben Abacha\"\\n          },\\n          {\\n            \"authorId\": \"38845771\",\\n            \"name\": \"Wen-wai Yim\"\\n          },\\n          {\\n            \"authorId\": \"49496422\",\\n            \"name\": \"Griffin Adams\"\\n          },\\n          {\\n            \"authorId\": \"2370273\",\\n            \"name\": \"N. Snider\"\\n          },\\n          {\\n            \"authorId\": \"1398215463\",\\n            \"name\": \"Meliha Yetisgen-Yildiz\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic generation of clinical notes from doctor-patient conversations can play a key role in reducing daily doctors\\\\u2019 workload and improving their interactions with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective solutions through shared tasks on the automatic summarization of doctor-patient conversations and on the generation of synthetic dialogues from clinical notes for data augmentation. Seventeen teams participated in the challenge and experimented with a broad range of approaches and models. In this paper, we describe the three MEDIQA-Chat 2023 tasks, the datasets, and the participants\\\\u2019 results and methods. We hope that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes.\"\\n      },\\n      {\\n        \"paperId\": \"de546efe75ef22c5471e9e92c21cc093975ca9d6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/de546efe75ef22c5471e9e92c21cc093975ca9d6\",\\n        \"title\": \"Prompting Large Language Models with Audio for General-Purpose Speech Summarization\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2406.05968\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.05968, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An end-to-end system that combines an instruction-tuned LLM with an audio encoder that converts speech into token representations that the LLM can interpret that allows the LLM to process speech inputs in the same way as text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2113751566\",\\n            \"name\": \"Wonjune Kang\"\\n          },\\n          {\\n            \"authorId\": \"2305604637\",\\n            \"name\": \"Deb Roy\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, we introduce a framework for speech summarization that leverages the processing and reasoning capabilities of large language models (LLMs). We propose an end-to-end system that combines an instruction-tuned LLM with an audio encoder that converts speech into token representations that the LLM can interpret. Using a dataset with paired speech-text data, the overall system is trained to generate consistent responses to prompts with the same semantic information regardless of the input modality. The resulting framework allows the LLM to process speech inputs in the same way as text, enabling speech summarization by simply prompting the LLM. Unlike prior approaches, our method is able to summarize spoken content from any arbitrary domain, and it can produce summaries in different styles by varying the LLM prompting strategy. Experiments demonstrate that our approach outperforms a cascade baseline of speech recognition followed by LLM text processing.\"\\n      },\\n      {\\n        \"paperId\": \"5272b2e11769a0e4bcfb9ff7cdf4c58e648afa18\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5272b2e11769a0e4bcfb9ff7cdf4c58e648afa18\",\\n        \"title\": \"EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3640457.3688185\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.11441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EmbSum is introduced, a novel framework that enables offline pre-computations of users and candidate items while capturing the interactions within the user engagement history, surpassing state-of-the-art (SoTA) methods with higher accuracy and fewer parameters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2024-05-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50445559\",\\n            \"name\": \"Chiyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2284621464\",\\n            \"name\": \"Yifei Sun\"\\n          },\\n          {\\n            \"authorId\": \"2145209409\",\\n            \"name\": \"Minghao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2284643679\",\\n            \"name\": \"Jun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284594046\",\\n            \"name\": \"Jie Lei\"\\n          },\\n          {\\n            \"authorId\": \"2065312024\",\\n            \"name\": \"M. Abdul-Mageed\"\\n          },\\n          {\\n            \"authorId\": \"2284592470\",\\n            \"name\": \"Rong Jin\"\\n          },\\n          {\\n            \"authorId\": \"2302524479\",\\n            \"name\": \"Angli Liu\"\\n          },\\n          {\\n            \"authorId\": \"2302370397\",\\n            \"name\": \"Ji Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2284639887\",\\n            \"name\": \"Sem Park\"\\n          },\\n          {\\n            \"authorId\": \"2284591939\",\\n            \"name\": \"Ning Yao\"\\n          },\\n          {\\n            \"authorId\": \"2284595625\",\\n            \"name\": \"Bo Long\"\\n          }\\n        ],\\n        \"abstract\": \"Content-based recommendation systems play a crucial role in delivering personalized content to users in the digital world. In this work, we introduce EmbSum, a novel framework that enables offline pre-computations of users and candidate items while capturing the interactions within the user engagement history. By utilizing the pretrained encoder-decoder model and poly-attention layers, EmbSum derives User Poly-Embedding (UPE) and Content Poly-Embedding (CPE) to calculate relevance scores between users and candidate items. EmbSum actively learns the long user engagement histories by generating user-interest summary with supervision from large language model (LLM). The effectiveness of EmbSum is validated on two datasets from different domains, surpassing state-of-the-art (SoTA) methods with higher accuracy and fewer parameters. Additionally, the model\\\\u2019s ability to generate summaries of user interests serves as a valuable by-product, enhancing its usefulness for personalized content recommendations.\"\\n      },\\n      {\\n        \"paperId\": \"60e7545ec5280ad944d2d6e49be52419301aa063\",\\n        \"url\": \"https://www.semanticscholar.org/paper/60e7545ec5280ad944d2d6e49be52419301aa063\",\\n        \"title\": \"VSS-Net: Visual Semantic Self-Mining Network for Video Summarization\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3312325?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3312325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108326383\",\\n            \"name\": \"Yunzuo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2190800936\",\\n            \"name\": \"Yameng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2194582294\",\\n            \"name\": \"Weili Kang\"\\n          },\\n          {\\n            \"authorId\": \"2070824819\",\\n            \"name\": \"Ran Tao\"\\n          }\\n        ],\\n        \"abstract\": \"Video summarization, with the target to detect valuable segments given untrimmed videos, is a meaningful yet understudied topic. Previous methods primarily consider inter-frame and inter-shot temporal dependencies, which might be insufficient to pinpoint important content due to limited valuable information that can be learned. To address this limitation, we elaborate on a Visual Semantic Self-mining Network (VSS-Net), a novel summarization framework motivated by the widespread success of cross-modality learning tasks. VSS-Net initially adopts a two-stream structure consisting of a Context Representation Graph (CRG) and a Video Semantics Encoder (VSE). They are jointly exploited to establish the groundwork for further boosting the capability of content awareness. Specifically, CRG is constructed using an edge-set strategy tailored to the hierarchical structure of videos, enriching visual features with local and non-local temporal cues from temporal order and visual relationship perspectives. Meanwhile, by learning visual similarity across features, VSE adaptively acquires an instructive video-level semantic representation of the input video from coarse to fine. Subsequently, the two streams converge in a Context-Semantics Interaction Layer (CSIL) to achieve sophisticated information exchange across frame-level temporal cues and video-level semantic representation, guaranteeing informative representations and boosting the sensitivity to important segments. Eventually, importance scores are predicted utilizing a prediction head, followed by key shot selection. We evaluate the proposed framework and demonstrate its effectiveness and superiority against state-of-the-art methods on the widely used benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"a36658b26ea4ccb58f85d8a578f6ec6767446095\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a36658b26ea4ccb58f85d8a578f6ec6767446095\",\\n        \"title\": \"Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The authors\\' model with 770M parameters consistently outperforms strong baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT itself, including models distilled from ChatGPT, and sometimes, even ChatGPT itself.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2122355046\",\\n            \"name\": \"Jaehun Jung\"\\n          },\\n          {\\n            \"authorId\": \"119659229\",\\n            \"name\": \"Peter West\"\\n          },\\n          {\\n            \"authorId\": \"2112504145\",\\n            \"name\": \"Liwei Jiang\"\\n          },\\n          {\\n            \"authorId\": \"9252833\",\\n            \"name\": \"Faeze Brahman\"\\n          },\\n          {\\n            \"authorId\": \"50085131\",\\n            \"name\": \"Ximing Lu\"\\n          },\\n          {\\n            \"authorId\": \"33772445\",\\n            \"name\": \"Jillian R. Fisher\"\\n          },\\n          {\\n            \"authorId\": \"122436831\",\\n            \"name\": \"Taylor Sorensen\"\\n          },\\n          {\\n            \"authorId\": \"1699545\",\\n            \"name\": \"Yejin Choi\"\\n          }\\n        ],\\n        \"abstract\": \"We present Impossible Distillation, a novel framework for paraphrasing and sentence summarization, that distills a high-quality dataset and model from a low-quality teacher that itself cannot perform these tasks. Unlike prior works that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific architecture, we hypothesize and verify the paraphrastic proximity intrinsic to pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in the LM distribution. By identifying and distilling generations from these subspaces, Impossible Distillation produces a high-quality dataset and model even from GPT2-scale LMs. We evaluate our method on multiple benchmarks spanning unconstrained / syntax-controlled paraphrase generation and sentence summarization. Our model with 770M parameters consistently outperforms strong baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher diversity and fidelity than up to 13 times larger datasets.\"\\n      },\\n      {\\n        \"paperId\": \"c747c6ce74bf80f1c0af97fb3aeb37875415d238\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c747c6ce74bf80f1c0af97fb3aeb37875415d238\",\\n        \"title\": \"Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13693\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that not only do automated metrics fail to capture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31860505\",\\n            \"name\": \"Lucy Lu Wang\"\\n          },\\n          {\\n            \"authorId\": \"1881453937\",\\n            \"name\": \"Yulia Otmakhova\"\\n          },\\n          {\\n            \"authorId\": \"48727916\",\\n            \"name\": \"Jay DeYoung\"\\n          },\\n          {\\n            \"authorId\": \"153574160\",\\n            \"name\": \"Thinh Hung Truong\"\\n          },\\n          {\\n            \"authorId\": \"2003338023\",\\n            \"name\": \"Bailey Kuehl\"\\n          },\\n          {\\n            \"authorId\": \"2203427167\",\\n            \"name\": \"Erin Bransom\"\\n          },\\n          {\\n            \"authorId\": \"2111879324\",\\n            \"name\": \"Byron Wallace\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better automated evaluation methods for literature review MDS. We take advantage of community submissions to the Multi-document Summarization for Literature Review (MSLR) shared task to compile a diverse and representative sample of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical features of generated summaries, to other automated metrics including several we propose in this work, and to aspects of human-assessed summary quality. We find that not only do automated metrics fail to capture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators.\"\\n      },\\n      {\\n        \"paperId\": \"be658d4482299d7a77be324bdb9946bfcbcbad25\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be658d4482299d7a77be324bdb9946bfcbcbad25\",\\n        \"title\": \"Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16784\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16784, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2167123387\",\\n            \"name\": \"Dongqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2293409512\",\\n            \"name\": \"Yifan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293393732\",\\n            \"name\": \"Vera Demberg\"\\n          }\\n        ],\\n        \"abstract\": \"For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the \\\\u2018RSTformer\\\\u2019, a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.\"\\n      },\\n      {\\n        \"paperId\": \"5f6c67f3111ca652e6709f5ae3b6821c13838a80\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5f6c67f3111ca652e6709f5ae3b6821c13838a80\",\\n        \"title\": \"Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.12324\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.12324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs into figure captions, and shows that it can be more effectively tackled as a text summarization task in scientific documents.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1414028847\",\\n            \"name\": \"Huang Chieh-Yang\"\\n          },\\n          {\\n            \"authorId\": \"4500245\",\\n            \"name\": \"Ting-Yao Hsu\"\\n          },\\n          {\\n            \"authorId\": \"2066337266\",\\n            \"name\": \"Ryan A. Rossi\"\\n          },\\n          {\\n            \"authorId\": \"3115414\",\\n            \"name\": \"A. Nenkova\"\\n          },\\n          {\\n            \"authorId\": \"2109571021\",\\n            \"name\": \"Sungchul Kim\"\\n          },\\n          {\\n            \"authorId\": \"51192588\",\\n            \"name\": \"G. Chan\"\\n          },\\n          {\\n            \"authorId\": \"35910905\",\\n            \"name\": \"Eunyee Koh\"\\n          },\\n          {\\n            \"authorId\": \"145157784\",\\n            \"name\": \"C. Lee Giles\"\\n          },\\n          {\\n            \"authorId\": \"2109026884\",\\n            \"name\": \"Ting Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., \\\\u201cFigure 3 shows...\\\\u201d) into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and data are available at: https://github.com/Crowd-AI-Lab/Generating-Figure-Captions-as-a-Text-Summarization-Task.\"\\n      },\\n      {\\n        \"paperId\": \"6b9b1409c013ed3fca1a2bc697ad442c337de462\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b9b1409c013ed3fca1a2bc697ad442c337de462\",\\n        \"title\": \"Abstractive Financial News Summarization via Transformer-BiLSTM Encoder and Graph Attention-Based Decoder\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASLP.2023.3304473?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASLP.2023.3304473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1509369579\",\\n            \"name\": \"Haozhou Li\"\\n          },\\n          {\\n            \"authorId\": \"144135237\",\\n            \"name\": \"Qinke Peng\"\\n          },\\n          {\\n            \"authorId\": \"2005209281\",\\n            \"name\": \"Xu Mou\"\\n          },\\n          {\\n            \"authorId\": \"2155516032\",\\n            \"name\": \"Ying Wang\"\\n          },\\n          {\\n            \"authorId\": \"2150468905\",\\n            \"name\": \"Zeyuan Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2131541300\",\\n            \"name\": \"Muhammad Fiaz Bashir\"\\n          }\\n        ],\\n        \"abstract\": \"Financial news summarization (FNS) has been an attractive research problem in recent years, which aims to generate a shorter highlight of the news article while preserving key factual aspects, emotions, and opinions, providing significant assistance in stock trading and investment decision-making. However, FNS faces two challenges compared to the common domain. Firstly, financial news involves professional qualitative and quantitative information and salient content always scatters across long-range interactions. Secondly, financial news contains latent causal relationships, where historical information in the early generated sequence can significantly affect the subsequent decoding process. To address these difficulties, we propose an enhanced Seq2Seq model named TLGA, where the hierarchical Transformer-BiLSTM encoder can capture long-range interactions and sequential semantics while the Graph Attention-based decoder can fully utilize the historical information of decoded tokens and capture key causal relations. Moreover, we propose history-enhanced attention to concentrate on salient input content based on history semantics, guiding our decoder to generate the summary around the corresponding contents. It is also the first attempt to reuse history information of previously generated summary sequences in FNS using the idea of the Graph Attention Mechanism. Additionally, we construct the LCFNS dataset with 430,820 news-summary pairs for the lack of large-scale high-quality datasets in FNS. Experimental results on two financial datasets and two benchmark datasets indicate that our model outperforms other baselines.\"\\n      },\\n      {\\n        \"paperId\": \"2b5e411e664a521bcc223329562e14ece650b6bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2b5e411e664a521bcc223329562e14ece650b6bf\",\\n        \"title\": \"Mr. HiSum: A Large-scale Dataset for Video Highlight Detection and Summarization\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents Mr. HiSum, a large-scale dataset for video highlight detection and summarization, containing 31,892 videos and reliable labels aggregated over 50,000+ users per video, and empirically prove reliability of the labels as frame importance by cross-dataset transfer and user study.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2231980663\",\\n            \"name\": \"Jinhwan Sul\"\\n          },\\n          {\\n            \"authorId\": \"2288035283\",\\n            \"name\": \"Jihoon Han\"\\n          },\\n          {\\n            \"authorId\": \"2288343910\",\\n            \"name\": \"Joonseok Lee\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"17330c07a59b9dfb6cb01612c4b8be8841735396\",\\n        \"url\": \"https://www.semanticscholar.org/paper/17330c07a59b9dfb6cb01612c4b8be8841735396\",\\n        \"title\": \"Leveraging Summary Guidance on Medical Report Summarization\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.04001\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.04001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5878432\",\\n            \"name\": \"Yunqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"1860612\",\\n            \"name\": \"Xuebing Yang\"\\n          },\\n          {\\n            \"authorId\": \"2204743673\",\\n            \"name\": \"Yuanyuan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2108167911\",\\n            \"name\": \"Wensheng Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"This study presents three deidentified large medical text datasets, named DISCHARGE, ECHO and RADIOLOGY, which contain 50 K, 16 K and 378 K pairs of report and summary that are derived from MIMIC-III, respectively. We implement convincing baselines of automated abstractive summarization on the created datasets with pre-trained encoder-decoder language models, including BERT2BERT, BERTShare, RoBERTaShare, Pegasus, ProphetNet, T5-large, BART and GSUM. Further, based on the BART model, we leverage the sampled summaries from the training set as prior knowledge guidance, for encoding additional contextual representations of the guidance with the encoder and enhancing the decoding representations in the decoder. The experimental results confirm the improvement of ROUGE scores and BERTScore made by the proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"0ff8c04c8bdbf93b39b49582c9195cf3fc894d03\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0ff8c04c8bdbf93b39b49582c9195cf3fc894d03\",\\n        \"title\": \"Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)\",\\n        \"citationCount\": 154,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3597503.3639183\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.06815, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that adding semantic facts to the code in the prompt actually does help and this approach improves performance in several different settings suggested by prior work, including for three different Large Language Models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-04-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2214523312\",\\n            \"name\": \"K. Pai\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"1757975\",\\n            \"name\": \"Earl T. Barr\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLM) are a new class of computation engines, \\\\u201cprogrammed\\\\u201d via prompt engineering. Researchers are still learning how to best \\\\u201cprogram\\\\u201d these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of \\\\u201ccode analysis\\\\u201d and extracting such information, while processing code: but are they, really? If they aren\\'t, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM\\'s prompt with semantic facts explicitly. actually helps. Prior work shows that LLM performance on code summarization benefits from embedding a few code & summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU 11Scores of 30\\\\u201340 BLEU are considered \\\\u201cGood\\\\u201d to \\\\u201cUnderstandable\\\\u201d for natural language translation; see https://cloud.google.com/translate/automl/docs/evaluate.. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs\\' line completion performance.\"\\n      },\\n      {\\n        \"paperId\": \"c91b2c65da5e50df129b877da57ec07bb7f9c363\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c91b2c65da5e50df129b877da57ec07bb7f9c363\",\\n        \"title\": \"DragAPart: Learning a Part-Level Motion Prior for Articulated Objects\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.15382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces DragAPart, a method that, given an image and a set of drags as input, generates a new image of the same object that responds to the action of the drags, and demonstrates much better part-level motion understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2213346491\",\\n            \"name\": \"Ruining Li\"\\n          },\\n          {\\n            \"authorId\": \"2272947462\",\\n            \"name\": \"Chuanxia Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275252600\",\\n            \"name\": \"Christian Rupprecht\"\\n          },\\n          {\\n            \"authorId\": \"2258587368\",\\n            \"name\": \"Andrea Vedaldi\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce DragAPart, a method that, given an image and a set of drags as input, generates a new image of the same object that responds to the action of the drags. Differently from prior works that focused on repositioning objects, DragAPart predicts part-level interactions, such as opening and closing a drawer. We study this problem as a proxy for learning a generalist motion model, not restricted to a specific kinematic structure or object category. We start from a pre-trained image generator and fine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce. Combined with a new encoding for the drags and dataset randomization, the model generalizes well to real images and different categories. Compared to prior motion-controlled generators, we demonstrate much better part-level motion understanding.\"\\n      },\\n      {\\n        \"paperId\": \"eaa0078838310afb0e9a92e929e85a274d696dbb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eaa0078838310afb0e9a92e929e85a274d696dbb\",\\n        \"title\": \"G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.12383\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.12383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category, represents a first approach that allows jointly generating both hand and object.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9653518\",\\n            \"name\": \"Yufei Ye\"\\n          },\\n          {\\n            \"authorId\": \"2240431852\",\\n            \"name\": \"Abhinav Gupta\"\\n          },\\n          {\\n            \"authorId\": \"2297185992\",\\n            \"name\": \"Kris Kitani\"\\n          },\\n          {\\n            \"authorId\": \"2757335\",\\n            \"name\": \"Shubham Tulsiani\"\\n          }\\n        ],\\n        \"abstract\": \"We propose G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category. To learn a 3D spatial diffusion model that can capture this joint distribution, we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object. We show that this hand-object prior can then serve as generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis. We believe that our model, trained by aggregating seven diverse real-world interaction datasets spanning across 155 cate-gories, represents a first approach that allows jointly generating both hand and object. Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis, outperforming current task-specific baselines.\"\\n      },\\n      {\\n        \"paperId\": \"68571a8c3598888035c5c4f0a99793434ad61ef4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/68571a8c3598888035c5c4f0a99793434ad61ef4\",\\n        \"title\": \"LoRaLay: A Multilingual and Multimodal Dataset for Long Range and Layout-Aware Summarization\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.11312\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.11312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LoRaLay is presented, a collection of datasets for long-range summarization with accompanying visual/layout information and four novel datasets \\\\u2013 consistently built from scholar resources \\\\u2013 covering French, Spanish, Portuguese, and Korean languages are proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-01-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2150610138\",\\n            \"name\": \"Laura Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"90745780\",\\n            \"name\": \"Thomas Scialom\"\\n          },\\n          {\\n            \"authorId\": \"1703777\",\\n            \"name\": \"Benjamin Piwowarski\"\\n          },\\n          {\\n            \"authorId\": \"1767493\",\\n            \"name\": \"Jacopo Staiano\"\\n          }\\n        ],\\n        \"abstract\": \"Text Summarization is a popular task and an active area of research for the Natural Language Processing community. By definition, it requires to account for long input texts, a characteristic which poses computational challenges for neural models. Moreover, real-world documents come in a variety of complex, visually-rich, layouts. This information is of great relevance, whether to highlight salient content or to encode long-range interactions between textual passages. Yet, all publicly available summarization datasets only provide plain text content. To facilitate research on how to exploit visual/layout information to better capture long-range dependencies in summarization models, we present LoRaLay, a collection of datasets for long-range summarization with accompanying visual/layout information. We extend existing and popular English datasets (arXiv and PubMed) with layout information and propose four novel datasets \\\\u2013 consistently built from scholar resources \\\\u2013 covering French, Spanish, Portuguese, and Korean languages. Further, we propose new baselines merging layout-aware and long-range models \\\\u2013 two orthogonal approaches \\\\u2013 and obtain state-of-the-art results, showing the importance of combining both lines of research.\"\\n      },\\n      {\\n        \"paperId\": \"7ed835ecf9f3f6a222ec830e7d8ee40fd809dae5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7ed835ecf9f3f6a222ec830e7d8ee40fd809dae5\",\\n        \"title\": \"Context-aware Decoding Reduces Hallucination in Query-focused Summarization\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.14335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A large-scale reproducibility study on one recently proposed decoding method -- Context-aware Decoding (CAD), which improves QFS quality by reducing factuality errors/hallucinations while also at a cost of increased inference-time FLOPs and reduced decoding speed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-12-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284763197\",\\n            \"name\": \"Zhichao Xu\"\\n          }\\n        ],\\n        \"abstract\": \"Query-focused summarization (QFS) aims to provide a summary of a single document/multi documents that can satisfy the information needs of a given query. It is useful for various real-world applications, such as abstractive snippet generation or more recent retrieval augmented generation (RAG). A prototypical QFS pipeline consists of a retriever (sparse or dense retrieval) and a generator (usually a large language model). However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs. There has been growing interest in developing new decoding methods to improve generation quality and reduce hallucination. In this work, we conduct a large-scale reproducibility study on one recently proposed decoding method -- Context-aware Decoding (CAD). In addition to replicating CAD\\'s experiments on news summarization datasets, we include experiments on QFS datasets, and conduct more rigorous analysis on computational complexity and hyperparameter sensitivity. Experiments with eight different language models show that performance-wise, CAD improves QFS quality by (1) reducing factuality errors/hallucinations while (2) mostly retaining the match of lexical patterns, measured by ROUGE scores, while also at a cost of increased inference-time FLOPs and reduced decoding speed. The code implementation based on Huggingface Library is made available https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs\"\\n      },\\n      {\\n        \"paperId\": \"4e444759dc61f26dee627fbb909a15a5ad32b184\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4e444759dc61f26dee627fbb909a15a5ad32b184\",\\n        \"title\": \"AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.09521\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.09521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AMRFact is proposed, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs) and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261278256\",\\n            \"name\": \"Haoyi Qiu\"\\n          },\\n          {\\n            \"authorId\": \"1956008001\",\\n            \"name\": \"Kung-Hsiang Huang\"\\n          },\\n          {\\n            \"authorId\": \"2266841207\",\\n            \"name\": \"Jingnong Qu\"\\n          },\\n          {\\n            \"authorId\": \"2261277442\",\\n            \"name\": \"Nanyun Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate our approach significantly outperforms previous systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating factuality of abstractive summarization.\"\\n      },\\n      {\\n        \"paperId\": \"143b5ef2b4b8f80923e0b46a09dc6b3fac7e3575\",\\n        \"url\": \"https://www.semanticscholar.org/paper/143b5ef2b4b8f80923e0b46a09dc6b3fac7e3575\",\\n        \"title\": \"A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.08503\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.08503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work enforces hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task, and outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2152867918\",\\n            \"name\": \"Chenhui Shen\"\\n          },\\n          {\\n            \"authorId\": \"123962152\",\\n            \"name\": \"Liying Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2147330214\",\\n            \"name\": \"Yang You\"\\n          },\\n          {\\n            \"authorId\": \"1996394\",\\n            \"name\": \"Lidong Bing\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained language models (PLMs) have achieved outstanding achievements in abstractive single-document summarization (SDS). However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex. Previous works either design new MDS architectures or apply PLMs bluntly with concatenated source documents as a reformulated SDS task. While the former does not utilize previous pre-training efforts and may not generalize well across different domains, the latter may not sufficiently attend to the intricate cross-document relationships unique to MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task. Across 10 MDS benchmarks from various domains, our method outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters. It outperforms its corresponding PLM backbone by up to 3 Rouge-L and is favored by humans.\"\\n      },\\n      {\\n        \"paperId\": \"12f881588e6b461b86b34209c30a2bdfe67f904d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/12f881588e6b461b86b34209c30a2bdfe67f904d\",\\n        \"title\": \"Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.14981\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.14981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that using a contrastive learning framework with the refined candidate summaries leads to significant gains on both factuality and similarity-based metrics, and a ranking strategy in which it effectively combine two metrics, thereby preventing any conflict during training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2126503480\",\\n            \"name\": \"Tanay Dixit\"\\n          },\\n          {\\n            \"authorId\": \"47939052\",\\n            \"name\": \"Fei Wang\"\\n          },\\n          {\\n            \"authorId\": \"1998918\",\\n            \"name\": \"Muhao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose {pasted macro \\\\u2018MODEL\\\\u2019}name (i.e. Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.\"\\n      },\\n      {\\n        \"paperId\": \"c014d501abdeb23c6c1c5075e66989a72d3f6f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c014d501abdeb23c6c1c5075e66989a72d3f6f69\",\\n        \"title\": \"Generic protein\\\\u2013ligand interaction scoring by integrating physical prior knowledge and data augmentation modelling\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s42256-024-00849-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s42256-024-00849-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"EquiScore, which utilizes a heterogeneous graph neural network to integrate physical prior knowledge and characterize protein\\\\u2013ligand interactions in equivariant geometric space, is introduced and is shown to have robust capabilities when applied to unseen protein targets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2220518044\",\\n            \"name\": \"Duanhua Cao\"\\n          },\\n          {\\n            \"authorId\": \"2175243828\",\\n            \"name\": \"Geng Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305221348\",\\n            \"name\": \"Jiaxin Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2155427582\",\\n            \"name\": \"Jie Yu\"\\n          },\\n          {\\n            \"authorId\": \"2219042185\",\\n            \"name\": \"Runze Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2276034718\",\\n            \"name\": \"Mingan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2256596397\",\\n            \"name\": \"Wei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2109298404\",\\n            \"name\": \"Lifan Chen\"\\n          },\\n          {\\n            \"authorId\": \"51135036\",\\n            \"name\": \"Feisheng Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2220532589\",\\n            \"name\": \"Yingying Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2220589745\",\\n            \"name\": \"Chenghao Lu\"\\n          },\\n          {\\n            \"authorId\": \"47056625\",\\n            \"name\": \"Xutong Li\"\\n          },\\n          {\\n            \"authorId\": \"2261477527\",\\n            \"name\": \"Xiaomin Luo\"\\n          },\\n          {\\n            \"authorId\": \"2145313647\",\\n            \"name\": \"Sulin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2114135568\",\\n            \"name\": \"Mingyue Zheng\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"ee1ef7b70dc34adcc90c42cc28168165ea56501f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ee1ef7b70dc34adcc90c42cc28168165ea56501f\",\\n        \"title\": \"SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization\",\\n        \"citationCount\": 435,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00453/1987014/tacl_a_00453.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2111.09525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work revisits the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-11-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46180754\",\\n            \"name\": \"Philippe Laban\"\\n          },\\n          {\\n            \"authorId\": \"48839531\",\\n            \"name\": \"Tobias Schnabel\"\\n          },\\n          {\\n            \"authorId\": \"144609235\",\\n            \"name\": \"Paul N. Bennett\"\\n          },\\n          {\\n            \"authorId\": \"1716902\",\\n            \"name\": \"Marti A. Hearst\"\\n          }\\n        ],\\n        \"abstract\": \"In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work.\"\\n      },\\n      {\\n        \"paperId\": \"1e2b6cd7247d9066c97a5f7befa5c4f890075f7b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e2b6cd7247d9066c97a5f7befa5c4f890075f7b\",\\n        \"title\": \"Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2407.12192\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.12192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A feature-oriented workflow for systematic prompt evaluation of summarization prompts is introduced and found that it helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and has the potential to generalize to other NLG and image-generation tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-07-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2289789049\",\\n            \"name\": \"Sam Yu-Te Lee\"\\n          },\\n          {\\n            \"authorId\": \"2311699054\",\\n            \"name\": \"Aryaman Bahukhandi\"\\n          },\\n          {\\n            \"authorId\": \"2312133175\",\\n            \"name\": \"Dongyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2239167039\",\\n            \"name\": \"Kwan-Liu Ma\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.\"\\n      },\\n      {\\n        \"paperId\": \"1b95053af03b5a06809a4967c6cf5ca137bbcde4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1b95053af03b5a06809a4967c6cf5ca137bbcde4\",\\n        \"title\": \"Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.09177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a first multi-turn approach that leverages benign preliminary questions to interact with the LLM, and focuses on a new attack form, called Contextual Interaction Attack, which is black-box and can also transfer across LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2282098821\",\\n            \"name\": \"Yixin Cheng\"\\n          },\\n          {\\n            \"authorId\": \"34291068\",\\n            \"name\": \"Markos Georgopoulos\"\\n          },\\n          {\\n            \"authorId\": \"1678641\",\\n            \"name\": \"V. Cevher\"\\n          },\\n          {\\n            \"authorId\": \"2140285545\",\\n            \"name\": \"Grigorios G. Chrysos\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired from Chomsky\\'s transformational-generative grammar theory and human practices of indirect context to elicit harmful information, we focus on a new attack form, called Contextual Interaction Attack. We contend that the prior context\\\\\\\\u2014the information preceding the attack query\\\\\\\\u2014plays a pivotal role in enabling strong Jailbreaking attacks. Specifically, we propose a first multi-turn approach that leverages benign preliminary questions to interact with the LLM. Due to the autoregressive nature of LLMs, which use previous conversation rounds as context during generation, we guide the model\\'s question-response pair to construct a context that is semantically aligned with the attack query to execute the attack. We conduct experiments on seven different LLMs and demonstrate the efficacy of this attack, which is black-box and can also transfer across LLMs. We believe this can lead to further developments and understanding of security in LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"ab36a9ced3ade6ca6e48055b4579c4ee8bed52f5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ab36a9ced3ade6ca6e48055b4579c4ee8bed52f5\",\\n        \"title\": \"InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion\",\\n        \"citationCount\": 162,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2308.16905\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.16905, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes InterDiff, a framework comprising two key steps: interaction diffusion, where a diffusion model is leverage to encode the distribution of future human-object interactions; and interaction correction, where a physics-informed predictor is introduced to correct denoised HOIs in a diffusion step.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"8775961\",\\n            \"name\": \"Sirui Xu\"\\n          },\\n          {\\n            \"authorId\": \"2221242078\",\\n            \"name\": \"Zhengyu Li\"\\n          },\\n          {\\n            \"authorId\": \"2236683644\",\\n            \"name\": \"Yu-Xiong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2587808\",\\n            \"name\": \"Liangyan Gui\"\\n          }\\n        ],\\n        \"abstract\": \"This paper addresses a novel task of anticipating 3D human-object interactions (HOIs). Most existing research on HOI synthesis lacks comprehensive whole-body interactions with dynamic objects, e.g., often limited to manipulating small or static objects. Our task is significantly more challenging, as it requires modeling dynamic objects with various shapes, capturing whole-body motion, and ensuring physically valid interactions. To this end, we propose InterDiff, a framework comprising two key steps: (i) interaction diffusion, where we leverage a diffusion model to encode the distribution of future human-object interactions; (ii) interaction correction, where we introduce a physics-informed predictor to correct denoised HOIs in a diffusion step. Our key insight is to inject prior knowledge that the interactions under reference with respect to contact points follow a simple pattern and are easily predictable. Experiments on multiple human-object interaction datasets demonstrate the effectiveness of our method for this task, capable of producing realistic, vivid, and remarkably longterm 3D HOI predictions.\"\\n      },\\n      {\\n        \"paperId\": \"59982844ae41696577f3fe4c396a8569f7d0fbd3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/59982844ae41696577f3fe4c396a8569f7d0fbd3\",\\n        \"title\": \"Calibration of Large Language Models on Code Summarization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151266129\",\\n            \"name\": \"Yuvraj Virk\"\\n          },\\n          {\\n            \"authorId\": \"114875459\",\\n            \"name\": \"Prem Devanbu\"\\n          },\\n          {\\n            \"authorId\": \"3432275\",\\n            \"name\": \"Toufique Ahmed\"\\n          }\\n        ],\\n        \"abstract\": \"A brief, fluent, and relevant summary can be helpful during program comprehension; however, such a summary does require significant human effort to produce. Often, good summaries are unavailable in software projects, which makes maintenance more difficult. There has been a considerable body of research into automated AI-based methods, using Large Language models (LLMs), to generate summaries of code; there also has been quite a bit of work on ways to measure the performance of such summarization methods, with special attention paid to how closely these AI-generated summaries resemble a summary a human might have produced. Measures such as BERTScore and BLEU have been suggested and evaluated with human-subject studies. However, LLM-generated summaries can be inaccurate, incomplete, etc: generally, too dissimilar to one that a good developer might write. Given an LLM-generated code summary, how can a user rationally judge if a summary is sufficiently good and reliable? Given just some input source code, and an LLM-generated summary, existing approaches can help judge brevity, fluency and relevance of the summary; however, it\\\\u2019s difficult to gauge whether an LLM-generated summary sufficiently resembles what a human might produce, without a \\\\u201cgolden\\\\u201d human-produced summary to compare against. Prior research indicates that human-produced summaries are generally preferred by human-raters, so we explore this issue in this paper. We study this resemblance question as a calibration problem: given just the code & the summary from an LLM, can we compute a confidence measure, that provides a reliable indication of whether the summary sufficiently resembles what a human would have produced in this situation? We examine this question using several LLMs, for several languages, and in several different settings. Our investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code.\"\\n      },\\n      {\\n        \"paperId\": \"c4960e3cd29f0af3c9120a4173bf681ed2131814\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c4960e3cd29f0af3c9120a4173bf681ed2131814\",\\n        \"title\": \"FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations\",\\n        \"citationCount\": 63,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2204.06508\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.06508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"FactGraph is proposed, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation and improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-04-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"10430740\",\\n            \"name\": \"Leonardo F. R. Ribeiro\"\\n          }\\n        ],\\n        \"abstract\": \"Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.\"\\n      },\\n      {\\n        \"paperId\": \"3e9b4218f6b943de8e57fe70f903911db11d24b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e9b4218f6b943de8e57fe70f903911db11d24b6\",\\n        \"title\": \"Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling\",\\n        \"citationCount\": 53,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.12378\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.12378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes to generate hard, representative synthetic examples of non-factual summaries through infilling language models to improve factual consistency inAbstractive summarization models and shows that this approach vastly outperforms prior methods in correcting erroneous summaries.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"143820870\",\\n            \"name\": \"Vidhisha Balachandran\"\\n          },\\n          {\\n            \"authorId\": \"2548384\",\\n            \"name\": \"Hannaneh Hajishirzi\"\\n          },\\n          {\\n            \"authorId\": \"2058480371\",\\n            \"name\": \"W. Cohen\"\\n          },\\n          {\\n            \"authorId\": \"2073587169\",\\n            \"name\": \"Yulia Tsvetkov\"\\n          }\\n        ],\\n        \"abstract\": \"Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets\\\\u2014 CNN/DM and XSum\\\\u2014we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model\\\\u2014FactEdit\\\\u2014improves factuality scores by over ~11 points on CNN/DM and over ~31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality.\"\\n      },\\n      {\\n        \"paperId\": \"4746b30f271a4acc472930dde079aeffa2e47367\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4746b30f271a4acc472930dde079aeffa2e47367\",\\n        \"title\": \"Evolution of coordination in pairwise and multi-player interactions via prior commitments\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://journals.sagepub.com/doi/pdf/10.1177/1059712321993166\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2009.11727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Whether prior commitment would be a viable evolutionary mechanism for enhancing coordination and the overall population social welfare strongly depends on the collective benefit and severity of competition, and more importantly, how asymmetric benefits are resolved in a commitment deal.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2164337059\",\\n            \"name\": \"Ndidi Bianca Ogbo\"\\n          },\\n          {\\n            \"authorId\": \"12078870\",\\n            \"name\": \"A. Elragig\"\\n          },\\n          {\\n            \"authorId\": \"2244073417\",\\n            \"name\": \"The Anh Han\"\\n          }\\n        ],\\n        \"abstract\": \"Upon starting a collective endeavour, it is important to understand your partners\\\\u2019 preferences and how strongly they commit to a common goal. Establishing a prior commitment or agreement in terms of posterior benefits and consequences from those engaging in it provides an important mechanism for securing cooperation. Resorting to methods from Evolutionary Game Theory (EGT), here we analyse how prior commitments can also be adopted as a tool for enhancing coordination when its outcomes exhibit an asymmetric payoff structure, in both pairwise and multi-party interactions. Arguably, coordination is more complex to achieve than cooperation since there might be several desirable collective outcomes in a coordination problem (compared to mutual cooperation, the only desirable collective outcome in cooperation dilemmas). Our analysis, both analytically and via numerical simulations, shows that whether prior commitment would be a viable evolutionary mechanism for enhancing coordination and the overall population social welfare strongly depends on the collective benefit and severity of competition, and more importantly, how asymmetric benefits are resolved in a commitment deal. Moreover, in multi-party interactions, prior commitments prove to be crucial when a high level of group diversity is required for optimal coordination. The results are robust for different selection intensities. Overall, our analysis provides new insights into the complexity and beauty of behavioural evolution driven by humans\\\\u2019 capacity for commitment, as well as for the design of self-organised and distributed multi-agent systems for ensuring coordination among autonomous agents.\"\\n      },\\n      {\\n        \"paperId\": \"903ce14de7226276a66e1bd3e3d3d97934d2dfd2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/903ce14de7226276a66e1bd3e3d3d97934d2dfd2\",\\n        \"title\": \"Mapping the Design Space of Human-AI Interaction in Text Summarization\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2206.14863\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2206.14863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work conducts a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions, and designed text summarization prototypes for each interaction.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2022-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"117397243\",\\n            \"name\": \"Ruijia Cheng\"\\n          },\\n          {\\n            \"authorId\": \"1405364873\",\\n            \"name\": \"Alison Smith-Renner\"\\n          },\\n          {\\n            \"authorId\": \"152645158\",\\n            \"name\": \"Kecheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1739099\",\\n            \"name\": \"Joel R. Tetreault\"\\n          },\\n          {\\n            \"authorId\": \"144633617\",\\n            \"name\": \"A. Jaimes\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans\\\\u2019 roles, experience, and needs when interacting with or being assisted by AI. From a human-centered perspective, we map the design opportunities and considerations for human-AI interaction in text summarization and broader text generation tasks. We first conducted a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions. We designed text summarization prototypes for each interaction. We then interviewed 16 users, aided by the prototypes, to understand their expectations, experience, and needs regarding efficiency, control, and trust with AI in text summarization and propose design considerations accordingly.\"\\n      },\\n      {\\n        \"paperId\": \"64c1f37f75d08902b460ff2c670293d903a74067\",\\n        \"url\": \"https://www.semanticscholar.org/paper/64c1f37f75d08902b460ff2c670293d903a74067\",\\n        \"title\": \"Expanding the coverage of regulons from high-confidence prior knowledge for accurate estimation of transcription factor activities\",\\n        \"citationCount\": 146,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://academic.oup.com/nar/advance-article-pdf/doi/10.1093/nar/gkad841/52174356/gkad841.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10639077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that the signed CollecTRI-derived regulons outperform other public collections of regulatory interactions in accurately inferring changes in TF activities in perturbation experiments and enable the accurate and comprehensive estimation of TF activities and thereby help to interpret transcriptomics data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2139664698\",\\n            \"name\": \"S. M\\\\u00fcller-Dott\"\\n          },\\n          {\\n            \"authorId\": \"1397165866\",\\n            \"name\": \"Eirini Tsirvouli\"\\n          },\\n          {\\n            \"authorId\": \"145019636\",\\n            \"name\": \"M. V\\\\u00e1zquez\"\\n          },\\n          {\\n            \"authorId\": \"1716214602\",\\n            \"name\": \"R. O. Ramirez Flores\"\\n          },\\n          {\\n            \"authorId\": \"1820842454\",\\n            \"name\": \"Pau Badia-i-Mompel\"\\n          },\\n          {\\n            \"authorId\": \"2213560713\",\\n            \"name\": \"Robin Fallegger\"\\n          },\\n          {\\n            \"authorId\": \"3124875\",\\n            \"name\": \"A. L\\\\u00e6greid\"\\n          },\\n          {\\n            \"authorId\": \"1400885451\",\\n            \"name\": \"J. Saez-Rodriguez\"\\n          }\\n        ],\\n        \"abstract\": \"Gene regulation plays a critical role in the cellular processes that underlie human health and disease. The regulatory relationship between transcription factors (TFs), key regulators of gene expression, and their target genes, the so called TF regulons, can be coupled with computational algorithms to estimate the activity of TFs. However, to interpret these findings accurately, regulons of high reliability and coverage are needed. In this study, we present and evaluate a collection of regulons created using the CollecTRI meta-resource containing signed TF-gene interactions for 1,183 TFs. In this context, we introduce a workflow to integrate information from multiple resources and assign the sign of regulation to TF-gene interactions that could be applied to other comprehensive knowledge bases. We find that the signed CollecTRI-derived regulons outperform other public collections of regulatory interactions in accurately inferring changes in TF activities in perturbation experiments. Furthermore, we showcase the value of the regulons by investigating hallmarks of TF activity profiles inferred from the transcriptomes of three different cancer types. Overall, the CollecTRI-derived TF regulons enable the accurate and comprehensive estimation of TF activities and thereby help to interpret transcriptomics data. GRAPHICAL ABSTRACT\"\\n      },\\n      {\\n        \"paperId\": \"ae1e48a74cb2f313e8e99c82f0aa4487b0805002\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ae1e48a74cb2f313e8e99c82f0aa4487b0805002\",\\n        \"title\": \"Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2204.13761\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.13761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that by utilizing an external knowledge base, one can improve the faithfulness of summaries without simply making them more extractive, and additionally, that external knowledge bases linked from the source can benefit the factuality of generated summaries.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-04-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49265991\",\\n            \"name\": \"Yue Dong\"\\n          },\\n          {\\n            \"authorId\": \"1771118\",\\n            \"name\": \"J. Wieting\"\\n          },\\n          {\\n            \"authorId\": \"2986975\",\\n            \"name\": \"Pat Verga\"\\n          }\\n        ],\\n        \"abstract\": \"Despite recent advances in abstractive summarization, current summarization systems still suffer from content hallucinations where models generate text that is either irrelevant or contradictory to the source document. However, prior work has been predicated on the assumption that any generated facts not appearing explicitly in the source are undesired hallucinations. Methods have been proposed to address this scenario by ultimately improving `faithfulness\\' to the source document, but in reality, there is a large portion of entities in the gold reference targets that are not directly in the source. In this work, we show that these entities are not aberrations, but they instead require utilizing external world knowledge to infer reasoning paths from entities in the source. We show that by utilizing an external knowledge base, we can improve the faithfulness of summaries without simply making them more extractive, and additionally, we show that external knowledge bases linked from the source can benefit the factuality of generated summaries.\"\\n      },\\n      {\\n        \"paperId\": \"81c83519b924bffca465f5a40183c62e711c0a69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/81c83519b924bffca465f5a40183c62e711c0a69\",\\n        \"title\": \"Toward Expanding the Scope of Radiology Report Summarization to Multiple Anatomies and Modalities\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.acl-short.41.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.08584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A dataset involving three new modalities and seven new anatomies based on the MIMIC-III and MIMic-CXR datasets is proposed and the performance of models both within and across modality-anatomy pairs in MIM IC-RRS is evaluated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-11-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35935570\",\\n            \"name\": \"Jean-Benoit Delbrouck\"\\n          },\\n          {\\n            \"authorId\": \"145859954\",\\n            \"name\": \"M. Varma\"\\n          },\\n          {\\n            \"authorId\": \"1584627064\",\\n            \"name\": \"C. Langlotz\"\\n          }\\n        ],\\n        \"abstract\": \"Radiology report summarization (RRS) is a growing area of research. Given the Findings section of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. However, RRS currently faces essential limitations.First, many prior studies conduct experiments on private datasets, preventing reproduction of results and fair comparisons across different systems and solutions. Second, most prior approaches are evaluated solely on chest X-rays. To address these limitations, we propose a dataset (MIMIC-RRS) involving three new modalities and seven new anatomies based on the MIMIC-III and MIMIC-CXR datasets. We then conduct extensive experiments to evaluate the performance of models both within and across modality-anatomy pairs in MIMIC-RRS. In addition, we evaluate their clinical efficacy via RadGraph, a factual correctness metric.\"\\n      },\\n      {\\n        \"paperId\": \"cbc05849bd04058e4aee09738dd73d9ad6621964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cbc05849bd04058e4aee09738dd73d9ad6621964\",\\n        \"title\": \"Dual-Prior Augmented Decoding Network for Long Tail Distribution in HOI Detection\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/27949/27918\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i3.27949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i3.27949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a dual-prior augmented decoding network by decomposing the HOI task into two sub-tasks: human-object pair detection and interaction recognition and leverages external knowledge to enhance the model\\'s ability at a finer granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293390597\",\\n            \"name\": \"Jiayi Gao\"\\n          },\\n          {\\n            \"authorId\": \"2582309\",\\n            \"name\": \"Kongming Liang\"\\n          },\\n          {\\n            \"authorId\": \"2240049431\",\\n            \"name\": \"Tao Wei\"\\n          },\\n          {\\n            \"authorId\": \"2256715874\",\\n            \"name\": \"Wei Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268678016\",\\n            \"name\": \"Zhanyu Ma\"\\n          },\\n          {\\n            \"authorId\": \"2273915241\",\\n            \"name\": \"Jun Guo\"\\n          }\\n        ],\\n        \"abstract\": \"Human object interaction detection aims at localizing human-object pairs and recognizing their interactions. Trapped by the long-tailed distribution of the data, existing HOI detection methods often have difficulty recognizing the tail categories. Many approaches try to improve the recognition of HOI tasks by utilizing external knowledge (e.g. pre-trained visual-language models). However, these approaches mainly utilize external knowledge at the HOI combination level and achieve limited improvement in the tail categories. In this paper, we propose a dual-prior augmented decoding network by decomposing the HOI task into two sub-tasks: human-object pair detection and interaction recognition. For each subtask, we leverage external knowledge to enhance the model\\'s ability at a finer granularity. Specifically, we acquire the prior candidates from an external classifier and embed them to assist the subsequent decoding process. Thus, the long-tail problem is mitigated from a coarse-to-fine level with the corresponding external knowledge. Our approach outperforms existing state-of-the-art models in various settings and significantly boosts the performance on the tail HOI categories. The source code is available at https://github.com/PRIS-CV/DP-ADN.\"\\n      },\\n      {\\n        \"paperId\": \"18f86a1f9896795069d8bf7d5098e3debf2fce81\",\\n        \"url\": \"https://www.semanticscholar.org/paper/18f86a1f9896795069d8bf7d5098e3debf2fce81\",\\n        \"title\": \"Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2203.08257\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.08257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work designs a two-step approach: extractive summarization followed by abstractive summarizations of radiology report summarization, and additionally breaks down the extractive part into two independent tasks: extraction of salient sentences and keywords.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-03-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34112357\",\\n            \"name\": \"Sanjeev Kumar Karn\"\\n          },\\n          {\\n            \"authorId\": \"47695637\",\\n            \"name\": \"Ning Liu\"\\n          },\\n          {\\n            \"authorId\": \"144418438\",\\n            \"name\": \"Hinrich Sch\\\\u00fctze\"\\n          },\\n          {\\n            \"authorId\": \"2211973\",\\n            \"name\": \"Oladimeji Farri\"\\n          }\\n        ],\\n        \"abstract\": \"The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist\\\\u2019s reasoning and conclusions, and it also aids the referring physician in confirming or excluding certain diagnoses. A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report. These tasks include acquisition of salient content from the report and generation of a concise, easily consumable IMPRESSIONS section. Prior research on radiology report summarization has focused on single-step end-to-end models \\\\u2013 which subsume the task of salient content acquisition. To fully explore the cascade structure and explainability of radiology report summarization, we introduce two innovations. First, we design a two-step approach: extractive summarization followed by abstractive summarization. Second, we additionally break down the extractive part into two independent tasks: extraction of salient (1) sentences and (2) keywords. Experiments on a publicly available radiology report dataset show our novel approach leads to a more precise summary compared to single-step and to two-step-with-single-extractive-process baselines with an overall improvement in F1 score of 3-4%.\"\\n      },\\n      {\\n        \"paperId\": \"a9dc25d11fadca961b762c173f5b27a445f645d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a9dc25d11fadca961b762c173f5b27a445f645d7\",\\n        \"title\": \"SARS-CoV-2 biology and host interactions\",\\n        \"citationCount\": 117,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s41579-023-01003-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s41579-023-01003-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An updated summary of the SARS-CoV-2 life cycle, gene function and virus\\\\u2013host interactions, including recent landmark findings on general aspects of coronavirus biology and newly discovered host factors necessary for virus replication are provided.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40397706\",\\n            \"name\": \"Silvio Steiner\"\\n          },\\n          {\\n            \"authorId\": \"1580760257\",\\n            \"name\": \"A. Kratzel\"\\n          },\\n          {\\n            \"authorId\": \"2322491040\",\\n            \"name\": \"G. Barut\"\\n          },\\n          {\\n            \"authorId\": \"2054345681\",\\n            \"name\": \"Reto M. Lang\"\\n          },\\n          {\\n            \"authorId\": \"1398526410\",\\n            \"name\": \"Etori Aguiar Moreira\"\\n          },\\n          {\\n            \"authorId\": \"2007902383\",\\n            \"name\": \"Lisa Thomann\"\\n          },\\n          {\\n            \"authorId\": \"152841375\",\\n            \"name\": \"Jenna N. Kelly\"\\n          },\\n          {\\n            \"authorId\": \"2262194481\",\\n            \"name\": \"Volker Thiel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"e9bc2a727d54ad514cfcacae01f65a6a4f0d0159\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e9bc2a727d54ad514cfcacae01f65a6a4f0d0159\",\\n        \"title\": \"Faithful Abstractive Summarization via Fact-aware Consistency-constrained Transformer\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3511808.3557319?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3511808.3557319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel fact-aware abstractive summarization model, named Entity-Relation Pointer Generator Network (ERPGN), which attempts to formalize the facts in original document as a factual knowledge graph, and then generates the high-quality summary via directly modeling consistency between summary and the factualknowledge graph.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2187857206\",\\n            \"name\": \"Yuanjie Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2118043140\",\\n            \"name\": \"Chen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"50383766\",\\n            \"name\": \"Tong Xu\"\\n          },\\n          {\\n            \"authorId\": \"1422036168\",\\n            \"name\": \"Zikai Yin\"\\n          },\\n          {\\n            \"authorId\": \"2173129111\",\\n            \"name\": \"Enhong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Abstractive summarization is a classic task in Natural Language Generation (NLG), which aims to produce a concise summary of the original document. Recently, great efforts have been made on sequence-to-sequence neural networks to generate abstractive sum- maries with a high level of fluency. However, prior arts mainly focus on the optimization of token-level likelihood, while the rich semantic information in documents has been largely ignored. In this way, the summarization results could be vulnerable to hallucinations, i.e., the semantic-level inconsistency between a summary and corresponding original document. To deal with this challenge, in this paper, we propose a novel fact-aware abstractive summarization model, named Entity-Relation Pointer Generator Network (ERPGN). Specially, we attempt to formalize the facts in original document as a factual knowledge graph, and then generate the high-quality summary via directly modeling consistency between summary and the factual knowledge graph. To that end, we first leverage two pointer net- work structures to capture the fact in original documents. Then, to enhance the traditional token-level likelihood loss, we design two extra semantic-level losses to measure the disagreement between a summary and facts from its original document. Extensive experi- ments on public datasets demonstrate that our ERPGN framework could outperform both classic abstractive summarization models and the state-of-the-art fact-aware baseline methods, with significant improvement in terms of faithfulness.\"\\n      },\\n      {\\n        \"paperId\": \"ac652b7aee8ba769a698f55c61a25e1f1aa270d9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac652b7aee8ba769a698f55c61a25e1f1aa270d9\",\\n        \"title\": \"Answer Summarization for Technical Queries: Benchmark and New Approach\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3551349.3560421\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.10868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A high-quality benchmark to enable automatic evaluation of answer summarization for the technical queries for SQA sites is constructed and it is demonstrated that automatic evaluation has a much lower cost and can uncover findings similar to the ones found through user studies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-09-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2154172271\",\\n            \"name\": \"Chengran Yang\"\\n          },\\n          {\\n            \"authorId\": \"2203459\",\\n            \"name\": \"Bowen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2121315\",\\n            \"name\": \"Ferdian Thung\"\\n          },\\n          {\\n            \"authorId\": \"2185772392\",\\n            \"name\": \"Yucen Shi\"\\n          },\\n          {\\n            \"authorId\": \"2146322053\",\\n            \"name\": \"Ting Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2139059234\",\\n            \"name\": \"Zhou Yang\"\\n          },\\n          {\\n            \"authorId\": \"2148928671\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"151496516\",\\n            \"name\": \"Jieke Shi\"\\n          },\\n          {\\n            \"authorId\": \"2158107537\",\\n            \"name\": \"Junda He\"\\n          },\\n          {\\n            \"authorId\": \"7883212\",\\n            \"name\": \"Donggyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2150912791\",\\n            \"name\": \"David Lo\"\\n          }\\n        ],\\n        \"abstract\": \"Prior studies have demonstrated that approaches to generate an answer summary for a given technical query in Software Question and Answer (SQA) sites are desired. We find that existing approaches are assessed solely through user studies. Hence, a new user study needs to be performed every time a new approach is introduced; this is time-consuming, slows down the development of the new approach, and results from different user studies may not be comparable to each other. There is a need for a benchmark with ground truth summaries as a complement assessment through user studies. Unfortunately, such a benchmark is non-existent for answer summarization for technical queries from SQA sites. To fill the gap, we manually construct a high-quality benchmark to enable automatic evaluation of answer summarization for the technical queries for SQA sites. It contains 111 query-summary pairs extracted from 382 Stack Overflow answers with 2,014 sentence candidates. Using the benchmark, we comprehensively evaluate the performance of existing approaches and find that there is still a big room for improvements. Motivated by the results, we propose a new approach TechSumBot with three key modules:1) Usefulness Ranking module; 2) Centrality Estimation module; and 3) Redundancy Removal module. We evaluate TechSumBot in both automatic (i.e., using our benchmark) and manual (i.e., via a user study) manners. The results from both evaluations consistently demonstrate that TechSumBot outperforms the best performing baseline approaches from both SE and NLP domains by a large margin, i.e., 10.83%\\\\u201314.90%, 32.75%\\\\u201336.59%, and 12.61%\\\\u201317.54%, in terms of ROUGE-1, ROUGE-2, and ROUGE-L on automatic evaluation, and 5.79%\\\\u20139.23% and 17.03%\\\\u201317.68%, in terms of average usefulness and diversity score on human evaluation. This highlights that automatic evaluation on our benchmark can uncover findings similar to the ones found through user studies. More importantly, the automatic evaluation has a much lower cost, especially when it is used to assess a new approach. Additionally, we also conducted an ablation study, which demonstrates that each module in TechSumBot contributes to boosting the overall performance of TechSumBot. We release the benchmark as well as the replication package of our experiment at https://github.com/TechSumBot/TechSumBot.\"\\n      },\\n      {\\n        \"paperId\": \"e391740cd4b5865868b86770479860c3911f5844\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e391740cd4b5865868b86770479860c3911f5844\",\\n        \"title\": \"Static Video Summarization Using Video Coding Features with Frame-Level Temporal Subsampling and Deep Learning\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2076-3417/13/10/6065/pdf?version=1684152118\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app13106065?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app13106065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper offers innovative approaches to the problem of key frame extraction for the purpose of video summarization, which includes the extraction of feature variables from the bit streams of coded videos, followed by optional stepwise regression for dimensionality reduction.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2217753873\",\\n            \"name\": \"Obada Issa\"\\n          },\\n          {\\n            \"authorId\": \"1805004\",\\n            \"name\": \"T. Shanableh\"\\n          }\\n        ],\\n        \"abstract\": \"There is an abundance of digital video content due to the cloud\\\\u2019s phenomenal growth and security footage; it is therefore essential to summarize these videos in data centers. This paper offers innovative approaches to the problem of key frame extraction for the purpose of video summarization. Our approach includes the extraction of feature variables from the bit streams of coded videos, followed by optional stepwise regression for dimensionality reduction. Once the features are extracted and their dimensionality is reduced, we apply innovative frame-level temporal subsampling techniques, followed by training and testing using deep learning architectures. The frame-level temporal subsampling techniques are based on cosine similarity and the PCA projections of feature vectors. We create three different learning architectures by utilizing LSTM networks, 1D-CNN networks, and random forests. The four most popular video summarization datasets, namely, TVSum, SumMe, OVP, and VSUMM, are used to evaluate the accuracy of the proposed solutions. This includes the precision, recall, F-score measures, and computational time. It is shown that the proposed solutions, when trained and tested on all subjective user summaries, achieved F-scores of 0.79, 0.74, 0.88, and 0.81, respectively, for the aforementioned datasets, showing clear improvements over prior studies.\"\\n      },\\n      {\\n        \"paperId\": \"7bb09bedd2d356667dc8aa286b62425c0246cd60\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7bb09bedd2d356667dc8aa286b62425c0246cd60\",\\n        \"title\": \"CAVES: A Dataset to facilitate Explainable Classification and Summarization of Concerns towards COVID Vaccines\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.13746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CAVES is the first large-scale dataset containing about 10k COVID-19 anti-vaccine tweets labelled into various specific anti- Vaccine concerns in a multi-label setting and is also the first multi- label classification dataset that provides explanations for each of the labels.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-04-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1388367454\",\\n            \"name\": \"Soham Poddar\"\\n          },\\n          {\\n            \"authorId\": \"2163948982\",\\n            \"name\": \"Azlaan Mustafa Samad\"\\n          },\\n          {\\n            \"authorId\": \"2182282\",\\n            \"name\": \"Rajdeep Mukherjee\"\\n          },\\n          {\\n            \"authorId\": \"4213990\",\\n            \"name\": \"Niloy Ganguly\"\\n          },\\n          {\\n            \"authorId\": \"143841814\",\\n            \"name\": \"Saptarshi Ghosh\"\\n          }\\n        ],\\n        \"abstract\": \"Convincing people to get vaccinated against COVID-19 is a key societal challenge in the present times. As a first step towards this goal, many prior works have relied on social media analysis to understand the specific concerns that people have towards these vaccines, such as potential side-effects, ineffectiveness, political factors, and so on. Though there are datasets that broadly classify social media posts into Anti-vax and Pro-Vax labels, there is no dataset (to our knowledge) that labels social media posts according to the specific anti-vaccine concerns mentioned in the posts. In this paper, we have curated CAVES, the first large-scale dataset containing about 10k COVID-19 anti-vaccine tweets labelled into various specific anti-vaccine concerns in a multi-label setting. This is also the first multi-label classification dataset that provides explanations for each of the labels. Additionally, the dataset also provides class-wise summaries of all the tweets. We also perform preliminary experiments on the dataset and show that this is a very challenging dataset for multi-label explainable classification and tweet summarization, as is evident by the moderate scores achieved by some state-of-the-art models.\"\\n      }\\n    ]\\n  },\\n  \"iterative prompt templating\": {\\n    \"total\": 1387,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"0f49fed824a915418b9dec3429673ed6d13c829c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f49fed824a915418b9dec3429673ed6d13c829c\",\\n        \"title\": \"Claude 2.0 large language model: Tackling a real-world classification problem with a new iterative prompt engineering approach\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1016/j.iswa.2024.200336\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.iswa.2024.200336?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.iswa.2024.200336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3053303\",\\n            \"name\": \"Loredana Caruccio\"\\n          },\\n          {\\n            \"authorId\": \"2053231377\",\\n            \"name\": \"Stefano Cirillo\"\\n          },\\n          {\\n            \"authorId\": \"1745736\",\\n            \"name\": \"G. Polese\"\\n          },\\n          {\\n            \"authorId\": \"1818311573\",\\n            \"name\": \"Giandomenico Solimando\"\\n          },\\n          {\\n            \"authorId\": \"2431234\",\\n            \"name\": \"Shanmugam Sundaramurthy\"\\n          },\\n          {\\n            \"authorId\": \"66533440\",\\n            \"name\": \"G. Tortora\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"e7476b944f8e8a88242bc3f7e3fa267c5709de97\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e7476b944f8e8a88242bc3f7e3fa267c5709de97\",\\n        \"title\": \"Iterative Prompt Learning for Unsupervised Backlit Image Enhancement\",\\n        \"citationCount\": 151,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.17569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel unsupervised backlit image enhancement method by exploring the potential of Contrastive Language-Image Pre-Training (CLIP) for pixel-level image enhancement, which outperforms state-of-the-art methods in terms of visual quality and generalization ability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1972230748\",\\n            \"name\": \"Zhexin Liang\"\\n          },\\n          {\\n            \"authorId\": \"2185949399\",\\n            \"name\": \"Chongyi Li\"\\n          },\\n          {\\n            \"authorId\": \"7523259\",\\n            \"name\": \"Shangchen Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2054599089\",\\n            \"name\": \"Ruicheng Feng\"\\n          },\\n          {\\n            \"authorId\": \"1717179\",\\n            \"name\": \"Chen Change Loy\"\\n          }\\n        ],\\n        \"abstract\": \"We propose a novel unsupervised backlit image enhancement method, abbreviated as CLIP-LIT, by exploring the potential of Contrastive Language-Image Pre-Training (CLIP) for pixel-level image enhancement. We show that the openworld CLIP prior not only aids in distinguishing between backlit and well-lit images, but also in perceiving heterogeneous regions with different luminance, facilitating the optimization of the enhancement network. Unlike high-level and image manipulation tasks, directly applying CLIP to enhancement tasks is non-trivial, owing to the difficulty in finding accurate prompts. To solve this issue, we devise a prompt learning framework that first learns an initial prompt pair by constraining the text-image similarity between the prompt (negative/positive sample) and the corresponding image (backlit image/well-lit image) in the CLIP latent space. Then, we train the enhancement network based on the textimage similarity between the enhanced result and the initial prompt pair. To further improve the accuracy of the initial prompt pair, we iteratively fine-tune the prompt learning framework to reduce the distribution gaps between the backlit images, enhanced results, and well-lit images via rank learning, boosting the enhancement performance. Our method alternates between updating the prompt learning framework and enhancement network until visually pleasing results are achieved. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in terms of visual quality and generalization ability, without requiring any paired data.\"\\n      },\\n      {\\n        \"paperId\": \"1ca54f9a79ffca3eeb187f81bad5c1a1df790c06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1ca54f9a79ffca3eeb187f81bad5c1a1df790c06\",\\n        \"title\": \"Iterative Prompt Refinement for Mining Gene Relationships from ChatGPT\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10769373\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10769373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study systematically evaluated ChatGPT\\\\u2019s capacity for predicting gene relationships using GPT-3.5-turbo and GPT-4, and introduced an innovative iterative prompt refinement technique that significantly enhances the performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-12-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2265518204\",\\n            \"name\": \"Yibo Chen\"\\n          },\\n          {\\n            \"authorId\": \"2276572987\",\\n            \"name\": \"Jeffrey Gao\"\\n          },\\n          {\\n            \"authorId\": \"2849198\",\\n            \"name\": \"Marius Petruc\"\\n          },\\n          {\\n            \"authorId\": \"2265390717\",\\n            \"name\": \"Richard D. Hammer\"\\n          },\\n          {\\n            \"authorId\": \"2072374137\",\\n            \"name\": \"Mihail Popescu\"\\n          },\\n          {\\n            \"authorId\": \"2318663840\",\\n            \"name\": \"Dong Xu\"\\n          }\\n        ],\\n        \"abstract\": \"ChatGPT has demonstrated its potential as a surrogate knowledge graph. Trained on extensive data sources, including open-access publications, peer-reviewed research articles and biomedical websites, ChatGPT extracted information on gene relationships and biological pathways. However, a major challenge is model hallucination, i.e., high false positive rates. To assess and address this challenge, we systematically evaluated ChatGPT\\\\u2019s capacity for predicting gene relationships using GPT-3.5-turbo and GPT-4. Benchmarking against the KEGG Pathway Database as the ground truth, we experimented with diverse prompting strategies, targeting gene relationships of activation, inhibition, and phosphorylation. We introduced an innovative iterative prompt refinement technique. By assessing prompt efficacy using metrics like F-1 score, precision, and recall, GPT-4 was re-engaged to suggest improved prompts. A refined prompt, which combines a specialized role with explanatory text, significantly enhances the performance. Going beyond pairwise gene relationships, we also deciphered complex gene interplays, such as gene interaction chains and pathways pertinent to diseases like non-small cell lung cancer. Direct prompts showed limited success, but \\\\u201cleast-to-most\\\\u201d prompting exhibited significant potentials for such network constructions. The methods in this study may be used for some other bioinformatics prediction problems.\"\\n      },\\n      {\\n        \"paperId\": \"1e39f3ca4aff09dccc3b951cdd355c7d8e7cbc2f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1e39f3ca4aff09dccc3b951cdd355c7d8e7cbc2f\",\\n        \"title\": \"Generative AI tools in art education: Exploring prompt engineering and iterative processes for enhanced creativity\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.54517/m.v4i1.2164\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54517/m.v4i1.2164?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54517/m.v4i1.2164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI\\\\u2019s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"48965602\",\\n            \"name\": \"Peter Cotroneo\"\\n          },\\n          {\\n            \"authorId\": \"145099528\",\\n            \"name\": \"James Hutson\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid development and adoption of generative artificial intelligence (AI) tools in the art and design education landscape have introduced both opportunities and challenges. This timely study addresses the need to effectively integrate these tools into the classroom while considering ethical implications and the importance of prompt engineering. By examining the iterative process of refining original ideas through multiple iterations, verbal expansion, and the use of OpenAI\\\\u2019s DALL-E2 for generating diverse visual outcomes, researchers gain insights into the potential benefits and pitfalls of these tools in an educational context. Students in the digital at case study were taught prompt engineering techniques and were tasked with crafting multiple prompts, focusing on refining their ideas over time. Participants demonstrated an increased understanding of the potential and limitations of generative AI tools and how to manipulate subject matter for more effective results. The iterative process encouraged students to explore and experiment with their creative ideas, leading to a deeper understanding of the possibilities offered by AI tools. Despite acknowledging the ethical concerns regarding copyright and the potential replacement of artists, students appreciated the value of generative AI tools for enhancing their sketchbooks and ideation process. Through prompt engineering and iterative processes, students developed a more detail-oriented approach to their work. The challenge of using AI-generated images as final products was conceptually intriguing, requiring further investigation and consideration of the prompts. This study highlights the potential benefits and challenges of integrating generative AI tools into art and design classrooms, emphasizing the importance of prompt engineering, iterative processes, and ethical considerations as these technologies continue to evolve.\"\\n      },\\n      {\\n        \"paperId\": \"0f4219a1a0f89c7364aabb27632cdcc833ab53b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0f4219a1a0f89c7364aabb27632cdcc833ab53b7\",\\n        \"title\": \"Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.15843\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.15843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a bi-directional iterative prompt-tuning method for EAE, where the EAE task is treated as a cloze-style task to take full advantage of entity information and pre-trained language models (PLMs).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2072645045\",\\n            \"name\": \"Lu Dai\"\\n          },\\n          {\\n            \"authorId\": \"2156645305\",\\n            \"name\": \"Bang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2052727822\",\\n            \"name\": \"Wei Xiang\"\\n          },\\n          {\\n            \"authorId\": \"1683407\",\\n            \"name\": \"Yijun Mo\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, prompt-tuning has attracted growing interests in event argument extraction (EAE). However, the existing prompt-tuning methods have not achieved satisfactory performance due to the lack of consideration of entity information. In this paper, we propose a bi-directional iterative prompt-tuning method for EAE, where the EAE task is treated as a cloze-style task to take full advantage of entity information and pre-trained language models (PLMs). Furthermore, our method explores event argument interactions by introducing the argument roles of contextual entities into prompt construction. Since template and verbalizer are two crucial components in a cloze-style prompt, we propose to utilize the role label semantic knowledge to construct a semantic verbalizer and design three kind of templates for the EAE task. Experiments on the ACE 2005 English dataset with standard and low-resource settings show that the proposed method significantly outperforms the peer state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1566d96346927ad4dced85de4d55356f6aee6fb6\",\\n        \"title\": \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\",\\n        \"citationCount\": 88,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new approach to code generation by LLMs is proposed, which is called AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"72729733\",\\n            \"name\": \"T. Ridnik\"\\n          },\\n          {\\n            \"authorId\": \"2279758170\",\\n            \"name\": \"Dedy Kredo\"\\n          },\\n          {\\n            \"authorId\": \"49668367\",\\n            \"name\": \"Itamar Friedman\"\\n          }\\n        ],\\n        \"abstract\": \"Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium\"\\n      },\\n      {\\n        \"paperId\": \"58f14bc6de4a005b439f38fe11db6e4f95e343bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/58f14bc6de4a005b439f38fe11db6e4f95e343bf\",\\n        \"title\": \"Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers, and develops a capable Gradient-inspired LLM-based Prompt Optimizer called GPO.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2109887979\",\\n            \"name\": \"Xinyu Tang\"\\n          },\\n          {\\n            \"authorId\": \"72541556\",\\n            \"name\": \"Xiaolei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2257376413\",\\n            \"name\": \"Wayne Xin Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2287879620\",\\n            \"name\": \"Siyuan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2273472188\",\\n            \"name\": \"Yaliang Li\"\\n          },\\n          {\\n            \"authorId\": \"2274218622\",\\n            \"name\": \"Ji-Rong Wen\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. By systematically analyzing a rich set of improvement strategies on the two aspects, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the optimization trajectory as the update direction. Then, it utilizes the generation-based refinement strategy to perform the update, while controlling the edit distance through a cosine-based decay strategy. Extensive experiments demonstrate the effectiveness and efficiency of GPO. In particular, GPO brings an additional improvement of up to 56.8% on Big-Bench Hard and 62.6% on MMLU compared to baseline methods.\"\\n      },\\n      {\\n        \"paperId\": \"015b1f127b6c31654e3597b75876eed8e445d866\",\\n        \"url\": \"https://www.semanticscholar.org/paper/015b1f127b6c31654e3597b75876eed8e445d866\",\\n        \"title\": \"PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2412.00596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"PhyT2V is presented, a new data-independent T2V technique that expands the current T2V model\\\\u2019s capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-11-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2333357012\",\\n            \"name\": \"Qiyao Xue\"\\n          },\\n          {\\n            \"authorId\": \"2202556933\",\\n            \"name\": \"Xiangyu Yin\"\\n          },\\n          {\\n            \"authorId\": \"2119657961\",\\n            \"name\": \"Boyuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2274008610\",\\n            \"name\": \"Wei Gao\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation has been recently enabled by transformer-based diffusion models, but current T2V models lack capabilities in adhering to the real-world common knowledge and physical rules, due to their limited understanding of physical realism and deficiency in temporal modeling. Existing solutions are either data-driven or require extra model inputs, but cannot be generalizable to out-of-distribution domains. In this paper, we present PhyT2V, a new data-independent T2V technique that expands the current T2V model\\\\u2019s capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting. Our experiments show that PhyT2V improves existing T2V models\\\\u2019 adherence to real-world physical rules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.\"\\n      },\\n      {\\n        \"paperId\": \"2ed64d90670177bf58cdce6bda04a48a8731a18f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2ed64d90670177bf58cdce6bda04a48a8731a18f\",\\n        \"title\": \"ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing\",\\n        \"citationCount\": 160,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613904.3642016\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.09128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs, and identifies three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243191212\",\\n            \"name\": \"Ian Arawjo\"\\n          },\\n          {\\n            \"authorId\": \"40964240\",\\n            \"name\": \"Chelse Swoopes\"\\n          },\\n          {\\n            \"authorId\": \"40961780\",\\n            \"name\": \"Priyan Vaithilingam\"\\n          },\\n          {\\n            \"authorId\": \"2237803620\",\\n            \"name\": \"Martin Wattenberg\"\\n          },\\n          {\\n            \"authorId\": \"143730651\",\\n            \"name\": \"Elena L. Glassman\"\\n          }\\n        ],\\n        \"abstract\": \"Evaluating outputs of large language models (LLMs) is challenging, requiring making\\\\u2014and making sense of\\\\u2014many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.\"\\n      },\\n      {\\n        \"paperId\": \"26bdde27ff70384ef7c43f2670d8ec6559c71f45\",\\n        \"url\": \"https://www.semanticscholar.org/paper/26bdde27ff70384ef7c43f2670d8ec6559c71f45\",\\n        \"title\": \"Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.00507, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Examining and comparing prompt chaining and stepwise prompt in the context of text summarization to ascertain which method stands out as the most effective has the potential to be extrapolated to other applications.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256995981\",\\n            \"name\": \"Shichao Sun\"\\n          },\\n          {\\n            \"authorId\": \"2273661632\",\\n            \"name\": \"Ruifeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2314396\",\\n            \"name\": \"Ziqiang Cao\"\\n          },\\n          {\\n            \"authorId\": \"2237591981\",\\n            \"name\": \"Wenjie Li\"\\n          },\\n          {\\n            \"authorId\": \"2256991660\",\\n            \"name\": \"Pengfei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have demonstrated the capacity to improve summary quality by mirroring a human-like iterative process of critique and refinement starting from the initial draft. Two strategies are designed to perform this iterative process: Prompt Chaining and Stepwise Prompt. Prompt chaining orchestrates the drafting, critiquing, and refining phases through a series of three discrete prompts, while Stepwise prompt integrates these phases within a single prompt. However, the relative effectiveness of the two methods has not been extensively studied. This paper is dedicated to examining and comparing these two methods in the context of text summarization to ascertain which method stands out as the most effective. Experimental results show that the prompt chaining method can produce a more favorable outcome. This might be because stepwise prompt might produce a simulated refinement process according to our various experiments. Since refinement is adaptable to diverse tasks, our conclusions have the potential to be extrapolated to other applications, thereby offering insights that may contribute to the broader development of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"a02482f81aa2651848ba060766f850fa3261b42a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a02482f81aa2651848ba060766f850fa3261b42a\",\\n        \"title\": \"Prompt engineering on leveraging large language models in generating response to InBasket messages\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1093/jamia/ocae172\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11413421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Informed by clinician and patient feedback synergistically, tuning in LLM prompt alone can be effective in creating clinically relevant and useful draft responses to PMARs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2312683202\",\\n            \"name\": \"Sherry Yan\"\\n          },\\n          {\\n            \"authorId\": \"2312112991\",\\n            \"name\": \"Wendi Knapp\"\\n          },\\n          {\\n            \"authorId\": \"2312112445\",\\n            \"name\": \"Andrew Leong\"\\n          },\\n          {\\n            \"authorId\": \"2312112555\",\\n            \"name\": \"Sarira Kadkhodazadeh\"\\n          },\\n          {\\n            \"authorId\": \"2312177380\",\\n            \"name\": \"Souvik Das\"\\n          },\\n          {\\n            \"authorId\": \"2312113058\",\\n            \"name\": \"Veena G Jones\"\\n          },\\n          {\\n            \"authorId\": \"2312408017\",\\n            \"name\": \"Robert Clark\"\\n          },\\n          {\\n            \"authorId\": \"2312112645\",\\n            \"name\": \"David Grattendick\"\\n          },\\n          {\\n            \"authorId\": \"2312166178\",\\n            \"name\": \"Kevin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2312113594\",\\n            \"name\": \"Lisa Hladik\"\\n          },\\n          {\\n            \"authorId\": \"2312112709\",\\n            \"name\": \"Lawrence Fagan\"\\n          },\\n          {\\n            \"authorId\": \"2312111094\",\\n            \"name\": \"Albert Chan\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Objectives Large Language Models (LLMs) have been proposed as a solution to address high volumes of Patient Medical Advice Requests (PMARs). This study addresses whether LLMs can generate high quality draft responses to PMARs that satisfies both patients and clinicians with prompt engineering. Materials and Methods We designed a novel human-involved iterative processes to train and validate prompts to LLM in creating appropriate responses to PMARs. GPT-4 was used to generate response to the messages. We updated the prompts, and evaluated both clinician and patient acceptance of LLM-generated draft responses at each iteration, and tested the optimized prompt on independent validation data sets. The optimized prompt was implemented in the electronic health record production environment and tested by 69 primary care clinicians. Results After 3 iterations of prompt engineering, physician acceptance of draft suitability increased from 62% to 84% (P\\\\u2009<.001) in the validation dataset (N\\\\u2009=\\\\u2009200), and 74% of drafts in the test dataset were rated as \\\\u201chelpful.\\\\u201d Patients also noted significantly increased favorability of message tone (78%) and overall quality (80%) for the optimized prompt compared to the original prompt in the training dataset, patients were unable to differentiate human and LLM-generated draft PMAR responses for 76% of the messages, in contrast to the earlier preference for human-generated responses. Majority (72%) of clinicians believed it can reduce cognitive load in dealing with InBasket messages. Discussion and Conclusion Informed by clinician and patient feedback synergistically, tuning in LLM prompt alone can be effective in creating clinically relevant and useful draft responses to PMARs.\"\\n      },\\n      {\\n        \"paperId\": \"c8e1dc10f302fb5cd748672d5d10d95a27c3bd0b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c8e1dc10f302fb5cd748672d5d10d95a27c3bd0b\",\\n        \"title\": \"Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.08995, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Self-prompt tuning is proposed, making LLMs themselves generate role-play prompts through fine-tuning, thereby automating complex prompting strategies and illustrating the great potential of utilizing fine-tuning to enable LLMs to self-prompt, thereby automating complex prompting strategies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2216561498\",\\n            \"name\": \"Aobo Kong\"\\n          },\\n          {\\n            \"authorId\": \"2275765829\",\\n            \"name\": \"Shiwan Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2242179099\",\\n            \"name\": \"Hao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2311304336\",\\n            \"name\": \"Qicheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2275736194\",\\n            \"name\": \"Yong Qin\"\\n          },\\n          {\\n            \"authorId\": \"2216621163\",\\n            \"name\": \"Ruiqi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2311736771\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2275780366\",\\n            \"name\": \"Jiaming Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2275584632\",\\n            \"name\": \"Haoqin Sun\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advancements in LLMs have showcased their remarkable role-playing capabilities, able to accurately simulate the dialogue styles and cognitive processes of various roles based on different instructions and contexts. Studies indicate that assigning LLMs the roles of experts, a strategy known as role-play prompting, can enhance their performance in the corresponding domains. However, the prompt needs to be manually designed for the given problem, requiring certain expertise and iterative modifications. To this end, we propose self-prompt tuning, making LLMs themselves generate role-play prompts through fine-tuning. Leveraging the LIMA dataset as our foundational corpus, we employ GPT-4 to annotate role-play prompts for each data points, resulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like Llama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned LLMs can automatically generate expert role prompts for any given question. We extensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and open-ended question test. Our empirical results illustrate that self-prompt tuned LLMs outperform standard instruction tuned baselines across most datasets. This highlights the great potential of utilizing fine-tuning to enable LLMs to self-prompt, thereby automating complex prompting strategies. We release the dataset, models, and code at this \\\\\\\\href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.\"\\n      },\\n      {\\n        \"paperId\": \"b503050327ba9e01d27dd8d06589544f89f40cbd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b503050327ba9e01d27dd8d06589544f89f40cbd\",\\n        \"title\": \"LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.16929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Inspired by structured reusable programming languages, LangGPT, a dual-layer prompt design framework as the programming language for LLMs is proposed, which has an easy-to-learn normative structure and provides an extended structure for migration and reuse.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-02-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2249764692\",\\n            \"name\": \"Ming Wang\"\\n          },\\n          {\\n            \"authorId\": \"2288139031\",\\n            \"name\": \"Yuanzhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2287879683\",\\n            \"name\": \"Xiaoming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2287801318\",\\n            \"name\": \"Songlian Li\"\\n          },\\n          {\\n            \"authorId\": \"2287879297\",\\n            \"name\": \"Yijie Huang\"\\n          },\\n          {\\n            \"authorId\": \"2287880544\",\\n            \"name\": \"Chi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2111226672\",\\n            \"name\": \"Daling Wang\"\\n          },\\n          {\\n            \"authorId\": \"2087586948\",\\n            \"name\": \"Shi Feng\"\\n          },\\n          {\\n            \"authorId\": \"2287871665\",\\n            \"name\": \"Jigang Li\"\\n          }\\n        ],\\n        \"abstract\": \"LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. In addition, it is not conducive to the iterative updating of prompts. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the performance of LLMs. Moreover, the case study shows that LangGPT leads LLMs to generate higher-quality responses. Furthermore, we analyzed the ease of use and reusability of LangGPT through a user survey in our online community.\"\\n      },\\n      {\\n        \"paperId\": \"0b8e991c334bfaa5d8ede08bb0fdbe446622931a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0b8e991c334bfaa5d8ede08bb0fdbe446622931a\",\\n        \"title\": \"Leveraging enhanced egret swarm optimization algorithm and artificial intelligence-driven prompt strategies for portfolio selection\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11535209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A structured prompt framework specifically designed for stock selection, aiming to provide direct and interpretable stock-selecting tools for investors of various levels and a novel algorithm that combines the Nonlinear-Activated Beetle Antennae Search strategy with the Egret Swarm Optimization Algorithm to address the Mean-Variance Portfolio Selection problem with Transaction Costs and Cardinality Constraints.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2329413534\",\\n            \"name\": \"Zhendai Huang\"\\n          },\\n          {\\n            \"authorId\": \"2294153042\",\\n            \"name\": \"Zhen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2181171694\",\\n            \"name\": \"Cheng Hua\"\\n          },\\n          {\\n            \"authorId\": \"2329153233\",\\n            \"name\": \"Bolin Liao\"\\n          },\\n          {\\n            \"authorId\": \"2133437125\",\\n            \"name\": \"Shuai Li\"\\n          }\\n        ],\\n        \"abstract\": \"In the financial field, constructing efficient investment portfolios is a focal point of research, encompassing asset selection and optimization of asset allocation. With the advancements in Large Language Models (LLMs), generative Artificial Intelligence (AI) tools have showcased capabilities never seen before. However, the black-box nature of these tools renders their outputs difficult to interpret directly, often necessitating iterative fine-tuning to align with users\\\\u2019 expected outcomes. This study presents a structured prompt framework specifically designed for stock selection, aiming to provide direct and interpretable stock-selecting tools for investors of various levels. By creating representative scenarios and combining them into different cases for experimentation, we can explore how the construction of prompts influences the responses generated by generative AI tools. Additionally, this paper proposes a novel algorithm that combines the Nonlinear-Activated Beetle Antennae Search strategy with the Egret Swarm Optimization Algorithm (NBESOA) to address the Mean-Variance Portfolio Selection problem with Transaction Costs and Cardinality Constraints (MVPS-TCCC), utilizing real stock market data to construct portfolios based on generative AI tools recommendations. Simulation results indicate that, compared to other algorithms, NBESOA prefers optimizing portfolio configurations to achieve the highest Sharpe Ratio with the strictest constraints, bringing the outcomes closer to the portfolio\\\\u2019s efficient frontier.\"\\n      },\\n      {\\n        \"paperId\": \"168ce6b5844f8cd4a3375f69952ccf630874689d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/168ce6b5844f8cd4a3375f69952ccf630874689d\",\\n        \"title\": \"Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.11105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method called Source Prompt Disentangled Inversion (SPDInv), which aims at reducing the impact of source prompt, thereby enhancing the text-driven image editing performance by employing diffusion models and can easily adapt customized image generation models to localized editing tasks and produce promising performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2284415331\",\\n            \"name\": \"Rui Li\"\\n          },\\n          {\\n            \"authorId\": \"151482572\",\\n            \"name\": \"Ruihuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2279757730\",\\n            \"name\": \"Song Guo\"\\n          },\\n          {\\n            \"authorId\": \"2277508910\",\\n            \"name\": \"Lei Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-driven diffusion models have significantly advanced the image editing performance by using text prompts as inputs. One crucial step in text-driven image editing is to invert the original image into a latent noise code conditioned on the source prompt. While previous methods have achieved promising results by refactoring the image synthesizing process, the inverted latent noise code is tightly coupled with the source prompt, limiting the image editability by target text prompts. To address this issue, we propose a novel method called Source Prompt Disentangled Inversion (SPDInv), which aims at reducing the impact of source prompt, thereby enhancing the text-driven image editing performance by employing diffusion models. To make the inverted noise code be independent of the given source prompt as much as possible, we indicate that the iterative inversion process should satisfy a fixed-point constraint. Consequently, we transform the inversion problem into a searching problem to find the fixed-point solution, and utilize the pre-trained diffusion models to facilitate the searching process. The experimental results show that our proposed SPDInv method can effectively mitigate the conflicts between the target editing prompt and the source prompt, leading to a significant decrease in editing artifacts. In addition to text-driven image editing, with SPDInv we can easily adapt customized image generation models to localized editing tasks and produce promising performance. The source code are available at https://github.com/leeruibin/SPDInv.\"\\n      },\\n      {\\n        \"paperId\": \"f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"title\": \"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.01701?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"a0a3d3349497782861ecc436c3807bfe1a7d90a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0a3d3349497782861ecc436c3807bfe1a7d90a1\",\\n        \"title\": \"Prompting Change: Exploring Prompt Engineering in Large Language Model AI and Its Potential to Transform Education\",\\n        \"citationCount\": 127,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11528-023-00896-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11528-023-00896-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The paper underscores how the natural language capabilities of LLM AI tools can help students and educators transition from passive recipients to active co-creators of their learning experiences, and charts the evolving trajectory of LLM AI as a tool poised to reshape educational practices and assumptions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260528529\",\\n            \"name\": \"William Cain\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"27af585e0e48741a18a042a7b7590f021483e151\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27af585e0e48741a18a042a7b7590f021483e151\",\\n        \"title\": \"Semantic-guided Prompt Organization for Universal Goal Hijacking against LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2405.14189?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2405.14189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A universal goal hijacking method called POUGH that incorporates semantic-guided prompt processing strategies and employs an iterative optimization algorithm to generate the universal fixed suffix for the prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142367654\",\\n            \"name\": \"Yihao Huang\"\\n          },\\n          {\\n            \"authorId\": \"2276510804\",\\n            \"name\": \"Chong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2275762052\",\\n            \"name\": \"Xiaojun Jia\"\\n          },\\n          {\\n            \"authorId\": \"2260611391\",\\n            \"name\": \"Qing Guo\"\\n          },\\n          {\\n            \"authorId\": \"2119277493\",\\n            \"name\": \"Felix Juefei-Xu\"\\n          },\\n          {\\n            \"authorId\": \"2265725720\",\\n            \"name\": \"Jian Zhang\"\\n          },\\n          {\\n            \"authorId\": \"4166714\",\\n            \"name\": \"G. Pu\"\\n          },\\n          {\\n            \"authorId\": \"2265753294\",\\n            \"name\": \"Yang Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"d5544571fa37a6eaac972153cec57df591c38a04\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d5544571fa37a6eaac972153cec57df591c38a04\",\\n        \"title\": \"MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.13191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MCQG-SRefine is proposed, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions, and introduces an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-10-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1576489304\",\\n            \"name\": \"Zonghai Yao\"\\n          },\\n          {\\n            \"authorId\": \"2326302479\",\\n            \"name\": \"Aditya Parashar\"\\n          },\\n          {\\n            \"authorId\": \"2261556443\",\\n            \"name\": \"Huixue Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2323786086\",\\n            \"name\": \"Won Seok Jang\"\\n          },\\n          {\\n            \"authorId\": \"2263773255\",\\n            \"name\": \"Feiyun Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2322987613\",\\n            \"name\": \"Zhichao Yang\"\\n          },\\n          {\\n            \"authorId\": \"2261455807\",\\n            \"name\": \"Hong Yu\"\\n          }\\n        ],\\n        \"abstract\": \"Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty. To address these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions. By integrating expert-driven prompt engineering with iterative self-critique and self-correction feedback, MCQG-SRefine significantly enhances human expert satisfaction regarding both the quality and difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.\"\\n      },\\n      {\\n        \"paperId\": \"e314d182fd9d35a05870b38a56ee38eb3149b47d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e314d182fd9d35a05870b38a56ee38eb3149b47d\",\\n        \"title\": \"Attack Prompt Generation for Red Teaming and Defending Large Language Models\",\\n        \"citationCount\": 81,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.12505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning to enhance their safety against red teaming attacks, and proposes a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2260342358\",\\n            \"name\": \"Boyi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2117833732\",\\n            \"name\": \"Wenjie Wang\"\\n          },\\n          {\\n            \"authorId\": \"2163400298\",\\n            \"name\": \"Fuli Feng\"\\n          },\\n          {\\n            \"authorId\": \"2367303636\",\\n            \"name\": \"Yang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2260433198\",\\n            \"name\": \"Qifan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2239071206\",\\n            \"name\": \"Xiangnan He\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facilitating the safety evaluation and enhancement of more LLMs. Our code and dataset is available on https://github.com/Aatrox103/SAP .\"\\n      },\\n      {\\n        \"paperId\": \"848909fbae167f21589bfc7a54fbf27e306b883c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/848909fbae167f21589bfc7a54fbf27e306b883c\",\\n        \"title\": \"An Iterative Optimizing Framework for Radiology Report Summarization With ChatGPT\",\\n        \"citationCount\": 123,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2304.08448\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.08448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed ImpressionGPT model achieves superior performance of AIG task on both the Medical Information Mart for Intensive Care - Chest X-ray database and Open Access Biomedical Image Search Engine datasets without requiring additional training data or fine-tuning the LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2132543537\",\\n            \"name\": \"Chong Ma\"\\n          },\\n          {\\n            \"authorId\": \"47039788\",\\n            \"name\": \"Zihao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2110238778\",\\n            \"name\": \"Jiaqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2211904452\",\\n            \"name\": \"Shaochen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2163675860\",\\n            \"name\": \"Yaonai Wei\"\\n          },\\n          {\\n            \"authorId\": \"2145977326\",\\n            \"name\": \"Zheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"143621713\",\\n            \"name\": \"Lei Guo\"\\n          },\\n          {\\n            \"authorId\": \"2174965546\",\\n            \"name\": \"Xiaoya Cai\"\\n          },\\n          {\\n            \"authorId\": \"2108086798\",\\n            \"name\": \"Shu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"49104946\",\\n            \"name\": \"Tuo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2181182\",\\n            \"name\": \"Dajiang Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2150038187\",\\n            \"name\": \"Dinggang Shen\"\\n          },\\n          {\\n            \"authorId\": \"2115345993\",\\n            \"name\": \"Tianming Liu\"\\n          },\\n          {\\n            \"authorId\": \"2144438902\",\\n            \"name\": \"Xiang Li\"\\n          }\\n        ],\\n        \"abstract\": \"The \\\\u201cImpression\\\\u201d section of a radiology report is a critical basis for communication between radiologists and other physicians. Typically written by radiologists, this part is derived from the \\\\u201cFindings\\\\u201d section, which can be laborious and error-prone. Although deep-learning-based models, such as bidirectional encoder representation from transformers (BERT), have achieved promising results in automatic impression generation (AIG), such models often require substantial amounts of medical data and have poor generalization performance. Recently, large language models (LLMs) like Chat Generative Pre-trained Transformer (ChatGPT) have shown strong generalization capabilities and performance, but their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, leveraging the contextual learning capabilities of LLMs through our dynamic prompt and iterative optimization algorithm to accomplish the AIG task. ImpressionGPT initially employs a small amount of domain-specific data to create a dynamic prompt, extracting contextual semantic information closely related to the test data. Subsequently, the iterative optimization algorithm automatically evaluates the output of LLMs and provides optimization suggestions, continuously refining the output results. The proposed ImpressionGPT model achieves superior performance of AIG task on both the Medical Information Mart for Intensive Care - Chest X-ray database (MIMIC-CXR) and Open Access Biomedical Image Search Engine (OpenI) datasets without requiring additional training data or fine-tuning the LLMs. This work presents a paradigm for localizing LLMs that can be applied in a wide range of similar application scenarios, bridging the gap between general-purpose LLMs and the specific language processing needs of various domains.\"\\n      },\\n      {\\n        \"paperId\": \"2d77b7203824e617206634277bce7eec2b71a2bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d77b7203824e617206634277bce7eec2b71a2bd\",\\n        \"title\": \"Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.20701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is revealed that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans, and a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE) is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105515515\",\\n            \"name\": \"Pengwei Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2294783971\",\\n            \"name\": \"Zhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2149324430\",\\n            \"name\": \"Qian Tan\"\\n          },\\n          {\\n            \"authorId\": \"2304366173\",\\n            \"name\": \"Jie Song\"\\n          },\\n          {\\n            \"authorId\": \"2304326919\",\\n            \"name\": \"Ru Xie\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.\"\\n      },\\n      {\\n        \"paperId\": \"6c7a1bcaa69dd29cb5fa89ac4a9ec61621c211fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6c7a1bcaa69dd29cb5fa89ac4a9ec61621c211fb\",\\n        \"title\": \"SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.06426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research highlights the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse, and introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2330190651\",\\n            \"name\": \"Bijoy Ahmed Saiem\"\\n          },\\n          {\\n            \"authorId\": \"2330191650\",\\n            \"name\": \"MD Sadik Hossain Shanto\"\\n          },\\n          {\\n            \"authorId\": \"2330191231\",\\n            \"name\": \"Rakib Ahsan\"\\n          },\\n          {\\n            \"authorId\": \"2150249269\",\\n            \"name\": \"Md. Rafi Ur Rashid\"\\n          }\\n        ],\\n        \"abstract\": \"As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.\"\\n      },\\n      {\\n        \"paperId\": \"113873a4e58e2ff15ce3523ee9fb629ff6dddfe4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/113873a4e58e2ff15ce3523ee9fb629ff6dddfe4\",\\n        \"title\": \"Does Prompt Formatting Have Any Impact on LLM Performance?\",\\n        \"citationCount\": 116,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2411.10541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper examines the impact of different prompt templates on LLM performance, and highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2331365723\",\\n            \"name\": \"Jia He\"\\n          },\\n          {\\n            \"authorId\": \"35315626\",\\n            \"name\": \"Mukund Rungta\"\\n          },\\n          {\\n            \"authorId\": \"2120266368\",\\n            \"name\": \"David Koleczek\"\\n          },\\n          {\\n            \"authorId\": \"5968525\",\\n            \"name\": \"Arshdeep Sekhon\"\\n          },\\n          {\\n            \"authorId\": \"2331375861\",\\n            \"name\": \"Franklin X Wang\"\\n          },\\n          {\\n            \"authorId\": \"2322137989\",\\n            \"name\": \"Sadid A. Hasan\"\\n          }\\n        ],\\n        \"abstract\": \"In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI\\'s GPT models. Experiments show that GPT-3.5-turbo\\'s performance varies by up to 40\\\\\\\\% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.\"\\n      },\\n      {\\n        \"paperId\": \"3f8d2977a7753f9a3b0b5d18e6bee6afb0028ed8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f8d2977a7753f9a3b0b5d18e6bee6afb0028ed8\",\\n        \"title\": \"PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs\",\\n        \"citationCount\": 164,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a novel visual prompting approach for VLMs that is called Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering, and finds that it enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3457048\",\\n            \"name\": \"Soroush Nasiriany\"\\n          },\\n          {\\n            \"authorId\": \"2267320085\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2265397442\",\\n            \"name\": \"Wenhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"9961095\",\\n            \"name\": \"Ted Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2271707306\",\\n            \"name\": \"Jacky Liang\"\\n          },\\n          {\\n            \"authorId\": \"2263548023\",\\n            \"name\": \"Ishita Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"14484808\",\\n            \"name\": \"Annie Xie\"\\n          },\\n          {\\n            \"authorId\": \"2283848260\",\\n            \"name\": \"Danny Driess\"\\n          },\\n          {\\n            \"authorId\": \"88728227\",\\n            \"name\": \"Ayzaan Wahid\"\\n          },\\n          {\\n            \"authorId\": \"2265456732\",\\n            \"name\": \"Zhuo Xu\"\\n          },\\n          {\\n            \"authorId\": \"144579461\",\\n            \"name\": \"Q. Vuong\"\\n          },\\n          {\\n            \"authorId\": \"2240715659\",\\n            \"name\": \"Tingnan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2110662889\",\\n            \"name\": \"T. Lee\"\\n          },\\n          {\\n            \"authorId\": \"2145145412\",\\n            \"name\": \"Kuang-Huei Lee\"\\n          },\\n          {\\n            \"authorId\": \"2153917744\",\\n            \"name\": \"Peng Xu\"\\n          },\\n          {\\n            \"authorId\": \"51881277\",\\n            \"name\": \"Sean Kirmani\"\\n          },\\n          {\\n            \"authorId\": \"2253507326\",\\n            \"name\": \"Yuke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2267246149\",\\n            \"name\": \"Andy Zeng\"\\n          },\\n          {\\n            \"authorId\": \"1944801\",\\n            \"name\": \"Karol Hausman\"\\n          },\\n          {\\n            \"authorId\": \"2801204\",\\n            \"name\": \"N. Heess\"\\n          },\\n          {\\n            \"authorId\": \"2257346440\",\\n            \"name\": \"Chelsea Finn\"\\n          },\\n          {\\n            \"authorId\": \"2257062067\",\\n            \"name\": \"Sergey Levine\"\\n          },\\n          {\\n            \"authorId\": \"2704814\",\\n            \"name\": \"Brian Ichter\"\\n          }\\n        ],\\n        \"abstract\": \"Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.\"\\n      },\\n      {\\n        \"paperId\": \"b9b93b2946ed13f33f3dd75254fff0ca4fd4e651\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b9b93b2946ed13f33f3dd75254fff0ca4fd4e651\",\\n        \"title\": \"Unsupervised Human Activity Recognition Via Large Language Models and Iterative Evolution\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICASSP48485.2024.10446819?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICASSP48485.2024.10446819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292733359\",\\n            \"name\": \"Jiayuan Gao\"\\n          },\\n          {\\n            \"authorId\": \"2145025105\",\\n            \"name\": \"Yingwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2243383757\",\\n            \"name\": \"Yiqiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"19264986\",\\n            \"name\": \"Tengxiang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2292488219\",\\n            \"name\": \"Boshi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2214851453\",\\n            \"name\": \"Xiaoyu Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Human activity recognition (HAR) is crucial for health monitoring and disease diagnosis in Internet-of-Things environments. However, existing HAR approaches either suffer from poor accuracy or achieve high accuracy at the expense of costly manual annotations. To overcome the challenge above, we propose a novel method named LLMIE-UHAR that that leverages LLMs and Iterative Evolution to realize Unsupervised HAR. Specifically, with our designed prompt engineering mechanism, we employ large language models to fuse both contextual and semantic information, and annotate key samples selected by a clustering algorithm. Moreover, LLMIE-UHAR enhances the recognition accuracy with iterative evolution of clustering algorithm, large language models and the neural network based recognition model. Experiments conducted on the public ARAS datasets show the efficiency of our method, achieving an accuracy of 96.00%. This highlights the practical value of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6\",\\n        \"title\": \"Ignore Previous Prompt: Attack Techniques For Language Models\",\\n        \"citationCount\": 575,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2211.09527\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2211.09527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates two types of attacks -- goal hijacking and prompt leaking -- and demonstrates that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3\\'s stochastic nature, creating long-tail risks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-11-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2191070702\",\\n            \"name\": \"F\\\\u00e1bio Perez\"\\n          },\\n          {\\n            \"authorId\": \"2191076673\",\\n            \"name\": \"Ian Ribeiro\"\\n          }\\n        ],\\n        \"abstract\": \"Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3\\'s stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.\"\\n      },\\n      {\\n        \"paperId\": \"e4503969a4cc5c6a1930a2845109221c3f544e64\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e4503969a4cc5c6a1930a2845109221c3f544e64\",\\n        \"title\": \"De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks via Iterative Grounding\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, a technique that grounds the predictions of an LLM through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt through a novel combination of retrieving suitable API references and iteratively querying the model.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-01-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) trained on datasets of publicly available source code have established a new state of the art in code generation tasks. However, these models are mostly unaware of the code that exists within a specific project, preventing the models from making good use of existing APIs. Instead, LLMs often invent, or\\\\\"hallucinate\\\\\", non-existent APIs or produce variants of already existing code. This paper presents De-Hallucinator, a technique that grounds the predictions of an LLM through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt. The approach exploits the observation that predictions by LLMs often resemble the desired code, but they fail to correctly refer to already existing APIs. De-Hallucinator automatically identifies project-specific API references related to the model\\'s initial predictions and adds these references into the prompt. Unlike retrieval-augmented generation (RAG), our approach uses the initial prediction(s) by the model to iteratively retrieve increasingly suitable API references. Our evaluation applies the approach to two tasks: predicting API usages in Python and generating tests in JavaScript. We show that De-Hallucinator consistently improves the generated code across five LLMs. In particular, the approach improves the edit distance by 23.3-50.6% and the recall of correctly predicted API usages by 23.9-61.0% for code completion, and improves the number of fixed tests that initially failed because of hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage for test generation.\"\\n      },\\n      {\\n        \"paperId\": \"7eaaa5aec72ffb95cf538462ed205fa4d4faef9c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7eaaa5aec72ffb95cf538462ed205fa4d4faef9c\",\\n        \"title\": \"Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.09993\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.09993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Reprompting, an iterative sampling algorithm that automatically learns the Chain-of-Thought recipes for a given task without human intervention, achieves consistently better performance than the state-of-the-art prompt optimization and decoding algorithms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47210642\",\\n            \"name\": \"Weijia Xu\"\\n          },\\n          {\\n            \"authorId\": \"2168558581\",\\n            \"name\": \"Andrzej Banburski-Fahey\"\\n          },\\n          {\\n            \"authorId\": \"1698689\",\\n            \"name\": \"N. Jojic\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce Reprompting, an iterative sampling algorithm that automatically learns the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, Reprompting infers the CoT recipes that work consistently well for a set of training samples by iteratively sampling new recipes using previously sampled recipes as parent prompts to solve other training problems. We conduct extensive experiments on 20 challenging reasoning tasks. Results show that Reprompting outperforms human-written CoT prompts substantially by +9.4 points on average. It also achieves consistently better performance than the state-of-the-art prompt optimization and decoding algorithms.\"\\n      },\\n      {\\n        \"paperId\": \"f53a4f34757d1f237446b4d887d5323f2a17ed02\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f53a4f34757d1f237446b4d887d5323f2a17ed02\",\\n        \"title\": \"PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2308.12033\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.12033, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations of prompt ensemble and builds a feedback mechanism for reflecting on the inadequacies of existing weak learners.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2313691609\",\\n            \"name\": \"Chenrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2166025432\",\\n            \"name\": \"Lina Liu\"\\n          },\\n          {\\n            \"authorId\": \"48093158\",\\n            \"name\": \"Jinpeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260827558\",\\n            \"name\": \"Chuyuan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2168803085\",\\n            \"name\": \"Xiaodi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2109800642\",\\n            \"name\": \"Hongyu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2149926941\",\\n            \"name\": \"Mingchen Cai\"\\n          }\\n        ],\\n        \"abstract\": \"As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1\",\\n        \"title\": \"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.04749\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.04749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment is explored and it is found that the assertion level alignment scores provide a useful feedback which can be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2112713578\",\\n            \"name\": \"Jaskirat Singh\"\\n          },\\n          {\\n            \"authorId\": \"144802394\",\\n            \"name\": \"Liang Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs. Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy. Project page for our paper is available at https://1jsingh.github.io/divide-evaluate-and-refine\"\\n      },\\n      {\\n        \"paperId\": \"2809a68abc9be8f1118ac2dea2af50e619b659c0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2809a68abc9be8f1118ac2dea2af50e619b659c0\",\\n        \"title\": \"DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2304.02827\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.02827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DITTO-NeRF is proposed, a novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a single image that outperforms state-of-the-art methods in terms of fidelity and diversity qualitatively and quantitatively with much faster training times than prior arts on image/text-to-3D such as DreamFusion, and NeuralLift-360.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"51245215\",\\n            \"name\": \"H. Seo\"\\n          },\\n          {\\n            \"authorId\": \"47298448\",\\n            \"name\": \"Hayeon Kim\"\\n          },\\n          {\\n            \"authorId\": \"2109334279\",\\n            \"name\": \"Gwanghyun Kim\"\\n          },\\n          {\\n            \"authorId\": \"34971370\",\\n            \"name\": \"S. Chun\"\\n          }\\n        ],\\n        \"abstract\": \"The increasing demand for high-quality 3D content creation has motivated the development of automated methods for creating 3D object models from a single image and/or from a text prompt. However, the reconstructed 3D objects using state-of-the-art image-to-3D methods still exhibit low correspondence to the given image and low multi-view consistency. Recent state-of-the-art text-to-3D methods are also limited, yielding 3D samples with low diversity per prompt with long synthesis time. To address these challenges, we propose DITTO-NeRF, a novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a single image. Our DITTO-NeRF consists of constructing high-quality partial 3D object for limited in-boundary (IB) angles using the given or text-generated 2D image from the frontal view and then iteratively reconstructing the remaining 3D NeRF using inpainting latent diffusion model. We propose progressive 3D object reconstruction schemes in terms of scales (low to high resolution), angles (IB angles initially to outer-boundary (OB) later), and masks (object to background boundary) in our DITTO-NeRF so that high-quality information on IB can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods in terms of fidelity and diversity qualitatively and quantitatively with much faster training times than prior arts on image/text-to-3D such as DreamFusion, and NeuralLift-360.\"\\n      },\\n      {\\n        \"paperId\": \"9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2\",\\n        \"title\": \"Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates\",\\n        \"citationCount\": 80,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.18540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing\\'\\' (PTST) strategy -- fine-tune models without a safety prompt, but include it at test time.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41049476\",\\n            \"name\": \"Kaifeng Lyu\"\\n          },\\n          {\\n            \"authorId\": \"2288581125\",\\n            \"name\": \"Haoyu Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2261386415\",\\n            \"name\": \"Xinran Gu\"\\n          },\\n          {\\n            \"authorId\": \"2141026731\",\\n            \"name\": \"Dingli Yu\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2261737783\",\\n            \"name\": \"Sanjeev Arora\"\\n          }\\n        ],\\n        \"abstract\": \"Public LLMs such as the Llama 2-Chat underwent alignment training and were considered safe. Recently Qi et al. [2024] reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment. Through extensive experiments on several chat models (Meta\\'s Llama 2-Chat, Mistral AI\\'s Mistral 7B Instruct v0.2, and OpenAI\\'s GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing\\'\\' (PTST) strategy -- fine-tune models without a safety prompt, but include it at test time. This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.\"\\n      },\\n      {\\n        \"paperId\": \"149ab16bed8554e71358b3642cb6979884c786b9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/149ab16bed8554e71358b3642cb6979884c786b9\",\\n        \"title\": \"Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt\",\\n        \"citationCount\": 70,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.04031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Bi-Modal Adversarial Prompt Attack (BAP) is introduced, which executes jailbreaks by optimizing textual and visual prompts cohesively and significantly outperforms other methods by large margins.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2304955104\",\\n            \"name\": \"Zonghao Ying\"\\n          },\\n          {\\n            \"authorId\": \"2257572247\",\\n            \"name\": \"Aishan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2300719164\",\\n            \"name\": \"Tianyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305061690\",\\n            \"name\": \"Zhengmin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2114786732\",\\n            \"name\": \"Siyuan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2237942988\",\\n            \"name\": \"Xianglong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2237906923\",\\n            \"name\": \"Dacheng Tao\"\\n          }\\n        ],\\n        \"abstract\": \"In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation. To address this limitation, this paper introduces the Bi-Modal Adversarial Prompt Attack (BAP), which executes jailbreaks by optimizing textual and visual prompts cohesively. Initially, we adversarially embed universally adversarial perturbations in an image, guided by a few-shot query-agnostic corpus (e.g., affirmative prefixes and negative inhibitions). This process ensures that the adversarial image prompt LVLMs to respond positively to harmful queries. Subsequently, leveraging the image, we optimize textual prompts with specific harmful intent. In particular, we utilize a large language model to analyze jailbreak failures and employ chain-of-thought reasoning to refine textual prompts through a feedback-iteration manner. To validate the efficacy of our approach, we conducted extensive evaluations on various datasets and LVLMs, demonstrating that our BAP significantly outperforms other methods by large margins (+29.03% in attack success rate on average). Additionally, we showcase the potential of our attacks on black-box commercial LVLMs, such as GPT-4o and Gemini. Our code is available at https://anonymous.4open.science/r/BAP-Jailbreak-Vision-Language-Models-via-Bi-Modal-Adversarial-Prompt-5496\"\\n      },\\n      {\\n        \"paperId\": \"1ecd677e3d12bffec77d23db696610a7744cf33f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1ecd677e3d12bffec77d23db696610a7744cf33f\",\\n        \"title\": \"WorldSmith: Iterative and Expressive Prompting for World Building with a Generative AI\",\\n        \"citationCount\": 38,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2308.13355\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.13355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates the use of recent multi-modal image generation systems to enable users iteratively visualize and modify elements of their fictional world using a combination of text input, sketching, and region-based filling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2767073\",\\n            \"name\": \"Hai Dang\"\\n          },\\n          {\\n            \"authorId\": \"2408148\",\\n            \"name\": \"Frederik Brudy\"\\n          },\\n          {\\n            \"authorId\": \"1703735\",\\n            \"name\": \"G. Fitzmaurice\"\\n          },\\n          {\\n            \"authorId\": \"2408187\",\\n            \"name\": \"Fraser Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Crafting a rich and unique environment is crucial for fictional world-building, but can be difficult to achieve since illustrating a world from scratch requires time and significant skill. We investigate the use of recent multi-modal image generation systems to enable users iteratively visualize and modify elements of their fictional world using a combination of text input, sketching, and region-based filling. WorldSmith enables novice world builders to quickly visualize a fictional world with layered edits and hierarchical compositions. Through a formative study (4 participants) and first-use study (13 participants) we demonstrate that WorldSmith offers more expressive interactions with prompt-based models. With this work, we explore how creatives can be empowered to leverage prompt-based generative AI as a tool in their creative process, beyond current \\\\\"click-once\\\\\" prompting UI paradigms.\"\\n      },\\n      {\\n        \"paperId\": \"1d14a708622917da4b9820ada6d32af24fc1651a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d14a708622917da4b9820ada6d32af24fc1651a\",\\n        \"title\": \"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.08541\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.08541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2149231840\",\\n            \"name\": \"Zhengyuan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2124948371\",\\n            \"name\": \"Jianfeng Wang\"\\n          },\\n          {\\n            \"authorId\": \"50703697\",\\n            \"name\": \"Linjie Li\"\\n          },\\n          {\\n            \"authorId\": \"143786724\",\\n            \"name\": \"Kevin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2249709105\",\\n            \"name\": \"Chung-Ching Lin\"\\n          },\\n          {\\n            \"authorId\": \"2251726216\",\\n            \"name\": \"Zicheng Liu\"\\n          },\\n          {\\n            \"authorId\": \"29957038\",\\n            \"name\": \"Lijuan Wang\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce ``Idea to Image,\\'\\' a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model\\'s characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative self-refinement on automatic image design and generation.\"\\n      },\\n      {\\n        \"paperId\": \"a2fae006e6c5ac346fd51bc8a009127f9abe22df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a2fae006e6c5ac346fd51bc8a009127f9abe22df\",\\n        \"title\": \"Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates\",\\n        \"citationCount\": 60,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.13006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An open-source framework to evaluate, compare, and visualize the reliability and alignment of LLM judges, which facilitates practitioners to choose LLM judges for alignment tasks and indicates a significant impact of prompt templates on LLM judge performance, as well as a mediocre alignment level between the tested LLM judges and human evaluators.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316959296\",\\n            \"name\": \"Hui Wei\"\\n          },\\n          {\\n            \"authorId\": \"2318243914\",\\n            \"name\": \"Shenghua He\"\\n          },\\n          {\\n            \"authorId\": \"2316860672\",\\n            \"name\": \"Tian Xia\"\\n          },\\n          {\\n            \"authorId\": \"2316960662\",\\n            \"name\": \"Andy Wong\"\\n          },\\n          {\\n            \"authorId\": \"2317016595\",\\n            \"name\": \"Jingyang Lin\"\\n          },\\n          {\\n            \"authorId\": \"2317047664\",\\n            \"name\": \"Mei Han\"\\n          }\\n        ],\\n        \"abstract\": \"LLM-as-a-Judge has been widely applied to evaluate and compare different LLM alignmnet approaches (e.g., RLHF and DPO). However, concerns regarding its reliability have emerged, due to LLM judges\\' biases and inconsistent decision-making. Previous research has developed evaluation frameworks to assess reliability of LLM judges and their alignment with human preferences. However, the employed evaluation metrics often lack adequate explainability and fail to address LLM internal inconsistency. Additionally, existing studies inadequately explore the impact of various prompt templates when applying LLM-as-a-Judge methods, leading to potentially inconsistent comparisons between different alignment algorithms. In this work, we systematically evaluate LLM-as-a-Judge on alignment tasks by defining more theoretically interpretable evaluation metrics and explicitly mitigating LLM internal inconsistency from reliability metrics. We develop an open-source framework to evaluate, compare, and visualize the reliability and alignment of LLM judges, which facilitates practitioners to choose LLM judges for alignment tasks. In the experiments, we examine effects of diverse prompt templates on LLM-judge reliability and also demonstrate our developed framework by comparing various LLM judges on two common alignment datasets (i.e., TL;DR Summarization and HH-RLHF-Helpfulness). Our results indicate a significant impact of prompt templates on LLM judge performance, as well as a mediocre alignment level between the tested LLM judges and human evaluators.\"\\n      },\\n      {\\n        \"paperId\": \"4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4777305738fd1aa30243f96a1687d57d8f70fa5d\",\\n        \"title\": \"Improving Text-to-Image Consistency via Automatic Prompt Optimization\",\\n        \"citationCount\": 57,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.17804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models and paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1796269096\",\\n            \"name\": \"Oscar Ma\\\\u00f1as\"\\n          },\\n          {\\n            \"authorId\": \"2274101827\",\\n            \"name\": \"Pietro Astolfi\"\\n          },\\n          {\\n            \"authorId\": \"2293590162\",\\n            \"name\": \"Melissa Hall\"\\n          },\\n          {\\n            \"authorId\": \"2256372432\",\\n            \"name\": \"Candace Ross\"\\n          },\\n          {\\n            \"authorId\": \"39219656\",\\n            \"name\": \"Jack Urbanek\"\\n          },\\n          {\\n            \"authorId\": \"2293907712\",\\n            \"name\": \"Adina Williams\"\\n          },\\n          {\\n            \"authorId\": \"2801949\",\\n            \"name\": \"Aishwarya Agrawal\"\\n          },\\n          {\\n            \"authorId\": \"1456285042\",\\n            \"name\": \"Adriana Romero-Soriano\"\\n          },\\n          {\\n            \"authorId\": \"3325894\",\\n            \"name\": \"M. Drozdzal\"\\n          }\\n        ],\\n        \"abstract\": \"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"ac0235cbeefb7b638c4de6af0f29e9650180150d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac0235cbeefb7b638c4de6af0f29e9650180150d\",\\n        \"title\": \"PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.08985\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.08985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt and supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"92480323\",\\n            \"name\": \"Seungho Baek\"\\n          },\\n          {\\n            \"authorId\": \"151351626\",\\n            \"name\": \"Hyerin Im\"\\n          },\\n          {\\n            \"authorId\": \"2223757354\",\\n            \"name\": \"Jiseung Ryu\"\\n          },\\n          {\\n            \"authorId\": \"2223886488\",\\n            \"name\": \"Juhyeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2110755619\",\\n            \"name\": \"T. Lee\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation model is able to generate images across a diverse range of subjects and styles based on a single prompt. Recent works have proposed a variety of interaction methods that help users understand the capabilities of models and utilize them. However, how to support users to efficiently explore the model\\'s capability and to create effective prompts are still open-ended research questions. In this paper, we present PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt. Through the iterative process, users can efficiently explore the model\\'s capability, and clarify their intent. PromptCrafter also supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model. Lastly, users can revert to a desired step by reviewing the work history. In this workshop paper, we discuss the design process of PromptCrafter and our plans for follow-up studies.\"\\n      },\\n      {\\n        \"paperId\": \"a1e2557fa6d5373c8f89b8c4d426168cdf31d7d5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1e2557fa6d5373c8f89b8c4d426168cdf31d7d5\",\\n        \"title\": \"Efficient multi-prompt evaluation of LLMs\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.17202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is proved that PromptEval consistently estimates the performance distribution and its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry; for example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490941219\",\\n            \"name\": \"Felipe Maia Polo\"\\n          },\\n          {\\n            \"authorId\": \"2304142327\",\\n            \"name\": \"Ronald Xu\"\\n          },\\n          {\\n            \"authorId\": \"2286343827\",\\n            \"name\": \"Lucas Weber\"\\n          },\\n          {\\n            \"authorId\": \"2247874639\",\\n            \"name\": \"M\\'irian Silva\"\\n          },\\n          {\\n            \"authorId\": \"2303400299\",\\n            \"name\": \"Onkar Bhardwaj\"\\n          },\\n          {\\n            \"authorId\": \"2283849613\",\\n            \"name\": \"Leshem Choshen\"\\n          },\\n          {\\n            \"authorId\": \"2303415785\",\\n            \"name\": \"Allysson Flavio Melo de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"2247880508\",\\n            \"name\": \"Yuekai Sun\"\\n          },\\n          {\\n            \"authorId\": \"8202372\",\\n            \"name\": \"M. Yurochkin\"\\n          }\\n        ],\\n        \"abstract\": \"Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs\\' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry; for example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Moreover, we show how PromptEval can be useful in LLM-as-a-judge and best prompt identification applications.\"\\n      },\\n      {\\n        \"paperId\": \"bb57af33e505681b1649366e494681e5684bdec7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bb57af33e505681b1649366e494681e5684bdec7\",\\n        \"title\": \"SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.13684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a two-stage adaptation approach termed SPTNet, which iteratively optimizes model parameters and data parameters and proposes a novel spatial prompt tuning method (SPT) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292339349\",\\n            \"name\": \"Hongjun Wang\"\\n          },\\n          {\\n            \"authorId\": \"51286373\",\\n            \"name\": \"S. Vaze\"\\n          },\\n          {\\n            \"authorId\": \"2335536188\",\\n            \"name\": \"Kai Han\"\\n          }\\n        ],\\n        \"abstract\": \"Generalized Category Discovery (GCD) aims to classify unlabelled images from both `seen\\' and `unseen\\' classes by transferring knowledge from a set of labelled `seen\\' class images. A key theme in existing GCD approaches is adapting large-scale pre-trained models for the GCD task. An alternate perspective, however, is to adapt the data representation itself for better alignment with the pre-trained model. As such, in this paper, we introduce a two-stage adaptation approach termed SPTNet, which iteratively optimizes model parameters (i.e., model-finetuning) and data parameters (i.e., prompt learning). Furthermore, we propose a novel spatial prompt tuning method (SPT) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes. We thoroughly evaluate our SPTNet on standard benchmarks and demonstrate that our method outperforms existing GCD methods. Notably, we find our method achieves an average accuracy of 61.4% on the SSB, surpassing prior state-of-the-art methods by approximately 10%. The improvement is particularly remarkable as our method yields extra parameters amounting to only 0.117% of those in the backbone architecture. Project page: https://visual-ai.github.io/sptnet.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"2bb9a87bdfc8a35bc1813e5a88180f43615785a8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2bb9a87bdfc8a35bc1813e5a88180f43615785a8\",\\n        \"title\": \"Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions\",\\n        \"citationCount\": 59,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.00727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts, and describes the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2048053804\",\\n            \"name\": \"Guangzhi Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2261406713\",\\n            \"name\": \"Qiao Jin\"\\n          },\\n          {\\n            \"authorId\": \"2314146159\",\\n            \"name\": \"Xiao Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314147661\",\\n            \"name\": \"Minjia Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2237094367\",\\n            \"name\": \"Zhiyong Lu\"\\n          },\\n          {\\n            \"authorId\": \"2265729351\",\\n            \"name\": \"Aidong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"The emergent abilities of large language models (LLMs) have demonstrated great potential in solving medical questions. They can possess considerable medical knowledge, but may still hallucinate and are inflexible in the knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed to enhance the medical question-answering capabilities of LLMs with external knowledge bases, it may still fail in complex cases where multiple rounds of information-seeking are required. To address such an issue, we propose iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts. In each iteration of i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and they will be further used to guide the query generation in the next iteration. Our experiments show the improved performance of various LLMs brought by i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes in the United States Medical Licensing Examination (USMLE), as well as various knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68% on the MedQA dataset. In addition, we characterize the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration. Our case studies show that i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing an in-depth analysis of medical questions. To the best of our knowledge, this is the first-of-its-kind study on incorporating follow-up queries into medical RAG.\"\\n      },\\n      {\\n        \"paperId\": \"51b7b3ad7645a69e3c1c80cae69473b8bd472f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51b7b3ad7645a69e3c1c80cae69473b8bd472f67\",\\n        \"title\": \"ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models\",\\n        \"citationCount\": 99,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.07738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes ResearchAgent, a system that automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents, to enhance the productivity of researchers.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-04-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"90765684\",\\n            \"name\": \"Jinheon Baek\"\\n          },\\n          {\\n            \"authorId\": \"3001990\",\\n            \"name\": \"S. Jauhar\"\\n          },\\n          {\\n            \"authorId\": \"73040249\",\\n            \"name\": \"Silviu Cucerzan\"\\n          },\\n          {\\n            \"authorId\": \"2260611009\",\\n            \"name\": \"Sung Ju Hwang\"\\n          }\\n        ],\\n        \"abstract\": \"The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically, starting with a core scientific paper, ResearchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers. Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based ReviewingAgents that provide reviews and feedback via iterative revision processes. These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and model-based evaluation results. Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work.\"\\n      },\\n      {\\n        \"paperId\": \"192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/192b808eba1232ba3b1d1481230db122b22c97e4\",\\n        \"title\": \"Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2312.01663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.01663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269468811\",\\n            \"name\": \"Runze He\"\\n          },\\n          {\\n            \"authorId\": \"2052151521\",\\n            \"name\": \"Shaofei Huang\"\\n          },\\n          {\\n            \"authorId\": \"2269461105\",\\n            \"name\": \"Xuecheng Nie\"\\n          },\\n          {\\n            \"authorId\": \"151475424\",\\n            \"name\": \"Tianrui Hui\"\\n          },\\n          {\\n            \"authorId\": \"1776665\",\\n            \"name\": \"Luoqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2108984\",\\n            \"name\": \"Jiao Dai\"\\n          },\\n          {\\n            \"authorId\": \"2269685669\",\\n            \"name\": \"Jizhong Han\"\\n          },\\n          {\\n            \"authorId\": \"2269748083\",\\n            \"name\": \"Guanbin Li\"\\n          },\\n          {\\n            \"authorId\": \"2269687302\",\\n            \"name\": \"Si Liu\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings. The code is available at: https://github.com/hrz2000/CustomNeRF.\"\\n      },\\n      {\\n        \"paperId\": \"d0482bd01de9d0912acf4e5338c7799eba4b9360\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d0482bd01de9d0912acf4e5338c7799eba4b9360\",\\n        \"title\": \"NAISTeacher: A Prompt and Rerank Approach to Generating Teacher Utterances in Educational Dialogues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2023.bea-1.63.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://aclanthology.org/2023.bea-1.63, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Through the proposed methodology, the BEA 2023 shared task of generating teacher responses in educational dialogues achieved the top score on both automated metrics and human evaluation, surpassing the reference human teachers on the latter.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1572641769\",\\n            \"name\": \"Justin Vasselli\"\\n          },\\n          {\\n            \"authorId\": \"2221318238\",\\n            \"name\": \"Christopher Vasselli\"\\n          },\\n          {\\n            \"authorId\": \"103680841\",\\n            \"name\": \"Adam Nohejl\"\\n          },\\n          {\\n            \"authorId\": \"2110694221\",\\n            \"name\": \"Taro Watanabe\"\\n          }\\n        ],\\n        \"abstract\": \"This paper presents our approach to the BEA 2023 shared task of generating teacher responses in educational dialogues, using the Teacher-Student Chatroom Corpus. Our system prompts GPT-3.5-turbo to generate initial suggestions, which are then subjected to reranking. We explore multiple strategies for candidate generation, including prompting for multiple candidates and employing iterative few-shot prompts with negative examples. We aggregate all candidate responses and rerank them based on DialogRPT scores. To handle consecutive turns in the dialogue data, we divide the task of generating teacher utterances into two components: teacher replies to the student and teacher continuations of previously sent messages. Through our proposed methodology, our system achieved the top score on both automated metrics and human evaluation, surpassing the reference human teachers on the latter.\"\\n      },\\n      {\\n        \"paperId\": \"b56df33446b37bdf5f17fb494201da728fb62502\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b56df33446b37bdf5f17fb494201da728fb62502\",\\n        \"title\": \"Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation\",\\n        \"citationCount\": 24,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2408.15205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is argued that MLLM hallucinations can reveal valuable contextual insights when leveraged correctly, as they represent pre-trained large-scale knowledge beyond individual images as they represent pre-trained large-scale knowledge beyond individual images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-08-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"123925137\",\\n            \"name\": \"Jian Hu\"\\n          },\\n          {\\n            \"authorId\": \"2273817980\",\\n            \"name\": \"Jiayi Lin\"\\n          },\\n          {\\n            \"authorId\": \"2317041351\",\\n            \"name\": \"Junchi Yan\"\\n          },\\n          {\\n            \"authorId\": \"2273646767\",\\n            \"name\": \"Shaogang Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Promptable segmentation typically requires instance-specific manual prompts to guide the segmentation of each desired object. To minimize such a need, task-generic promptable segmentation has been introduced, which employs a single task-generic prompt to segment various images of different objects in the same task. Current methods use Multimodal Large Language Models (MLLMs) to reason detailed instance-specific prompts from a task-generic prompt for improving segmentation accuracy. The effectiveness of this segmentation heavily depends on the precision of these derived prompts. However, MLLMs often suffer hallucinations during reasoning, resulting in inaccurate prompting. While existing methods focus on eliminating hallucinations to improve a model, we argue that MLLM hallucinations can reveal valuable contextual insights when leveraged correctly, as they represent pre-trained large-scale knowledge beyond individual images. In this paper, we utilize hallucinations to mine task-related information from images and verify its accuracy for enhancing precision of the generated prompts. Specifically, we introduce an iterative Prompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a mask generator.The prompt generator uses a multi-scale chain of thought prompting, initially exploring hallucinations for extracting extended contextual knowledge on a test image.These hallucinations are then reduced to formulate precise instance-specific prompts, directing the mask generator to produce masks that are consistent with task semantics by mask semantic alignment. The generated masks iteratively induce the prompt generator to focus more on task-relevant image areas and reduce irrelevant hallucinations, resulting jointly in better prompts and masks. Experiments on 5 benchmarks demonstrate the effectiveness of ProMaC. Code given in https://lwpyh.github.io/ProMaC/.\"\\n      },\\n      {\\n        \"paperId\": \"03ccf2e13cdbcc156ee48aa26f500b574da07af8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/03ccf2e13cdbcc156ee48aa26f500b574da07af8\",\\n        \"title\": \"PromptWizard: Task-Aware Prompt Optimization Framework\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.18369, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving, self-adapting mechanism, and demonstrates PromptWizard\\'s efficiency, scalability, and advantages over existing prompt optimization strategies.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-05-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2303465593\",\\n            \"name\": \"Eshaan Agarwal\"\\n          },\\n          {\\n            \"authorId\": \"2308075211\",\\n            \"name\": \"Joykirat Singh\"\\n          },\\n          {\\n            \"authorId\": \"2303466479\",\\n            \"name\": \"Vivek Dani\"\\n          },\\n          {\\n            \"authorId\": \"2324053080\",\\n            \"name\": \"Raghav Magazine\"\\n          },\\n          {\\n            \"authorId\": \"1785978\",\\n            \"name\": \"Tanuja Ganu\"\\n          },\\n          {\\n            \"authorId\": \"51464520\",\\n            \"name\": \"A. Nambi\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have transformed AI across diverse domains, with prompting being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving, self-adapting mechanism. Through a feedback-driven critique and synthesis process, PromptWizard achieves an effective balance between exploration and exploitation, iteratively refining both prompt instructions and in-context examples to generate human-readable, task-specific prompts. This guided approach systematically improves prompt quality, resulting in superior performance across 45 tasks. PromptWizard excels even with limited training data, smaller LLMs, and various LLM architectures. Additionally, our cost analysis reveals a substantial reduction in API calls, token usage, and overall cost, demonstrating PromptWizard\\'s efficiency, scalability, and advantages over existing prompt optimization strategies.\"\\n      },\\n      {\\n        \"paperId\": \"69a7ae20845c47c5efe3a93c94e6888b852fe532\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69a7ae20845c47c5efe3a93c94e6888b852fe532\",\\n        \"title\": \"DivLog: Log Parsing with Prompt Enhanced In-Context Learning\",\\n        \"citationCount\": 77,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3597503.3639155?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3597503.3639155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2186630124\",\\n            \"name\": \"Junjielong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2115429277\",\\n            \"name\": \"Ruichun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2036553451\",\\n            \"name\": \"Yintong Huo\"\\n          },\\n          {\\n            \"authorId\": \"2287118778\",\\n            \"name\": \"Chengyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265706586\",\\n            \"name\": \"Pinjia He\"\\n          }\\n        ],\\n        \"abstract\": \"Log parsing, which involves log template extraction from semi-structured logs to produce structured logs, is the first and the most critical step in automated log analysis. However, current log parsers suffer from limited effectiveness for two reasons. First, traditional data-driven log parsers solely rely on heuristics or handcrafted features designed by domain experts, which may not consistently perform well on logs from diverse systems. Second, existing super-vised log parsers require model tuning, which is often limited to fixed training samples and causes sub-optimal performance across the entire log source. To address this limitation, we propose Di-vLog, an effective log parsing framework based on the in-context learning (ICL) ability of large language models (LLMs). Specifically, before log parsing, DivLog samples a small amount of offline logs as candidates by maximizing their diversity. Then, during log parsing, DivLog selects five appropriate labeled candidates as examples for each target log and constructs them into a prompt. By mining the semantics of examples in the prompt, DivLog generates a target log template in a training-free manner. In addition, we design a straightforward yet effective prompt format to extract the output and enhance the quality of the generated log templates. We conducted experiments on 16 widely-used public datasets. The results show that DivLog achieves (1) 98.1% Parsing Accuracy, (2) 92.1% Precision Template Accuracy, and (3) 92.9% Recall Template Accuracy on average, exhibiting state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"2ca32be903662ec64f0a2fd014265bc8c00d8254\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2ca32be903662ec64f0a2fd014265bc8c00d8254\",\\n        \"title\": \"Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.03099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent, and demonstrates the effectiveness of the method with respect to strong proprietary models on real-world tasks such as moderation and generation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34490455\",\\n            \"name\": \"Elad Levi\"\\n          },\\n          {\\n            \"authorId\": \"2282532582\",\\n            \"name\": \"Eli Brosh\"\\n          },\\n          {\\n            \"authorId\": \"2282531918\",\\n            \"name\": \"Matan Friedmann\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system\\'s key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\\\\\\\\href{https://github.com/Eladlev/AutoPrompt}{here}$.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='d91bd339-39e5-4007-b93a-ef5d26028fec'), AIMessage(content='{\\n  \"papers\": [\\n    {\\n      \"paper_id\": \"6f75404b0d01f9a09afe428f9efd5cbcd7825469\",\\n      \"title\": \"Dynamic Prompting: A Unified Framework for Prompt Tuning\",\\n      \"overlap_score\": 0.88,\\n      \"methodology_overlap\": 0.85,\\n      \"problem_overlap\": 0.85,\\n      \"domain_overlap\": 0.7,\\n      \"key_overlaps\": [\\n        \"Dynamic prompt tuning\",\\n        \"Cross-domain applicability\",\\n        \"Parameter-efficient prompt tuning\"\\n      ],\\n      \"key_differences\": [\\n        \"More general theoretical framework across NLP/vision modalities; not restricted to single-task fine-tuning or test-time adaptation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"51ac2dceecc75431eab5b2c320822e081b282b27\",\\n      \"title\": \"SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models\",\\n      \"overlap_score\": 0.86,\\n      \"methodology_overlap\": 0.84,\\n      \"problem_overlap\": 0.82,\\n      \"domain_overlap\": 0.78,\\n      \"key_overlaps\": [\\n        \"Test-time prompt adaptation\",\\n        \"Vision-Language Models\",\\n        \"Prompt optimization across domains\"\\n      ],\\n      \"key_differences\": [\\n        \"Focuses on test-time prompt adaptation, not end-to-end fine-tuning or multi-turn reasoning prompts.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"1d24f2dfe942e2d67381d8fcfac3423145bd557f\",\\n      \"title\": \"DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval\",\\n      \"overlap_score\": 0.85,\\n      \"methodology_overlap\": 0.83,\\n      \"problem_overlap\": 0.82,\\n      \"domain_overlap\": 0.8,\\n      \"key_overlaps\": [\\n        \"Cross-modal prompt tuning\",\\n        \"Global-local video attention\",\\n        \"Prompt-based retrieval alignment\"\\n      ],\\n      \"key_differences\": [\\n        \"Specifically targets video retrieval with global-local prompts; emphasis on cross-modal prompt design rather than dialog/story coherence.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n      \"title\": \"Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning\",\\n      \"overlap_score\": 0.72,\\n      \"methodology_overlap\": 0.7,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.75,\\n      \"key_overlaps\": [\\n        \"Image-specific prompts\",\\n        \"Zero-shot domain adaptation\",\\n        \"Prompt-based adaptation for generative models\"\\n      ],\\n      \"key_differences\": [\\n        \"Centers on image-domain prompt specialization for generative models rather than iterative, multi-turn adaptation for narratives or dialogues.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"5a8541bb468afb45d68e16d2f83b327768f97c67\",\\n      \"title\": \"Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval\",\\n      \"overlap_score\": 0.8,\\n      \"methodology_overlap\": 0.78,\\n      \"problem_overlap\": 0.75,\\n      \"domain_overlap\": 0.76,\\n      \"key_overlaps\": [\\n        \"Prompt-based CLIP adaptation\",\\n        \"Video retrieval\",\\n        \"Semantics-enhancing prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Uses prompt switching to adapt CLIP for video tasks, rather than dynamic, narrative coherence prompts.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"81ca1ca9b81470988d29e0d0def9712320243e06a\",\\n      \"title\": \"HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target Binding Affinity Prediction\",\\n      \"overlap_score\": 0.7,\\n      \"methodology_overlap\": 0.72,\\n      \"problem_overlap\": 0.68,\\n      \"domain_overlap\": 0.65,\\n      \"key_overlaps\": [\\n        \"Dynamic prompts for domain-specific interactions\",\\n        \"Hybrid Graph-Transformer models\",\\n        \"Context-aware feature prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Applied to bioinformatics DTA prediction; uses dynamic prompts in a graph-transformer setting rather than natural language generation tasks.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"3967189742efab8859da542ce3953d4c72957aca\",\\n      \"title\": \"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection\",\\n      \"overlap_score\": 0.82,\\n      \"methodology_overlap\": 0.8,\\n      \"problem_overlap\": 0.8,\\n      \"domain_overlap\": 0.8,\\n      \"key_overlaps\": [\\n        \"Prompt learning for VLM adaptation\",\\n        \"Zero-shot HOI detection\",\\n        \"LLM/VLM guidance for prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Focus on visual-human-object interaction detection rather than narrative generation or text-only prompts.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"8d531cb8cf51eec3b8f1106d189295fa3c81c02a\",\\n      \"title\": \"Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing\",\\n      \"overlap_score\": 0.74,\\n      \"methodology_overlap\": 0.72,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.7,\\n      \"key_overlaps\": [\\n        \"Dynamic prompt learning\",\\n        \"Image editing with cross-attention control\",\\n        \"Noun-word prompt targeting\"\\n      ],\\n      \"key_differences\": [\\n        \"Targets fine-grained image editing; not about long-form narrative coherence or dialogue.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n      \"title\": \"Prompt-and-Transfer: Dynamic Class-Aware Enhancement for Few-Shot Segmentation\",\\n      \"overlap_score\": 0.68,\\n      \"methodology_overlap\": 0.66,\\n      \"problem_overlap\": 0.66,\\n      \"domain_overlap\": 0.7,\\n      \"key_overlaps\": [\\n        \"Dynamic class-aware prompts\",\\n        \"Few-shot segmentation\",\\n        \"Prompt-driven encoder adaptation\"\\n      ],\\n      \"key_differences\": [\\n        \"Applied to segmentation tasks with few-shot data; different domain and evaluation than narrative prompt adaptation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"64ffcf31b03aaae2905e06f9e24be58f51e2f776\",\\n      \"title\": \"Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation\",\\n      \"overlap_score\": 0.7,\\n      \"methodology_overlap\": 0.68,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.72,\\n      \"key_overlaps\": [\\n        \"Test-time visual prompts\",\\n        \"Continual test-time adaptation\",\\n        \"Memory-informed prompt initialization\"\\n      ],\\n      \"key_differences\": [\\n        \"Medical imaging application; continual test-time adaptation with prompts rather than narrative generation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"4f9766356478ddf606457af735f70682839194b1\",\\n      \"title\": \"VPA: Fully Test-Time Visual Prompt Adaptation\",\\n      \"overlap_score\": 0.83,\\n      \"methodology_overlap\": 0.81,\\n      \"problem_overlap\": 0.82,\\n      \"domain_overlap\": 0.79,\\n      \"key_overlaps\": [\\n        \"Visual prompt tuning\",\\n        \"Test-time adaptation\",\\n        \"Domain robustness via prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Visual prompts for OOD/generalization; not narrative multi-turn prompt adaptation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"5d48b087d3d4a16904c7e5442a4a815ff901cc0d\",\\n      \"title\": \"CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning\",\\n      \"overlap_score\": 0.75,\\n      \"methodology_overlap\": 0.73,\\n      \"problem_overlap\": 0.74,\\n      \"domain_overlap\": 0.72,\\n      \"key_overlaps\": [\\n        \"Chain-of-domain adaptation\",\\n        \"Severity-aware visual prompts\",\\n        \"Prompt-based domain adaptation\"\\n      ],\\n      \"key_differences\": [\\n        \"Domain leverage within computer vision domain adaptation; separate from dialogue/storytelling coherence prompts.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"7be9a0708300e42fd3c376b4f4027dd530f240af\",\\n      \"title\": \"Dynamic Prompt Optimizing for Text-to-Image Generation\",\\n      \"overlap_score\": 0.79,\\n      \"methodology_overlap\": 0.77,\\n      \"problem_overlap\": 0.76,\\n      \"domain_overlap\": 0.75,\\n      \"key_overlaps\": [\\n        \"Prompt auto-editing\",\\n        \"Weight and step optimization for prompts\",\\n        \"Online reinforcement learning for prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Image generation prompts optimization; tasks differ from dialogue coherence and long-context prompt adaptation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"b1b688cf506f64bc4a06e037bfcdfa8b22be51aa\",\\n      \"title\": \"Cascade Prompt Learning for Vision-Language Model Adaptation\",\\n      \"overlap_score\": 0.82,\\n      \"methodology_overlap\": 0.8,\\n      \"problem_overlap\": 0.78,\\n      \"domain_overlap\": 0.79,\\n      \"key_overlaps\": [\\n        \"Cascade prompting\",\\n        \"Domain-general and task-specific prompts\",\\n        \"Teacher-student prompt learning\"\\n      ],\\n      \"key_differences\": [\\n        \"Two-stage prompting to separate domain-general and task-specific prompts; still about prompting rather than long narrative coherence.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"03ccf2e13cdbcc156ee48aa26f500b574da07af8\",\\n      \"title\": \"PromptWizard: Task-Aware Prompt Optimization Framework\",\\n      \"overlap_score\": 0.74,\\n      \"methodology_overlap\": 0.72,\\n      \"problem_overlap\": 0.74,\\n      \"domain_overlap\": 0.72,\\n      \"key_overlaps\": [\\n        \"Automated discrete prompt optimization\",\\n        \"Self-evolving critique and synthesis\",\\n        \"Task-aware prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Focuses on automating discrete prompt optimization with self-evolving mechanisms; while related, less focus on continual narrative coherence.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"aec0dfbd00ab2a863028e97d1ead7a437ea00056\",\\n      \"title\": \"Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning\",\\n      \"overlap_score\": 0.72,\\n      \"methodology_overlap\": 0.7,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.75,\\n      \"key_overlaps\": [\\n        \"Image-specific prompts\",\\n        \"Domain adaptation for generative models\",\\n        \"Prompt-based adaptation\"\\n      ],\\n      \"key_differences\": [\\n        \"Generative modeling domain; not about long-form dialogue coherence or iterative narrative prompting.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"0f4219a1a0f89c7364aabb27632cdcc833ab53b7\",\\n      \"title\": \"Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction\",\\n      \"overlap_score\": 0.66,\\n      \"methodology_overlap\": 0.64,\\n      \"problem_overlap\": 0.66,\\n      \"domain_overlap\": 0.68,\\n      \"key_overlaps\": [\\n        \"Iterative prompt-tuning\",\\n        \"Event argument extraction\",\\n        \"Bidirectional prompting\"\\n      ],\\n      \"key_differences\": [\\n        \"NLP event extraction-focused; not narrative coherence or long-form dialogue generation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"e49e4fa5f8f1b99f44c1c5fe404231049b7e3bb6\",\\n      \"title\": \"Prompt-and-Transfer: Dynamic Class-Aware Enhancement for Few-Shot Segmentation\",\\n      \"overlap_score\": 0.68,\\n      \"methodology_overlap\": 0.66,\\n      \"problem_overlap\": 0.66,\\n      \"domain_overlap\": 0.7,\\n      \"key_overlaps\": [\\n        \"Dynamic class-aware prompting\",\\n        \"Few-shot segmentation\",\\n        \"Encoder adaptation via prompts\"\\n      ],\\n      \"key_differences\": [\\n        \"Segmentation domain; uses prompts for feature adaptation rather than narrative continuation.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"e39? (placeholder)\",\\n      \"title\": \"Generative AI tools in art education: prompt engineering and iterative processes for enhanced creativity\",\\n      \"overlap_score\": 0.6,\\n      \"methodology_overlap\": 0.58,\\n      \"problem_overlap\": 0.62,\\n      \"domain_overlap\": 0.6,\\n      \"key_overlaps\": [\\n        \"Prompt engineering\",\\n        \"Iterative design processes\",\\n        \"Creative generation education\"\\n      ],\\n      \"key_differences\": [\\n        \"Educational context; not primarily about LLM prompt adaptation in long interactions.\"\\n      ]\\n    },\\n    {\\n      \"paper_id\": \"ac0235cbeefb7b638c4de6af0f29e9650180150d\",\\n      \"title\": \"PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM\",\\n      \"overlap_score\": 0.7,\\n      \"methodology_overlap\": 0.68,\\n      \"problem_overlap\": 0.7,\\n      \"domain_overlap\": 0.72,\\n      \"key_overlaps\": [\\n        \"Mixed-initiative prompt crafting\",\\n        \"Dialogue-guided prompt design\",\\n        \"LLM-based prompt optimization\"\\n      ],\\n      \"key_differences\": [\\n        \"Prompts for T2I in visual tasks; not specifically about long-running interactive coherence.\"\\n      ]\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='646e4d96-0582-4af8-8717-fdc99de2f427')]}\n"
     ]
    }
   ],
   "source": [
    "graph = workflow.compile()\n",
    "graph.get_graph().draw_mermaid_png(output_file_path='story.png')\n",
    "\n",
    "result_llm = graph.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"Dynamic Prompt Adaptation:\n",
    "                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\n",
    "                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\n",
    "                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\n",
    "                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\n",
    "                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\"\"\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3d43f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_papers = json.loads(result_llm[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d1b4c8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>methodology_overlap</th>\n",
       "      <th>problem_overlap</th>\n",
       "      <th>domain_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    methodology_overlap  problem_overlap  domain_overlap\n",
       "0                  0.85             0.85            0.70\n",
       "1                  0.84             0.82            0.78\n",
       "2                  0.83             0.82            0.80\n",
       "3                  0.70             0.70            0.75\n",
       "4                  0.78             0.75            0.76\n",
       "5                  0.72             0.68            0.65\n",
       "6                  0.80             0.80            0.80\n",
       "7                  0.72             0.70            0.70\n",
       "8                  0.66             0.66            0.70\n",
       "9                  0.68             0.70            0.72\n",
       "10                 0.81             0.82            0.79\n",
       "11                 0.73             0.74            0.72\n",
       "12                 0.77             0.76            0.75\n",
       "13                 0.80             0.78            0.79\n",
       "14                 0.72             0.74            0.72\n",
       "15                 0.70             0.70            0.75\n",
       "16                 0.64             0.66            0.68\n",
       "17                 0.66             0.66            0.70\n",
       "18                 0.58             0.62            0.60\n",
       "19                 0.68             0.70            0.72"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Parse the results\n",
    "result_content = json.loads(result_llm[\"messages\"][-1].content)\n",
    "\n",
    "# Create DataFrame\n",
    "papers_df = pd.DataFrame(result_content['papers'])\n",
    "\n",
    "papers_df[[\"methodology_overlap\",\"problem_overlap\",\"domain_overlap\"]]\n",
    "\n",
    "\n",
    "## NOTE\n",
    "## Wrong overlap calculation\n",
    "\n",
    "### Development Note\n",
    "## might be interesting to make the output of this papers analysis passed into \n",
    "## a GAN-like architecture (So not using overlapping score like the current one)\n",
    "## where one agent is argumenting why the idea is novel\n",
    "## and another agent is criticising why the idea is not novel, both based on the prior work\n",
    "## and then another agent act as a judge to decide which argument is stronger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idea_evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
